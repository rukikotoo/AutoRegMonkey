[
  {
    "text": "ECONOMETRICS B E. H RUCE ANSEN Â©2000,20211 University of Wisconsin Departmentof Economics ThisRevision: February18,2021 CommentsWelcome 1Thismanuscriptmaybeprintedandreproducedforindividualorinstructionaluse,butmaynotbeprintedfor commercialpurposes.",
    "page": 1,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Contents Preface xviii AbouttheAuthor xix 1 Introduction 1 1.1 WhatisEconometrics? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.2 TheProbabilityApproachtoEconometrics . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.3 EconometricTermsandNotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.4 ObservationalData . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.5 StandardDataStructures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.6 EconometricSoftware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1.7 Replication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 1.8 DataFilesforTextbook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 1.9 ReadingtheManuscript . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 1.10 CommonSymbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 I Regression 13 2 ConditionalExpectationandProjection 14 2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.2 TheDistributionofWages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.3 ConditionalExpectation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 2.4 LogsandPercentages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 2.5 ConditionalExpectationFunction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 2.6 ContinuousVariables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 2.7 LawofIteratedExpectations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 2.8 CEFError . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 2.9 Intercept-OnlyModel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 2.10 RegressionVariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 2.11 BestPredictor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 2.12 ConditionalVariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 2.13 HomoskedasticityandHeteroskedasticity . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 2.14 RegressionDerivative . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 2.15 LinearCEF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 2.16 LinearCEFwithNonlinearEffects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 2.17 LinearCEFwithDummyVariables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 2.18 BestLinearPredictor . . . . .",
    "page": 2,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". . . . . . . . . . . . . . . . . . . . 29 2.14 RegressionDerivative . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 2.15 LinearCEF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 2.16 LinearCEFwithNonlinearEffects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 2.17 LinearCEFwithDummyVariables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 2.18 BestLinearPredictor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 2.19 IllustrationsofBestLinearPredictor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 i",
    "page": 2,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CONTENTS ii 2.20 LinearPredictorErrorVariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 2.21 RegressionCoefficients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 2.22 RegressionSub-Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 2.23 CoefficientDecomposition. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 2.24 OmittedVariableBias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 2.25 BestLinearApproximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 2.26 RegressiontotheMean . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 2.27 ReverseRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 2.28 LimitationsoftheBestLinearProjection . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 2.29 RandomCoefficientModel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 2.30 CausalEffects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 2.31 ExistenceandUniquenessoftheConditionalExpectation*. . . . . . . . . . . . . . . . . . 55 2.32 Identification* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 2.33 TechnicalProofs* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 2.34 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60 3 TheAlgebraofLeastSquares 62 3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 3.2 Samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 3.3 MomentEstimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 3.4 LeastSquaresEstimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 3.5 SolvingforLeastSquareswithOneRegressor . . . . . . . . . . . . . . . . . . . . . . . . . . 65 3.6 SolvingforLeastSquareswithMultipleRegressors . . . . . . . . . . . . . . . . . . . . . . . 66 3.7 Illustration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 3.8 LeastSquaresResiduals. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 3.9 DemeanedRegressors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 3.10 ModelinMatrixNotation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 3.11 ProjectionMatrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 3.12 AnnihilatorMatrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 3.13 EstimationofErrorVariance . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "page": 3,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 3.10 ModelinMatrixNotation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 3.11 ProjectionMatrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 3.12 AnnihilatorMatrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 3.13 EstimationofErrorVariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 3.14 AnalysisofVariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 3.15 Projections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 3.16 RegressionComponents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 3.17 RegressionComponents(AlternativeDerivation)* . . . . . . . . . . . . . . . . . . . . . . . 79 3.18 ResidualRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80 3.19 LeverageValues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 3.20 Leave-One-OutRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82 3.21 InfluentialObservations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 3.22 CPSDataSet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86 3.23 NumericalComputation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86 3.24 CollinearityErrors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87 3.25 Programming. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89 3.26 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92 4 LeastSquaresRegression 97 4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97 4.2 RandomSampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97 4.3 SampleMean . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98",
    "page": 3,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CONTENTS iii 4.4 LinearRegressionModel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99 4.5 ExpectationofLeastSquaresEstimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99 4.6 VarianceofLeastSquaresEstimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 4.7 UnconditionalMoments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102 4.8 Gauss-MarkovTheorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103 4.9 ModernGauss-MarkovTheorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 4.10 GeneralizedLeastSquares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 4.11 ModernGeneralizedGaussMarkovTheorem . . . . . . . . . . . . . . . . . . . . . . . . . . 106 4.12 Residuals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107 4.13 EstimationofErrorVariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108 4.14 Mean-SquareForecastError . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109 4.15 CovarianceMatrixEstimationUnderHomoskedasticity . . . . . . . . . . . . . . . . . . . 110 4.16 CovarianceMatrixEstimationUnderHeteroskedasticity . . . . . . . . . . . . . . . . . . . 111 4.17 StandardErrors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114 4.18 EstimationwithSparseDummyVariables . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116 4.19 Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117 4.20 MeasuresofFit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119 4.21 EmpiricalExample . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120 4.22 Multicollinearity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121 4.23 ClusteredSampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123 4.24 InferencewithClusteredSamples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128 4.25 AtWhatLeveltoCluster? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129 4.26 TechnicalProofs* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130 4.27 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132 5 NormalRegression 137 5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137 5.2 TheNormalDistribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137 5.3 MultivariateNormalDistribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139 5.4 JointNormalityandLinearRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "page": 4,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". . . . . . . . . . . . . . . . . . . . . . . . . . . 132 5 NormalRegression 137 5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137 5.2 TheNormalDistribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137 5.3 MultivariateNormalDistribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139 5.4 JointNormalityandLinearRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141 5.5 NormalRegressionModel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141 5.6 DistributionofOLSCoefficientVector . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143 5.7 DistributionofOLSResidualVector . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144 5.8 DistributionofVarianceEstimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145 5.9 t-statistic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145 5.10 ConfidenceIntervalsforRegressionCoefficients . . . . . . . . . . . . . . . . . . . . . . . . 146 5.11 ConfidenceIntervalsforErrorVariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148 5.12 tTest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149 5.13 LikelihoodRatioTest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150 5.14 InformationBoundforNormalRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . 152 5.15 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152 II LargeSampleMethods 154 6 AReviewofLargeSampleAsymptotics 155 6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155 6.2 ModesofConvergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155",
    "page": 4,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CONTENTS iv 6.3 WeakLawofLargeNumbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156 6.4 CentralLimitTheorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156 6.5 ContinuousMappingTheoremandDeltaMethod . . . . . . . . . . . . . . . . . . . . . . . 157 6.6 SmoothFunctionModel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158 6.7 BestUnbiasedEstimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159 6.8 StochasticOrderSymbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159 6.9 ConvergenceofMoments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160 6.10 UniformStochasticBounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161 7 AsymptoticTheoryforLeastSquares 162 7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162 7.2 ConsistencyofLeastSquaresEstimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162 7.3 AsymptoticNormality. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164 7.4 JointDistribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168 7.5 ConsistencyofErrorVarianceEstimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170 7.6 HomoskedasticCovarianceMatrixEstimation . . . . . . . . . . . . . . . . . . . . . . . . . 171 7.7 HeteroskedasticCovarianceMatrixEstimation . . . . . . . . . . . . . . . . . . . . . . . . . 171 7.8 SummaryofCovarianceMatrixNotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173 7.9 AlternativeCovarianceMatrixEstimators* . . . . . . . . . . . . . . . . . . . . . . . . . . . 173 7.10 FunctionsofParameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175 7.11 AsymptoticStandardErrors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177 7.12 t-statistic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179 7.13 ConfidenceIntervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180 7.14 RegressionIntervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182 7.15 ForecastIntervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183 7.16 WaldStatistic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184 7.17 HomoskedasticWaldStatistic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184 7.18 ConfidenceRegions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185 7.19 EdgeworthExpansion* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186 7.20 UniformlyConsistentResiduals* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "page": 5,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184 7.17 HomoskedasticWaldStatistic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184 7.18 ConfidenceRegions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185 7.19 EdgeworthExpansion* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186 7.20 UniformlyConsistentResiduals* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187 7.21 AsymptoticLeverage* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188 7.22 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189 8 RestrictedEstimation 196 8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196 8.2 ConstrainedLeastSquares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197 8.3 ExclusionRestriction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198 8.4 FiniteSampleProperties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199 8.5 MinimumDistance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202 8.6 AsymptoticDistribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203 8.7 VarianceEstimationandStandardErrors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205 8.8 EfficientMinimumDistanceEstimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205 8.9 ExclusionRestrictionRevisited . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206 8.10 VarianceandStandardErrorEstimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208 8.11 HausmanEquality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208 8.12 Example:Mankiw,RomerandWeil(1992) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208 8.13 Misspecification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213 8.14 NonlinearConstraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215",
    "page": 5,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CONTENTS v 8.15 InequalityRestrictions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216 8.16 TechnicalProofs* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217 8.17 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218 9 HypothesisTesting 221 9.1 Hypotheses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221 9.2 AcceptanceandRejection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222 9.3 TypeIError . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223 9.4 ttests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224 9.5 TypeIIErrorandPower . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225 9.6 StatisticalSignificance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226 9.7 P-Values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227 9.8 t-ratiosandtheAbuseofTesting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228 9.9 WaldTests. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229 9.10 HomoskedasticWaldTests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231 9.11 Criterion-BasedTests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232 9.12 MinimumDistanceTests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232 9.13 MinimumDistanceTestsUnderHomoskedasticity . . . . . . . . . . . . . . . . . . . . . . 233 9.14 FTests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234 9.15 HausmanTests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235 9.16 ScoreTests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236 9.17 ProblemswithTestsofNonlinearHypotheses . . . . . . . . . . . . . . . . . . . . . . . . . 238 9.18 MonteCarloSimulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241 9.19 ConfidenceIntervalsbyTestInversion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243 9.20 MultipleTestsandBonferroniCorrections . . . . . . . . . . . . . . . . . . . . . . . . . . . 244 9.21 PowerandTestConsistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245 9.22 AsymptoticLocalPower . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246 9.23 AsymptoticLocalPower,VectorCase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249 9.24 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250 10 ResamplingMethods 257 10.1 Introduction . . . . . . . .",
    "page": 6,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". . . . . 244 9.21 PowerandTestConsistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245 9.22 AsymptoticLocalPower . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246 9.23 AsymptoticLocalPower,VectorCase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249 9.24 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250 10 ResamplingMethods 257 10.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257 10.2 Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257 10.3 JackknifeEstimationofVariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258 10.4 Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261 10.5 JackknifeforClusteredObservations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262 10.6 TheBootstrapAlgorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263 10.7 BootstrapVarianceandStandardErrors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265 10.8 PercentileInterval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267 10.9 TheBootstrapDistribution. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267 10.10 TheDistributionoftheBootstrapObservations . . . . . . . . . . . . . . . . . . . . . . . . 269 10.11 TheDistributionoftheBootstrapSampleMean . . . . . . . . . . . . . . . . . . . . . . . . 270 10.12 BootstrapAsymptotics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270 10.13 ConsistencyoftheBootstrapEstimateofVariance . . . . . . . . . . . . . . . . . . . . . . . 273 10.14 TrimmedEstimatorofBootstrapVariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275 10.15 UnreliabilityofUntrimmedBootstrapStandardErrors . . . . . . . . . . . . . . . . . . . . 276 10.16 ConsistencyofthePercentileInterval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277 10.17 Bias-CorrectedPercentileInterval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279",
    "page": 6,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CONTENTS vi 10.18 BC PercentileInterval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281 a 10.19 Percentile-tInterval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283 10.20 Percentile-tAsymptoticRefinement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285 10.21 BootstrapHypothesisTests. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286 10.22 Wald-TypeBootstrapTests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288 10.23 Criterion-BasedBootstrapTests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289 10.24 ParametricBootstrap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290 10.25 HowManyBootstrapReplications? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291 10.26 SettingtheBootstrapSeed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292 10.27 BootstrapRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292 10.28 BootstrapRegressionAsymptoticTheory . . . . . . . . . . . . . . . . . . . . . . . . . . . . 294 10.29 WildBootstrap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295 10.30 BootstrapforClusteredObservations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296 10.31 TechnicalProofs* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298 10.32 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301 III MultipleEquationModels 306 11 MultivariateRegression 307 11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307 11.2 RegressionSystems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307 11.3 LeastSquaresEstimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308 11.4 MeanandVarianceofSystemsLeastSquares . . . . . . . . . . . . . . . . . . . . . . . . . . 310 11.5 AsymptoticDistribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311 11.6 CovarianceMatrixEstimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312 11.7 SeeminglyUnrelatedRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313 11.8 EquivalenceofSURandLeastSquares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315 11.9 MaximumLikelihoodEstimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316 11.10 RestrictedEstimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317 11.11 ReducedRankRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317 11.12 PrincipalComponentAnalysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 320 11.13 FactorModels . . . .",
    "page": 7,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". . . . . . . . . . . . . . . . . . . . . 315 11.9 MaximumLikelihoodEstimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316 11.10 RestrictedEstimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317 11.11 ReducedRankRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317 11.12 PrincipalComponentAnalysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 320 11.13 FactorModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322 11.14 ApproximateFactorModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324 11.15 FactorModelswithAdditionalRegressors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327 11.16 Factor-AugmentedRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327 11.17 MultivariateNormal* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 328 11.18 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330 12 InstrumentalVariables 332 12.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332 12.2 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332 12.3 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333 12.4 EndogenousRegressors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335 12.5 Instruments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335 12.6 Example:CollegeProximity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336 12.7 ReducedForm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338 12.8 Identification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339",
    "page": 7,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CONTENTS vii 12.9 InstrumentalVariablesEstimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340 12.10 DemeanedRepresentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342 12.11 WaldEstimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343 12.12 Two-StageLeastSquares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344 12.13 LimitedInformationMaximumLikelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . 346 12.14 Split-SampleIVandJIVE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349 12.15 Consistencyof2SLS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350 12.16 AsymptoticDistributionof2SLS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351 12.17 Determinantsof2SLSVariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353 12.18 CovarianceMatrixEstimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354 12.19 LIMLAsymptoticDistribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355 12.20 FunctionsofParameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357 12.21 HypothesisTests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357 12.22 FiniteSampleTheory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358 12.23 Bootstrapfor2SLS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359 12.24 ThePerilofBootstrap2SLSStandardErrors . . . . . . . . . . . . . . . . . . . . . . . . . . . 361 12.25 ClusteredDependence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362 12.26 GeneratedRegressors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363 12.27 RegressionwithExpectationErrors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 366 12.28 ControlFunctionRegression. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 369 12.29 EndogeneityTests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371 12.30 SubsetEndogeneityTests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 374 12.31 OverIdentificationTests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375 12.32 SubsetOverIdentificationTests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378 12.33 BootstrapOveridentificationTests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 380 12.34 LocalAverageTreatmentEffects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381 12.35 IdentificationFailure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384 12.36 WeakInstruments . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "page": 8,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378 12.33 BootstrapOveridentificationTests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 380 12.34 LocalAverageTreatmentEffects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381 12.35 IdentificationFailure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384 12.36 WeakInstruments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 386 12.37 ManyInstruments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 388 12.38 TestingforWeakInstruments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 392 12.39 WeakInstrumentswithk >1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 398 2 12.40 Example:Acemoglu,JohnsonandRobinson(2001) . . . . . . . . . . . . . . . . . . . . . . 400 12.41 Example:AngristandKrueger(1991) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402 12.42 Programming. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404 12.43 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 405 13 GeneralizedMethodofMoments 412 13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 412 13.2 MomentEquationModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 412 13.3 MethodofMomentsEstimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 413 13.4 OveridentifiedMomentEquations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 414 13.5 LinearMomentModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415 13.6 GMMEstimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415 13.7 DistributionofGMMEstimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 416 13.8 EfficientGMM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 417 13.9 EfficientGMMversus2SLS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 418 13.10 EstimationoftheEfficientWeightMatrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . 418 13.11 IteratedGMM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419",
    "page": 8,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CONTENTS viii 13.12 CovarianceMatrixEstimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419 13.13 ClusteredDependence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 420 13.14 WaldTest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421 13.15 RestrictedGMM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 422 13.16 NonlinearRestrictedGMM. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 423 13.17 ConstrainedRegression. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 424 13.18 MultivariateRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 424 13.19 DistanceTest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425 13.20 Continuously-UpdatedGMM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 427 13.21 OverIdentificationTest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 427 13.22 SubsetOverIdentificationTests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 428 13.23 EndogeneityTest. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429 13.24 SubsetEndogeneityTest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429 13.25 NonlinearGMM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 430 13.26 BootstrapforGMM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 431 13.27 ConditionalMomentEquationModels. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 432 13.28 TechnicalProofs* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433 13.29 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 435 IV DependentandPanelData 441 14 TimeSeries 442 14.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 442 14.2 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 442 14.3 DifferencesandGrowthRates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 444 14.4 Stationarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 445 14.5 TransformationsofStationaryProcesses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 448 14.6 ConvergentSeries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 448 14.7 Ergodicity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 449 14.8 ErgodicTheorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 450 14.9 ConditioningonInformationSets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "page": 9,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . 448 14.6 ConvergentSeries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 448 14.7 Ergodicity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 449 14.8 ErgodicTheorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 450 14.9 ConditioningonInformationSets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451 14.10 MartingaleDifferenceSequences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452 14.11 CLTforMartingaleDifferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455 14.12 Mixing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455 14.13 CLTforCorrelatedObservations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457 14.14 LinearProjection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458 14.15 WhiteNoise. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 459 14.16 TheWoldDecomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 459 14.17 LagOperator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461 14.18 AutoregressiveWoldRepresentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461 14.19 LinearModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 462 14.20 MovingAverageProcesses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 462 14.21 Infinite-OrderMovingAverageProcess. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 463 14.22 First-OrderAutoregressiveProcess . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 464 14.23 UnitRootandExplosiveAR(1)Processes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 467 14.24 Second-OrderAutoregressiveProcess . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 469 14.25 AR(p)Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 471",
    "page": 9,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CONTENTS ix 14.26 ImpulseResponseFunction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 472 14.27 ARMAandARIMAProcesses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 473 14.28 MixingPropertiesofLinearProcesses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 474 14.29 Identification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 475 14.30 EstimationofAutoregressiveModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 477 14.31 AsymptoticDistributionofLeastSquaresEstimator . . . . . . . . . . . . . . . . . . . . . . 478 14.32 DistributionUnderHomoskedasticity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 479 14.33 AsymptoticDistributionUnderGeneralDependence . . . . . . . . . . . . . . . . . . . . . 479 14.34 CovarianceMatrixEstimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 480 14.35 CovarianceMatrixEstimationUnderGeneralDependence. . . . . . . . . . . . . . . . . . 481 14.36 TestingtheHypothesisofNoSerialCorrelation . . . . . . . . . . . . . . . . . . . . . . . . . 482 14.37 TestingforOmittedSerialCorrelation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 483 14.38 ModelSelection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 484 14.39 Illustrations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 485 14.40 TimeSeriesRegressionModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 487 14.41 Static,DistributedLag,andAutoregressiveDistributedLagModels . . . . . . . . . . . . . 488 14.42 TimeTrends . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 489 14.43 Illustration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491 14.44 GrangerCausality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 492 14.45 TestingforSerialCorrelationinRegressionModels . . . . . . . . . . . . . . . . . . . . . . 494 14.46 BootstrapforTimeSeries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 495 14.47 TechnicalProofs* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 496 14.48 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 505 15 MultivariateTimeSeries 509 15.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 509 15.2 MultipleEquationTimeSeriesModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 509 15.3 LinearProjection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 510 15.4 MultivariateWoldDecomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 511 15.5 ImpulseResponse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "page": 10,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 509 15.2 MultipleEquationTimeSeriesModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 509 15.3 LinearProjection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 510 15.4 MultivariateWoldDecomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 511 15.5 ImpulseResponse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 512 15.6 VAR(1)Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 514 15.7 VAR(p)Model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 514 15.8 RegressionNotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 515 15.9 Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 515 15.10 AsymptoticDistribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 516 15.11 CovarianceMatrixEstimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 517 15.12 SelectionofLagLengthinanVAR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 518 15.13 Illustration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 518 15.14 PredictiveRegressions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 519 15.15 ImpulseResponseEstimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 521 15.16 LocalProjectionEstimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 522 15.17 RegressiononResiduals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 523 15.18 OrthogonalizedShocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 524 15.19 OrthogonalizedImpulseResponseFunction . . . . . . . . . . . . . . . . . . . . . . . . . . 525 15.20 OrthogonalizedImpulseResponseEstimation . . . . . . . . . . . . . . . . . . . . . . . . . 526 15.21 Illustration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 526 15.22 ForecastErrorDecomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 526 15.23 IdentificationofRecursiveVARs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 528",
    "page": 10,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CONTENTS x 15.24 OilPriceShocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 529 15.25 StructuralVARs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 531 15.26 IdentificationofStructuralVARs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 534 15.27 Long-RunRestrictions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 535 15.28 BlanchardandQuah(1989)Illustration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 537 15.29 ExternalInstruments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 539 15.30 DynamicFactorModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 540 15.31 TechnicalProofs* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 541 15.32 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 543 16 Non-StationaryTimeSeries 547 16.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 547 16.2 PartialSumProcessandFunctionalConvergence . . . . . . . . . . . . . . . . . . . . . . . 547 16.3 Beveridge-NelsonDecomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 549 16.4 FunctionalCLT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 551 16.5 OrdersofIntegration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 552 16.6 Means,LocalMeans,andTrends . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 553 16.7 DemeaningandDetrending . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 555 16.8 StochasticIntegrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 556 16.9 EstimationofanAR(1) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 558 16.10 AR(1)EstimationwithanIntercept . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 560 16.11 SampleCovariancesofIntegratedandStationaryProcesses . . . . . . . . . . . . . . . . . 562 16.12 AR(p)ModelswithaUnitRoot. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 562 16.13 TestingforaUnitRoot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 564 16.14 KPSSStationarityTest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 567 16.15 SpuriousRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 570 16.16 NonStationaryVARs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 574 16.17 Cointegration. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 575 16.18 RoleofInterceptandTrend . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579 16.19 CointegratingRegression . . . . . . . . . . . .",
    "page": 11,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". 567 16.15 SpuriousRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 570 16.16 NonStationaryVARs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 574 16.17 Cointegration. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 575 16.18 RoleofInterceptandTrend . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579 16.19 CointegratingRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579 16.20 VECMEstimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 582 16.21 TestingforCointegrationinaVECM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 584 16.22 TechnicalProofs* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 587 16.23 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 595 17 PanelData 597 17.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 597 17.2 TimeIndexingandUnbalancedPanels. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 598 17.3 Notation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 599 17.4 PooledRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 599 17.5 One-WayErrorComponentModel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 600 17.6 RandomEffects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 601 17.7 FixedEffectModel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 603 17.8 WithinTransformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 605 17.9 FixedEffectsEstimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 607 17.10 DifferencedEstimator. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 608 17.11 DummyVariablesRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 609 17.12 FixedEffectsCovarianceMatrixEstimation . . . . . . . . . . . . . . . . . . . . . . . . . . . 611",
    "page": 11,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CONTENTS xi 17.13 FixedEffectsEstimationinStata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 612 17.14 BetweenEstimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 613 17.15 FeasibleGLS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 614 17.16 InterceptinFixedEffectsRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 615 17.17 EstimationofFixedEffects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 616 17.18 GMMInterpretationofFixedEffects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 616 17.19 IdentificationintheFixedEffectsModel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 618 17.20 AsymptoticDistributionofFixedEffectsEstimator . . . . . . . . . . . . . . . . . . . . . . 618 17.21 AsymptoticDistributionforUnbalancedPanels . . . . . . . . . . . . . . . . . . . . . . . . 620 17.22 Heteroskedasticity-RobustCovarianceMatrixEstimation . . . . . . . . . . . . . . . . . . 622 17.23 Heteroskedasticity-RobustEstimationâUnbalancedCase . . . . . . . . . . . . . . . . . . 623 17.24 HausmanTestforRandomvsFixedEffects . . . . . . . . . . . . . . . . . . . . . . . . . . . 624 17.25 RandomEffectsorFixedEffects? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624 17.26 TimeTrends . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 625 17.27 Two-WayErrorComponents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 625 17.28 InstrumentalVariables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 627 17.29 IdentificationwithInstrumentalVariables. . . . . . . . . . . . . . . . . . . . . . . . . . . . 628 17.30 AsymptoticDistributionofFixedEffects2SLSEstimator . . . . . . . . . . . . . . . . . . . 629 17.31 LinearGMM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 630 17.32 EstimationwithTime-InvariantRegressors . . . . . . . . . . . . . . . . . . . . . . . . . . . 630 17.33 Hausman-TaylorModel. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 632 17.34 JackknifeCovarianceMatrixEstimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 634 17.35 PanelBootstrap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 635 17.36 DynamicPanelModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 635 17.37 TheBiasofFixedEffectsEstimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 636 17.38 Anderson-HsiaoEstimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 637 17.39 Arellano-BondEstimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 638 17.40 WeakInstruments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "page": 12,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 635 17.37 TheBiasofFixedEffectsEstimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 636 17.38 Anderson-HsiaoEstimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 637 17.39 Arellano-BondEstimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 638 17.40 WeakInstruments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 640 17.41 DynamicPanelswithPredeterminedRegressors . . . . . . . . . . . . . . . . . . . . . . . . 641 17.42 Blundell-BondEstimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 642 17.43 ForwardOrthogonalTransformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 645 17.44 EmpiricalIllustration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 646 17.45 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 647 18 DifferenceinDifferences 650 18.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 650 18.2 MinimumWageinNewJersey . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 650 18.3 Identification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 653 18.4 MultipleUnits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 654 18.5 DoPoliceReduceCrime? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 656 18.6 TrendSpecification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 657 18.7 DoBlueLawsAffectLiquorSales? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 658 18.8 CheckYourCode:DoesAbortionImpactCrime? . . . . . . . . . . . . . . . . . . . . . . . . 660 18.9 Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 661 18.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 662",
    "page": 12,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CONTENTS xii V NonparametricMethods 665 19 NonparametricRegression 666 19.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 666 19.2 BinnedMeansEstimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 666 19.3 KernelRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 668 19.4 LocalLinearEstimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 669 19.5 LocalPolynomialEstimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 670 19.6 AsymptoticBias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 671 19.7 AsymptoticVariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 673 19.8 AIMSE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 674 19.9 ReferenceBandwidth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 675 19.10 EstimationataBoundary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 677 19.11 NonparametricResidualsandPredictionErrors . . . . . . . . . . . . . . . . . . . . . . . . 679 19.12 Cross-ValidationBandwidthSelection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 679 19.13 AsymptoticDistribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 681 19.14 Undersmoothing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 683 19.15 ConditionalVarianceEstimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 683 19.16 VarianceEstimationandStandardErrors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 684 19.17 ConfidenceBands . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 685 19.18 TheLocalNatureofKernelRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 685 19.19 ApplicationtoWageRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 686 19.20 ClusteredObservations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 687 19.21 ApplicationtoTestscores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 689 19.22 MultipleRegressors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 690 19.23 CurseofDimensionality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 691 19.24 PartiallyLinearRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 692 19.25 Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 693 19.26 TechnicalProofs* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 693 19.27 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "page": 13,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". . . . . . . . . . . . . . . . . . . . . . . . . . . 691 19.24 PartiallyLinearRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 692 19.25 Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 693 19.26 TechnicalProofs* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 693 19.27 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 698 20 SeriesRegression 700 20.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 700 20.2 PolynomialRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 701 20.3 IllustratingPolynomialRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 701 20.4 OrthogonalPolynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 702 20.5 Splines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 704 20.6 IllustratingSplineRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 705 20.7 TheGlobal/LocalNatureofSeriesRegression . . . . . . . . . . . . . . . . . . . . . . . . . . 706 20.8 Stone-WeierstrassandJacksonApproximationTheory . . . . . . . . . . . . . . . . . . . . 708 20.9 RegressorBounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 710 20.10 MatrixConvergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 711 20.11 ConsistentEstimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 713 20.12 ConvergenceRate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 713 20.13 AsymptoticNormality. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 714 20.14 RegressionEstimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 715 20.15 Undersmoothing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 716 20.16 ResidualsandRegressionFit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 717",
    "page": 13,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CONTENTS xiii 20.17 Cross-ValidationModelSelection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 717 20.18 VarianceandStandardErrorEstimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 718 20.19 ClusteredObservations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 719 20.20 ConfidenceBands . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 720 20.21 UniformApproximations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 721 20.22 PartiallyLinearModel. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 721 20.23 PanelFixedEffects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 722 20.24 MultipleRegressors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 722 20.25 AdditivelySeparableModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 722 20.26 NonparametricInstrumentalVariablesRegression. . . . . . . . . . . . . . . . . . . . . . . 723 20.27 NPIVIdentification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 724 20.28 NPIVConvergenceRate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 726 20.29 NonparametricvsParametricIdentification. . . . . . . . . . . . . . . . . . . . . . . . . . . 726 20.30 Example:AngristandLavy(1999) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 727 20.31 TechnicalProofs* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 729 20.32 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 735 21 RegressionDiscontinuity 739 21.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 739 21.2 SharpRegressionDiscontinuity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 739 21.3 Identification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 740 21.4 Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 741 21.5 Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 743 21.6 BandwidthSelection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 744 21.7 RDDwithCovariates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 746 21.8 ASimpleRDDEstimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 747 21.9 DensityDiscontinuityTest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 748 21.10 FuzzyRegressionDiscontinuity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 748 21.11 EstimationofFRD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 750 21.12 Exercises . . . .",
    "page": 14,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". . . . . . . . . . . . . . . . . . . . . 746 21.8 ASimpleRDDEstimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 747 21.9 DensityDiscontinuityTest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 748 21.10 FuzzyRegressionDiscontinuity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 748 21.11 EstimationofFRD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 750 21.12 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 751 VI NonLinearMethods 752 22 M-Estimators 753 22.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 753 22.2 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 753 22.3 IdentificationandEstimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 754 22.4 Consistency. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 754 22.5 UniformLawofLargeNumbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 756 22.6 AsymptoticDistribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 757 22.7 CovarianceMatrixEstimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 758 22.8 TechnicalProofs* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 759 22.9 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 761 23 NonlinearLeastSquares 762 23.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 762 23.2 Identification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 763",
    "page": 14,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CONTENTS xiv 23.3 Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 764 23.4 AsymptoticDistribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 766 23.5 CovarianceMatrixEstimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 769 23.6 PanelData . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 770 23.7 ThresholdModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 770 23.8 TestingforNonlinearComponents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 774 23.9 Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 776 23.10 TechnicalProofs* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 776 23.11 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 778 24 QuantileRegression 780 24.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 780 24.2 MedianRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 780 24.3 LeastAbsoluteDeviations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 782 24.4 QuantileRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 783 24.5 ExampleQuantileShapes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 786 24.6 Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 787 24.7 AsymptoticDistribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 788 24.8 CovarianceMatrixEstimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 789 24.9 ClusteredDependence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 790 24.10 QuantileCrossings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 791 24.11 QuantileCausalEffects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 793 24.12 RandomCoefficientRepresentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 794 24.13 NonparametricQuantileRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 795 24.14 PanelData . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 795 24.15 IVQuantileRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 797 24.16 TechnicalProofs* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 797 24.17 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 799 25 BinaryChoice 801 25.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "page": 15,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". . . . . . . . . . . . . . . . . . 795 24.15 IVQuantileRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 797 24.16 TechnicalProofs* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 797 24.17 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 799 25 BinaryChoice 801 25.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 801 25.2 BinaryChoiceModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 801 25.3 ModelsfortheResponseProbability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 802 25.4 LatentVariableInterpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 804 25.5 Likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 805 25.6 Pseudo-TrueValues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 807 25.7 AsymptoticDistribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 808 25.8 CovarianceMatrixEstimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 810 25.9 MarginalEffects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 810 25.10 Application . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 811 25.11 SemiparametricBinaryChoice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 811 25.12 IVProbit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 813 25.13 BinaryPanelData . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 814 25.14 TechnicalProofs* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 815 25.15 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 817 26 MultipleChoice 819 26.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 819",
    "page": 15,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CONTENTS xv 26.2 MultinomialResponse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 819 26.3 MultinomialLogit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 820 26.4 ConditionalLogit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 822 26.5 IndependenceofIrrelevantAlternatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 826 26.6 NestedLogit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 827 26.7 MixedLogit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 829 26.8 SimpleMultinomialProbit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 831 26.9 GeneralMultinomialProbit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 832 26.10 OrderedResponse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 833 26.11 CountData . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 835 26.12 BLPDemandModel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 836 26.13 TechnicalProofs* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 838 26.14 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 840 27 CensoringandSelection 842 27.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 842 27.2 Censoring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 842 27.3 CensoredRegressionFunctions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 844 27.4 TheBiasofLeastSquaresEstimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 845 27.5 TobitEstimator. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 846 27.6 IdentificationinTobitRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 847 27.7 CLADandCQREstimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 849 27.8 IllustratingCensoredRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 850 27.9 SampleSelectionBias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 851 27.10 HeckmanâsModel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 852 27.11 NonparametricSelection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 854 27.12 PanelData . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 855 27.13 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 856 28 ModelSelection,SteinShrinkage,andModelAveraging 859 28.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . .",
    "page": 16,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". . . . . . . . . . . . . . . . . . . 852 27.11 NonparametricSelection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 854 27.12 PanelData . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 855 27.13 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 856 28 ModelSelection,SteinShrinkage,andModelAveraging 859 28.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 859 28.2 ModelSelection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 859 28.3 BayesianInformationCriterion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 861 28.4 AkaikeInformationCriterionforRegression . . . . . . . . . . . . . . . . . . . . . . . . . . 862 28.5 AkaikeInformationCriterionforLikelihood. . . . . . . . . . . . . . . . . . . . . . . . . . . 865 28.6 MallowsCriterion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 866 28.7 Hold-OutCriterion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 867 28.8 Cross-ValidationCriterion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 868 28.9 K-FoldCross-Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 869 28.10 ManySelectionCriteriaareSimilar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 870 28.11 RelationwithLikelihoodRatioTesting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 872 28.12 ConsistentSelection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 872 28.13 AsymptoticSelectionOptimality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 875 28.14 FocusedInformationCriterion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 876 28.15 BestSubsetandStepwiseRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 879 28.16 TheMSEofModelSelectionEstimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 880 28.17 InferenceAfterModelSelection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 882 28.18 EmpiricalIllustration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 883",
    "page": 16,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CONTENTS xvi 28.19 ShrinkageMethods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 884 28.20 James-SteinShrinkageEstimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 885 28.21 InterpretationoftheSteinEffect . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 886 28.22 PositivePartEstimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 886 28.23 ShrinkageTowardsRestrictions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 888 28.24 GroupJames-Stein . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 889 28.25 EmpiricalIllustrations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 890 28.26 ModelAveraging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 892 28.27 SmoothedBICandAIC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 894 28.28 MallowsModelAveraging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 897 28.29 Jackknife(CV)ModelAveraging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 899 28.30 Granger-RamanathanAveraging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 900 28.31 EmpiricalIllustration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 900 28.32 TechnicalProofs* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 901 28.33 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 907 29 MachineLearning 910 29.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 910 29.2 BigData,HighDimensionality,andMachineLearning . . . . . . . . . . . . . . . . . . . . 910 29.3 HighDimensionalRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 911 29.4 p-norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 912 29.5 RidgeRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 913 29.6 StatisticalPropertiesofRidgeRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . 916 29.7 IllustratingRidgeRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 917 29.8 Lasso. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 918 29.9 LassoPenaltySelection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 920 29.10 LassoComputation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 921 29.11 AsymptoticTheoryfortheLasso . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 921 29.12 ApproximateSparsity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 923 29.13 ElasticNet . . . . . . . . . . .",
    "page": 17,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". . . . . . . . . . . . 918 29.9 LassoPenaltySelection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 920 29.10 LassoComputation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 921 29.11 AsymptoticTheoryfortheLasso . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 921 29.12 ApproximateSparsity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 923 29.13 ElasticNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 925 29.14 Post-Lasso . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 925 29.15 RegressionTrees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 925 29.16 Bagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 927 29.17 RandomForests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 929 29.18 Ensembling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 931 29.19 LassoIV . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 931 29.20 DoubleSelectionLasso . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 933 29.21 Post-RegularizationLasso . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 934 29.22 Double/DebiasedMachineLearning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 936 29.23 TechnicalProofs* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 938 29.24 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 943 Appendices 945 A MatrixAlgebra 945 A.1 Notation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 945 A.2 ComplexMatrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 946",
    "page": 17,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CONTENTS xvii A.3 MatrixAddition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 947 A.4 MatrixMultiplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 947 A.5 Trace . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 948 A.6 RankandInverse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 948 A.7 OrthogonalandOrthonormalMatrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 950 A.8 Determinant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 950 A.9 Eigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 951 A.10 PositiveDefiniteMatrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 952 A.11 IdempotentMatrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 952 A.12 SingularValues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 953 A.13 MatrixDecompositions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 954 A.14 GeneralizedEigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 954 A.15 ExtremaofQuadraticForms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 956 A.16 CholeskyDecomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 957 A.17 QRDecomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 958 A.18 SolvingLinearSystems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 959 A.19 AlgorithmicMatrixInversion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 961 A.20 MatrixCalculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 961 A.21 KroneckerProductsandtheVecOperator . . . . . . . . . . . . . . . . . . . . . . . . . . . . 963 A.22 VectorNorms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 963 A.23 MatrixNorms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 964 B UsefulInequalities 966 B.1 InequalitiesforRealNumbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 966 B.2 InequalitiesforVectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 967 B.3 InequalitiesforMatrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 968 B.4 ProbabilityInequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 968 B.5 Proofs*. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 972 References 984",
    "page": 18,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Preface Thistextbookisthesecondinatwo-partseriescoveringthecorematerialtypicallytaughtinaone- yearPh.D.courseineconometrics.Thesequenceis: 1. IntroductiontoEconometrics(firstvolume) 2. Econometrics(thisvolume) Econometricsassumesthatstudentshaveabackgroundinmultivariatecalculus,probabilitytheory, linear algebra, and mathematical statistics. A prior course in undergraduate econometrics would be helpfulbutnotrequired. TwoexcellentundergraduatetextbooksareWooldridge(2015)andStockand Watson(2014).Therelevantbackgroundinprobabilitytheoryandmathematicalstatisticsisprovidedin IntroductiontoEconometrics. For reference, the basic tools of matrix algebra and probability inequalites are reviewed in the Ap- pendix. Forstudentswishingtodeepentheirknowledgeofmatrixalgebrainrelationtoeconometrics,Irec- ommendMatrixAlgebrabyAbadirandMagnus(2005). For further study in econometrics beyond this text, I recommend Davidson (2020) for asymptotic theory,Hamilton(1994)andKilianandLÃ¼tkepohl(2017)fortimeseriesmethods,CameronandTrivedi (2005)andWooldridge(2010)forpaneldataanddiscreteresponsemodels,andLiandRacine(2007)for nonparametricsandsemiparametriceconometrics. Beyondthesetexts,theHandbookofEconometrics seriesprovidesadvancedsummariesofcontemporaryeconometricmethodsandtheory. AlternativePhD-leveleconometricstextbooksincludeTheil(1971),Amemiya(1985),Judge,Griffiths, Hill,LÃ¼tkepohl,andLee(1985),Goldberger(1991),DavidsonandMacKinnon(1993),JohnstonandDi- Nardo(1997),Davidson(2000),Hayashi(2000),Ruud(2000),DavidsonandMacKinnon(2004),Greene (2017)andMagnus(2017).ForafocusonappliedmethodsseeAngristandPischke(2009). Theend-of-chapterexercisesareimportantpartsofthetextandaremeanttohelpteachstudentsof econometrics.Answersarenotprovided,andthisisintentional. IwouldliketothankYing-YingLeeandWooyoungKimforprovidingresearchassistanceinpreparing someofthenumericalanalysis,graphics,andempiricalexamplespresentedinthetext. xviii",
    "page": 19,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "About the Author BruceE.HansenistheMaryClaireAschenbrennerPhippsDistinguishedChairofEconomicsatthe University of Wisconsin-Madison. Bruce is originally from Los Angeles, California, has an undergrad- uatedegreeineconomicsfromOccidentalCollege,andaPh.D.ineconomicsfromYaleUniversity. He previouslytaughtattheUniversityofRochesterandBostonCollege. Bruce is a Fellow of the Econometric Society, the Journal of Econometrics, and the International AssociationofAppliedEconometrics.HehasservedasCo-EditorofEconometricTheoryandasAssociate EditorofEconometrica.Hehaspublished63papersinrefereedjournalswhichhavereceivedover36,000 citations. xix",
    "page": 20,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Chapter 1 Introduction 1.1 WhatisEconometrics? ThetermâeconometricsâisbelievedtohavebeencraftedbyRagnarFrisch(1895-1973)ofNorway, oneofthethreeprincipalfoundersoftheEconometricSociety,firsteditorofthejournalEconometrica, and co-winner of the first Nobel Memorial Prize in Economic Sciences in 1969. It is therefore fitting thatweturntoFrischâsownwordsintheintroductiontothefirstissueofEconometricatodescribethe discipline. A word of explanation regarding the term econometrics may be in order. Its definition is implied in the statement of the scope of the [Econometric] Society, in Section I of the Constitution,whichreads: âTheEconometricSocietyisaninternationalsocietyforthead- vancementofeconomictheoryinitsrelationtostatisticsandmathematics....Itsmainobject shallbetopromotestudiesthataimataunificationofthetheoretical-quantitativeandthe empirical-quantitativeapproachtoeconomicproblems....â But there are several aspects of the quantitative approach to economics, and no single oneoftheseaspects,takenbyitself,shouldbeconfoundedwitheconometrics.Thus,econo- metricsisbynomeansthesameaseconomicstatistics. Norisitidenticalwithwhatwecall general economictheory, althougha considerable portionofthistheory has adefininitely quantitative character. Nor should econometrics be taken as synonomous with the appli- cationofmathematicstoeconomics. Experiencehasshownthateachofthesethreeview- points,thatofstatistics,economictheory,andmathematics,isanecessary,butnotbyitself asufficient,conditionforarealunderstandingofthequantitativerelationsinmoderneco- nomic life. It is the unification of all three that is powerful. And it is this unification that constituteseconometrics. RagnarFrisch,Econometrica,(1933),1,pp.1-2. This definition remains valid today, although some terms have evolved somewhat in their usage. Today,wewouldsaythateconometricsistheunifiedstudyofeconomicmodels,mathematicalstatistics, andeconomicdata. Within the field of econometrics there are sub-divisions and specializations. Econometrictheory concernsthedevelopmentoftoolsandmethods,andthestudyofthepropertiesofeconometricmeth- ods. Appliedeconometricsisatermdescribingthedevelopmentofquantitativeeconomicmodelsand theapplicationofeconometricmethodstothesemodelsusingeconomicdata. 1",
    "page": 21,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER1. INTRODUCTION 2 1.2 TheProbabilityApproachtoEconometrics TheunifyingmethodologyofmoderneconometricswasarticulatedbyTrygveHaavelmo(1911-1999) ofNorway, winnerofthe1989NobelMemorialPrizeinEconomicSciences, inhisseminalpaperâThe probability approach in econometricsâ (1944). Haavelmo argued that quantitative economic models mustnecessarilybeprobabilitymodels(bywhichtodaywewouldmeanstochastic).Deterministicmod- elsareblatentlyinconsistentwithobservedeconomicquantities,anditisincoherenttoapplydetermin- isticmodelstonon-deterministicdata. Economicmodelsshouldbeexplicitlydesignedtoincorporate randomness; stochasticerrorsshouldnotbesimplyaddedtodeterministicmodelstomakethemran- dom. Onceweacknowledgethataneconomicmodelisaprobabilitymodel,itfollowsnaturallythatan appropriate tool way to quantify, estimate, and conduct inferences about the economy is through the powerfultheoryofmathematicalstatistics. Theappropriatemethodforaquantitativeeconomicanaly- sisfollowsfromtheprobabilisticconstructionoftheeconomicmodel. Haavelmoâs probability approach was quickly embraced by the economics profession. Today no quantitativeworkineconomicsshunsitsfundamentalvision. Whilealleconomistsembracetheprobabilityapproach,therehasbeensomeevolutioninitsimple- mentation. ThestructuralapproachistheclosesttoHaavelmoâsoriginalidea. Aprobabilisticeconomicmodel is specified, and the quantitative analysis performed under the assumption that the economic model iscorrectlyspecified. Researchersoftendescribethisasâtakingtheirmodelseriouslyâ. Thestructural approachtypicallyleadstolikelihood-basedanalysis,includingmaximumlikelihoodandBayesianesti- mation. Acriticismofthestructuralapproachisthatitismisleadingtotreataneconomicmodelascorrectly specified. Rather,itismoreaccuratetoviewamodelasausefulabstractionorapproximation. Inthis case,howshouldweinterpretstructuraleconometricanalysis?Thequasi-structuralapproachtoinfer- enceviewsastructuraleconomicmodelasanapproximationratherthanthetruth. Thistheoryhasled totheconceptsofthepseudo-truevalue(theparametervaluedefinedbytheestimationproblem),the quasi-likelihoodfunction,quasi-MLE,andquasi-likelihoodinference. Closelyrelatedisthesemiparametricapproach. Aprobabilisticeconomicmodelispartiallyspec- ifiedbutsomefeaturesareleftunspecified. Thisapproachtypicallyleadstoestimationmethodssuch as least squares and the Generalized Method of Moments. The semiparametric approach dominates contemporaryeconometrics,andisthemainfocusofthistextbook. Another branch of quantitative structural economics is the calibration approach. Similar to the quasi-structuralapproach,thecalibrationapproachinterpretsstructuralmodelsasapproximationsand henceinherentlyfalse. Thedifferenceisthatthecalibrationistliteraturerejectsmathematicalstatistics (deemingclassicaltheoryasinappropriateforapproximatemodels)andinsteadselectsparametersby matchingmodelanddatamomentsusingnon-statisticaladhoc1methods",
    "page": 22,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Another branch of quantitative structural economics is the calibration approach. Similar to the quasi-structuralapproach,thecalibrationapproachinterpretsstructuralmodelsasapproximationsand henceinherentlyfalse. Thedifferenceisthatthecalibrationistliteraturerejectsmathematicalstatistics (deemingclassicaltheoryasinappropriateforapproximatemodels)andinsteadselectsparametersby matchingmodelanddatamomentsusingnon-statisticaladhoc1methods. TrygveHaavelmo The founding ideas of the field of econometrics are largely due to the Nor- weigeneconometricianTrygveHaavelmo(1911-1999). Hisadvocacyofproba- bility models revolutionized the field, and his use of formal mathematical rea- soninglaidthefoundationforsubsequentgenerations.HewasawardedtheNo- belMemorialPrizeinEconomicSciencesin1989. 1Adhocmeansâforthispurposeââamethoddesignedforaspecificproblemâandnotbasedonageneralizableprinciple.",
    "page": 22,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER1. INTRODUCTION 3 1.3 EconometricTermsandNotation Inatypicalapplication,aneconometricianhasasetofrepeatedmeasurementsonasetofvariables. Forexample,inalaborapplicationthevariablescouldincludeweeklyearnings,educationalattainment, age,andotherdescriptivecharacteristics.Wecallthisinformationthedata,dataset,orsample. Weusethetermobservationstorefertodistinctrepeatedmeasurementsonthevariables. Anindi- vidualobservationoftencorrespondstoaspecificeconomicunit,suchasaperson,household,corpo- ration, firm, organization, country, state, city or other geographical region. An individual observation couldalsobeameasurementatapointintime,suchasquarterlyGDPoradailyinterestrate. EconomiststypicallydenotevariablesbytheitalicizedromancharactersY, X,and/or Z. Thecon- ventionineconometricsistousethecharacterY todenotethevariabletobeexplained,whilethechar- actersX andZ areusedtodenotetheconditioning(explaining)variables.Followingmathematicalprac- tice, randomvariablesandvectorsaredenotedbyuppercaseromancharacterssuchasY and X. We makeanexceptionforequationerrorswhichwetypicallydenotebythelowercaseletterse,uorv. Realnumbers(elementsoftherealline(cid:82),alsocalledscalars)arewrittenusinglowercaseitalicssuch asx.Vectors(elementsof(cid:82)k)areeitherwrittensimilarlyusinglowercaseitalics,e.g. ï£« ï£¶ x 1 ï£¬ x ï£· x=ï£¬ ï£¬ . 2 ï£· ï£· ï£¬ . . ï£· ï£­ ï£¸ x k orusingbylowercasebolditalicssuchasx.Weuseboldinmatrixalgebraicexpressionsforcompatibility withmatrixnotation. MatricesarewrittenusinguppercasebolditalicssuchasX.Ournotationwillnotmakeadistinction betweenrandomandnon-randommatrices.TypicallyweuseU,V,X,Y,Z todenoterandommatrices anduse A,B,C,W todenotenon-randommatrices. Wedenotethenumberofobservationsbythenaturalnumbern,andsubscriptthevariablesbythe indexi todenotetheindividualobservation,e.g. Y . Insomecontextsweuseindicesotherthani,such i asintimeseriesapplicationswheretheindext iscommon.Inpanelstudieswetypicallyusethedouble indexit torefertoindividuali atatimeperiodt. WetypicallyuseGreekletterssuchasÎ²,Î¸andÏ2todenoteunknownparameters(scalarorvectors). ParametermatricesarewrittenusinguppercaseLatinboldface,e.g. A.Estimatorsaretypicallydenoted byputtingahatâ^â,tildeâ~âorbarâ-âoverthecorrespondingletter,e.g. Î² (cid:98)andÎ² (cid:101)areestimatorsofÎ², and A(cid:98) isanestimatorof A. The covariance matrix of an econometric estimator will typically be written using the upper case boldfaceV,oftenwithasubscripttodenotetheestimator,e.g.V Î²(cid:98) =var (cid:163)Î² (cid:98) (cid:164) asthecovariancematrixfor Î² (cid:98). Hopefully without caus (cid:112) ing confusion, we will use the notationVÎ² =avar (cid:163)Î² (cid:98) (cid:164) to denote the asymp- totic covariance matrix of n (cid:161)Î² (cid:98) âÎ²(cid:162) (the variance of the asymptotic distribution). Covariance matrix estimatorswillbedenotedbyappendinghatsortildes,e.g.V(cid:98)Î²isanestimatorofVÎ²",
    "page": 23,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Hopefully without caus (cid:112) ing confusion, we will use the notationVÎ² =avar (cid:163)Î² (cid:98) (cid:164) to denote the asymp- totic covariance matrix of n (cid:161)Î² (cid:98) âÎ²(cid:162) (the variance of the asymptotic distribution). Covariance matrix estimatorswillbedenotedbyappendinghatsortildes,e.g.V(cid:98)Î²isanestimatorofVÎ². 1.4 ObservationalData Acommoneconometricquestionistoquantifythecausalimpactofonesetofvariablesonanother variable.Forexample,aconcerninlaboreconomicsisthereturnstoschoolingâthechangeinearnings inducedbyincreasingaworkerâseducation,holdingothervariablesconstant. Anotherissueofinterest istheearningsgapbetweenmenandwomen.",
    "page": 23,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER1. INTRODUCTION 4 Ideally, we would use experimental data to answer these questions. To measure the returns to schooling, anexperimentmightrandomlydividechildrenintogroups, mandatedifferentlevelsofed- ucationtothedifferentgroups,andthenfollowthechildrenâswagepathaftertheymatureandenterthe laborforce. Thedifferencesbetweenthegroupswouldbedirectmeasurementsoftheeffectsofdiffer- ent levels of education. However, experiments such as this would be widely condemned as immoral! Consequently,ineconomicsexperimentaldatasetsaretypicallynarrowinscope. Instead,mosteconomicdataisobservational.Tocontinuetheaboveexample,throughdatacollec- tionwecanrecordthelevelofapersonâseducationandtheirwage. Withsuchdatawecanmeasurethe joint distribution of these variables and assess their joint dependence. But from observational data it isdifficulttoinfercausalityaswearenotabletomanipulateonevariabletoseethedirecteffectonthe other.Forexample,apersonâslevelofeducationis(atleastpartially)determinedbythatpersonâschoices. Thesefactorsarelikelytobeaffectedbytheirpersonalabilitiesandattitudestowardswork.Thefactthat apersonishighlyeducatedsuggestsahighlevelofability,whichsuggestsahighrelativewage.Thisisan alternativeexplanationforanobservedpositivecorrelationbetweeneducationallevelsandwages.High abilityindividualsdobetterinschool,andthereforechoosetoattainhigherlevelsofeducation,andtheir highabilityisthefundamentalreasonfortheirhighwages. Thepointisthatmultipleexplanationsare consistentwithapositivecorrelationbetweenschoolinglevelsandeducation. Knowledgeofthejoint distributionalonemaynotbeabletodistinguishbetweentheseexplanations. Mosteconomicdatasetsareobservational,notexperimental.Thismeansthat allvariablesmustbetreatedasrandomandpossiblyjointlydetermined. This discussion means that it is difficult to infer causality from observational data alone. Causal inferencerequiresidentification,andthisisbasedonstrongassumptions. Wewilldiscusstheseissues onoccasionthroughoutthetext. 1.5 StandardDataStructures Therearefivemajortypesofeconomicdatasets:cross-sectional,timeseries,panel,clustered,and spatial.Theyaredistinguishedbythedependencestructureacrossobservations. Cross-sectionaldatasetshaveoneobservationperindividual. Surveysandadministrativerecords are a typical source for cross-sectional data. In typical applications, the individuals surveyed are per- sons, households, firms or other economic agents. In many contemporary econometric cross-section studiesthesamplesizen isquitelarge. Itisconventionaltoassumethatcross-sectionalobservations aremutuallyindependent.Mostofthistextisdevotedtothestudyofcross-sectiondata. Timeseriesdataareindexedbytime. Typicalexamplesincludemacroeconomicaggregates,prices and interest rates. This type of data is characterized by serial dependence. Most aggregate economic dataisonlyavailableatalowfrequency(annual, quarterly, ormonthly)sothesamplesizeistypically muchsmallerthanincross-sectionstudies",
    "page": 24,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Itisconventionaltoassumethatcross-sectionalobservations aremutuallyindependent.Mostofthistextisdevotedtothestudyofcross-sectiondata. Timeseriesdataareindexedbytime. Typicalexamplesincludemacroeconomicaggregates,prices and interest rates. This type of data is characterized by serial dependence. Most aggregate economic dataisonlyavailableatalowfrequency(annual, quarterly, ormonthly)sothesamplesizeistypically muchsmallerthanincross-sectionstudies. Anexceptionisfinancialdatawheredataareavailableata highfrequency(daily,hourly,orbytransaction)sosamplesizescanbequitelarge. Panel data combines elements of cross-section and time series. These data sets consist of a set of individuals(typicallypersons,households,orcorporations)measuredrepeatedlyovertime. Thecom- monmodelingassumptionisthattheindividualsaremutuallyindependentofoneanother,butagiven individualâsobservationsaremutuallydependent. Insomepaneldatacontextsthenumberoftimese- riesobservationsT perindividualissmallwhilethenumberofindividualsnislarge.Inotherpaneldata",
    "page": 24,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER1. INTRODUCTION 5 contexts (for example when countries or states are taken as the unit of measurement) the number of individualsn canbesmallwhilethenumberoftimeseriesobservationsT canbemoderatelylarge. An importantissueineconometricpaneldataisthetreatmentoferrorcomponents. Clusteredsamplesareincreasingpopularinappliedeconomicsandarerelatedtopaneldata.Inclus- teredsamplingtheobservationsaregroupedintoâclustersâwhicharetreatedasmutuallyindependent yetallowedtobedependentwithinthecluster. Themajordifferencewithpaneldataisthatclustered sampling typically does not explicitly model error component structures, nor the dependence within clusters,butratherisconcernedwithinferencewhichisrobusttoarbitraryformsofwithin-clustercor- relation. Spatialdependenceisanothermodelofinterdependence. Theobservationsaretreatedasmutually dependentaccordingtoaspatialmeasure(forexample,geographicproximity).Unlikeclustering,spatial modelsallowallobservationstobemutuallydependent, andtypicallyrelyonexplicitmodelingofthe dependence relationships. Spatial dependence can also be viewed as a generalization of time series dependence. DataStructures â¢ Cross-section â¢ Time-series â¢ Panel â¢ Clustered â¢ Spatial Aswementionedabove,mostofthistextwillbedevotedtocross-sectionaldataundertheassump- tionofmutuallyindependentobservations. Bymutualindependencewemeanthattheith observation (Y ,X )isindependentofthe jth observation (cid:161) Y ,X (cid:162) fori (cid:54)=j.Inthiscasewesaythatthedataareinde- i i j j pendentlydistributed.(Sometimesthelabelâindependentâismisconstrued.Itisastatementaboutthe relationshipbetweenobservationsi and j,notastatementabouttherelationshipbetweenY andX .) i i Furthermore,ifthedataisrandomlygathered,itisreasonabletomodeleachobservationasadraw from the same probability distribution. In this case we say that the data are identically distributed. Iftheobservationsaremutuallyindependentandidenticallydistributed, wesaythattheobservations areindependentandidenticallydistributed, i.i.d., orarandomsample. Formostofthistextwewill assumethatourobservationscomefromarandomsample. Definition1.1 The variables (Y ,X ) are a sample from the distribution F if i i theyareidenticallydistributedwithdistributionF.",
    "page": 25,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER1. INTRODUCTION 6 Definition1.2 Thevariables(Y ,X )arearandomsampleiftheyaremutually i i independentandidenticallydistributed(i.i.d.)acrossi =1,...,n. In the random sampling framework, we think of an individual observation (Y ,X ) as a realization i i (cid:161) (cid:162) fromajointprobabilitydistributionF y,x whichwecancallthepopulation. Thisâpopulationâisin- finitelylarge.Thisabstractioncanbeasourceofconfusionasitdoesnotcorrespondtoaphysicalpopu- lationintherealworld.ItisanabstractionsincethedistributionF isunknown,andthegoalofstatistical inferenceistolearnaboutfeaturesofF fromthesample.Theassumptionofrandomsamplingprovides themathematicalfoundationfortreatingeconomicstatisticswiththetoolsofmathematicalstatistics. The random sampling framework was a major intellectual breakthrough of the late 19th century, allowingtheapplicationofmathematicalstatisticstothesocialsciences. Beforethisconceptualdevel- opment,methodsfrommathematicalstatisticshadnotbeenappliedtoeconomicdataasthelatterwas viewedasnon-random. Therandomsamplingframeworkenabledeconomicsamplestobetreatedas random,anecessarypreconditionfortheapplicationofstatisticalmethods. 1.6 EconometricSoftware Economistsuseavarietyofeconometric,statistical,andprogrammingsoftware. Stataisapowerfulstatisticalprogramwithabroadsetofpre-programmedeconometricandstatisti- caltools. Itisquitepopularamongeconomists,andiscontinuouslybeingupdatedwithnewmethods. Itisanexcellentpackageformosteconometricanalysis,butislimitedwhenyouwanttouseneworless- common econometric methods which have not yet been programed. At many points in this textbook specific Stata estimation methods and commands are described. These commands are valid for Stata version16. MATLAB,GAUSS,andOxMetricsarehigh-levelmatrixprogramminglanguageswithawidevarietyof built-instatisticalfunctions. Manyeconometricmethodshavebeenprogramedintheselanguagesand areavailableontheweb. Theadvantageofthesepackagesisthatyouareincompletecontrolofyour analysis,anditiseasiertoprogramnewmethodsthaninStata. Somedisadvantagesarethatyouhave todomuchoftheprogrammingyourself,programmingcomplicatedprocedurestakessignificanttime, andprogrammingerrorsarehardtopreventanddifficulttodetectandeliminate. Oftheselanguages, GAUSSusedtobequitepopularamongeconometricians,butcurrentlyMATLABismorepopular. AnintermediatechoiceisR.Rhasthecapabilitiesoftheabovehigh-levelmatrixprogramminglan- guages,butalsohasmanybuilt-instatisticalenvironmentswhichcanreplicatemuchofthefunctionality ofStata. Risthedominantprogramminglanguageinthestatisticsfield,somethodsdevelopedinthat arenaaremostcommonlyavailableinR.Uniquely,Risopen-source,user-contributed,andbestofall, completelyfree!AgrowinggroupofeconometriciansareenthusiasticfansofR. Forhighly-intensivecomputationaltasks,someeconomistswritetheirprogramsinastandardpro- gramminglanguagesuchasFortranorC.Thiscanleadtomajorgainsincomputationalspeed, atthe costofincreasedtimeinprogramminganddebugging",
    "page": 26,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Risthedominantprogramminglanguageinthestatisticsfield,somethodsdevelopedinthat arenaaremostcommonlyavailableinR.Uniquely,Risopen-source,user-contributed,andbestofall, completelyfree!AgrowinggroupofeconometriciansareenthusiasticfansofR. Forhighly-intensivecomputationaltasks,someeconomistswritetheirprogramsinastandardpro- gramminglanguagesuchasFortranorC.Thiscanleadtomajorgainsincomputationalspeed, atthe costofincreasedtimeinprogramminganddebugging. Therearemanyotherpackageswhichareusedbyeconometricians, includeEviews, Gretl, PcGive, Python,Julia,RATS,andSAS. Asthepackagesdescribedabovehavedistinctadvantagesmanyempiricaleconomistsendupusing morethanonepackage. Asastudentofeconometricsyouwilllearnatleastoneofthesepackagesand probablymorethanone. Myadviceisthatallstudentsofeconometricsshoulddevelopabasiclevelof familiaritywithStata,MATLAB,andR.",
    "page": 26,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER1. INTRODUCTION 7 1.7 Replication Scientificresearchneedstobedocumentedandreplicable. Forsocialscienceresearchusingobser- vationaldatathisrequirescarefuldocumentationandarchivingoftheresearchmethods,datamanipu- lations,andcoding. Thebestpracticeisasfollows.Accompanyingeachpublishedpaperanauthorshouldcreateacom- pletereplicationpackage(setofdatafiles,documentation,andprogramcodefiles).Thispackageshould containthesource(raw)datausedforanalysis,andcodewhichexecutestheempiricalanalysisandother numericalworkreportedinthepaper. Inmostcasesthisisasetofprogramswhichmayneedtobeex- ecuted sequentially. (For example, there may be an initial program which âcleansâ and manipulates thedata,andthenasecondsetofprogramswhichestimatethereportedmodels.)Theidealisfulldocu- mentationandclarity.Thispackageshouldbepostedontheauthor(s)website,andpostedatthejournal websitewhenthatisanoption. Acomplicatingfactoristhatmanycurrenteconomicdatasetshaverestrictedaccessandcannotbe sharedwithoutpermission. Inthesecasesthedatacannotbepostednorshared. Thecomputedcode, however,canandshouldbeposted. Most journals in economics require authors of published papers to make their datasets generally available.Forexample: Econometricastates: Econometrica has the policy that all empirical, experimental and simulation results must bereplicable. Therefore,authorsofacceptedpapersmustsubmitdatasets,programs,and informationonempiricalanalysis,experimentsandsimulationsthatareneededforreplica- tionandsomelimitedsensitivityanalysis. TheAmericanEconomicReviewstates: Alldatausedinanalysismustbemadeavailabletoanyresearcherforpurposesofreplica- tion. TheJournalofPoliticalEconomystates: ItisthepolicyoftheJournalofPoliticalEconomytopublishpapersonlyifthedatausedin theanalysisareclearlyandpreciselydocumentedandarereadilyavailabletoanyresearcher forpurposesofreplication. If you are interested in using the data from a published paper, first check the journalâs website, as manyjournalsarchivedataandreplicationprogramsonline.Second,checkthewebsite(s)ofthepaperâs author(s). Most academic economists maintain webpages, and some make available replication files completewithdataandprograms.Iftheseinvestigationsfail,emailtheauthor(s),politelyrequestingthe data.Youmayneedtobepersistent. Asamatterofprofessionaletiquette,allauthorsabsolutelyhavetheobligationtomaketheirdataand programsavailable. Unfortunately,manyfailtodoso,andtypicallyforpoorreasons. Theironyofthe situationisthatitistypicallyinthebestinterestsofascholartomakeasmuchoftheirwork(including alldataandprograms)freelyavailable,asthisonlyincreasesthelikelihoodoftheirworkbeingcitedand havinganimpact. Keepthisinmindasyoustartyourownempiricalproject. Rememberthataspartofyourendprod- uct,youwillneed(andwant)toprovidealldataandprogramstothecommunityofscholars.Thegreatest",
    "page": 27,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER1. INTRODUCTION 8 formofflatteryistolearnthatanotherscholarhasreadyourpaper,wantstoextendyourwork,orwants to use your empirical methods. In addition, public openness provides a healthy incentive for trans- parencyandintegrityinempiricalanalysis. 1.8 DataFilesforTextbook Onthetextbookwebpagehttp://www.ssc.wisc.edu/~bhansen/econometrics/therearepostedanum- ber of files containing data sets which are used in this textbook both for illustration and for end-of- chapterempiricalexercises. Formostofthedatasetstherearefourfiles: (1)Description(pdfformat); (2)Exceldatafile; (3)Textdatafile; (4)Statadatafile. Thethreedatafilesareidenticalincontent: the observationsandvariablesarelistedinthesameorderineach,andallhavevariablelabels. Forexample,thetextmakesfrequentreferencetoawagedatasetextractedfromtheCurrentPopula- tionSurvey.Thisdatasetisnamedcps09mar,andisrepresentedbythefilescps09mar_description.pdf, cps09mar.xlsx,cps09mar.txt,andcps09mar.dta. Thedatasetscurrentlyincludedare â¢ AB1991 â DatafilefromArellanoandBond(1991) â¢ AJR2001 â DatafilefromAcemoglu,JohnsonandRobinson(2001) â¢ AK1991 â DatafilefromAngristandKrueger(1991) â¢ AL1999 â DatafilefromAngristandLavy(1999) â¢ BMN2016 â DatafilefromBernheim,MeerandNovarro(2016) â¢ cps09mar â householdsurveydataextractedfromtheMarch2009CurrentPopulationSurvey â¢ Card1995 â DatafilefromCard(1995) â¢ CHJ2004 â DatafilefromCox,B.E.HansenandJimenez(2004) â¢ CK1994",
    "page": 28,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER1. INTRODUCTION 9 â DatafilefromCardandKrueger(1994) â¢ CMR2008 â DatefilefromCard,Mas,andRothstein(2008) â¢ DDK2011 â DatafilefromDuflo,DupasandKremer(2011) â¢ DS2004 â DatafilefromDiTellaandSchargrodsky(2004) â¢ FRED-MDandFRED-QD â U.S.monthlyandquarterlymacroeconomicdatabasesfromMcCrackenandNg(2015) â¢ Invest1993 â DatafilefromHallandHall(1993) â¢ LM2007 â DatafilefromLudwigandMiller(2007)andCattaneo,Titiunik,andVazquez-Bare(2017) â¢ Kilian2009 â DatafilefromKilian(2009) â¢ Koppelman â Data file from Forinash and Koppelman (1993), Koppelman and Wen (2000) and Wen and Koppelman(2001) â¢ MRW1992 â DatafilefromMankiw,RomerandWeil(1992) â¢ Nerlove1963 â DatafilefromNerlov(1963) â¢ PSS2017 â DatafilefromPapageorgiou,Saam,andSchulte(2017) â¢ RR2010 â DatafilefromReinhardandRogoff(2010)",
    "page": 29,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER1. INTRODUCTION 10 1.9 ReadingtheManuscript Ihaveendeavoredtouseaunifiednotationandnomenclature. Thedevelopmentofthematerialis cumulative,withlaterchaptersbuildingontheearlierones.Nevertheless,everyattempthasbeenmade tomakeeachchapterself-containedsoreaderscanpickandchoosetopicsaccordingtotheirinterests. Tofullyunderstandeconometricmethodsitisnecessarytohaveamathematicalunderstandingofits mechanics,andthisincludesthemathematicalproofsofthemainresults.Consequently,thistextisself- contained with nearly all results proved with full mathematical rigor. The mathematical development andproofsaimatbrevityandconciseness(sometimesdescribedasmathematicalelegance),butalsoat pedagogy. Tounderstandamathematicalproofitisnotsufficienttosimplyreadtheproof,youneedto followitandre-createitforyourself. Nevertheless,manyreaderswillnotbeinterestedineachmathematicaldetail,explanation,orproof. Thisisokay. Touseamethoditmaynotbenecessarytounderstandthemathematicaldetails. Accord- ingly I have placed the more technical mathematical proofs and details in chapter appendices. These appendicesandothertechnicalsectionsaremarkedwithanasterisk(*). Thesesectionscanbeskipped withoutanylossinexposition. ThekeyconceptsofmatrixalgebraandprobabilityinequalitiesarereviewedinAppendicesA&B. ItmaybeusefultoreadorreviewAppendixA.1-A.11beforestartingChapter3,andreviewAppendixB beforeChapter6.Itisnotnecessarytounderstandallthematerialintheappendices.Theyareintended tobereferencematerialandsomeoftheresultsarenotusedinthistextbook.",
    "page": 30,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER1. INTRODUCTION 11 1.10 CommonSymbols a scalar aora vector A matrix X randomvariableorvector (cid:82) realline (cid:82) + positiverealline (cid:82)k Euclideank space (cid:80)[A] probability (cid:80)[A|B] conditionalprobability F(x) cumulativedistributionfunction Ï(x) probabilitymassfunction f(x) probabilitydensityfunction (cid:69)[X] mathematicalexpectation (cid:69)[Y |X =x],(cid:69)[Y |X] conditionalexpectation var[X] varianceorcovariancematrix var[Y |X =x],var[Y |X] conditionalvariance cov(X,Y) covariance P [Y |X =x],P [Y |X] bestlinearpredictor corr(X,Y) correlation X samplemean n Ï2 samplevariance (cid:98) s2 biased-correctedsamplevariance Î¸ (cid:98) estimator s (cid:161)Î¸ (cid:98) (cid:162) standarderrorofestimator lim limit nââ plim probabilitylimit nââ ââ convergence ââ convergenceinprobability p ââ convergenceindistribution d L (Î¸) likelihoodfunction n (cid:96) (Î¸) log-likelihoodfunction n I Î¸ informationmatrix N(0,1) standardnormaldistribution N(Âµ,Ï2) normaldistributionwithmeanÂµandvarianceÏ2 Ï2 chi-squaredistributionwithk degreesoffreedom k",
    "page": 31,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER1. INTRODUCTION 12 I nÃnidentitymatrix n 1 nÃ1vectorofones n trA trace (cid:48) A vectorormatrixtranspose A â1 matrixinverse A>0 positivedefinite Aâ¥0 positivesemi-definite (cid:107)a(cid:107) Euclideannorm (cid:107)A(cid:107) matrixnorm d=ef definitionalequality 1 {a} indicatorfunction(1ifaistrue,else0) (cid:39) approximateequality â¼ isdistributedas log(x) naturallogarithm exp(x) exponentialfunction n (cid:88) summationfromi =1ton i=1",
    "page": 32,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Part I Regression 13",
    "page": 33,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Chapter 2 Conditional Expectation and Projection 2.1 Introduction Themostcommonlyappliedeconometrictoolisleastsquaresestimation,alsoknownasregression. Leastsquaresisatooltoestimatetheconditionalmeanofonevariable(thedependentvariable)given anothersetofvariables(theregressors,conditioningvariables,orcovariates). Inthischapterweabstractfromestimationandfocusontheprobabilisticfoundationofthecondi- tionalexpectationmodelanditsprojectionapproximation.Thisincludesareviewofprobabilitytheory. ForabackgroundinintermediateprobabilitytheoryseeChapters1-5ofIntroductiontoEconometrics. 2.2 TheDistributionofWages SupposethatweareinterestedinwageratesintheUnitedStates.Sincewageratesvaryacrosswork- erswecannotdescribewageratesbyasinglenumber.Instead,wecandescribewagesusingaprobabil- itydistribution. Formally,weviewthewageofanindividualworkerasarandomvariablewagewiththe probabilitydistribution F(u)=(cid:80)(cid:163) wageâ¤u (cid:164) . Whenwesaythatapersonâswageisrandomwemeanthatwedonotknowtheirwagebeforeitismea- sured, and we treat observed wage rates as realizations from the distribution F. Treating unobserved wagesasrandomvariablesandobservedwagesasrealizationsisapowerfulmathematicalabstraction whichallowsustousethetoolsofmathematicalprobability. Ausefulthoughtexperimentistoimaginedialingatelephonenumberselectedatrandom,andthen askingthepersonwhorespondstotellustheirwagerate. (Assumeforsimplicitythatallworkershave equal access to telephones and that the person who answers your call will answer honestly.) In this thoughtexperiment,thewageofthepersonyouhavecalledisasingledrawfromthedistributionF of wagesinthepopulation.Bymakingmanysuchphonecallswecanlearnthefulldistribution. WhenadistributionfunctionF isdifferentiablewedefinetheprobabilitydensityfunction d f(u)= F(u). du Thedensitycontainsthesameinformationasthedistributionfunction,butthedensityistypicallyeasier tovisuallyinterpret. 14",
    "page": 34,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 15 0 10 20 30 40 50 60 0.1 9.0 8.0 7.0 6.0 5.0 4.0 3.0 2.0 1.0 0.0 0 10 20 30 40 50 60 70 0 1 2 3 4 5 6 (a)Distribution (b)Density (c)Densityoflog(wage) Figure2.1:WageDistributionandDensity.AllFull-timeU.S.Workers In Figure 2.1 we display estimates1 of the probability distribution function (panel (a)) and density function(panel(b))ofU.S.wageratesin2009. Weseethatthedensityispeakedaround$15,andmost oftheprobabilitymassappearstoliebetween$10and$40.Thesearerangesfortypicalwageratesinthe U.S.population. Importantmeasuresofcentraltendencyarethemedianandthemean.Themedianmofacontinu- ousdistributionF istheuniquesolutionto 1 F(m)= . 2 ThemedianU.S.wageis$19.23. Themedianisarobust2measureofcentraltendency,butitistrickyto useformanycalculationsasitisnotalinearoperator. ThemeanorexpectationofarandomvariableY withdiscretesupportis â Âµ=(cid:69)[Y]= (cid:88) Ï (cid:80)(cid:163) Y =Ï (cid:164) . j j j=1 Foracontinuousrandomvariablewithdensity f(y)theexpectationis (cid:90) â Âµ=(cid:69)[Y]= yf(y)dy. ââ HerewehaveusedthecommonandconvenientconventionofusingthesinglecharacterY todenotea randomvariable,ratherthanthemorecumbersomelabelwage. Analternativenotationwhichincludes (cid:82)â bothdiscreteandcontinuousrandomvariablesasspecialcasesistowritetheintegralas ydF(y). ââ Theexpectationisaconvenientmeasureofcentraltendencybecauseitisalinearoperatorandarises naturallyinmanyeconomicmodels.Adisadvantageoftheexpectationisthatitisnotrobust3especially inthepresenceofsubstantialskewnessorthicktails,whicharebothfeaturesofthewagedistributionas 1The distribution and density are estimated nonparametrically from the sample of 50,742 full-time non-military wage- earnersreportedintheMarch2009CurrentPopulationSurvey. Thewagerateisconstructedasannualindividualwageand salaryearningsdividedbyhoursworked. 2Themedianisnotsensitivetopertubationsinthetailsofthedistribution. 3Theexpectationissensitivetopertubationsinthetailsofthedistribution.",
    "page": 35,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 16 canbeseeneasilyinFigure2.1(b). Anotherwayofviewingthisisthat64%ofworkersearnlessthanthe meanwageof$23.90,suggestingthatitisincorrecttodescribethemean$23.90asaâtypicalâwagerate. Inthiscontextitisusefultotransformthedatabytakingthenaturallogarithm4.Figure2.1(c)shows thedensityofloghourlywageslog(wage)forthesamepopulation.Thedensityoflogwagesismuchless skewedandfat-tailedthanthedensityofthelevelofwages,soitsmean (cid:69)(cid:163) log(wage) (cid:164)=2.95 isamuchbetter(morerobust)measure5 ofcentraltendencyofthedistribution. Forthisreason,wage regressionstypicallyuselogwagesasadependentvariableratherthanthelevelofwages. AnotherusefulwaytosummarizetheprobabilitydistributionF(u)isintermsofitsquantiles. For any Î±â(0,1), the Î±th quantile ofthe continuous6 distributionF is the real number qÎ± which satisfies F (cid:161) qÎ± (cid:162)=Î±.ThequantilefunctionqÎ±,viewedasafunctionofÎ±,istheinverseofthedistributionfunction F.Themostcommonlyusedquantileisthemedian,thatis,q =m.Wesometimesrefertoquantilesby 0.5 thepercentilerepresentationofÎ±andinthiscasetheyarecalledpercentiles.E.g.themedianisthe50th percentile. 2.3 ConditionalExpectation We saw in Figure 2.1(c) the density of log wages. Is this distribution the same for all workers, or doesthewagedistributionvaryacrosssubpopulations? Toanswerthisquestion,wecancomparewage distributionsfordifferentgroupsâforexample,menandwomen. TheplotinFigure2.2(a)displaysthe densities of log wages for U.S. men and women. We can see that the two wage densities take similar shapesbutthedensityformenissomewhatshiftedtotheright. white men white women black men black women Women Men 0 1 2 3 4 5 6 1.8 3.2 4.6 (a)WomenandMen (b)ByGenderandRace Figure2.2:LogWageDensitybyGenderandRace 4Throughoutthetext,wewilluselog(y)orlogytodenotethenaturallogarithmofy. 5Moreprecisely,thegeometricmeanexp (cid:161)(cid:69)(cid:163) logW (cid:164)(cid:162)=$19.11isarobustmeasureofcentraltendency. 6IfFisnotcontinuousthedefinitionisqÎ±=inf{u:F(u)â¥Î±}",
    "page": 36,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 17 Thevalues3.05and2.81arethemeanlogwagesinthesubpopulationsofmenandwomenworkers. Theyarecalledtheconditionalmeans(orconditionalexpectations)oflogwagesgivengender.Wecan writetheirspecificvaluesas (cid:69)(cid:163) log(wage)|gender=man (cid:164)=3.05 (cid:69)(cid:163) log(wage)|gender=woman (cid:164)=2.81. We call these means conditional as they are conditioning on a fixed value of the variable gender. Whileyoumightnotthinkofapersonâsgenderasarandomvariable,itisrandomfromtheviewpointof econometricanalysis.Ifyourandomlyselectanindividual,thegenderoftheindividualisunknownand thusrandom.(InthepopulationofU.S.workers,theprobabilitythataworkerisawomanhappenstobe 43%.)Inobservationaldata,itismostappropriatetoviewallmeasurementsasrandomvariables,and themeansofsubpopulationsarethenconditionalmeans. Itisimportanttomentionatthispointthatweinnowayattributecausalityorinterpretationtothe difference in the conditional expectation of log wages between men and women. There are multiple potentialexplanations. AsthetwodensitiesinFigure2.2appearsimilar,ahastyinferencemightbethatthereisnotamean- ingfuldifferencebetweenthewagedistributionsofmenandwomen.Beforejumpingtothisconclusion letusexaminethedifferencesinthedistributionsmorecarefully. Aswementionedabove,theprimary differencebetweenthetwodensitiesappearstobetheirmeans.Thisdifferenceequals (cid:69)(cid:163) log(wage)|gender=man (cid:164)â(cid:69)(cid:163) log(wage)|gender=woman (cid:164)=3.05â2.81 =0.24. (2.1) Adifferenceinexpectedlogwagesof0.24isofteninterpretedasanaverage24%differencebetweenthe wagesofmenandwomen,whichisquitesubstantial.(ForamorecompleteexplanationseeSection2.4.) Considerfurthersplittingthemaleandfemalesubpopulationsbyrace,dividingthepopulationinto whites, Blacks, and other races. We display the log wage density functions of four of these groups in Figure2.2(b).Againweseethattheprimarydifferencebetweenthefourdensityfunctionsistheircentral tendency. Focusingonthemeansofthesedistributions,Table2.1reportsthemeanlogwageforeachofthesix sub-populations. Table2.1:MeanLogWagesbyGenderandRace men women white 3.07 2.82 Black 2.86 2.73 other 3.03 2.86 Onceagainwestressthatweinnowayattributecausalityorinterpretationtothedifferencesacross the entries of the table. The reason why we use these particular sub-populations to illustrate condi- tionalexpectationisbecausedifferencesineconomicoutcomesbetweengenderandracialgroupsinthe UnitedStates(andelsewhere)arewidelydiscussed;partoftheroleofsocialscienceistocarefullydoc- umentsuchpatterns,andpartofitsroleistocraftmodelsandexplanations. Conditionalexpectations (bythemselves)canhelpinthedocumentationanddescription;conditionalexpectationsbythemselves areneitheramodelnoranexplanation. TheentriesinTable2.1aretheconditionalmeansoflog(wage)givengenderandrace.Forexample (cid:69)(cid:163) log(wage)|gender=man, race=white (cid:164)=3.07",
    "page": 37,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 18 and (cid:69)(cid:163) log(wage)|gender=woman, race=Black (cid:164)=2.73. Onebenefitoffocusingonconditionalmeansisthattheyreducecomplicateddistributionstoasin- gle summary measure, and thereby facilitate comparisons across groups. Because of this simplifying property, conditional means are the primary interest of regression analysis and are a major focus in econometrics. Table2.1allowsustoeasilycalculateaveragewagedifferencesbetweengroups.Forexample,wecan seethatthewagegapbetweenmenandwomencontinuesafterdisaggregationbyrace,astheaverage gapbetweenwhitemenandwhitewomenis25%,andthatbetweenBlackmenandBlackwomenis13%. Wealsocanseethatthereisaracegap,astheaveragewagesofBlacksaresubstantiallylessthantheother racecategories. Inparticular,theaveragewagegapbetweenwhitemenandBlackmenis21%,andthat betweenwhitewomenandBlackwomenis9%. 2.4 LogsandPercentages Inthissectionwewanttomotivateandclarifytheuseofthelogarithminregressionanalysisbymak- ingtwoobservations.First,whenappliedtonumbersthedifferenceoflogarithmsapproximatelyequals thepercentagedifference.Second,whenappliedtoaveragesthedifferenceinlogarithmsapproximately equalsthepercentagedifferenceinthegeometricmean. Wenowexplaintheseideasandthenatureof theapproximationsinvolved. Taketwopositivenumbersaandb.Thepercentagedifferencebetweenaandbis (cid:181) aâb (cid:182) p=100 . b Rewriting a p =1+ . b 100 Takingnaturallogarithms, (cid:179) p (cid:180) logaâlogb=log 1+ . (2.2) 100 Ausefulapproximationforsmallxis log(1+x)(cid:39)x. (2.3) Thiscanbederivedfromtheinfiniteseriesexpansionoflog(1+x): x2 x3 x4 log(1+x)=xâ + â +Â·Â·Â·=x+O(x2). 2 3 4 ThesymbolO(x2)meansthattheremainderisboundedby Ax2 asxâ0forsome A<â.Numerically, theapproximationlog(1+x)(cid:39)xiswithin0.001for|x|â¤0.1.Theapproximationerrorincreaseswith|x|. Applying(2.3)to(2.2)andmultiplyingby100wefind p(cid:39)100 (cid:161) loga)âlogb (cid:162) . This shows that 100 multiplied by the difference in logarithms is approximately the percentage differ- ence.Numerically,theapproximationerrorislessthan0.1percentagepointsfor|c|â¤10. Nowconsiderthedifferenceintheexpectationoflogtransformedrandomvariables. Taketworan- dom variables X ,X > 0. It will be useful to define their geometric means Î¸ = exp (cid:161)(cid:69)(cid:163) logX (cid:164)(cid:162) and 1 2 1 1 Î¸ =exp (cid:161)(cid:69)(cid:163) logX (cid:164)(cid:162) .Thedifferenceintheexpectationofthelogtransforms(multipliedby100)is 2 2 100 (cid:161)(cid:69)(cid:163) logX (cid:164)â(cid:69)(cid:163) logX (cid:164)(cid:162)=100 (cid:161) logÎ¸ âlogÎ¸ (cid:162)(cid:39)p 2 1 2 1",
    "page": 38,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 19 the percentage difference between Î¸ and Î¸ . In words, the difference between the average of the log 2 1 transformedvariablesis(approximately)thepercentagedifferenceinthegeometricmeans. Thereasonwhythislatterobservationisimportantisbecausemanyeconometricequationstakethe semi-logform (cid:69)(cid:163) logY |group=1 (cid:164)=Âµ 1 (cid:69)(cid:163) logY |group=2 (cid:164)=Âµ 2 and considerable attention is given to the difference Âµ âÂµ . For example, in the previous section we 1 2 comparedtheaveragelogwagesformenandwomenandfoundthatthedifferenceis0.24. Inthatsec- tionwestatedthatthisdifferenceisofteninterpretedastheaveragepercentagedifference. Thisisnot quite right, but is not quite wrong either. What the above calculation shows is that this difference is approximately the percentage difference in the geometric mean. So Âµ âÂµ is an average percentage 1 2 difference,whereâaverageâreferstogeometricratherthanarithmeticmean. TocomparedifferentmeasuresofpercentagedifferenceseeTable2.2. Inthefirsttwocolumnswe reportaveragewagesformenandwomenintheCPSpopulationusingfourâaveragesâ:arithmeticmean, median,geometricmean,andmeanlog. Forbothgroupsthemeanishigherthanthemedianandgeo- metricmean,andthelattertwoaresimilartooneanother.Thisisacommonfeatureofskeweddistribu- tionssuchasthewagedistribution.Thethirdcolumnreportsthepercentagedifferencebetweenthefirst twocolumns(usingmenâswagesasthebase). Forexample,thefirstentryof34%statesthatthemean wageformenis34%higherthanthemeanwageforwomen.Thenextentriesshowthatthemedianand geometricmeanformenis26%higherthanthoseforwomen.Thefinalentryinthiscolumnis100times thesimpledifferencebetweenthemeanlogwage,whichis24%. Asshownabove,thedifferenceinthe meanofthelogtransformationisapproximatelythepercentagedifferenceinthegeometricmean,and thisapproximationisexcellentfordifferencesunder10%. Letâssummarizethisanalysis. Itiscommontotakelogarithmsofvariablesandmakecomparisons betweenconditionalmeans. Wehaveshownthatthesedifferencesaremeasuresofthepercentagedif- ferenceinthegeometricmean. Thusthecommondescriptionthatthedifferencebetweenexpectedlog transforms(suchasthe0.24differencebetweenthoseformenandwomenâswages)isanapproximate percentagedifference(e.g.a24%differenceinmenâswagesrelativetowomenâs)iscorrect,solongaswe realizethatweareimplicitlycomparinggeometricmeans. Inotherwords: Inpracticewhenwetransformourdatabytakinglogarithms(asiscommonineco- nomics) and then compare means (including regression coefficients) we are computing approximate percentagedifferencesinthegeometricmean. Table2.2:AverageWagesandPercentageDifferences men women %Difference ArithmeticMean $26.80 $20.00 34% Median $21.14 $16.83 26% GeometricMean $21.03 $16.64 26% â MeanlogWage 3.05 2.81 24%",
    "page": 39,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 20 2.5 ConditionalExpectationFunction An important determinant of wages is education. In many empirical studies economists measure educationalattainmentbythenumberofyears7ofschooling.Wewillwritethisvariableaseducation. The conditional expectation of log(wage) given gender, race, and education is a single number for eachcategory.Forexample (cid:69)(cid:163) log(wage)|gender=man, race=white, education=12 (cid:164)=2.84. WedisplayinFigure2.3theconditionalexpectationsoflog(wage)forwhitemenandwhitewomenas afunctionofeducation.Theplotisquiterevealing.Weseethattheconditionalexpectationisincreasing in years of education, but at a different rate for schooling levels above and below nine years. Another strikingfeatureofFigure2.3isthatthegapbetweenmenandwomenisroughlyconstantforalleducation levels.Asthevariablesaremeasuredinlogsthisimpliesaconstantaveragepercentagegapbetweenmen andwomenregardlessofeducationalattainment. l l l l l l l l l l l l Years of Education ruoH rep sralloD goL 4 6 8 10 12 14 16 18 20 0.4 5.3 0.3 5.2 0.2 l white men white women Figure2.3:ExpectedLogWageasaFunctionofYearsofEducation In many cases it is convenient to simplify the notation by writing variables using single charac- ters,typicallyY, X and/or Z. Itisconventionalineconometricstodenotethedependentvariable(e.g. log(wage))bytheletterY,aconditioningvariable(suchasgender)bytheletter X,andmultiplecondi- tioningvariables(suchasrace,educationandgender)bythesubscriptedlettersX ,X ,...,X . 1 2 k Conditionalexpectationscanbewrittenwiththegenericnotation (cid:69)[Y |X =x ,X =x ,...,X =x ]=m(x ,x ,...,x ). 1 1 2 2 k k 1 2 k Wecallthistheconditionalexpectationfunction(CEF).TheCEFisafunctionof(x ,x ,...,x )asitvaries 1 2 k withthevariables. Forexample,theconditionalexpectationofY =log(wage)given(X ,X )=(gender, 1 2 race)isgivenbythesixentriesofTable2.1. 7Here,educationisdefinedasyearsofschoolingbeyondkindergarten. Ahighschoolgraduatehaseducation=12,acollege graduatehaseducation=16,aMasterâsdegreehaseducation=18,andaprofessionaldegree(medical,laworPhD)haseduca- tion=20.",
    "page": 40,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 21 Forgreatercompactnesswetypicallywritetheconditioningvariablesasavectorin(cid:82)k: ï£« ï£¶ X 1 ï£¬ X ï£· X =ï£¬ ï£¬ . 2 ï£· ï£·. (2.4) ï£¬ . . ï£· ï£­ ï£¸ X k Giventhisnotation,theCEFcanbecompactlywrittenas (cid:69)[Y |X =x]=m(x). The CEF m(x)=(cid:69)[Y |X =x] is a function of x â(cid:82)k. It says: âWhen X takes the value x then the averagevalueofY ism(x).âSometimesitisusefultoviewtheCEFasafunctionoftherandomvariable X. Inthiscaseweevaluatethefunctionm(x)atX,andwritem(X)or(cid:69)[Y |X]. Thisisrandomasitisa functionoftherandomvariableX. 2.6 ContinuousVariables Intheprevioussectionsweimplicitlyassumedthattheconditioningvariablesarediscrete.However, manyconditioningvariablesarecontinuous. Inthissection,wetakeupthiscaseandassumethatthe variables(Y,X)arecontinuouslydistributedwithajointdensityfunction f(y,x). As an example, take Y =log(wage) and X = experience, the latter the number of years of potential labormarketexperience8.ThecontoursoftheirjointdensityareplottedontheleftsideofFigure2.4for thepopulationofwhitemenwith12yearsofeducation. Labor Market Experience (Years) ruoH rep sralloD goL 0.4 5.3 0.3 5.2 0.2 5.1 exp=5 exp=10 exp=25 exp=40 Conditional Mean 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 0 5 10 15 20 25 30 35 40 45 Log Dollars per Hour (b)ConditionalDensityoflog(wage)givenexperi- (a)JointDensityoflog(wage)andexperience ence Figure2.4:WhiteMenwithHighSchoolDegree Giventhejointdensity f(y,x)thevariablexhasthemarginaldensity (cid:90) â f (x)= f(y,x)dy. X ââ 8Asthereisnodirectmeasureforexperience,weinsteaddefineexperienceasageâeducationâ6",
    "page": 41,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 22 Foranyxsuchthat f (x)>0theconditionaldensityofY givenX isdefinedas X f Y|X (cid:161) y|x (cid:162)= f(y,x) . (2.5) f (x) X The conditional density is a renormalized slice of the jointdensity f(y,x) holding x fixed. The slice is renormalized(dividedby f (x)sothatitintegratestoone)andisthusadensity. Wecanvisualizethis X by slicing the joint density function at a specific value of x parallel with the y-axis. For example, take thedensitycontoursontheleftsideofFigure2.4andslicethroughthecontourplotataspecificvalue ofexperience,andthenrenormalizetheslicesothatitisaproperdensity. Thisgivesustheconditional densityoflog(wage)forwhitemenwith12yearsofeducationandthislevelofexperience. Wedothis forfourlevelsofexperience(5,10,25,and40years),andplotthesedensitiesontherightsideofFigure 2.4. We can see that the distribution of wages shifts to the right and becomes more diffuse as experi- enceincreasesfrom5to10years,andfrom10to25years,butthereislittlechangefrom25to40years experience. TheCEFofY givenX =xistheexpectationoftheconditionaldensity(2.5) (cid:90) â m(x)=(cid:69)[Y |X =x]= yf Y|X (cid:161) y|x (cid:162) dy. (2.6) ââ Intuitively,m(x)istheexpectationofY fortheidealizedsubpopulationwheretheconditioningvariables arefixedatx.WhenX iscontinuouslydistributedthissubpopulationisinfinitelysmall. Thisdefinition(2.6)isappropriatewhentheconditionaldensity(2.5)iswelldefined.However,The- orem2.13inSection2.31willshowthatm(x)canbedefinedforanyrandomvariables(Y,X)solongas (cid:69)|Y|<â. InFigure2.4theCEFoflog(wage)givenexperience isplottedasthesolidline. Wecanseethatthe CEFisasmoothbutnonlinearfunction.TheCEFisinitiallyincreasinginexperience,flattensoutaround experience=30,andthendecreasesforhighlevelsofexperience. 2.7 LawofIteratedExpectations Anextremelyusefultoolfromprobabilitytheoryisthelawofiteratedexpectations. Animportant specialcaseistheknownastheSimpleLaw. Theorem2.1 SimpleLawofIteratedExpectations If(cid:69)|Y|<âthenforanyrandomvectorX, (cid:69)[(cid:69)[Y |X]]=(cid:69)[Y]. Thisstatesthattheexpectationoftheconditionalexpectationistheunconditionalexpectation. In otherwordstheaverageoftheconditionalaveragesistheunconditionalaverage.FordiscreteX â (cid:69)[(cid:69)[Y |X]]= (cid:88) (cid:69)(cid:163) Y |X =x (cid:164)(cid:80)(cid:163) X =x (cid:164) . j j j=1 ForcontinuousX (cid:90) (cid:69)[(cid:69)[Y |X]]= (cid:69)[Y |X =x]f (x)dx. X (cid:82)k",
    "page": 42,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 23 Goingbacktoourinvestigationofaveragelogwagesformenandwomen,thesimplelawstatesthat (cid:69)(cid:163) log(wage)|gender=man (cid:164)(cid:80)(cid:163) gender=man (cid:164) +(cid:69)(cid:163) log(wage)|gender=woman (cid:164)(cid:80)(cid:163) gender=woman (cid:164) =(cid:69)(cid:163) log(wage) (cid:164) . Ornumerically, 3.05Ã0.57+2.81Ã0.43=2.95. Thegenerallawofiteratedexpectationsallowstwosetsofconditioningvariables. Theorem2.2 LawofIteratedExpectations If(cid:69)|Y|<âthenforanyrandomvectorsX andX , 1 2 (cid:69)[(cid:69)[Y |X ,X ]|X ]=(cid:69)[Y |X ]. 1 2 1 1 Noticethewaythelawisapplied. Theinnerexpectationconditionson X and X ,whiletheouter 1 2 expectationconditionsonlyon X .Theiteratedexpectationyieldsthesimpleanswer(cid:69)[Y |X ],theex- 1 1 pectationconditionalonX alone.Sometimeswephrasethisas:âThesmallerinformationsetwins.â 1 Asanexample (cid:69)(cid:163) log(wage)|gender=man, race=white (cid:164)(cid:80)(cid:163) race=white|gender=man (cid:164) +(cid:69)(cid:163) log(wage)|gender=man, race=Black (cid:164)(cid:80)(cid:163) race=Black|gender=man (cid:164) +(cid:69)(cid:163) log(wage)|gender=man, race=other (cid:164)(cid:80)(cid:163) race=other|gender=man (cid:164) =(cid:69)(cid:163) log(wage)|gender=man (cid:164) ornumerically 3.07Ã0.84+2.86Ã0.08+3.03Ã0.08=3.05. A property of conditional expectations is that when you condition on a random vector X you can effectivelytreatitasifitisconstant. Forexample,(cid:69)[X |X]=X and(cid:69)(cid:163) g(X)|X (cid:164)=g(X)foranyfunction g(Â·).ThegeneralpropertyisknownastheConditioningTheorem. Theorem2.3 ConditioningTheorem If(cid:69)|Y|<âthen (cid:69)(cid:163) g(X)Y |X (cid:164)=g(X)(cid:69)[Y |X]. (2.7) Ifinaddition(cid:69)(cid:175) (cid:175)g(X) (cid:175) (cid:175) <âthen (cid:69)(cid:163) g(X)Y (cid:164)=(cid:69)(cid:163) g(X)(cid:69)[Y |X] (cid:164) . (2.8) TheproofsofTheorems2.1,2.2and2.3aregiveninSection2.33.",
    "page": 43,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 24 2.8 CEFError TheCEFerroreisdefinedasthedifferencebetweenY andtheCEFevaluatedatX: e=Y âm(X). Byconstruction,thisyieldstheformula Y =m(X)+e. (2.9) In(2.9)itisusefultounderstandthattheerrore isderivedfromthejointdistributionof(Y,X),and soitspropertiesarederivedfromthisconstruction. ManyauthorsineconometricsdenotetheCEFerrorusingtheGreekletterÎµ.Idonotfollowthiscon- ventionsincetheerroreisarandomvariablesimilartoY andX,anditistypicaltouseLatincharacters forrandomvariables. AkeypropertyoftheCEFerroristhatithasaconditionalmeanofzero. Toseethis,bythelinearity ofexpectations,thedefinitionm(X)=(cid:69)[Y |X]andtheConditioningTheorem (cid:69)[e|X]=(cid:69)[(Y âm(X))|X] =(cid:69)[Y |X]â(cid:69)[m(X)|X] =m(X)âm(X)=0. Thisfactcanbecombinedwiththelawofiteratedexpectationstoshowthattheunconditionalmean isalsozero. (cid:69)[e]=(cid:69)[(cid:69)[e|X]]=(cid:69)[0]=0. Westatethisandsomeotherresultsformally. Theorem2.4 PropertiesoftheCEFerror If(cid:69)|Y|<âthen 1. (cid:69)[e|X]=0. 2. (cid:69)[e]=0. 3. If(cid:69)|Y|r <âforr â¥1then(cid:69)|e|r <â. 4. Foranyfunctionh(x)suchthat(cid:69)|h(X)e|<âthen(cid:69)[h(X)e]=0. The proof of the third result is deferred to Section 2.33. The fourth result, whose proof is left to Exercise2.3,impliesthateisuncorrelatedwithanyfunctionoftheregressors. Theequations Y =m(X)+e (cid:69)[e|X]=0 togetherimplythatm(X)istheCEFofY givenX.Itisimportanttounderstandthatthisisnotarestric- tion.Theseequationsholdtruebydefinition. Thecondition(cid:69)[e|X]=0isimpliedbythedefinitionofe asthedifferencebetweenY andtheCEF m(X). The equation (cid:69)[e|X]=0 is sometimes called a conditional mean restriction, since the condi- tionalmeanoftheerrore isrestrictedtoequalzero. Thepropertyisalsosometimescalledmeaninde- pendence,fortheconditionalmeanofeis0andthusindependentofX.However,itdoesnotimplythat",
    "page": 44,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 25 thedistributionofeisindependentofX.SometimestheassumptionâeisindependentofXâisaddedas aconvenientsimplification,butitisnotgenericfeatureoftheconditionalmean.Typicallyandgenerally, eandX arejointlydependenteventhoughtheconditionalmeanofeiszero. Asanexample,thecontoursofthejointdensityofe andexperienceareplottedinFigure2.5forthe samepopulationasFigure2.4.Noticethattheshapeoftheconditionaldistributionvarieswiththelevel ofexperience. Labor Market Experience (Years) e 0 5 10 15 20 25 30 35 40 45 0.1 5.0 0.0 5.0â 0.1â Figure2.5:JointDensityofErroreandexperienceforWhiteMenwithHighSchoolEducation AsasimpleexampleofacasewhereX ande aremeanindependentyetdependentlete=Xuwhere X andu areindependentN(0,1).Thenconditionalon X theerrore hasthedistributionN(0,X2).Thus (cid:69)[e|X]=0 and e is mean independent of X, yet e is not fully independent of X. Mean independence doesnotimplyfullindependence. 2.9 Intercept-OnlyModel AspecialcaseoftheregressionmodeliswhentherearenoregressorsX.Inthiscasem(X)=(cid:69)[Y]=Âµ, theunconditionalmeanofY.WecanstillwriteanequationforY intheregressionformat: Y =Âµ+e (cid:69)[e]=0. Thisisusefulforitunifiesthenotation. 2.10 RegressionVariance AnimportantmeasureofthedispersionabouttheCEFfunctionistheunconditionalvarianceofthe CEFerrore.Wewritethisas Ï2=var[e]=(cid:69)(cid:163) (eâ(cid:69)[e])2(cid:164)=(cid:69)(cid:163) e2(cid:164) . Theorem2.4.3impliesthefollowingsimplebutusefulresult.",
    "page": 45,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 26 Theorem2.5 If(cid:69)(cid:163) Y2(cid:164)<âthenÏ2<â. We can call Ï2 the regression variance or the variance of the regression error. The magnitude of Ï2 measurestheamountofvariationinY whichisnotâexplainedâoraccountedforintheconditional expectation(cid:69)[Y |X]. TheregressionvariancedependsontheregressorsX.Considertworegressions Y =(cid:69)[Y |X ]+e 1 1 Y =(cid:69)[Y |X ,X ]+e . 1 2 2 Wewritethetwoerrorsdistinctlyase ande astheyaredifferentâchangingtheconditioninginforma- 1 2 tionchangestheconditionalexpectationandthereforetheregressionerroraswell. Inourdiscussionofiteratedexpectationswehaveseenthatbyincreasingtheconditioningsetthe conditionalexpectationrevealsgreaterdetailaboutthedistributionofY.Whatistheimplicationforthe regressionerror? Itturnsoutthatthereisasimplerelationship. Wecanthinkoftheconditionalexpectation(cid:69)[Y |X] astheâexplainedportionâofY.Theremaindere =Y â(cid:69)[Y |X]istheâunexplainedportionâ. Thesim- plerelationshipwenowderiveshowsthatthevarianceofthisunexplainedportiondecreaseswhenwe conditiononmorevariables. Thisrelationshipismonotonicinthesensethatincreasingtheamountof informationalwaysdecreasesthevarianceoftheunexplainedportion. Theorem2.6 If(cid:69)(cid:163) Y2(cid:164)<âthen var[Y]â¥var[Y â(cid:69)[Y |X ]]â¥var[Y â(cid:69)[Y |X ,X ]]. 1 1 2 Theorem 2.6 says that the variance of the difference between Y and its conditional expectation (weakly)decreaseswheneveranadditionalvariableisaddedtotheconditioninginformation. TheproofofTheorem2.6isgiveninSection2.33. 2.11 BestPredictor SupposethatgivenarandomvectorX wewanttopredictorforecastY.Wecanwriteanypredictoras afunctiong(X)ofX.The(ex-post)predictionerroristherealizeddifferenceY âg(X).Anon-stochastic measureofthemagnitudeofthepredictionerroristheexpectationofitssquare (cid:104) (cid:105) (cid:69) (cid:161) Y âg(X) (cid:162)2 . (2.10) Wecandefinethebestpredictorasthefunctiong(X)whichminimizes(2.10). Whatfunctionisthe bestpredictor?ItturnsoutthattheansweristheCEFm(X).Thisholdsregardlessofthejointdistribution of(Y,X).",
    "page": 46,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 27 Toseethis,notethatthemeansquarederrorofapredictorg(X)is (cid:104) (cid:105) (cid:104) (cid:105) (cid:69) (cid:161) Y âg(X) (cid:162)2 =(cid:69) (cid:161) e+m(X)âg(X) (cid:162)2 (cid:104) (cid:105) =(cid:69)(cid:163) e2(cid:164)+2(cid:69)(cid:163) e (cid:161) m(X)âg(X) (cid:162)(cid:164)+(cid:69) (cid:161) m(X)âg(X) (cid:162)2 (cid:104) (cid:105) =(cid:69)(cid:163) e2(cid:164)+(cid:69) (cid:161) m(X)âg(X) (cid:162)2 â¥(cid:69)(cid:163) e2(cid:164) =(cid:69)(cid:163) (Y âm(X))2(cid:164) wherethefirstequalitymakesthesubstitutionY =m(X)+e andthethirdequalityusesTheorem2.4.4. Theright-hand-sideafterthethirdequalityisminimizedbysettingg(X)=m(X),yieldingtheinequality inthefourthline.Theminimumisfiniteundertheassumption(cid:69)(cid:163) Y2(cid:164)<âasshownbyTheorem2.5. Westatethisformallyinthefollowingresult. Theorem2.7 ConditionalExpectationasBestPredictor If(cid:69)(cid:163) Y2(cid:164)<â,thenforanypredictorg(X), (cid:104) (cid:105) (cid:69) (cid:161) Y âg(X) (cid:162)2 â¥(cid:69)(cid:163) (Y âm(X))2(cid:164) wherem(X)=(cid:69)[Y |X]. Itmaybehelpfultoconsiderthisresultinthecontextoftheintercept-onlymodel Y =Âµ+e (cid:69)[e]=0. Theorem 2.7 shows that the best predictor for Y (in the class of constants) is the unconditional mean Âµ=(cid:69)[Y]inthesensethatthemeanminimizesthemeansquaredpredictionerror. 2.12 ConditionalVariance While the conditional mean is a good measure of the location of a conditional distribution it does notprovideinformationaboutthespreadofthedistribution. Acommonmeasureofthedispersionis the conditionalvariance. We first give the general definition of the conditional variance of a random variableW. Definition2.1 If(cid:69)(cid:163) W2(cid:164)<â,theconditionalvarianceofW givenX =xis Ï2(x)=var[W |X =x]=(cid:69)(cid:163) (W â(cid:69)[W |X =x])2|X =x (cid:164) . Theconditionalvariancetreatedasarandomvariableisvar[W |X]=Ï2(X).",
    "page": 47,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 28 Theconditionalvarianceisdistinctfromtheunconditionalvariancevar[W]. Thedifferenceisthat theconditionalvarianceisafunctionoftheconditioningvariables.Noticethattheconditionalvariance istheconditionalsecondmoment,centeredaroundtheconditionalfirstmoment. Giventhisdefinitionwedefinetheconditionalvarianceoftheregressionerror. Definition2.2 If(cid:69)(cid:163) e2(cid:164)<â,theconditionalvarianceoftheregressionerrore givenX =xis Ï2(x)=var[e|X =x]=(cid:69)(cid:163) e2|X =x (cid:164) . Theconditionalvarianceofetreatedasarandomvariableisvar[e|X]=Ï2(X). Again,theconditionalvarianceÏ2(x)isdistinctfromtheunconditionalvarianceÏ2.Theconditional varianceisafunctionoftheregressors,theunconditionalvarianceisnot.Generally,Ï2(x)isanon-trivial functionofx andcantakeanyformsubjecttotherestrictionthatitisnon-negative. Onewaytothink aboutÏ2(x)isthatitistheconditionalmeanofe2 givenX. NoticeaswellthatÏ2(x)=var[Y |X =x]so itisequivalentlytheconditionalvarianceofthedependentvariable. ThevarianceofY isinadifferentunitofmeasurementthanY. Toconvertthevariancetothesame (cid:112) unitofmeasurewedefinetheconditionalstandarddeviationasitssquarerootÏ(x)= Ï2(x). As an example of how the conditional variance depends on observables, compare the conditional logwagedensitiesformenandwomendisplayedinFigure2.2. Thedifferencebetweenthedensitiesis notpurelyalocationshiftbutisalsoadifferenceinspread. Specifically,wecanseethatthedensityfor menâslogwagesissomewhatmorespreadoutthanthatforwomen,whilethedensityforwomenâswages issomewhatmorepeaked. Indeed,theconditionalstandarddeviationformenâswagesis3.05andthat forwomenis2.81.Sowhilemenhavehigheraveragewagestheyarealsosomewhatmoredispersed. Theunconditionalvarianceisrelatedtotheconditionalvariancebythefollowingidentity. Theorem2.8 If(cid:69)(cid:163) X2(cid:164)<âthen var[X]=(cid:69)[var[X |W]]+var[(cid:69)[X |W]]. SeeTheorem4.14ofIntroductiontoEconometrics.Theorem2.8decomposestheunconditionalvari- anceintowhataresometimescalledtheâwithingroupvarianceâandtheâacrossgroupvarianceâ. For example,ifX iseducationlevel,thenthefirsttermistheexpectedvarianceoftheconditionalmeanby educationlevel.Thesecondtermisthevarianceaftercontrollingforeducation. Theregressionerrorhasaconditionalmeanofzero, soitsunconditionalerrorvarianceequalsthe expectedconditionalvariance,orequivalentlycanbefoundbythelawofiteratedexpectations Ï2=(cid:69)(cid:163) e2(cid:164)=(cid:69)(cid:163)(cid:69)(cid:163) e2|X (cid:164)(cid:164)=(cid:69)(cid:163)Ï2(X) (cid:164) . Thatis,theunconditionalerrorvarianceistheaverageconditionalvariance. Giventheconditionalvariancewecandefinearescalederror e u= . (2.11) Ï(X)",
    "page": 48,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 29 WecancalculatethatsinceÏ(X)isafunctionofX (cid:183) (cid:175) (cid:184) (cid:69)[u|X]=(cid:69) e (cid:175) (cid:175)X = 1 (cid:69)[e|X]=0 Ï(X)(cid:175) Ï(X) and var[u|X]=(cid:69)(cid:163) u2|X (cid:164)=(cid:69) (cid:183) e2 (cid:175) (cid:175) (cid:175)X (cid:184) = 1 (cid:69)(cid:163) e2|X (cid:164)= Ï2(X) =1. Ï2(X)(cid:175) Ï2(X) Ï2(X) Thusuhasaconditionalmeanofzeroandaconditionalvarianceof1. Noticethat(2.11)canberewrittenas e=Ï(X)u. andsubstitutingthisforeintheCEFequation(2.9),wefindthat Y =m(X)+Ï(X)u. Thisisanalternative(mean-variance)representationoftheCEFequation. Many econometric studies focus on the conditional mean m(x) and either ignore the conditional varianceÏ2(x),treatitasaconstantÏ2(x)=Ï2,ortreatitasanuisanceparameter(aparameternotof primaryinterest).Thisisappropriatewhentheprimaryvariationintheconditionaldistributionisinthe meanbutcanbeshort-sightedinothercases. Dispersionisrelevanttomanyeconomictopics,includ- ingincomeandwealthdistribution,economicinequality,andpricedispersion. Conditionaldispersion (variance)canbeafruitfulsubjectforinvestigation. Theperverseconsequencesofanarrow-mindedfocusonthemeanisparodiedinaclassicjoke: An economist was standing with one foot in a bucket of boiling water andtheotherfootinabucketofice.Whenaskedhowhefelt,hereplied, âOnaverageIfeeljustfine.â Clearly,theeconomistinquestionignoredvariance! 2.13 HomoskedasticityandHeteroskedasticity AnimportantspecialcaseobtainswhentheconditionalvarianceÏ2(x)isaconstantandindepen- dentofx.Thisiscalledhomoskedasticity. Definition2.3 The error is homoskedastic if Ï2(x) = Ï2 does not de- pendonx. InthegeneralcasewhereÏ2(x)dependsonxwesaythattheerroreisheteroskedastic. Definition2.4 TheerrorisheteroskedasticifÏ2(x)dependsonx.",
    "page": 49,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 30 It is helpful to understand that the concepts homoskedasticity and heteroskedasticity concern the conditionalvariance,nottheunconditionalvariance. Bydefinition,theunconditionalvarianceÏ2 isa constantandindependentoftheregressors X. Sowhenwetalkaboutthevarianceasafunctionofthe regressorswearetalkingabouttheconditionalvarianceÏ2(x). Someolderorintroductorytextbooksdescribeheteroskedasticityasthecasewhereâthevarianceofe variesacrossobservationsâ.Thisisapoorandconfusingdefinition.Itismoreconstructivetounderstand thatheteroskedasticitymeansthattheconditionalvarianceÏ2(x)dependsonobservables. Oldertextbooksalsotendtodescribehomoskedasticityasacomponentofacorrectregressionspec- ificationanddescribeheteroskedasticityasanexceptionordeviance. Thisdescriptionhasinfluenced manygenerationsofeconomistsbutitisunfortunatelybackwards.Thecorrectviewisthatheteroskedas- ticityisgenericandâstandardâ,whilehomoskedasticityisunusualandexceptional. Thedefaultinem- piricalworkshouldbetoassumethattheerrorsareheteroskedastic,nottheconverse. Inapparentcontradictiontotheabovestatementwewillstillfrequentlyimposethehomoskedastic- ityassumptionwhenmakingtheoreticalinvestigationsintothepropertiesofestimationandinference methods. Thereasonisthatinmanycaseshomoskedasticitygreatlysimplifiesthetheoreticalcalcula- tionsanditisthereforequiteadvantageousforteachingandlearning.Itshouldalwaysberemembered, however,thathomoskedasticityisneverimposedbecauseitisbelievedtobeacorrectfeatureofanem- piricalmodelbutratherbecauseofitssimplicity. HeteroskedasticorHeteroscedastic? Thespellingofthewordshomoskedasticandheteroskedastichavebeen somewhatcontroversial. Earlyeconometricstextbooksweresplit,with someusingaâcâasinheteroscedasticandsomeâkâasinheteroskedastic. McCulloch(1985)pointedoutthatthewordisderivedfromGreekroots. oÂµoÎ¹oÏmeansâsameâ. ÎµÏÎµÏomeansâotherâorâdifferentâ. ÏÎºÎµÎ´Î±Î½Î½ÏÂµÎ¹ meansâtoscatterâ. SincethepropertransliterationoftheGreekletterÎº inÏÎºÎµÎ´Î±Î½Î½ÏÂµÎ¹isâkâ,thisimpliesthatthecorrectEnglishspellingofthe twowordsiswithaâkâasinhomoskedasticandheteroskedastic. 2.14 RegressionDerivative OnewaytointerprettheCEFm(x)=(cid:69)[Y |X =x]isintermsofhowmarginalchangesintheregres- sorsximplychangesintheconditionalmeanoftheresponsevariableY.Itistypicaltoconsidermarginal changesinasingleregressor,say X ,holdingtheremainderfixed. Whenaregressor X iscontinuously 1 1 distributed,wedefinethemarginaleffectofachangeinX ,holdingthevariablesX ,...,X fixed,asthe 1 2 k partialderivativeoftheCEF â m(x ,...,x ). âx 1 k 1 When X isdiscretewedefinethemarginaleffectasadiscretedifference. Forexample, if x isbinary, 1 1 thenthemarginaleffectofX ontheCEFis 1 m(1,x ,...,x )âm(0,x ,...,x ). 2 k 2 k",
    "page": 50,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 31 Wecanunifythecontinuousanddiscretecaseswiththenotation ï£± â ï£´ ï£´ ï£² âx m(x 1 ,...,x k ), ifX 1 iscontinuous â m(x)= 1 1 ï£´ ï£´ ï£³ m(1,x ,...,x )âm(0,x ,...,x ), ifX isbinary. 2 k 2 k 1 Collectingthek effectsintoonekÃ1vector,wedefinetheregressionderivativewithrespecttoX : ï£® ï£¹ â m(x) 1 ï£¯ â m(x) ï£º âm(x)=ï£¯ ï£¯ 2 . ï£º ï£º. ï£¯ . . ï£º ï£° ï£» â m(x) k â WhenallelementsofX arecontinuous,thenwehavethesimplificationâm(x)= m(x),thevectorof âx partialderivatives. Therearetwoimportantpointstorememberconcerningourdefinitionoftheregressionderivative. First,theeffectofeachvariableiscalculatedholdingtheothervariablesconstant.Thisistheceteris paribus concept commonly used in economics. But in the case of a regression derivative, the condi- tionalmeandoesnotliterallyholdallelseconstant. Itonlyholdsconstantthevariablesincludedinthe conditionalmean. Thismeansthattheregressionderivativedependsonwhichregressorsareincluded. Forexample,inaregressionofwagesoneducation,experience,raceandgender,theregressionderiva- tivewithrespecttoeducationshowsthemarginaleffectofeducationonmeanwages,holdingconstant experience,raceandgender. Butitdoesnotholdconstantanindividualâsunobservablecharacteristics (suchasability),norvariablesnotincludedintheregression(suchasthequalityofeducation). Second,theregressionderivativeisthechangeintheconditionalexpectationofY,notthechangein theactualvalueofY foranindividual. Itistemptingtothinkoftheregressionderivativeasthechange intheactualvalueofY,butthisisnotacorrectinterpretation. Theregressionderivativeâm(x)isthe change in the actual value of Y only if the error e is unaffected by the change in the regressor X. We returntoadiscussionofcausaleffectsinSection2.30. 2.15 LinearCEF AnimportantspecialcaseiswhentheCEFm(x)=(cid:69)[Y |X =x]islinearinx.Inthiscasewecanwrite themeanequationas m(x)=x 1 Î² 1 +x 2 Î² 2 +Â·Â·Â·+x k Î² k +Î² k+1 . Notationallyitisconvenienttowritethisasasimplefunctionofthevectorx. Aneasywaytodosoisto augmenttheregressorvectorX bylistingthenumberâ1âasanelement. Wecallthistheâconstantâand thecorrespondingcoefficientiscalledtheâinterceptâ.Equivalently,specifythatthefinalelement9ofthe vectorxisx =1.Thus(2.4)hasbeenredefinedasthekÃ1vector k ï£« ï£¶ X 1 ï£¬ X ï£· ï£¬ 2 ï£· X =ï£¬ ï£¬ . . . ï£· ï£·. (2.12) ï£¬ ï£· ï£¬ ï£· ï£­ X kâ1 ï£¸ 1 9Theorderdoesnâtmatter.Itcouldbeanyelement.",
    "page": 51,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 32 Withthisredefinition,theCEFis m(x)=x Î² +x Î² +Â·Â·Â·+Î² =x (cid:48)Î² (2.13) 1 1 2 2 k where ï£« Î² ï£¶ 1 Î²=ï£¬ . . ï£· ï£­ . ï£¸ Î² k is a kÃ1 coefficient vector. This is the linearCEFmodel. It is also often called the linearregression model,ortheregressionofY onX. InthelinearCEFmodeltheregressionderivativeissimplythecoefficientvector. Thatisâm(x)=Î². ThisisoneoftheappealingfeaturesofthelinearCEFmodel. Thecoefficientshavesimpleandnatural interpretationsasthemarginaleffectsofchangingonevariable,holdingtheothersconstant. LinearCEFModel Y =X (cid:48)Î²+e (cid:69)[e|X]=0 IfinadditiontheerrorishomoskedasticwecallthisthehomoskedasticlinearCEFmodel. HomoskedasticLinearCEFModel Y =X (cid:48)Î²+e (cid:69)[e|X]=0 (cid:69)(cid:163) e2|X (cid:164)=Ï2 2.16 LinearCEFwithNonlinearEffects ThelinearCEFmodeloftheprevioussectionislessrestrictivethanitmightappear,aswecaninclude asregressorsnonlineartransformationsoftheoriginalvariables.Inthissense,thelinearCEFframework isflexibleandcancapturemanynonlineareffects. Forexample,supposewehavetwoscalarvariablesX andX .TheCEFcouldtakethequadraticform 1 2 m(x ,x )=x Î² +x Î² +x2Î² +x2Î² +x x Î² +Î² . (2.14) 1 2 1 1 2 2 1 3 2 4 1 2 5 6 Thisequationisquadraticintheregressors(x ,x )yetlinearinthecoefficientsÎ²=(Î² ,...,Î² ) (cid:48) .Westill 1 2 1 6 call(2.14)alinearCEFbecauseitisalinearfunctionofthecoefficients.Atthesametime,ithasnonlinear effectsbecauseitisnonlinearintheunderlyingvariablesx andx . Thekeyistounderstandthat(2.14) 1 2 isquadraticinthevariables(x ,x )yetlinearinthecoefficientsÎ². 1 2",
    "page": 52,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 33 Tosimplifytheexpressionwedefinethetransformationsx =x2,x =x2,x =x x ,andx =1,and 3 1 4 2 5 1 2 6 redefinetheregressorvectorasx=(x ,...,x ) (cid:48) .Withthisredefinition,m(x ,x )=x (cid:48)Î²whichislinearinÎ². 1 6 1 2 Formosteconometricpurposes(estimationandinferenceonÎ²)thelinearityinÎ²isallthatisimportant. Anexceptionisintheanalysisofregressionderivatives.Innonlinearequationssuchas(2.14)there- gressionderivativeshouldbedefinedwithrespecttotheoriginalvariablesnotwithrespecttothetrans- formedvariables.Thus â m(x ,x )=Î² +2x Î² +x Î² âx 1 2 1 1 3 2 5 1 â m(x ,x )=Î² +2x Î² +x Î² . âx 1 2 2 2 4 1 5 2 Weseethatinthemodel(2.14),theregressionderivativesarenotasimplecoefficient,butarefunctions of several coefficients plus the levels of (x x ). Consequently it is difficult to interpret the coefficients 1, 2 individually.Itismoreusefultointerpretthemasagroup. WetypicallycallÎ² theinteractioneffect. Noticethatitappearsinbothregressionderivativeequa- 5 tionsandhasasymmetricinterpretationineach. IfÎ² >0thentheregressionderivativewithrespectto 5 x isincreasinginthelevelofx (andtheregressionderivativewithrespecttox isincreasinginthelevel 1 2 2 ofx ),whileifÎ² <0thereverseistrue. 1 5 2.17 LinearCEFwithDummyVariables WhenallregressorstakeafinitesetofvaluesitturnsouttheCEFcanbewrittenasalinearfunction ofregressors. This simplest example is a binary variable which takes only two distinct values. For example, in traditional data sets the variable gender takes only the values man and woman (or male and female). Binaryvariablesareextremelycommonineconometricapplicationsandarealternativelycalleddummy variablesorindicatorvariables. Considerthesimplecaseofasinglebinaryregressor.Inthiscasetheconditionalmeancanonlytake twodistinctvalues.Forexample, ï£± Âµ if gender=man ï£² 0 (cid:69)(cid:163) Y |gender (cid:164)= ï£³ Âµ if gender=woman. 1 Tofacilitateamathematicaltreatmentwerecorddummyvariableswiththevalues{0,1}.Forexample (cid:189) 0 if gender=man X = (2.15) 1 1 if gender=woman. Given this notation we write the conditional mean as a linear function of the dummy variable X . 1 Thus(cid:69)[Y |X ]=Î² X +Î² whereÎ² =Âµ âÂµ andÎ² =Âµ . Inthissimpleregressionequationthein- 1 1 1 2 1 1 0 2 0 terceptÎ² isequaltotheconditionalmeanofY fortheX =0subpopulation(men)andtheslopeÎ² is 2 1 1 equaltothedifferenceintheconditionalmeansbetweenthetwosubpopulations. Alternatively,wecouldhavedefinedX as 1 (cid:189) 1 if gender=man X = (2.16) 1 0 if gender=woman. In this case, the regression intercept is the mean for women (rather than for men) and the regression slope has switched signs. The two regressions are equivalent but the interpretation of the coefficients",
    "page": 53,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 34 haschanged.Thereforeitisalwaysimportanttounderstandtheprecisedefinitionsofthevariables,and illuminatinglabelsarehelpful. Forexample,labellingX asâgenderâdoesnothelpdistinguishbetween 1 definitions(2.15)and(2.16). Instead,itisbettertolabelX asâwomenâorâfemaleâifdefinition(2.15)is 1 used,orasâmenâorâmaleâif(2.16)isused. NowsupposewehavetwodummyvariablesX andX .Forexample,X =1ifthepersonismarried, 1 2 2 elseX =0.TheconditionalmeangivenX andX takesatmostfourpossiblevalues: 2 1 2 ï£± Âµ if X =0andX =0 (unmarriedmen) ï£´ ï£´ 00 1 2 ï£´ ï£² Âµ if X =0andX =1 (marriedmen) (cid:69)[Y |X ,X ]= 01 1 2 1 2 Âµ if X =1andX =0 (unmarriedwomen) ï£´ ï£´ 10 1 2 ï£´ ï£³ Âµ if X =1andX =1 (marriedwomen). 11 1 2 InthiscasewecanwritetheconditionalmeanasalinearfunctionofX,X andtheirproductX X : 2 1 2 (cid:69)[Y |X ,X ]=Î² X +Î² X +Î² X X +Î² 1 2 1 1 2 2 3 1 2 4 whereÎ² =Âµ âÂµ ,Î² =Âµ âÂµ ,Î² =Âµ âÂµ âÂµ +Âµ ,andÎ² =Âµ . 1 10 00 2 01 00 3 11 10 01 00 4 00 We can view the coefficient Î² as the effect of gender on expected log wages for unmarried wage 1 earners,thecoefficientÎ² astheeffectofmarriageonexpectedlogwagesformenwageearners,andthe 2 coefficient Î² as the difference between the effects of marriage on expected log wages among women 3 andamongmen. Alternatively,itcanalsobeinterpretedasthedifferencebetweentheeffectsofgender onexpectedlogwagesamongmarriedandnon-marriedwageearners. Bothinterpretationsareequally valid. We often describe Î² as measuring the interaction between the two dummy variables, or the 3 interactioneffect,anddescribeÎ² =0asthecasewhentheinteractioneffectiszero. 3 InthissettingwecanseethattheCEFislinearinthethreevariables(X ,X ,X X ).Toputthemodel 1 2 1 2 intheframeworkofSection2.15wedefinetheregressorX =X X andtheregressorvectoras 3 1 2 ï£« ï£¶ X 1 ï£¬ X ï£· X =ï£¬ 2 ï£·. ï£¬ X ï£· ï£­ 3 ï£¸ 1 Soeventhoughwestartedwithonly2dummyvariables,thenumberofregressors(includingtheinter- cept)is4. Ifthereare3dummyvariables X ,X ,X ,then(cid:69)[Y |X ,X ,X ]takesatmost23 =8distinctvalues 1 2 3 1 2 3 andcanbewrittenasthelinearfunction (cid:69)[Y |X ,X ,X ]=Î² X +Î² X +Î² X +Î² X X +Î² X X +Î² X X +Î² X X +Î² 1 2 3 1 1 2 2 3 3 4 1 2 5 1 3 6 2 3 7X1 2 3 8 whichhaseightregressorsincludingtheintercept. Ingeneral,iftherearepdummyvariablesX ,...,X thentheCEF(cid:69)(cid:163) Y |X ,X ,...,X (cid:164) takesatmost2p 1 p 1 2 p distinctvaluesandcanbewrittenasalinearfunctionofthe2p regressorsincludingX ,X ,...,X andall 1 2 p cross-products.Alinearregressionmodelwhichincludesall2p binaryinteractionsiscalledasaturated dummyvariableregressionmodel.Itisacompletemodeloftheconditionalmean.Incontrast,amodel withnointeractionsequals (cid:69)(cid:163) Y |X ,X ,...,X (cid:164)=Î² X +Î² X +Â·Â·Â·+Î² X +Î² . 1 2 p 1 1 2 2 p p p Thishasp+1coefficientsinsteadof2p. We started this section by saying that the conditional mean is linear whenever all regressors take onlyafinitenumberofpossiblevalues. Howcanweseethis? Takeacategoricalvariable,suchasrace.",
    "page": 54,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 35 For example, we earlier divided race into three categories. We can record categorical variables using numberstoindicateeachcategory,forexample ï£± 1 if white ï£² X = 2 if Black 3 ï£³ 3 if other. Whendoingso,thevaluesofX havenomeaningintermsofmagnitude,theysimplyindicatetherele- 3 vantcategory. WhentheregressoriscategoricaltheconditionalmeanofY givenX takesadistinctvalueforeach 3 possibility: ï£± Âµ if X =1 ï£² 1 3 (cid:69)[Y |X ]= Âµ if X =2 3 2 3 ï£³ Âµ if X =3. 3 3 This is not a linear function of X itself, but it can be made a linear function by constructing dummy 3 variablesfortwoofthethreecategories.Forexample (cid:189) 1 if Black X = 4 0 if notBlack (cid:189) 1 if other X = 5 0 if notother. Inthiscase,thecategoricalvariableX isequivalenttothepairofdummyvariables(X ,X ).Theexplicit 3 4 5 relationshipis ï£± 1 if X =0andX =0 ï£² 4 5 X = 2 if X =1andX =0 3 4 5 ï£³ 3 if X =0andX =1. 4 5 Giventhesetransformations,wecanwritetheconditionalmeanofY asalinearfunctionofX andX 4 5 (cid:69)[Y |X ]=(cid:69)[Y |X ,X ]=Î² X +Î² X +Î² . 3 4 5 1 4 2 5 3 We can write the CEF as either (cid:69)[Y |X ] or (cid:69)[Y |X ,X ] (they are equivalent), but it is only linear as a 3 4 5 functionofX andX . 4 5 Thissettingissimilartothecaseoftwodummyvariables, withthedifferencethatwehavenotin- cludedtheinteractiontermX X .Thisisbecausetheevent{X =1andX =1}isemptybyconstruction, 4 5 4 5 soX X =0bydefinition. 4 5 2.18 BestLinearPredictor While the conditional mean m(X) =(cid:69)[Y |X] is the best predictor of Y among all functions of X, its functional form is typically unknown. In particular, the linear CEF model is empirically unlikely to beaccurateunlessX isdiscreteandlow-dimensionalsoallinteractionsareincluded. Consequently,in mostcasesitismorerealistictoviewthelinearspecification(2.13)asanapproximation. Inthissection wederiveaspecificapproximationwithasimpleinterpretation. Theorem2.7showedthattheconditionalmeanm(X)isthebestpredictorinthesensethatithasthe lowestmeansquarederroramongallpredictors. Byextension,wecandefineanapproximationtothe CEFbythelinearfunctionwiththelowestmeansquarederroramongalllinearpredictors. Forthisderivationwerequirethefollowingregularitycondition.",
    "page": 55,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 36 Assumption2.1 1. (cid:69)(cid:163) Y2(cid:164)<â. 2. (cid:69)(cid:107)X(cid:107)2<â. 3. Q =(cid:69)(cid:163) XX (cid:48)(cid:164) ispositivedefinite. XX InAssumption2.1.2weuse(cid:107)x(cid:107)=(cid:161) x (cid:48) x (cid:162)1/2 todenotetheEuclideanlengthofthevectorx. ThefirsttwopartsofAssumption2.1implythatthevariablesY andX havefinitemeans,variances, andcovariances. Thethirdpartoftheassumptionismoretechnical,anditsrolewillbecomeapparent shortly. ItisequivalenttoimposingthatthecolumnsofthematrixQ =(cid:69)(cid:163) XX (cid:48)(cid:164) arelinearlyindepen- XX dent,orthatthematrixisinvertible. AlinearpredictorforY isafunctionX (cid:48)Î²forsomeÎ²â(cid:82)k.Themeansquaredpredictionerroris (cid:104) (cid:105) S(Î²)=(cid:69) (cid:161) Y âX (cid:48)Î²(cid:162)2 . (2.17) ThebestlinearpredictorofY given X,writtenP [Y |X],isfoundbyselectingtheÎ²whichminimizes S(Î²). Definition2.5 TheBestLinearPredictorofY givenX is P [Y |X]=X (cid:48)Î² whereÎ²minimizesthemeansquaredpredictionerror (cid:104) (cid:105) S(Î²)=(cid:69) (cid:161) Y âX (cid:48)Î²(cid:162)2 . Theminimizer Î²=argminS(b) (2.18) bâ(cid:82)k iscalledtheLinearProjectionCoefficient. Wenowcalculateanexplicitexpressionforitsvalue. Themeansquaredpredictionerror(2.17)can bewrittenoutasaquadraticfunctionofÎ²: S(Î²)=(cid:69)(cid:163) Y2(cid:164)â2Î²(cid:48)(cid:69)[XY]+Î²(cid:48)(cid:69)(cid:163) XX (cid:48)(cid:164)Î². (2.19) The quadratic structure of S(Î²) means that we can solve explicitly for the minimizer. The first-order conditionforminimization(fromAppendixA.20)is â 0= S(Î²)=â2(cid:69)[XY]+2(cid:69)(cid:163) XX (cid:48)(cid:164)Î². (2.20) âÎ² Rewriting(2.20)as 2(cid:69)[XY]=2(cid:69)(cid:163) XX (cid:48)(cid:164)Î²",
    "page": 56,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 37 anddividingby2,thisequationtakestheform Q =Q Î² (2.21) XY XX whereQ =(cid:69)[XY]iskÃ1andQ =(cid:69)(cid:163) XX (cid:48)(cid:164) iskÃk. Thesolutionisfoundbyinvertingthematrix XY XX Q ,andiswritten XX Î²=Q â1 Q XX XY or Î²=(cid:161)(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:162)â1(cid:69)[XY]. (2.22) Itisworthtakingthetimetounderstandthenotationinvolvedintheexpression(2.22).Q isakÃkma- XX trixandQ isakÃ1columnvector.Therefore,alternativeexpressionssuchas (cid:69)[XY] or(cid:69)[XY] (cid:161)(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:162)â1 XY (cid:69)[XX(cid:48)] areincoherentandincorrect. WealsocannowseetheroleofAssumption2.1.3. Itisequivalenttoas- sumingthatQ hasaninverseQ â1 whichisnecessaryforthesolutiontothenormalequations(2.21) XX XX tobeuniqueorequivalentlyfor(2.22)tobeuniquelydefined. IntheabsenceofAssumption2.1.3there couldbemultiplesolutionstotheequation(2.21). Wenowhaveanexplicitexpressionforthebestlinearpredictor: P [Y |X]=X (cid:48)(cid:161)(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:162)â1(cid:69)[XY]. ThisexpressionisalsoreferredtoasthelinearprojectionofY onX. Theprojectionerroris e=Y âX (cid:48)Î². (2.23) Thisequalstheerror(2.9)fromtheregressionequationwhen(andonlywhen)theconditionalmeanis linearinX,otherwisetheyaredistinct. Rewriting,weobtainadecompositionofY intolinearpredictoranderror Y =X (cid:48)Î²+e. (2.24) Ingeneral,wecallequation(2.24)orX (cid:48)Î²thebestlinearpredictorofY givenX orthelinearprojectionof Y onX.Equation(2.24)isalsooftencalledtheregressionofY onX butthiscansometimesbeconfusing aseconomistsusethetermâregressionâinmanycontexts. (RecallthatwesaidinSection2.15thatthe linearCEFmodelisalsocalledthelinearregressionmodel.) Animportantpropertyoftheprojectionerroreis (cid:69)[Xe]=0. (2.25) Toseethis,usingthedefinitions(2.23)and(2.22)andthematrixproperties AA â1=I andIa=a, (cid:69)[Xe]=(cid:69)(cid:163) X (cid:161) Y âX (cid:48)Î²(cid:162)(cid:164) =(cid:69)[XY]â(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:161)(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:162)â1(cid:69)[XY] =0 (2.26) asclaimed. Equation(2.25)isasetofk equations,oneforeachregressor.Inotherwords,(2.25)isequivalentto (cid:69)(cid:163) X e (cid:164)=0 (2.27) j",
    "page": 57,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 38 for j =1,...,k.Asin(2.12),theregressorvector X typicallycontainsaconstant,e.g. X =1. Inthiscase k (2.27)for j =k isthesameas (cid:69)[e]=0. (2.28) Thustheprojectionerrorhasameanofzerowhentheregressorvectorcontainsaconstant. (When X doesnothaveaconstant(2.28)isnotguaranteed. Asitisdesirablefore tohaveazeromeanthisisa goodreasontoalwaysincludeaconstantinanyregressionmodel.) It is also useful to observe that since cov(X ,e) = (cid:69)(cid:163) X e (cid:164)â(cid:69)(cid:163) X (cid:164)(cid:69)[e], then (2.27)-(2.28) together j j j implythatthevariablesX andeareuncorrelated. j Thiscompletesthederivationofthemodel.Wesummarizesomeofthemostimportantproperties. Theorem2.9 PropertiesofLinearProjectionModel UnderAssumption2.1, 1. Themoments(cid:69)(cid:163) XX (cid:48)(cid:164) and(cid:69)[XY]existwithfiniteelements. 2. Thelinearprojectioncoefficient(2.18)exists,isunique,andequals Î²=(cid:161)(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:162)â1(cid:69)[XY]. 3. Thebestlinearpredictorofy givenxis P(Y |X)=X (cid:48)(cid:161)(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:162)â1(cid:69)[XY]. 4. Theprojectionerrore=Y âX (cid:48)Î²exists.Itsatisfies(cid:69)(cid:163) e2(cid:164)<âand(cid:69)[Xe]=0. 5. IfX containsanconstant,then(cid:69)[e]=0. 6. If(cid:69)|Y|r <âand(cid:69)(cid:107)X(cid:107)r <âforr â¥2then(cid:69)|e|r <â. AcompleteproofofTheorem2.9isgiveninSection2.33. ItisusefultoreflectonthegeneralityofTheorem2.9. TheonlyrestrictionisAssumption2.1. Thus for any random variables (Y,X) with finite variances we can define a linear equation (2.24) with the propertieslistedinTheorem2.9.Strongerassumptions(suchasthelinearCEFmodel)arenotnecessary. Inthissensethelinearmodel(2.24)existsquitegenerally. However,itisimportantnottomisinterpret thegeneralityofthisstatement.Thelinearequation(2.24)isdefinedasthebestlinearpredictor.Itisnot necessarilyaconditionalmean,noraparameterofastructuralorcausaleconomicmodel. LinearProjectionModel Y =X (cid:48)Î²+e (cid:69)[Xe]=0 Î²=(cid:161)(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:162)â1(cid:69)[XY]",
    "page": 58,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 39 InvertibilityandIdentification The linear projection coefficient Î² = (cid:161)(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:162)â1(cid:69)[XY] exists and is uniqueaslongasthekÃkmatrixQ =(cid:69)(cid:163) XX (cid:48)(cid:164) isinvertible.ThematrixQ XX XX isoftencalledthedesignmatrixasinexperimentalsettingstheresearcheris abletocontrolQ bymanipulatingthedistributionoftheregressorsX. XX Observethatforanynon-zeroÎ±â(cid:82)k, (cid:104) (cid:105) Î±(cid:48) Q Î±=(cid:69)(cid:163)Î±(cid:48) XX (cid:48)Î±(cid:164)=(cid:69) (cid:161)Î±(cid:48) X (cid:162)2 â¥0 XX so Q by construction is positive semi-definite, conventionally written as XX Q â¥0. Theassumptionthatitispositivedefinitemeansthatthisisastrict XX (cid:104) (cid:105) inequality, (cid:69) (cid:161)Î±(cid:48) X (cid:162)2 > 0. This is conventionally written as Q > 0. This XX condition means that there is no non-zero vector Î± such that Î±(cid:48) X =0 iden- tically. Positive definite matrices are invertible. Thus when Q > 0 then XX Î² = (cid:161)(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:162)â1(cid:69)[XY] exists and is uniquely defined. In other words, if we canexcludethepossibilitythatalinearfunctionof X isdegenerate,thenÎ²is uniquelydefined. Theorem 2.5 shows that the linear projection coefficient Î² is identified (uniquelydetermined)underAssumption2.1. ThekeyisinvertibilityofQ . XX Otherwise,thereisnouniquesolutiontotheequation Q Î²=Q . (2.29) XX XY WhenQ isnotinvertibletherearemultiplesolutionsto(2.29). Inthiscase XX thecoefficientÎ²isnotidentifiedasitdoesnothaveauniquevalue.",
    "page": 59,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 40 Minimization The mean squared prediction error (2.19) is a function with vector argu- mentoftheform f(x)=aâ2b (cid:48) x+x (cid:48) Cx whereC >0.Foranyfunctionofthisform,theuniqueminimizeris x=C â1b. (2.30) Toseethatthisistheuniqueminimizerwepresenttwoproofs. Thefirstuses matrixcalculus.FromAppendixA.20 â (cid:161) b (cid:48) x (cid:162)=b (2.31) âx â (cid:161) x (cid:48) Cx (cid:162)=2Cx (2.32) âx â2 (cid:161) x (cid:48) Cx (cid:162)=2C. (2.33) âxâx (cid:48) Using(2.31)and(2.32),wefind â f(x)=â2b+2Cx. âx The first-order condition for minimization sets this derivative equal to zero. Thusthesolutionsatisfiesâ2b+2Cx =0. Solvingfor x wefind(2.30). Using (2.33)wealsofind â2 f(x)=2C >0 âxâx (cid:48) whichisthesecond-orderconditionforminimization. Thisshowsthat(2.30) istheuniqueminimizerof f(x). Oursecondproofisalgebraic.Re-write f(x)as f(x)=(cid:161) aâb (cid:48) C â1b (cid:162)+(cid:161) xâC â1b (cid:162)(cid:48) C (cid:161) xâC â1b (cid:162) . The first term does not depend on x so does not affect the minimizer. The secondtermisaquadraticforminapositivedefinitematrix. Thismeansthat for any non-zero Î±, Î±(cid:48) CÎ±>0. Thus for x (cid:54)=C â1b, the second-term is strictly positive, yet for x =C â1b this term equals zero. It is therefore minimized at x=C â1basclaimed. 2.19 IllustrationsofBestLinearPredictor Weillustratethebestlinearpredictor(projection)usingthreelogwageequationsintroducedinear- liersections. Forourfirstexample,weconsideramodelwiththetwodummyvariablesforgenderandracesimilar toTable2.1. AswelearnedinSection2.17, theentriesinthistablecanbeequivalentlyexpressedbya",
    "page": 60,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 41 linearCEF.Forsimplicity,letâsconsidertheCEFoflog(wage)asafunctionofBlackandfemale. (cid:69)(cid:163) log(wage)|Black,female (cid:164)=â0.20Blackâ0.24female+0.10BlackÃfemale+3.06. (2.34) ThisisaCEFasthevariablesarebinaryandallinteractionsareincluded. Now consider a simpler model omitting the interaction effect. This is the linear projection on the variablesBlackandfemale P (cid:163) log(wage)|Black,female (cid:164)=â0.15Blackâ0.23female+3.06. (2.35) Whatisthedifference? ThefullCEF(2.34)showsthattheracegapisdifferentiatedbygender: itis20% for Black men (relative to non-Black men) and 10% for Black women (relative to non-Black women). Theprojectionmodel(2.35)simplifiesthisanalysis,calculatinganaverage15%wagegapforBlackwage earners,ignoringtheroleofgender.Noticethatthisisdespitethefactthatgenderisincludedin(2.35). Years of Education ruoH rep sralloD goL 4 6 8 10 12 14 16 18 20 0.4 5.3 0.3 5.2 0.2 Linear Projection Linear Spline Labor Market Experience (Years) (a)Projectionsontoeducation ruoH rep sralloD goL 0 5 10 15 20 25 30 35 40 45 50 5.3 0.3 5.2 0.2 Conditional Mean Linear Projection Quadratic Projection (b)Projectionsontoexperience Figure2.6:Projectionsoflog(wage)ontoeducationandexperience For our second example we consider the CEF of log wages as a function of years of education for white men which was illustrated in Figure 2.3 and is repeated in Figure 2.6(a). Superimposed on the figure are two projections. The first (given by the dashed line) is the linear projection of log wages on yearsofeducation P (cid:163) log(wage)|education (cid:164)=0.11education+1.5. This simple equation indicates an average 11% increase in wages for every year of education. An in- spection of the Figure shows that this approximation works well for educationâ¥9, but under-predicts forindividualswithlowerlevelsofeducation. Tocorrectthisimbalanceweusealinearsplineequation whichallowsdifferentratesofreturnaboveandbelow9yearsofeducation: P (cid:163) log(wage)|education,(educationâ9)Ã1 {education>9} (cid:164) =0.02education+0.10Ã(educationâ9)Ã1 {education>9}+2.3. ThisequationisdisplayedinFigure2.6(a)usingthesolidline,andappearstofitmuchbetter.Itindicates a2%increaseinmeanwagesforeveryyearofeducationbelow9,anda12%increaseinmeanwagesfor",
    "page": 61,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 42 everyyearofeducationabove9.Itisstillanapproximationtotheconditionalmeanbutitappearstobe fairlyreasonable. ForourthirdexamplewetaketheCEFoflogwagesasafunctionofyearsofexperienceforwhitemen with12yearsofeducation,whichwasillustratedinFigure2.4andisrepeatedasthesolidlineinFigure 2.6(b). Superimposedonthefigurearetwoprojections. Thefirst(givenbythedot-dashedline)isthe linearprojectiononexperience P (cid:163) log(wage)|experience (cid:164)=0.011experience+2.5 andthesecond(givenbythedashedline)isthelinearprojectiononexperienceanditssquare P (cid:163) log(wage)|experience (cid:164)=0.046experienceâ0.0007experience2+2.3. ItisfairlyclearfromanexaminationofFigure2.6(b)thatthefirstlinearprojectionisapoorapproxima- tion.Itover-predictswagesforyoungandoldworkers,andunder-predictsfortherest.Mostimportantly, itmissesthestrongdownturninexpectedwagesforolderwage-earners.Thesecondprojectionfitsmuch better.Wecancallthisequationaquadraticprojectionsincethefunctionisquadraticinexperience. 2.20 LinearPredictorErrorVariance AsintheCEFmodel, wedefinetheerrorvarianceasÏ2 =(cid:69)(cid:163) e2(cid:164) . SettingQ =(cid:69)(cid:163) Y2(cid:164) andQ = YY YX (cid:69)(cid:163) YX (cid:48)(cid:164) wecanwriteÏ2as (cid:104) (cid:105) Ï2=(cid:69) (cid:161) Y âX (cid:48)Î²(cid:162)2 =(cid:69)(cid:163) Y2(cid:164)â2(cid:69)(cid:163) YX (cid:48)(cid:164)Î²+Î²(cid:48)(cid:69)(cid:163) XX (cid:48)(cid:164)Î² =Q â2Q Q â1 Q +Q Q â1 Q Q â1 Q YY YX XX XY YX XX XX XX XY =Q âQ Q â1 Q YY YX XX XY d=ef Q YYÂ·X . (2.36) OneusefulfeatureofthisformulaisthatitshowsthatQ YYÂ·X =Q YY âQ YX Q â X 1 X Q XY equalsthevariance oftheerrorfromthelinearprojectionofY onX. 2.21 RegressionCoefficients Sometimesitisusefultoseparatetheconstantfromtheotherregressorsandwritethelinearprojec- tionequationintheformat Y =X (cid:48)Î²+Î±+e (2.37) whereÎ±istheinterceptandX doesnotcontainaconstant. Takingexpectationsofthisequation,wefind (cid:69)[Y]=(cid:69)(cid:163) X (cid:48)Î²(cid:164)+(cid:69)[Î±]+(cid:69)[e] orÂµ =Âµ(cid:48) Î²+Î±whereÂµ =(cid:69)[Y]andÂµ =(cid:69)[X],since(cid:69)[e]=0from(2.28). (WhileX doesnotcontain Y X Y X aconstant,theequationdoesso(2.28)stillapplies.)Rearranging,wefindÎ±=Âµ âÂµ(cid:48) Î².Subtractingthis Y X equationfrom(2.37)wefind Y âÂµ =(cid:161) XâÂµ (cid:162)(cid:48) Î²+e, (2.38) Y X",
    "page": 62,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 43 alinearequationbetweenthecenteredvariablesYâÂµ andXâÂµ .(Theyarecenteredattheirmeansso Y X aremean-zerorandomvariables.)BecauseXâÂµ isuncorrelatedwithe,(2.38)isalsoalinearprojection. X Thusbytheformulaforthelinearprojectionmodel, Î²= (cid:179) (cid:69) (cid:104) (cid:161) XâÂµ (cid:162)(cid:161) XâÂµ (cid:162)(cid:48)(cid:105)(cid:180)â1 (cid:69)(cid:163)(cid:161) XâÂµ (cid:162)(cid:161) yâÂµ (cid:162)(cid:164) X X X Y =var[X] â1cov(X,Y) afunctiononlyofthecovariances10ofX andY. Theorem2.10 InthelinearprojectionmodelY =X (cid:48)Î²+Î±+e, Î±=Âµ âÂµ(cid:48) Î² (2.39) Y X and Î²=var[X] â1cov(X,Y). (2.40) 2.22 RegressionSub-Vectors Lettheregressorsbepartitionedas (cid:181) (cid:182) X X = 1 . (2.41) X 2 WecanwritetheprojectionofY onX as Y =X (cid:48)Î²+e =X (cid:48)Î² +X (cid:48)Î² +e (2.42) 1 1 2 2 (cid:69)[Xe]=0. Inthissectionwederiveformulaeforthesub-vectorsÎ² andÎ² . 1 2 PartitionQ conformablywithX XX (cid:183) Q Q (cid:184) (cid:183) (cid:69)(cid:163) X X (cid:48)(cid:164) (cid:69)(cid:163) X X (cid:48)(cid:164) (cid:184) Q XX = Q 11 Q 12 = (cid:69)(cid:163) X 1 X 1 (cid:48)(cid:164) (cid:69)(cid:163) X 1 X 2 (cid:48)(cid:164) 21 22 2 1 2 2 andsimilarly (cid:183) Q (cid:184) (cid:183) (cid:69)[X Y] (cid:184) Q = 1Y = 1 . XY Q (cid:69)[X Y] 2Y 2 Bythepartitionedmatrixinversionformula(A.3) Q â X 1 X = (cid:183) Q Q 11 Q Q 12 (cid:184)â1 d=ef (cid:183) Q Q 1 2 1 1 Q Q 1 2 2 2 (cid:184) = (cid:183) âQ â Q 1 â 1 Q 1 1 Â·2 Q â1 âQ â 11 Q 1 Â·2 â Q 1 12 Q â 22 1 (cid:184) (2.43) 21 22 22Â·1 21 11 22Â·1 10Thecovariancematrixbetweenvectors X and Z iscov(X,Z)=(cid:69)(cid:163) (Xâ(cid:69)[X])(Zâ(cid:69)[Z]) (cid:48)(cid:164) .Thecovariancematrixofthe vectorX isvar[X]=cov(X,X)=(cid:69)(cid:163) (Xâ(cid:69)[X])(Xâ(cid:69)[X]) (cid:48)(cid:164) .",
    "page": 63,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 44 whereQ 11Â·2 d=ef Q 11 âQ 12 Q â 22 1Q 21 andQ 22Â·1 d=ef Q 22 âQ 21 Q â 11 1Q 12 .Thus (cid:181) Î² (cid:182) Î²= 1 Î² 2 (cid:183) Q â1 âQ â1 Q Q â1 (cid:184)(cid:183) Q (cid:184) = 11Â·2 11Â·2 12 22 1Y âQ â1 Q Q â1 Q â1 Q 22Â·1 21 11 22Â·1 2Y (cid:181) Q â1 (cid:161) Q âQ Q â1Q (cid:162) (cid:182) = 11Â·2 1y 12 22 2Y Q â1 (cid:161) Q âQ Q â1Q (cid:162) 22Â·1 2y 21 11 1Y = (cid:181) Q Q â 1 â 2 1 2 1 1 Â· Â· 2 1 Q Q 1 2 Y Y Â· Â· 2 1 (cid:182) . WehaveshownthatÎ² 1 =Q â 11 1 Â·2 Q 1YÂ·2 andÎ² 2 =Q â 22 1 Â·1 Q 2YÂ·1 . 2.23 CoefficientDecomposition Intheprevioussectionwederivedformulaeforthecoefficientsub-vectorsÎ² andÎ² .Wenowuse 1 2 theseformulaetogiveausefulinterpretationofthecoefficientsintermsofaniteratedprojection. Takeequation(2.42)forthecasedim(X )=1sothatÎ² â(cid:82). 1 1 Y =X Î² +X (cid:48)Î² +e. (2.44) 1 1 2 2 NowconsidertheprojectionofX onX : 1 2 X =X (cid:48)Î³ +u 1 2 2 1 (cid:69)[X u ]=0. 2 1 From(2.22)and(2.36),Î³ 2 =Q â 22 1Q 21 and(cid:69)(cid:163) u 1 2(cid:164)=Q 11Â·2 =Q 11 âQ 12 Q â 22 1Q 21 .Wecanalsocalculatethat (cid:69)[u 1 Y]=(cid:69)(cid:163)(cid:161) X 1 âÎ³(cid:48) 2 X 2 (cid:162) Y (cid:164)=(cid:69)[X 1 Y]âÎ³(cid:48) 2 (cid:69)[X 2 Y]=Q 1Y âQ 12 Q â 22 1Q 2Y =Q 1YÂ·2 . Wehavefoundthat Î² 1 =Q â 11 1 Â·2 Q 1YÂ·2 = (cid:69) (cid:69) [ (cid:163) u u 1 2 Y (cid:164) ] 1 thecoefficientfromthesimpleregressionofY onu . 1 Whatthismeansisthatinthemultivariateprojectionequation(2.44),thecoefficientÎ² equalsthe 1 projectioncoefficientfromaregressionofY onu ,theerrorfromaprojectionofX ontheotherregres- 1 1 sors X .Theerroru canbethoughtofasthecomponentof X whichisnotlinearlyexplainedbythe 2 1 1 otherregressors.ThusthecoefficientÎ² equalsthelineareffectofX onY afterstrippingouttheeffects 1 1 oftheothervariables. Therewasnothingspecialinthechoiceofthevariable X .Thisderivationappliessymmetricallyto 1 allcoefficientsinalinearprojection.EachcoefficientequalsthesimpleregressionofY ontheerrorfrom aprojectionofthatregressoronalltheotherregressors. Eachcoefficientequalsthelineareffectofthat variableonY afterlinearlycontrollingforalltheotherregressors.",
    "page": 64,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 45 2.24 OmittedVariableBias Again,lettheregressorsbepartitionedasin(2.41).ConsidertheprojectionofY onX only.Perhaps 1 thisisdonebecausethevariablesX arenotobserved.Thisistheequation 2 Y =X (cid:48)Î³ +u (2.45) 1 1 (cid:69)[X u]=0. 1 NoticethatwehavewrittenthecoefficientasÎ³ ratherthanÎ² andtheerrorasu ratherthane.Thisis 1 1 because(2.45)isdifferentthan(2.42). Goldberger(1991)introducedthecatchylabelslongregression for(2.42)andshortregressionfor(2.45)toemphasizethedistinction. Typically,Î² (cid:54)=Î³ ,exceptinspecialcases.Toseethis,wecalculate 1 1 Î³ =(cid:161)(cid:69)(cid:163) X X (cid:48)(cid:164)(cid:162)â1(cid:69)[X Y] 1 1 1 1 =(cid:161)(cid:69)(cid:163) X X (cid:48)(cid:164)(cid:162)â1(cid:69)(cid:163) X (cid:161) X (cid:48)Î² +X (cid:48)Î² +e (cid:162)(cid:164) 1 1 1 1 1 2 2 =Î² +(cid:161)(cid:69)(cid:163) X X (cid:48)(cid:164)(cid:162)â1(cid:69)(cid:163) X X (cid:48)(cid:164)Î² 1 1 1 1 2 2 =Î² +Î Î² 1 12 2 whereÎ =Q â1Q isthecoefficientmatrixfromaprojectionof X on X whereweusethenotation 12 11 12 2 1 fromSection2.22. ObservethatÎ³ =Î² +Î Î² (cid:54)=Î² unlessÎ =0orÎ² =0.Thustheshortandlongregressionshave 1 1 12 2 1 12 2 differentcoefficients. Theyarethesameonlyunderoneoftwoconditions. First,iftheprojectionofX 2 onX yieldsasetofzerocoefficients(theyareuncorrelated),orsecond,ifthecoefficientonX in(2.42) 1 2 iszero.ThedifferenceÎ Î² betweenÎ³ andÎ² isknownasomittedvariablebias.Itistheconsequence 12 2 1 1 ofomissionofarelevantcorrelatedvariable. Toavoidomittedvariablesbiasthestandardadviceistoincludeallpotentiallyrelevantvariablesin estimatedmodels. Byconstruction,thegeneralmodelwillbefreeofsuchbias. Unfortunatelyinmany casesitisnotfeasibletocompletelyfollowthisadviceasmanydesiredvariablesarenotobserved.Inthis case,thepossibilityofomittedvariablesbiasshouldbeacknowledgedanddiscussedinthecourseofan empiricalinvestigation. Forexample,supposeY islogwages,X iseducation,andX isintellectualability. Itseemsreason- 1 2 abletosupposethateducationandintellectualabilityarepositivelycorrelated(highlyableindividuals attainhigherlevelsofeducation)whichmeansÎ >0. Italsoseemsreasonabletosupposethatcon- 12 ditional on education, individuals with higher intelligence will earn higher wages on average, so that Î² >0.ThisimpliesthatÎ Î² >0andÎ³ =Î² +Î Î² >Î² .Therefore,itseemsreasonabletoexpectthat 2 12 2 1 1 12 2 1 inaregressionofwagesoneducationwithabilityomitted, thecoefficientoneducationishigherthan inaregressionwhereabilityisincluded. Inotherwords,inthiscontexttheomittedvariablebiasesthe regression coefficient upwards. It is possible, for example, that Î² =0 so that education has no direct 1 effectonwagesyetÎ³ =Î Î² >0meaningthattheregressioncoefficientoneducationaloneispositive, 1 12 2 butisaconsequenceoftheunmodeledcorrelationbetweeneducationandintellectualability. Unfortunatelytheabovesimplecharacterizationofomittedvariablebiasdoesnotimmediatelycarry overtomorecomplicatedsettings, asdiscoveredby Luca, Magnus, andPeracchi (2018)",
    "page": 65,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". It is possible, for example, that Î² =0 so that education has no direct 1 effectonwagesyetÎ³ =Î Î² >0meaningthattheregressioncoefficientoneducationaloneispositive, 1 12 2 butisaconsequenceoftheunmodeledcorrelationbetweeneducationandintellectualability. Unfortunatelytheabovesimplecharacterizationofomittedvariablebiasdoesnotimmediatelycarry overtomorecomplicatedsettings, asdiscoveredby Luca, Magnus, andPeracchi (2018). Forexample, supposewecomparethreenestedprojections Y =X (cid:48)Î³ +u 1 1 1 Y =X (cid:48)Î´ +X (cid:48)Î´ +u 1 1 2 2 2 Y =X (cid:48)Î² +X (cid:48)Î² +X (cid:48)Î² +e. 1 1 2 2 3 3",
    "page": 65,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 46 Wecancallthemtheshort,medium,andlongregressions.SupposethattheparameterofinterestisÎ² in 1 thelongregression.WeareinterestedintheconsequencesofomittingX whenestimatingthemedium 3 regression, andofomittingboth X and X whenestimatingtheshortregression. Inparticularweare 2 3 interestedinthequestion: Isitbettertoestimatetheshortormediumregression,giventhatbothomit X ?Intuitionsuggeststhatthemediumregressionshouldbeâlessbiasedâbutitisworthinvestigatingin 3 greaterdetail.Bysimilarcalculationstothoseabove,wefindthat Î³ =Î² +Î Î² +Î Î² 1 1 12 2 13 3 Î´ =Î² +Î Î² 1 1 13Â·2 3 whereÎ 13Â·2 =Q â 11 1 Â·2 Q 13Â·2 usingthenotationfromSection2.22. We see that the bias in the short regression coefficient is Î Î² +Î Î² which depends on both Î² 12 2 13 3 2 andÎ² 3 ,whilethatforthemediumregressioncoefficientisÎ 13Â·2 Î² 3 whichonlydependsonÎ² 3 .Sothebias forthemediumregressionislesscomplicatedandintuitivelyseemsmorelikelytobesmallerthanthat of the short regression. However it is impossible to strictly rank the two. It is quite possible that Î³ is 1 lessbiasedthanÎ´ .Thusasageneralruleitisstrictlyimpossibletostatethatestimationofthemedium 1 regressionwillbelessbiasedthanestimationoftheshortregression. 2.25 BestLinearApproximation Therearealternativewayswecouldconstructalinearapproximation X (cid:48)Î²totheconditionalmean m(X).Inthissectionweshowthatonealternativeapproachturnsouttoyieldthesameanswerasthe bestlinearpredictor. Westartbydefiningthemean-squareapproximationerrorof X (cid:48)Î²tom(X)astheexpectedsquared differencebetweenX (cid:48)Î²andtheconditionalmeanm(X) (cid:104) (cid:105) d(Î²)=(cid:69) (cid:161) m(X)âX (cid:48)Î²(cid:162)2 . Thefunctiond(Î²)isameasureofthedeviationofX (cid:48)Î²fromm(X).Ifthetwofunctionsareidenticalthen d(Î²)=0,otherwised(Î²)>0.Wecanalsoviewthemean-squaredifferenced(Î²)asadensity-weighted averageofthefunction (cid:161) m(X)âX (cid:48)Î²(cid:162)2 since (cid:90) d(Î²)= (cid:161) m(x)âx (cid:48)Î²(cid:162)2 f (x)dx X (cid:82)k where f (x)isthemarginaldensityofX. X Wecanthendefinethebestlinearapproximationtotheconditionalm(X)asthefunction X (cid:48)Î²ob- tainedbyselectingÎ²tominimized(Î²): Î²=argmind(b). (2.46) bâ(cid:82)k Similartothebestlinearpredictorwearemeasuringaccuracybyexpectedsquarederror.Thedifference isthatthebestlinearpredictor(2.18)selectsÎ²tominimizetheexpectedsquaredpredictionerror,while thebestlinearapproximation(2.46)selectsÎ²tominimizetheexpectedsquaredapproximationerror. Despitethedifferentdefinitions,itturnsoutthatthebestlinearpredictorandthebestlinearapprox- imationareidentical. Bythesamestepsasin(2.18)plusanapplicationofconditionalexpectationswe canfindthat Î²=(cid:161)(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:162)â1(cid:69)[Xm(X)] (2.47) =(cid:161)(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:162)â1(cid:69)[XY] (2.48) (seeExercise2.19). Thus(2.46)equals(2.18). Weconcludethatthedefinition(2.46)canbeviewedasan alternativemotivationforthelinearprojectioncoefficient.",
    "page": 66,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 47 2.26 RegressiontotheMean ThetermregressionoriginatedinaninfluentialpaperbyFrancisGalton(1886)whereheexamined thejointdistributionofthestature(height)ofparentsandchildren. Effectively, hewasestimatingthe conditionalmeanofchildrenâsheightgiventheirparentâsheight.Galtondiscoveredthatthisconditional meanwasapproximatelylinearwithaslopeof2/3. Thisimpliesthatonaverageachildâsheightismore mediocre (average) than his or her parentâs height. Galton called this phenomenon regressiontothe mean,andthelabelregressionhasstucktothisdaytodescribemostconditionalrelationships. OneofGaltonâsfundamentalinsightswastorecognizethatifthemarginaldistributionsofY andX arethesame(e.g.theheightsofchildrenandparentsinastableenvironment)thentheregressionslope inalinearprojectionisalwayslessthanone. Tobemoreprecise,takethesimplelinearprojection Y =XÎ²+Î±+e (2.49) whereY equalstheheightofthechildandX equalstheheightoftheparent.AssumethatY andX have thesamemeansothatÂµ =Âµ =Âµ.Thenfrom(2.39)Î±=(cid:161) 1âÎ²(cid:162)Âµsowecanwritethelinearprojection Y X (2.49)as P (Y |X)=(cid:161) 1âÎ²(cid:162)Âµ+XÎ². Thisshowsthattheprojectedheightofthechildisaweightedaverageofthepopulationaverageheight Âµ and the parentâs height X with the weight equal to Î². When the height distribution is stable across generationssothatvar[Y]=var[X],thenthisslopeisthesimplecorrelationofY andX.Using(2.40) cov(X,Y) Î²= =corr(X,Y). var[X] BytheCauchy-Schwarzinequality(B.32),â1â¤corr(X,Y)â¤1,withcorr(X,Y)=1onlyinthedegenerate caseY =X.Thusifweexcludedegeneracy,Î²isstrictlylessthan1. Thismeansthatonaverageachildâsheightismoremediocre(closertothepopulationaverage)than theparentâs. A common error â known as the regression fallacy â is to infer from Î²<1 that the population is convergingmeaningthatitsvarianceisdecliningtowardszero. Thisisafallacybecausewederivedthe implicationÎ²<1undertheassumptionofconstantmeansandvariances. SocertainlyÎ²<1doesnot implythatthevarianceY islessthanthanthevarianceofX. Anotherwayofseeingthisistoexaminetheconditionsforconvergenceinthecontextofequation (2.49).SinceX andeareuncorrelated,itfollowsthat var[Y]=Î²2var[X]+var[e]. Thenvar[Y]<var[X]ifandonlyif var[e] Î²2<1â var[X] whichisnotimpliedbythesimplecondition (cid:175) (cid:175) Î²(cid:175) (cid:175) <1. Theregressionfallacyarisesinrelatedempiricalsituations.Supposeyousortfamiliesintogroupsby theheightsoftheparents,andthenplottheaverageheightsofeachsubsequentgenerationovertime. Ifthepopulationisstable,theregressionpropertyimpliesthattheplotslineswillconvergeâchildrenâs heightwillbemoreaveragethantheirparents. Theregressionfallacyistoincorrectlyconcludethatthe populationisconverging. Amessagetobelearnedfromthisexampleisthatsuchplotsaremisleading forinferencesaboutconvergence.",
    "page": 67,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 48 Theregressionfallacyissubtle. Itiseasyforintelligenteconomiststosuccumbtoitstemptation. A famousexampleisTheTriumphofMediocrityinBusiness byHoraceSecristpublishedin1933. Inthis book,Secristcarefullyandwithgreatdetaildocumentedthatinasampleofdepartmentstoresover1920- 1930,whenhedividedthestoresintogroupsbasedon1920-1921profits,andplottedtheaverageprofits of these groups for the subsequent 10 years, he found clear and persuasive evidence for convergence âtowardmediocrityâ.Ofcourse,therewasnodiscoveryâregressiontothemeanisanecessaryfeatureof stabledistributions. 2.27 ReverseRegression Galton noticed another interesting feature of the bivariate distribution. There is nothing special aboutaregressionofY onX.WecanalsoregressX onY.(Inhisheredityexamplethisisthebestlinear predictoroftheheightofparentsgiventheheightoftheirchildren.)Thisregressiontakestheform X =YÎ²â+Î±â+e â . (2.50) Thisissometimescalledthereverseregression.Inthisequation,thecoefficientsÎ±â ,Î²â anderrore â are definedbylinearprojection.Inastablepopulationwefindthat Î²â=corr(X,Y)=Î² Î±â=(cid:161) 1âÎ²(cid:162)Âµ=Î± whichareexactlythesameasintheprojectionofY onX!Theinterceptandslopehaveexactlythesame valuesintheforwardandreverseprojections![Thisequalityisnotparticularlyimporant;itisanartifact oftheassumptionthatX andY havethesamevariances.] Whilethisalgebraicdiscoveryisquitesimple,itiscounter-intuitive.Instead,acommonyetmistaken guessfortheformofthereverseregressionistotaketheequation(2.49),dividethroughbyÎ²andrewrite tofindtheequation 1 Î± 1 X =Y â â e (2.51) Î² Î² Î² suggestingthattheprojectionofX onY shouldhaveaslopecoefficientof1/Î²insteadofÎ²,andintercept ofâÎ±/Î²ratherthanÎ±.Whatwentwrong? Equation(2.51)isperfectlyvalidbecauseitisasimplema- nipulationofthevalidequation(2.49).Thetroubleisthat(2.51)isneitheraCEFnoralinearprojection. Invertingaprojection(orCEF)doesnotyieldaprojection(orCEF).Instead,(2.50)isavalidprojection, not(2.51). Inanyevent,Galtonâsfindingwasthatwhenthevariablesarestandardizedtheslopeinbothprojec- tions(Y onX,andX onY)equalsthecorrelationandbothequationsexhibitregressiontothemean. It isnotacausalrelation,butanaturalfeatureofalljointdistributions. 2.28 LimitationsoftheBestLinearProjection LetâscomparethelinearprojectionandlinearCEFmodels. From Theorem 2.4.4 we know that the CEF error has the property (cid:69)[Xe]=0. Thus a linear CEF is thebestlinearprojection. However,theconverseisnottrueastheprojectionerrordoesnotnecessarily satisfy(cid:69)[e|X]=0.Furthermore,thelinearprojectionmaybeapoorapproximationtotheCEF. Toseethesepointsinasimpleexample,supposethatthetrueprocessisY =X+X2withX â¼N(0,1). InthiscasethetrueCEFism(x)=x+x2andthereisnoerror.NowconsiderthelinearprojectionofY on",
    "page": 68,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 49 X andaconstant,namelythemodelY =Î²X+Î±+e.SinceX â¼N(0,1)thenX andX2areuncorrelatedand thelinearprojectiontakestheformP [Y |X]=X +1.ThisisquitedifferentfromthetrueCEFm(X)= X+X2.Theprojectionerrorequalse=X2â1whichisadeterministicfunctionofX yetisuncorrelated with X. WeseeinthisexamplethataprojectionerrorneednotbeaCEFerrorandalinearprojection canbeapoorapproximationtotheCEF. 0 1 2 3 4 5 6 8 6 4 2 0 Conditional Mean Linear Projection, Group 1 Linear Projection, Group 2 Figure2.7:ConditionalMeanandTwoLinearProjections Anotherdefectoflinearprojectionisthatitissensitivetothemarginaldistributionoftheregressors whentheconditionalmeanisnonlinear.WeillustratetheissueinFigure2.7foraconstructed11jointdis- tributionofY andX.ThesolidlineisthenonlinearCEFofY givenX.Thedataaredividedintwogroups âGroup1andGroup2âwhichhavedifferentmarginaldistributionsfortheregressorX,andGroup1has alowermeanvalueof X thanGroup2. TheseparatelinearprojectionsofY on X forthesetwogroups aredisplayedinthefigurebythedashedlines. Thesetwoprojectionsaredistinctapproximationstothe CEF.AdefectwithlinearprojectionisthatitleadstotheincorrectconclusionthattheeffectofX onY is differentforindividualsinthetwogroups.Thisconclusionisincorrectbecauseinfactthereisnodiffer- enceintheconditionalmeanfunction.Theapparentdifferenceisaby-productoflinearapproximations toanonlinearmeancombinedwithdifferentmarginaldistributionsfortheconditioningvariables. 2.29 RandomCoefficientModel AmodelwhichisnotationallysimilartobutconceptuallydistinctfromthelinearCEFmodelisthe linearrandomcoefficientmodel. IttakestheformY =X (cid:48)Î·wheretheindividual-specificcoefficientÎ· is random and independent of X. For example, if X is years of schooling and Y is log wages, then Î· is the individual-specific returns to schooling. If a person obtains an extra year of schooling, Î· is the actualchangeintheirwage.Therandomcoefficientmodelallowsthereturnstoschoolingtovaryinthe population. Someindividualsmighthaveahighreturntoeducation(ahighÎ·)andothersalowreturn, possibly0,orevennegative. 11TheX inGroup1areN(2,1)andthoseinGroup2areN(4,1),andtheconditionaldistributionofY givenX isN(m(X),1) wherem(x)=2xâx2/6.",
    "page": 69,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 50 In the linear CEF model the regressor coefficient equals the regression derivative â the change in the conditional mean due to a change in the regressors, Î²=âm(X). This is not the effect on a given individual, itistheeffectonthepopulationaverage. Incontrast, intherandomcoefficientmodelthe randomvectorÎ·=â(cid:161) X (cid:48)Î·(cid:162) isthetruecausaleffectâthechangeintheresponsevariableY itselfduetoa changeintheregressors. Itisinteresting,however,todiscoverthatthelinearrandomcoefficientmodelimpliesalinearCEF.To seethis,letÎ²=(cid:69)(cid:163)Î·(cid:164) andÎ£=var (cid:163)Î·(cid:164) denotethemeanandcovariancematrixofÎ·andthendecomposethe randomcoefficientasÎ·=Î²+uwhereuisdistributedindependentlyofX withmeanzeroandcovariance matrixÎ£.Thenwecanwrite (cid:69)[Y |X]=X (cid:48)(cid:69)(cid:163)Î·|X (cid:164)=X (cid:48)(cid:69)(cid:163)Î·(cid:164)=X (cid:48)Î² sotheCEFislinearinX,andthecoefficientÎ²equalsthemeanoftherandomcoefficientÎ·. WecanthuswritetheequationasalinearCEFY =X (cid:48)Î²+e wheree=X (cid:48) uandu=Î·âÎ². Theerroris conditionallymeanzero:(cid:69)[e|X]=0.Furthermore var[e|X]=X (cid:48) var (cid:163)Î·(cid:164) X =X (cid:48)Î£X sotheerrorisconditionallyheteroskedasticwithitsvarianceaquadraticfunctionofX. Theorem2.11 In the linear random coefficient model Y = X (cid:48)Î· with Î· inde- pendentofX,(cid:69)(cid:107)X(cid:107)2<â,and(cid:69)(cid:176) (cid:176) Î·(cid:176) (cid:176) 2<â,then (cid:69)[Y |X]=X (cid:48)Î² var[Y |X]=X (cid:48)Î£X whereÎ²=(cid:69)(cid:163)Î·(cid:164) andÎ£=var (cid:163)Î·(cid:164) . 2.30 CausalEffects Sofarwehaveavoidedtheconceptofcausality,yetoftentheunderlyinggoalofaneconometricanal- ysisistomeasureacausalrelationshipbetweenvariables. Itisoftenofgreatinteresttounderstandthe causes and effects of decisions, actions, and policies. For example, we may be interested in the effect of class sizes on test scores, police expenditures on crime rates, climate change on economic activity, yearsofschoolingonwages,institutionalstructureongrowth,theeffectivenessofrewardsonbehavior, theconsequencesofmedicalproceduresforhealthoutcomes,oranyvarietyofpossiblecausalrelation- ships. Ineachcasethegoalistounderstandwhatistheactualeffectontheoutcomeduetoachangein aninput. Wearenotjustinterestedintheconditionalmeanorlinearprojection,wewouldliketoknow theactualchange. Twoinherentbarriersare:(1)thecausaleffectistypicallyspecifictoanindividual;and(2)thecausal effectistypicallyunobserved. Considertheeffectofschoolingonwages. Thecausaleffectistheactualdifferenceapersonwould receiveinwagesifwecouldchangetheirlevelofeducationholdingallelseconstant. Thisisspecificto eachindividualastheiremploymentoutcomesinthesetwodistinctsituationsareindividual.Thecausal effectisunobservedbecausethemostwecanobserveistheiractuallevelofeducationandtheiractual wage,butnotthecounterfactualwageiftheireducationhadbeendifferent.",
    "page": 70,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 51 Tobeconcretesupposethattherearetwoindividuals,JenniferandGeorge,andbothhavethepossi- bilityofbeinghigh-schoolgraduatesorcollegegraduates,andbothwouldhavereceiveddifferentwages giventheirchoices.Forexample,supposethatJenniferwouldhaveearned$10anhourasahigh-school graduate and $20 an hour as a college graduate while George would have earned $8 as a high-school graduateand$12asacollegegraduate. Inthisexamplethecausaleffectofschoolingis$10ahourfor Jenniferand$4anhourforGeorge. Thecausaleffectsarespecifictotheindividualandneithercausal effectisobserved. Rubin(1974)developedthepotentialoutcomesframework(alsoknownastheRubincausalmodel) toclarifytheissues. LetY beascalaroutcome(forexample, wages)andD beabinarytreatment(for example,collegeattendence).Thespecificationoftreatmentasbinaryisnotessentialbutsimplifiesthe notation.Aflexiblemodeldescribingtheimpactofthetreatmentontheoutcomeis Y =h(D,U) (2.52) whereU is an (cid:96)Ã1 unobserved random factor and h is a functional relationship. It is also common tousethesimplifiednotationY(0)=h(0,U)andY(1)=h(1,U)forthepotentialoutcomesassociated with non-treatment and treatment, respectively. The notation implicitly holdsU fixed. The potential outcomesarespecifictoeachindividualastheydependonU. Forexample,ifY isanindividualâswage, theunobservablesU couldincludecharacteristicssuchastheindividualsâsabilities, skills, workethic, interpersonalconnections,andpreferences,allofwhichpotentiallyinfluencetheirwage.Inourexample thesefactorsaresummarizedbythelabelsâJenniferâandâGeorgeâ. RubindescribedtheeffectascausalwhenwevaryD whileholdingU constant. Inourexamplethis meanschanginganindividualâseducationwhileholdingconstanttheirotherattributes. Definition2.6 Inthemodel(2.52)thecausaleffectofD onY is C(U)=Y(1)âY(0)=h(1,U)âh(0,U), (2.53) thechangeinY duetotreatmentwhileholdingU constant. Itmaybehelpfultounderstandthat(2.53)isadefinitionanddoesnotnecessarilydescribecausal- ity in a fundamental or experimental sense. Perhaps it would be more appropriate to label (2.53) as a structuraleffect(theeffectwithinthestructuralmodel). ThecausaleffectoftreatmentC(U)definedin(2.53)isheterogeneousandrandomasthepotential outcomes Y(0) and Y(1) vary across individuals. We do not observe both Y(0) and Y(1) for a given individualbutratheronlytherealizedvalue ï£± Y(0) if D=0 ï£² Y = ï£³ Y(1) if D=1. ConsequentlythecausaleffectC(U)isunobserved. RubinâsgoalwastolearnfeaturesofthedistributionofC(U)includingitsexpectedvaluewhichhe calledtheaveragecausaleffect.Hedefineditasfollows.",
    "page": 71,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 52 Definition2.7 Inthemodel(2.52)theaveragecausaleffectofD onY is (cid:90) ACE=(cid:69)[C(U)]= C(u)f(u)du (cid:82)(cid:96) where f(u)isthedensityofU. The ACE is the population average of the causal effect. Extending our Jennifer & George example, supposethathalfofthepopulationarelikeJenniferandtheotherhalfarelikeGeorge.Thentheaverage causaleffectofcollegeonwagesis(10+4)/2=$7anhour. ToestimatetheACEareasonablestartingplaceistocompareaverageY fortreatedanduntreated individuals. In our example this is the difference between the average wage among college graduates andhighschoolgraduates. ThisisthesameasthecoefficientinaregressionoftheoutcomeY onthe treatmentD.DoesthisequaltheACE? TheanswerdependsontherelationshipbetweentreatmentD andtheunobservedcomponentU.If DisrandomlyassignedasinanexperimentthenDandU areindependentandtheregressioncoefficient equalstheACE.However,ifD andU aredependentthentheregressioncoefficientandACEarediffer- ent. Toseethis,observethatthedifferencebetweentheaverageoutcomesofthetreatedanduntreated populationsare (cid:90) (cid:90) (cid:69)[Y |D=1]â(cid:69)[Y |D=0]= h(1,u)f (u|D=1)duâ h(1,u)f (u|D=0)du (cid:82)(cid:96) (cid:82)(cid:96) where f (u|D)istheconditionaldensityofU givenD.IfU isindependentofDthen f (u|D)=f (u)and theaboveexpressionequals (cid:82) (cid:82)(cid:96)(h(1,u)âh(0,u))f (u)du=ACE.However,ifU andDaredependentthis equalityfails. Toillustrate,letâsreturntoourexampleofJenniferandGeorge.Supposethatallhighschoolstudents takeanaptitudetest. Ifastudentgetsahigh(H)scoretheygotocollegewithprobability3/4,andifa studentgetsalow(L)scoretheygotocollegewithprobability1/4.SupposefurtherthatJennifergetsan aptitudescoreofHwithprobability3/4,whileGeorgegetsascoreofHwithprobability1/4. Giventhis situation,62.5%ofJenniferâswillgotocollege12while37.5%ofGeorgeâswillgotocollege13. An econometrician who randomly samples 32 individuals and collects data oneducational attain- mentandwageswillfindthewagedistributiondisplayedinTable2.3. Oureconometricianfindsthattheaveragewageamonghighschoolgraduatesis$8.75whiletheav- eragewageamongcollegegraduatesis$17.00.Thedifferenceof$8.25istheeconometricianâsregression coefficientfortheeffectofcollegeonwages. But$8.25overstatesthetrueACEof$7. Thereasonisthat collegeattendenceisdeterminedbyanaptitudetestwhichiscorrelatedwithanindividualâscausalef- fect.Jenniferhasbothahighcausaleffectandismorelikelytoattendcollege,sotheobserveddifference inwagesoverstatesthecausaleffectofcollege. 12(cid:80)(cid:163) college|Jennifer (cid:164)=(cid:80)(cid:163) college|H (cid:164)(cid:80)(cid:163) H|Jennifer (cid:164)+(cid:80)(cid:163) college|L (cid:164)(cid:80)(cid:163) L|Jennifer (cid:164)=(3/4)2+(1/4)2. 13(cid:80)(cid:163) college|George (cid:164)=(cid:80)(cid:163) college|H (cid:164)(cid:80)(cid:163) H|George (cid:164)+(cid:80)(cid:163) college|L (cid:164)(cid:80)(cid:163) L|George (cid:164)=(3/4)(1/4)+(1/4)(3/4)",
    "page": 72,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". 12(cid:80)(cid:163) college|Jennifer (cid:164)=(cid:80)(cid:163) college|H (cid:164)(cid:80)(cid:163) H|Jennifer (cid:164)+(cid:80)(cid:163) college|L (cid:164)(cid:80)(cid:163) L|Jennifer (cid:164)=(3/4)2+(1/4)2. 13(cid:80)(cid:163) college|George (cid:164)=(cid:80)(cid:163) college|H (cid:164)(cid:80)(cid:163) H|George (cid:164)+(cid:80)(cid:163) college|L (cid:164)(cid:80)(cid:163) L|George (cid:164)=(3/4)(1/4)+(1/4)(3/4). Table2.3:ExampleDistribution $8 $10 $12 $20 Mean High-SchoolGraduate 10 6 0 0 $8.75 CollegeGraduate 0 0 6 10 $17.00 Difference $8.25",
    "page": 72,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 53 Ourfirstlessonfromthisanalysisisthatweneedtobecautiousaboutinterpretingregressioncoeffi- cientsascausaleffects.Unlesstheregressors(e.g.educationattainment)canbeinterpretedasrandomly assigneditisinappropriatetointerprettheregressioncoefficientscausally. Oursecondlessonwillbethatacausalinterpretationcanbeobtainedifweconditiononasufficiently richsetofcovariates.Wenowexplorethisissue. Supposethattheobservablesincludeasetofcovariates X inadditiontotheoutcomeY andtreat- mentD.Weextendthepotentialoutcomesmodel(2.52)toincludeX: Y =h(D,X,U). (2.54) WealsoextendthedefinitionofacausaleffecttoallowconditioningonX. Definition2.8 Inthemodel(2.54)thecausaleffectofD onY is C(X,U)=h(1,X,U)âh(0,X,U), thechangeinY duetotreatmentholdingX andU constant. TheconditionalaveragecausaleffectofD onY conditionalonX =xis (cid:90) ACE(x)=(cid:69)[C(X,U)|X =x]= C(x,u)f(u|x)du (cid:82)(cid:96) where f(u|x)istheconditionaldensityofU givenX. TheunconditionalaveragecausaleffectofD onY is (cid:90) ACE=(cid:69)[C(X,U)]= ACE(x)f(x)dx where f(x)isthedensityofX. TheconditionalaveragecausaleffectACE(x)istheACEforthesub-populationwithcharacteristics X =x.Givenobservationson(Y,D,X)wewanttomeasurethecausaleffectofDonY,andareinterested ifthiscanbeobtainedbyaregressionofY on (D,X). WewouldliketointerpretthecoefficientonD as acausaleffect.Isthisappropriate? Our previous analysis showed that a causal interpretation obtains when U is independent of the regressors.Whilethisissufficientitisstrongerthannecessary.Instead,thefollowingissufficient. Definition2.9 ConditionalIndependenceAssumption(CIA).Conditionalon X therandomvariablesD andU arestatisticallyindependent. TheCIAimpliesthattheconditionaldensityofU given(D,X)onlydependsonX,thus f(u|D,X)= f(u|X).ThisimpliesthattheregressionofY on(D,X)equals m(d,x)=(cid:69)[Y |D=d,X =x] =(cid:69)[h(d,x,U)|D=d,X =x] (cid:90) = h(d,x,u)f(u|x)du.",
    "page": 73,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 54 UndertheCIAthetreatmenteffectmeasuredbytheregressionis âm(d,x)=m(1,x)âm(0,x) (cid:90) (cid:90) = h(1,x,u)f(u|x)duâ h(0,x,u)f(u|x)du (cid:90) = C(x,u)f(u|x)du =ACE(x). (2.55) ThisistheconditionalACE.ThusundertheCIAtheregressioncoefficientequalstheACE. WededucethattheregressionofY on(D,X)revealsthecausalimpactoftreatmentwhentheCIA holds. Thismeansthatregressionanalysiscanbeinterpretedcausallywhenwecanmakethecasethat theregressorsX aresufficienttocontrolforfactorswhicharecorrelatedwithtreatment. Theorem2.12 In the structural model (2.54), the Conditional Independence Assumptionimpliesâm(d,x)=ACE(x),thattheregressionderivativewithre- specttotreatmentequalstheconditionalACE. Thisisafascinatingresult.Itshowsthatwhenevertheunobservableisindependentofthetreatment variable after conditioning on appropriate regressors, the regression derivative equals the conditional causaleffect.ThismeanstheCEFhascausaleconomicmeaning,givingstrongjustificationtoestimation oftheCEF. It is important to understand the critical role of the CIA. If CIA fails then the equality (2.55) of the regressionderivativeandtheACEfails. TheCIAstatesthatconditionalon X thevariablesU andD are independent.ThismeansthattreatmentD isnotaffectedbytheunobservedindividualfactorsU andis effectivelyrandom.Itisastrongassumption.Inthewage/educationexampleitmeansthateducationis notselectedbyindividualsbasedontheirunobservedcharacteristics. However,itisalsohelpfultounderstandthattheCIAisweakerthanfullindependenceofU fromthe regressors(D,X).WhatisrequiredisonlythatU andD areindependentafterconditioningonX.IfX is sufficientlyrichthismaynotberestrictive. Returningtoourexample,werequireavariableX whichbreaksthedependencebetweenD andU. Inourexamplethisvariableistheaptitudetestscore,sincethedecisiontoattendcollegewasbasedon thetestscore.Itfollowsthateducationalattainmentandtypeareindependentonceweconditiononthe testscore. Toseethis,observethatifastudentâstestscoreisHtheprobabilitytheygotocollege(D =1)is3/4 forbothJennifersandGeorges. Similarly,iftheirtestscoreisLtheprobabilitytheygotocollegeis1/4 forbothtypes. Thismeansthatcollegeattendenceisindependentoftype,conditionalontheaptitude testscore. TheconditionalACEdependsonthetestscore. Amongstudentswhoreceiveahightestscore,3/4 areJenniferâsand1/4areGeorgeâs.ThustheconditionalACEforstudentswithascoreofHis(3/4)Ã10+ (1/4)Ã4=$8.50.Amongstudentswhoreceivealowtestscore, 1/4areJenniferâsand3/4areGeorgeâs. ThustheACEforstudentswithascoreofLis(1/4)Ã10+(3/4)Ã4=$5.50.TheunconditionalACEisthe average,ACE=(8.50+5.50)/2=$7,since50%ofstudentseachreceivescoresofHandL. Theorem2.12showsthattheconditionalACEisrevealedbyaregressionwhichincludestestscores. Toseethisinthewagedistribution,supposethattheeconometriciancollectsdataontheaptitudetest",
    "page": 74,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 55 Table2.4:ExampleDistribution2 $8 $10 $12 $20 Mean High-SchoolGraduate+HighTestScore 1 3 0 0 $9.50 CollegeGraduate+HighTestScore 0 0 3 9 $18.00 High-SchoolGraduate+LowTestScore 9 3 0 0 $8.50 CollegeGraduate+LowTestScore 0 0 3 1 $14.00 scoreaswellaseducationandwages. Givenarandomsampleof32individualswewouldexpecttofind thewagedistributioninTable2.4. Define a dummy highscore to indicate students who received a high test score. The regression of wagesoncollegeattendanceandtestscoreswiththeirinteractionis (cid:69)(cid:163) wage|college,highscore (cid:164)=1.00highscore+5.50college+3.00highscoreÃcollege+8.50. (2.56) Thecoefficientoncollege,$5.50,istheregressionderivativeofcollegeattendanceforthosewithlowtest scores, and the sum of this coefficient with the interaction coefficient $3.00 equals $8.50 which is the regression derivative for college attendance for those with high test scores. $5.50 and $8.50 equal the conditionalcausaleffectsascalculatedabove. Thisshowsthatfromtheregression(2.56)aneconometricianwillfindthattheeffectofcollegeon wages is $8.50 for those with high test scores and $5.50 for those with low test scores with an average effectof$7(since50%ofstudentsreceivehighandlowtestscores).Thisisthetrueaveragecausaleffect ofcollegeonwages.Thustheregressioncoefficientoncollegein(2.56)canbeinterpretedcausally,while aregressionomittingtheaptitudetestscoredoesnotrevealthecausaleffectofeducation. Tosummarizeourfindings,wehaveshownhowitispossiblethatasimpleregressionwillgiveafalse measurementofacausaleffect,butamorecarefulregressioncanrevealthetruecausaleffect. Thekey istoconditiononasuitablyrichsetofcovariatessuchthattheremainingunobservedfactorsaffecting theoutcomeareindependentofthetreatmentvariable. 2.31 ExistenceandUniquenessoftheConditionalExpectation* InSections2.3and2.6wedefinedtheconditionalexpectationwhentheconditioningvariablesX are discreteandwhenthevariables(Y,X)haveajointdensity. Wehaveexploredthesecasesbecausethese arethesituationswheretheconditionalmeaniseasiesttodescribeandunderstand. However,thecon- ditionalmeanexistsquitegenerallywithoutappealingtothepropertiesofeitherdiscreteorcontinuous randomvariables. To justify this claim we now present a deep result from probability theory. What it says is that the conditionalmeanexistsforalljointdistributions(Y,X)forwhichY hasafinitemean.",
    "page": 75,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 56 Theorem2.13 ExistenceoftheConditionalExpectation If(cid:69)|Y|<âthenthereexistsafunctionm(x)suchthatforallsetsX forwhich (cid:80)[X âX]isdefined, (cid:69)[ 1 {X âX}Y]=(cid:69)[ 1 {X âX}m(X)]. (2.57) Thefunctionm(X)isalmosteverywhereunique,inthesensethatifh(x)sat- isfies (2.57), then there is a set S such that (cid:80)[S] = 1 and m(x) = h(x) for x âS.Thefunctionm(x)iscalledtheconditionalexpectationandiswritten m(x)=(cid:69)[Y |X =x]. See,forexample,Ash(1972),Theorem6.3.3. Theconditionalexpectationm(x)definedby(2.57)specializesto(2.6)when(Y,X)haveajointden- sity.Theusefulnessofdefinition(2.57)isthatTheorem2.13showsthattheconditionalmeanm(X)exists forallfinite-meandistributions.ThisdefinitionallowsY tobediscreteorcontinuous,forX tobescalar orvector-valued,andforthecomponentsofX tobediscreteorcontinuouslydistributed. YoumayhavenoticedthatTheorem2.13appliesonlytosetsX forwhich(cid:80)[X âX]isdefined.Thisis atechnicalissueâmeasurabilityâwhichwelargelyside-stepinthistextbook.Formalprobabilitytheory onlyappliestosetswhicharemeasurableâforwhichprobabilitiesaredefinedâasitturnsoutthatnotall setssatisfymeasurability. Thisisnotapracticalconcernforapplications,sowedefersuchdistinctions forformaltheoreticaltreatments. 2.32 Identification* Acriticalandimportantissueinstructuraleconometricmodelingisidentification, meaningthata parameterisuniquelydeterminedbythedistributionoftheobservedvariables. Itisrelativelystraight- forwardinthecontextoftheunconditionalandconditionalmean,butitisworthwhiletointroduceand exploretheconceptatthispointforclarity. LetF denotethedistributionoftheobserveddata,forexamplethedistributionofthepair(Y,X).Let F beacollectionofdistributionsF.LetÎ¸beaparameterofinterest(forexample,theexpectation(cid:69)[Y]). Definition2.10 AparameterÎ¸â(cid:82)isidentifiedonF ifforallF âF,thereisa uniquelydeterminedvalueofÎ¸. Equivalently,Î¸isidentifiedifwecanwriteitasamappingÎ¸=g(F)onthesetF.Therestrictiontothe setF isimportant.Mostparametersareidentifiedonlyonastrictsubsetofthespaceofalldistributions. Take,forexample,themeanÂµ=(cid:69)[Y].Itisuniquelydeterminedif(cid:69)|Y|<â,soÂµisidentifiedforthe setF ={F :(cid:69)|Y|<â}. Next,considertheconditionalexpectation.Theorem2.13demonstratesthat(cid:69)|Y|<âisasufficient conditionforidentification. Theorem2.14 IdentificationoftheConditionalExpectation If (cid:69)|Y|<â, the conditional expectation m(x)=(cid:69)[Y |X =x] is identified al- mosteverywhere.",
    "page": 76,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 57 Itmightseemasifidentificationisageneralpropertyforparameterssolongasweexcludedegener- atecases. Thisistrueformomentsofobserveddata,butnotnecessarilyformorecomplicatedmodels. As a case in point, consider the context of censoring. Let Y be a random variable with distribution F. â InsteadofobservingY,weobserveY definedbythecensoringrule Y â= (cid:189) Y ifY â¤Ï Ï ifY >Ï. Thatis,Y â iscappedatthevalueÏ.Acommonexampleisincomesurveys,whereincomeresponsesare âtop-codedâ meaning that incomes above the top code Ï are recorded as the top code. The observed â variableY hasdistribution F â (u)= (cid:189) F(u) foruâ¤Ï 1 foruâ¥Ï. â WeareinterestedinfeaturesofthedistributionF notthecensoreddistributionF .Forexample,weare interestedinthemeanwageÂµ=(cid:69)[Y].ThedifficultyisthatwecannotcalculateÂµfromF â exceptinthe trivialcasewherethereisnocensoring(cid:80)[Y â¥Ï]=0.ThusthemeanÂµisnotgenericallyidentifiedfrom thecensoreddistribution. Atypicalsolutiontotheidentificationproblemistoassumeaparametricdistribution.Forexample, letF bethesetofnormaldistributionsY â¼N(Âµ,Ï2).Itispossibletoshowthattheparameters(Âµ,Ï2)are identifiedforallF âF.Thatis,ifweknowthattheuncensoreddistributionisnormalwecanuniquely determinetheparametersfromthecensoreddistribution.Thisisoftencalledparametricidentification asidentificationisrestrictedtoaparametricclassofdistributions. Inmoderneconometricsthisisgen- erallyviewedasasecond-bestsolutionasidentificationhasbeenachievedonlythroughtheuseofan arbitraryandunverifiableparametricassumption. Apessimisticconclusionmightbethatitisimpossibletoidentifyparametersofinterestfromcen- soreddatawithoutparametricassumptions. Interestingly,thispessimismisunwarranted. Itturnsout that we can identify the quantiles qÎ± of F for Î±â¤(cid:80)[Y â¤Ï]. For example, if 20% of the distribution is censoredwecanidentifyallquantilesforÎ±â(0,0.8).Thisisoftencallednonparametricidentification astheparametersareidentifiedwithoutrestrictiontoaparametricclass. Whatwehavelearnedfromthislittleexerciseisthatinthecontextofcensoreddatamomentscan onlybeparametricallyidentifiedwhilenon-censoredquantilesarenonparametricallyidentified.Partof themessageisthatastudyofidentificationcanhelpfocusattentiononwhatcanbelearnedfromthe datadistributionsavailable. 2.33 TechnicalProofs* (cid:161) (cid:162) Proof of Theorem 2.1 For convenience, assume that the variables have a joint density f y,x . Since (cid:69)[Y |X]isafunctionoftherandomvectorX only,tocalculateitsexpectationweintegratewithrespect tothedensity f (x)ofX,thatis X (cid:90) (cid:69)[(cid:69)[Y |X]]= (cid:69)[Y |X]f (x)dx. X (cid:82)k Substitutingin(2.6)andnotingthat f Y|X (cid:161) y|x (cid:162) f X (x)=f (cid:161) y,x (cid:162) ,wefindthattheaboveexpressionequals (cid:90) (cid:181)(cid:90) (cid:182) (cid:90) (cid:90) yf Y|X (cid:161) y|x (cid:162) dy f X (x)dx= yf (cid:161) y,x (cid:162) dydx=(cid:69)[Y] (cid:82)k (cid:82) (cid:82)k (cid:82) theunconditionalexpectationofY. â ",
    "page": 77,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 58 ProofofTheorem2.2Againassumethatthevariableshaveajointdensity.Itisusefultoobservethat (cid:161) (cid:162) f (cid:161) y|x ,x (cid:162) f (x |x )= f y,x 1 ,x 2 f (x 1 ,x 2 ) =f (cid:161) y,x |x (cid:162) , (2.58) 1 2 2 1 2 1 f (x ,x ) f (x ) 1 2 1 thedensityof(Y,X )givenX .Here,wehaveabusednotationandusedasinglesymbol f todenotethe 2 1 variousunconditionalandconditionaldensitiestoreducenotationalclutter. Notethat (cid:90) (cid:69)[Y |X =x ,X =x ]= yf (cid:161) y|x ,x (cid:162) dy. (2.59) 1 1 2 2 1 2 (cid:82) Integrating(2.59)withrespecttotheconditionaldensityofX givenX ,andapplying(2.58)wefindthat 2 1 (cid:90) (cid:69)[(cid:69)[Y |X ,X ]|X =x ]= (cid:69)[Y |X =x ,X =x ]f (x |x )dx 1 2 1 1 1 1 2 2 2 1 2 (cid:82)k2 (cid:90) (cid:181)(cid:90) (cid:182) = yf (cid:161) y|x ,x (cid:162) dy f (x |x )dx 1 2 2 1 2 (cid:82)k2 (cid:82) (cid:90) (cid:90) = yf (cid:161) y|x ,x (cid:162) f (x |x )dydx 1 2 2 1 2 (cid:82)k2 (cid:82) (cid:90) (cid:90) = yf (cid:161) y,x |x (cid:162) dydx 2 1 2 (cid:82)k2 (cid:82) =(cid:69)[Y |X =x ]. 1 1 Thisimplies(cid:69)[(cid:69)[Y |X ,X ]|X ]=(cid:69)[Y |X ]asstated. â  1 2 1 1 ProofofTheorem2.3 (cid:90) (cid:90) (cid:69)(cid:163) g(X)Y |X =x (cid:164)= g(x)yf Y|X (cid:161) y|x (cid:162) dy=g(x) yf Y|X (cid:161) y|x (cid:162) dy=g(x)(cid:69)[Y |X =x] (cid:82) (cid:82) Thisimplies(cid:69)(cid:163) g(X)Y |X (cid:164)=g(X)(cid:69)[Y |X]whichis(2.7). Equation(2.8)followsbyapplyingthesimple lawofiteratedexpectations(Theorem2.1)to(2.7). â  ProofofTheorem2.4ApplyingMinkowskiâsinequality(B.34)toe=Y âm(X), (cid:161)(cid:69)|e|r(cid:162)1/r =(cid:161)(cid:69)|Y âm(X)|r(cid:162)1/r â¤(cid:161)(cid:69)|Y|r(cid:162)1/r+(cid:161)(cid:69)|m(X)|r(cid:162)1/r <â, wherethetwopartsontheright-hand-sidearefinitesince(cid:69)|Y|r <âbyassumptionand(cid:69)|m(X)|r <â bytheconditionalexpectationinequality(B.29).Thefactthat((cid:69)|e|r)1/r <âimplies(cid:69)|e|r <â. â  ProofofTheorem2.6Theassumptionthat(cid:69)(cid:163) Y2(cid:164)<âimpliesthatalltheconditionalexpectationsbe- lowexist. Usingthelawofiteratedexpectations(Theorem2.2)(cid:69)[Y |X ]=(cid:69)((cid:69)[Y |X ,X ]|X )andthecondi- 1 1 2 1 tionalJensenâsinequality(B.28), ((cid:69)[Y |X ])2=((cid:69)((cid:69)[Y |X ,X ]|X ))2â¤(cid:69)(cid:163) ((cid:69)[Y |X ,X ])2|X (cid:164) . 1 1 2 1 1 2 1 Takingunconditionalexpectations,thisimplies (cid:69)(cid:163) ((cid:69)[Y |X ])2(cid:164)â¤(cid:69)(cid:163) ((cid:69)[Y |X ,X ])2(cid:164) . 1 1 2 Similarly, ((cid:69)[Y])2â¤(cid:69)(cid:163) ((cid:69)[Y |X ])2(cid:164)â¤(cid:69)(cid:163) ((cid:69)[Y |X ,X ])2(cid:164) . (2.60) 1 1 2",
    "page": 78,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 59 The variables Y, (cid:69)[Y |X ] and (cid:69)[Y |X ,X ] all have the same expectation (cid:69)[Y], so the inequality 1 1 2 (2.60)impliesthatthevariancesarerankedmonotonically: 0â¤var((cid:69)[Y |X ])â¤var((cid:69)[Y |X ,X ]). (2.61) 1 1 2 Define e=Y â(cid:69)[Y |X]andu=(cid:69)[Y |X]âÂµsothatwehavethedecompositionY âÂµ=e+u.Notice (cid:69)[e|X]=0anduisafunctionofX.Thusbytheconditioningtheorem(Theorem2.3),(cid:69)[eu]=0soeand uareuncorrelated.Itfollowsthat var[Y]=var[e]+var[u]=var[Y â(cid:69)[Y |X]]+var[(cid:69)[Y |X]]. (2.62) Themonotonicityofthevariancesoftheconditionalmean(2.61)appliedtothevariancedecomposition (2.62)impliesthereversemonotonicityofthevariancesofthedifferences,completingtheproof. â  ProofofTheorem2.9Forpart1,bytheexpectationinequality(B.30),(A.17)andAssumption2.1, (cid:176) (cid:176) (cid:69)(cid:163) XX (cid:48)(cid:164)(cid:176) (cid:176) â¤(cid:69)(cid:176) (cid:176)XX (cid:48)(cid:176) (cid:176) =(cid:69)(cid:107)X(cid:107)2<â. Similarly,usingtheexpectationinequality(B.30),theCauchy-Schwarzinequality(B.32)andAssumption 2.1, (cid:107)(cid:69)[XY](cid:107)â¤(cid:69)(cid:107)XY(cid:107)â¤(cid:161)(cid:69)(cid:107)X(cid:107)2(cid:162)1/2(cid:161)(cid:69)(cid:163) Y2(cid:164)(cid:162)1/2<â. Thusthemoments(cid:69)[XY]and(cid:69)(cid:163) XX (cid:48)(cid:164) arefiniteandwelldefined. For part 2, the coefficient Î²=(cid:161)(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:162)â1(cid:69)[XY] is well defined since (cid:161)(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:162)â1 exists under As- sumption2.1. Part3followsfromDefinition2.5andpart2. Forpart4,firstnotethat (cid:104) (cid:105) (cid:69)(cid:163) e2(cid:164)=(cid:69) (cid:161) Y âX (cid:48)Î²(cid:162)2 =(cid:69)(cid:163) Y2(cid:164)â2(cid:69)(cid:163) YX (cid:48)(cid:164)Î²+Î²(cid:48)(cid:69)(cid:163) XX (cid:48)(cid:164)Î² =(cid:69)(cid:163) Y2(cid:164)â(cid:69)(cid:163) YX (cid:48)(cid:164)(cid:161)(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:162)â1(cid:69)[XY] â¤(cid:69)(cid:163) Y2(cid:164)<â. Thefirstinequalityholdsbecause(cid:69)(cid:163) YX (cid:48)(cid:164)(cid:161)(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:162)â1(cid:69)[XY]isaquadraticformandthereforenecessar- ilynon-negative.Second,bytheexpectationinequality(B.30),theCauchy-Schwarzinequality(B.32)and Assumption2.1, (cid:107)(cid:69)[Xe](cid:107)â¤(cid:69)(cid:107)Xe(cid:107)=(cid:161)(cid:69)(cid:107)X(cid:107)2(cid:162)1/2(cid:161)(cid:69)(cid:163) e2(cid:164)(cid:162)1/2<â. Itfollowsthattheexpectation(cid:69)[Xe]isfinite,andiszerobythecalculation(2.26)",
    "page": 79,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Itfollowsthattheexpectation(cid:69)[Xe]isfinite,andiszerobythecalculation(2.26). Forpart6,applyingMinkowskiâsinequality(B.34)toe=Y âX (cid:48)Î², (cid:161)(cid:69)|e|r(cid:162)1/r =(cid:161)(cid:69)(cid:175) (cid:175)Y âX (cid:48)Î²(cid:175) (cid:175) r(cid:162)1/r â¤(cid:161)(cid:69)|Y|r(cid:162)1/r+(cid:161)(cid:69)(cid:175) (cid:175)X (cid:48)Î²(cid:175) (cid:175) r(cid:162)1/r â¤(cid:161)(cid:69)|Y|r(cid:162)1/r+(cid:161)(cid:69)(cid:107)X(cid:107)r(cid:162)1/r(cid:176) (cid:176) Î²(cid:176) (cid:176) <â, thefinalinequalitybyassumption. â  _____________________________________________________________________________________________",
    "page": 79,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 60 2.34 Exercises Exercise2.1 Find(cid:69)[(cid:69)[(cid:69)[Y |X ,X ,X ]|X ,X ]|X ]. 1 2 3 1 2 1 Exercise2.2 If(cid:69)[Y |X]=a+bX,find(cid:69)[YX]asafunctionofmomentsofX. Exercise2.3 ProveTheorem2.4.4usingthelawofiteratedexpectations. Exercise2.4 Suppose that the random variables Y and X only take the values 0 and 1, and have the followingjointprobabilitydistribution X =0 X =1 Y =0 .1 .2 Y =1 .4 .3 Find(cid:69)[Y |X],(cid:69)(cid:163) Y2|X (cid:164) andvar[Y |X]forX =0andX =1. Exercise2.5 ShowthatÏ2(X)isthebestpredictorofe2givenX: (a) Writedownthemean-squarederrorofapredictorh(X)fore2. (b) Whatdoesitmeantobepredictinge2? (c) ShowthatÏ2(X)minimizesthemean-squarederrorandisthusthebestpredictor. Exercise2.6 UseY =m(X)+etoshowthatvar[Y]=var[m(X)]+Ï2. Exercise2.7 ShowthattheconditionalvariancecanbewrittenasÏ2(X)=(cid:69)(cid:163) Y2|X (cid:164)â((cid:69)[Y |X])2. Exercise2.8 SupposethatY isdiscrete-valued,takingvaluesonlyonthenon-negativeintegers,andthe conditionaldistributionofY givenX =xisPoisson: exp (cid:161)âx (cid:48)Î²(cid:162)(cid:161) x (cid:48)Î²(cid:162)j (cid:80)(cid:163) Y =j |X =x (cid:164)= , j =0,1,2,... j! Compute(cid:69)[Y |X]andvar[Y |X].DoesthisjustifyalinearregressionmodeloftheformY =X (cid:48)Î²+e? Hint:If(cid:80)(cid:163) Y =j (cid:164)= exp(âÎ»)Î»j then(cid:69)[Y]=Î»andvar[Y]=Î». j! Exercise2.9 Supposeyouhavetworegressors: X isbinary(takesvalues0and1)and X iscategorical 1 2 with3categories(A,B,C).Write(cid:69)[Y |X ,X ]asalinearregression. 1 2 Exercise2.10 TrueorFalse.IfY =XÎ²+e,X â(cid:82),and(cid:69)[e|X]=0,then(cid:69)(cid:163) X2e (cid:164)=0. Exercise2.11 TrueorFalse.IfY =XÎ²+e,X â(cid:82),and(cid:69)[Xe]=0,then(cid:69)(cid:163) X2e (cid:164)=0. Exercise2.12 TrueorFalse.IfY =X (cid:48)Î²+eand(cid:69)[e|X]=0,theneisindependentofX. Exercise2.13 TrueorFalse.IfY =X (cid:48)Î²+eand(cid:69)[Xe]=0,then(cid:69)[e|X]=0. Exercise2.14 TrueorFalse.IfY =X (cid:48)Î²+e,(cid:69)[e|X]=0,and(cid:69)(cid:163) e2|X (cid:164)=Ï2,theneisindependentofX.",
    "page": 80,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER2. CONDITIONALEXPECTATIONANDPROJECTION 61 Exercise2.15 Considertheintercept-onlymodelY =Î±+e withÎ±thebestlinearpredictor. Showthat Î±=(cid:69)[Y]. Exercise2.16 Let X andY havethejointdensity f (cid:161) x,y (cid:162)= 3(cid:161) x2+y2(cid:162) on0â¤x â¤1,0â¤y â¤1.Compute 2 thecoefficientsofthebestlinearpredictorY =Î±+Î²X+e.Computetheconditionalexpectationm(x)= (cid:69)[Y |X =x].Arethebestlinearpredictorandconditionalexpectationdifferent? Exercise2.17 LetX bearandomvariablewithÂµ=(cid:69)[X]andÏ2=var[X].Define (cid:195) (cid:33) g (cid:161) x,Âµ,Ï2(cid:162)= (cid:161) xâ x Âµ â (cid:162)2 Âµ âÏ2 . Showthat(cid:69)(cid:163) g(X,m,s) (cid:164)=0ifandonlyifm=Âµands=Ï2. Exercise2.18 SupposethatX =(1,X ,X )whereX =Î± +Î± X isalinearfunctionofX . 2 3 3 1 2 2 2 (a) ShowthatQ =(cid:69)(cid:163) XX (cid:48)(cid:164) isnotinvertible. XX (b) Usealineartransformationof X tofindanexpressionforthebestlinearpredictorofY given X. (Beexplicit,donotjustusethegeneralizedinverseformula.) Exercise2.19 Show(2.47)-(2.48),namelythatfor (cid:104) (cid:105) d(Î²)=(cid:69) (cid:161) m(X)âX (cid:48)Î²(cid:162)2 then Î²=argmind(b)=(cid:161)(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:162)â1(cid:69)[Xm(X)]=(cid:161)(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:162)â1(cid:69)[XY]. bâ(cid:82)k Hint:Toshow(cid:69)[Xm(X)]=(cid:69)[XY]usethelawofiteratedexpectations. Exercise2.20 Verifythat(2.57)holdswithm(X)definedin(2.6)when(Y,X)haveajointdensity f(y,x). Exercise2.21 Considertheshortandlongprojections Y =XÎ³ +e 1 Y =XÎ² +X2Î² +u 1 2 (a) UnderwhatconditiondoesÎ³ =Î² ? 1 1 (b) TakethelongprojectionisY =XÎ¸ +X3Î¸ +v.IsthereaconditionunderwhichÎ³ =Î¸ ? 1 2 1 1 Exercise2.22 Takethehomoskedasticmodel Y =X (cid:48)Î² +X (cid:48)Î² +e 1 1 2 2 (cid:69)[e|X ,X ]=0 1 2 (cid:69)(cid:163) e2|X ,X (cid:164)=Ï2 1 2 (cid:69)[X |X ]=ÎX . 2 1 1 AssumeÎ(cid:54)=0. SupposetheparameterÎ² isofinterest. WeknowthattheexclusionofX createsomited 1 2 variablebiasintheprojectioncoefficientonX .Italsochangestheequationerror.Ourquestionis:what 2 istheeffectonthehomoskedasticitypropertyoftheinducedequationerror? Doestheexclusionof X 2 induceheteroskedasticityornot?Bespecific.",
    "page": 81,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Chapter 3 The Algebra of Least Squares 3.1 Introduction Inthischapterweintroducethepopularleastsquaresestimator.Mostofthediscussionwillbealge- braic,withquestionsofdistributionandinferencedeferredtolaterchapters. 3.2 Samples InSection2.18wederivedanddiscussedthebestlinearpredictorofY givenX forapairofrandom variables(Y,X)â(cid:82)Ã(cid:82)k andcalledthisthelinearprojectionmodel.Wearenowinterestedinestimating theparametersofthismodel,inparticulartheprojectioncoefficient Î²=(cid:161)(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:162)â1(cid:69)[XY]. (3.1) WecanestimateÎ²fromsampleswhichincludejointmeasurementsof(Y,X).Forexample,suppos- ingweareinterestedinestimatingawageequation,wewoulduseadatasetwithobservationsonwages (orweeklyearnings),education,experience(orage),anddemographiccharacteristics(gender,race,lo- cation).OnepossibledatasetistheCurrentPopulationSurvey(CPS),asurveyofU.S.householdswhich includesquestionsonemployment,income,education,anddemographiccharacteristics. Notationally we wish to distinguish observations (realizations) from the underlying random vari- ables.Therandomvariablesare(Y,X).Theobservationsare(Y ,X ).Fromthevantageoftheresearcher i i thelatterare numbers. Fromthevantage ofstatisticaltheory weview themasrealizationsofrandom variables. Forindividualobservationsweappendasubscripti whichrunsfrom1ton,thustheith ob- servationis(Y ,X ).Thenumbernisthesamplesize.Thedatasetorsampleis{(Y ,X ):i =1,...,n}. i i i i Fromtheviewpointofempiricalanalysisadatasetisanarrayofnumbers. Itistypicallyorganized as a table where each column is a variable and each row is an observation. For empirical analysis the datasetisfixedinthesensethattheyarenumberspresentedtotheresearcher.Forstatisticalanalysiswe viewthedatasetasrandom,ormorepreciselyasarealizationofarandomprocess. Theindividualobservationscouldbedrawsfromacommon(homogeneous)distributionorcould bedrawsfromheterogeneousdistributions.Thesimplestapproachistoassumehomogeneityâthatthe observationsarerealizationsfromanidenticalunderlyingpopulationF. Assumption3.1 The variables {(Y ,X ),...,(Y ,X ),...,(Y ,X )} are identically dis- 1 1 i i n n tributed;theyaredrawsfromacommondistributionF. 62",
    "page": 82,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 63 Thisassumptiondoesnotneedtobeviewedasliterallytrue.Ratheritisausefulmodelingdeviceso thatparameterssuchasÎ²arewelldefined. Thisassumptionshouldbeinterpretedashowweviewan observationapriori,beforeweactuallyobserveit. IfItellyouthatwehaveasamplewithn=59obser- vationssetinnoparticularorder,thenitmakessensetoviewtwoobservations,say17and58,asdraws fromthesamedistribution.Wehavenoreasontoexpectanythingspecialabouteitherobservation. IneconometrictheorywerefertotheunderlyingcommondistributionF asthepopulation. Some authors prefer the label the data-generating-process (DGP). You can think of it as a theoretical con- ceptoraninfinitely-largepotentialpopulation. Incontrast,werefertotheobservationsavailabletous {(Y ,X ):i =1,...,n}asthesampleordataset. Insomecontextsthedatasetconsistsofallpotentialob- i i servations,forexampleadministrativetaxrecordsmaycontaineverysingletaxpayerinapoliticalunit. Eveninthiscaseweviewtheobservationsasiftheyarerandomdrawsfromanunderlyinginfinitely-large populationasthiswillallowustoapplythetoolsofstatisticaltheory. The linear projection model applies to the random variables (Y,X). This is the probability model describedinSection2.18.Themodelis Y =X (cid:48)Î²+e (3.2) wherethelinearprojectioncoefficientÎ²isdefinedas Î²=argminS(b), (3.3) bâ(cid:82)k theminimizeroftheexpectedsquarederror (cid:104) (cid:105) S(Î²)=(cid:69) (cid:161) Y âX (cid:48)Î²(cid:162)2 . (3.4) Thecoefficienthastheexplicitsolution(3.1). 3.3 MomentEstimators WewanttoestimatethecoefficientÎ²definedin(3.1)fromthesampleofobservations. Noticethat Î²iswrittenasafunctionofcertainpopulationexpectations. Inthiscontextanappropriateestimatoris thesamefunctionofthesamplemoments.Letâsexplainthisindetail. Tostart,supposethatweareinterestedinthepopulationmeanÂµofarandomvariableY withdistri- butionfunctionF (cid:90) â Âµ=(cid:69)[Y]= ydF(y). (3.5) ââ TheexpectationÂµisafunctionofthedistributionF. ToestimateÂµgivenn randomvariablesY fromF i anaturalestimatoristhesamplemean 1 (cid:88) n Âµ=Y = Y . (cid:98) i n i=1 Notice that we have written this using two pieces of notation. The notation Y with the bar on top is conventional for a sample mean. The notation Âµ with the hat â^â is conventional in econometrics to (cid:98) denoteanestimatoroftheparameterÂµ.InthiscaseY istheestimatorofÂµ,soÂµandY arethesame.The (cid:98) samplemeanY canbeviewedasthenaturalanalogofthepopulationmean(3.5)becauseY equalsthe expectation(3.5)withrespecttotheempiricaldistributionâthediscretedistributionwhichputsweight 1/noneachobservationY .ThereareotherjustificationsforY asanestimatorforÂµ.Wewilldeferthese i discussionsfornow.Sufficeittosaythatitistheconventionalestimator.",
    "page": 83,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 64 Nowsupposethatweareinterestedinasetofpopulationexpectationsofpossiblynonlinearfunc- tionsofarandomvectorY,sayÂµ=(cid:69)[h(Y)].Forexample,wemaybeinterestedinthefirsttwomoments ofY,(cid:69)[Y]and(cid:69)(cid:163) Y2(cid:164) .Inthiscasethenaturalestimatoristhevectorofsamplemeans, 1 (cid:88) n Âµ= h(Y ). (cid:98) i n i=1 We call Âµ the moment estimator for Âµ. For example, if h(y)=(y,y2) (cid:48) then Âµ =n â1(cid:80)n Y and Âµ = (cid:98) (cid:98)1 i=1 i (cid:98)2 n â1(cid:80)n Y2. i=1 i Nowsupposethatweareinterestedinanonlinearfunctionofasetofmoments. Forexample,con- siderthevarianceofY Ï2=var[Y]=(cid:69)(cid:163) Y2(cid:164)â((cid:69)[Y])2. In general, many parameters of interest can be written as a function of moments of Y. Notationally, Î²=g(Âµ)andÂµ=(cid:69)[h(Y)]. Here,Y aretherandomvariables,h(Y)arefunctions(transformations)ofthe randomvariables,andÂµistheexpectationofthesefunctions. Î²istheparameterofinterest,andisthe (nonlinear)functiong(Â·)oftheseexpectations. InthiscontextanaturalestimatorofÎ²isobtainedbyreplacingÂµwithÂµ (cid:98) . ThusÎ² (cid:98) =g (cid:161)Âµ (cid:98) (cid:162) . Theesti- matorÎ² (cid:98)isoftencalledaplug-inestimator. WealsocallÎ² (cid:98)amoment,ormoment-based,estimatorofÎ² sinceitisanaturalextensionofthemomentestimatorÂµ. (cid:98) TaketheexampleofthevarianceÏ2=var[Y].Itsmomentestimatoris (cid:195) (cid:33)2 Ï2=Âµ âÂµ2= 1 (cid:88) n Y2â 1 (cid:88) n Y . (cid:98) (cid:98)2 (cid:98)1 n i n i i=1 i=1 ThisisnottheonlypossibleestimatorforÏ2(thereisalsothewell-knownbias-correctedestimator)but Ï2isastraightforwardandsimplechoice. (cid:98) 3.4 LeastSquaresEstimator ThelinearprojectioncoefficientÎ²isdefinedin(3.3)astheminimizeroftheexpectedsquarederror S(Î²) defined in (3.4). For given Î², the expected squared error is the expectation of the squared error (cid:161) Y âX (cid:48)Î²(cid:162)2 .ThemomentestimatorofS(Î²)isthesampleaverage: S(cid:98)(Î²)= n 1 (cid:88) n (cid:161) Y i âX i (cid:48)Î²(cid:162)2= n 1 SSE(Î²) (3.6) i=1 where n SSE(Î²)= (cid:88)(cid:161) Y âX (cid:48)Î²(cid:162)2 i i i=1 iscalledthesumofsquarederrorsfunction. SinceS(cid:98)(Î²)isasampleaveragewecaninterpretitasanestimatoroftheexpectedsquarederrorS(Î²). ExaminingS(cid:98)(Î²)asafunctionofÎ²isinformativeabouthowS(Î²)varieswithÎ².Sincetheprojectioncoef- ficientminimizesS(Î²)ananalogestimatorminimizes(3.6). WedefinetheestimatorÎ² (cid:98)astheminimizerofS(cid:98)(Î²).",
    "page": 84,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 65 Definition3.1 The least squares estimator is Î² (cid:98) = argminS(cid:98)(Î²) Î²â(cid:82)k whereS(cid:98)(Î²)= n 1(cid:88) n (cid:161) Y i âX i (cid:48)Î²(cid:162)2 . i=1 AsS(cid:98)(Î²)isascalemultipleofSSE(Î²)wemayequivalentlydefineÎ² (cid:98)astheminimizerofSSE(Î²).Hence Î² (cid:98)iscommonlycalledtheleastsquares(LS)estimatorofÎ². Theestimatorisalsocommonlyreferedto astheordinaryleastsquares(OLS)estimator.Fortheoriginofthislabelseethehistoricaldiscussionon Adrien-MarieLegendrebelow.Here,asiscommonineconometrics,weputahatâ^âovertheparameter Î²toindicatethatÎ² (cid:98)isasampleestimatorofÎ².Thisisahelpfulconvention. Justbyseeingthesymbol Î² (cid:98)wecanimmediatelyinterpretitasanestimator(becauseofthehat)oftheparameterÎ². Sometimes whenwewanttobeexplicitabouttheestimationmethod,wewillwriteÎ² (cid:98)ols tosignifythatitistheOLS estimator.ItisalsocommontoseethenotationÎ² (cid:98)n ,wherethesubscriptânâindicatesthattheestimator dependsonthesamplesizen. ItisimportanttounderstandthedistinctionbetweenpopulationparameterssuchasÎ²andsample estimatorssuchasÎ² (cid:98). ThepopulationparameterÎ²isanon-randomfeatureofthepopulationwhilethe sampleestimatorÎ² (cid:98)isarandomfeatureofarandomsample.Î²isfixed,whileÎ² (cid:98)variesacrosssamples. 3.5 SolvingforLeastSquareswithOneRegressor Forsimplicity,westartbyconsideringthecasek=1sothatthereisascalarregressorX andascalar coefficientÎ².Toillustrate,Figure3.1(a)displaysascatterplot1of20pairs(Y ,X ). i i The sum of squared errors SSE(Î²) is a function of Î². Given Î² we calculate the âerrorâ Y âX Î² by i i takingtheverticaldistancebetweenY and X Î². ThiscanbeseeninFigure3.1(a)bytheverticallines i i whichconnecttheobservationstothestraightline.TheseverticallinesaretheerrorsY âX Î².Thesum i i ofsquarederrorsisthesumofthe20squaredlengths. Thesumofsquarederrorsisthefunction (cid:195) (cid:33) (cid:195) (cid:33) (cid:195) (cid:33) n n n n SSE(Î²)= (cid:88)(cid:161) Y âX Î²(cid:162)2= (cid:88) Y2 â2Î² (cid:88) X Y +Î²2 (cid:88) X2 . i i i i i i i=1 i=1 i=1 i=1 ThisisaquadraticfunctionofÎ².ThesumofsquarederrorfunctionisdisplayedinFigure3.1(b)overthe range[2,4]. ThecoefficientÎ²rangesalongthex-axis. ThesumofsquarederrorsSSE(Î²)asafunctionof Î²isdisplayedonthey-axis. TheOLSestimatorÎ² (cid:98)minimizesthisfunction.Fromelementaryalgebraweknowthattheminimizer ofthequadraticfunctionaâ2bx+cx2isx=b/c.ThustheminimizerofSSE(Î²)is (cid:80)n X Y Î² (cid:98) = i=1 i i . (3.7) (cid:80)n X2 i=1 i Forexample,theminimizerofthesumofsquarederrorfunctiondisplayedinFigure3.1(b)isÎ² (cid:98) =3.07, andismarkedonthex-axis. Theintercept-onlymodelisthespecialcaseX =1.Inthiscasewefind i Î² (cid:98) = (cid:80) (cid:80) n i n =1 1 1 Y 2 i = n 1 (cid:88) n Y i =Y, (3.8) i=1 i=1 1TheobservationsweregeneratedbysimulationasXâ¼U[0,1]andY â¼N[3X,1].",
    "page": 85,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 66 l l l l l l l l l l l l l l l l l l l l 2.0 2.5 3.0 3.5 4.0 (a)DeviationfromFittedLine 81 61 41 21 01 b^ (b)SumofSquaredErrorFunction Figure3.1:RegressionWithOneRegressor â thesamplemeanofY .Here, asiscommon, weputabarâ âoverY toindicatethatthequantityisa i samplemean.ThisshowsthattheOLSestimatorintheintercept-onlymodelisthesamplemean. Technically, theestimatorÎ² (cid:98)in(3.7)onlyexistsifthedenominatorisnon-zero. Sinceitisasumof squaresitisnecessarilynon-negative.ThusÎ² (cid:98)existsif (cid:80)n i=1 X i 2>0. 3.6 SolvingforLeastSquareswithMultipleRegressors Wenowconsiderthecasewithk>1sothatthecoefficientÎ²â(cid:82)k isavector. Toillustrate,Figure3.2(a)displaysascatterplotof100triples(Y ,X ,X ). Theregressionfunction i 1i 2i x (cid:48)Î²=x Î² +x Î² isa2-dimensionalsurfaceandisshownastheplaneinFigure3.2(a). 1 1 2 2 Y 470 460 450 440 430 420 XX22 4.0 4.0 3.5 3.0 3.0 3.5 b 1 2.5 2.0 2.0 2.5 b 2 XXX111 b 1 (b)SumofSquaredErrorFunction (a)RegressionPlane b 2 2.5 3.0 3.5 4.0 5.3 0.3 5.2 b^ (c)SSEContour Figure3.2:RegressionwithTwoVariables The sum of squared errors SSE(Î²) is a function of the vector Î². For any Î² the error Y âX (cid:48)Î² is the i i",
    "page": 86,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 67 verticaldistancebetweenY andX (cid:48)Î².ThiscanbeseeninFigure3.2(a)bytheverticallineswhichconnect i i theobservationstotheplane. Asinthesingleregressorcasetheseverticallinesaretheerrorse =Y â i i X (cid:48)Î².Thesumofsquarederrorsisthesumofthe100squaredlengths. i Thesumofsquarederrorscanbewrittenas n n n SSE(Î²)= (cid:88) Y2â2Î²(cid:48)(cid:88) X Y +Î²(cid:48)(cid:88) X X (cid:48)Î². i i i i i i=1 i=1 i=1 As in the single regressor case this is a quadratic function in Î². The difference is that in the multiple regressorcasethisisavector-valuedquadraticfunction.Tovisualizethesumofsquarederrorsfunction Figure 3.2(b) displays SSE(Î²). Another way to visualize a 3-dimensional surface is by a contour plot. A contour plot of the same SSE(Î²) function is shown in Figure 3.2(c). The contour lines are points in the (Î² ,Î² ) space where SSE(Î²) takes the same value. The contour lines are elliptical since SSE(Î²) is 1 2 quadratic. TheleastsquaresestimatorÎ² (cid:98)minimizesSSE(Î²).Asimplewaytofindtheminimumisbysolvingthe first-orderconditions.Thelatterare â n n 0= âÎ² SSE(Î² (cid:98))=â2 (cid:88) X i Y i +2 (cid:88) X i X i (cid:48)Î² (cid:98). (3.9) i=1 i=1 Wehavewrittenthisusingasingleexpression,butitisactuallyasystemofkequationswithkunknowns (theelementsofÎ² (cid:98)). The solution for Î² (cid:98)may be found by solving the system of k equations in (3.9). We can write this solutioncompactlyusingmatrixalgebra.Dividing(3.9)by2weobtain n n (cid:88) X i X i (cid:48)Î² (cid:98) = (cid:88) X i Y i . (3.10) i=1 i=1 Thisisasystemofequationsoftheform Ab=c where A iskÃk andb andc arekÃ1. Thesolutionis b=A â1c,andcanbeobtainedbypre-multiplying Ab=c by A â1andusingthematrixinverseproperty A â1A=I .Appliedto(3.10)wefindanexplicitformulafortheleastsquaresestimator k (cid:195) (cid:33)â1(cid:195) (cid:33) n n Î² (cid:98) = (cid:88) X i X i (cid:48) (cid:88) X i Y i . (3.11) i=1 i=1 ThisisthenaturalestimatorofthebestlinearprojectioncoefficientÎ²definedin(3.3),andcouldalsobe calledthelinearprojectionestimator. RecallthatweclaimedthatÎ² (cid:98)in(3.11)istheminimizerofSSE(Î²), andfounditbysolvingthefirst- orderconditions.Tobecompleteweshouldverifythesecond-orderconditions.Wecalculatethat â2 n SSE(Î²)=2 (cid:88) X X (cid:48)>0 âÎ²âÎ²(cid:48) i i i=1 whichisapositivedefinitematrix. Thisshowsthatthesecond-orderconditionforminimizationissat- isfiedsoÎ² (cid:98)isindeedtheuniqueminimizerofSSE(Î²). ReturningtotheexamplesumofsquarederrorsfunctionSSE(Î²)displayedinFigure3.2(b),theleast squaresestimatorÎ² (cid:98)isthethepair(Î² (cid:98)1 ,Î² (cid:98)2 )whichminimizethisfunction;visuallyitisthelowspotinthe 3-dimensionalgraph,andismarkedinFigure3.2(c)asthecenterpointofthecontourplots. Returningtoequation(3.11),supposethatk=1. Inthiscase X isscalarso X X (cid:48)=X2. Then(3.11) i i i i simplifiestotheexpression(3.7)previouslyderived. Theexpression(3.11)isanotationallysimplegen- eralizationbutrequiresacarefulattentiontovectorandmatrixmanipulations.",
    "page": 87,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 68 Alternatively,equation(3.1)writestheprojectioncoefficientÎ²asanexplicitfunctionofthepopula- tionmomentsQ andQ .Theirmomentestimatorsarethesamplemoments XY XX 1 (cid:88) n Q(cid:98)XY = X i Y i n i=1 Q(cid:98)XX = n 1 (cid:88) n X i X i (cid:48) . i=1 ThemomentestimatorofÎ²replacesthepopulationmomentsin(3.1)withthesamplemoments: Î² (cid:98) =Q(cid:98) â X 1 X Q(cid:98)XY (cid:195) (cid:33)â1(cid:195) (cid:33) = 1 (cid:88) n X X (cid:48) 1 (cid:88) n X Y n i i n i i i=1 i=1 (cid:195) (cid:33)â1(cid:195) (cid:33) n n = (cid:88) X X (cid:48) (cid:88) X Y i i i i i=1 i=1 whichisidenticalwith(3.11). Technically,theestimatorÎ² (cid:98)isuniqueandequals(3.11)onlyiftheinvertedmatrixisactuallyinvert- ible,whichholdsif(andonlyif)thismatrixispositivedefinite. Thisexcludesthecasethat X contains i redundantregressors.ThiswillbediscussedfurtherinSection3.24. Theorem3.1 If (cid:80)n X X (cid:48)>0,theleastsquaresestimatorisuniqueandequals i=1 i i (cid:195) (cid:33)â1(cid:195) (cid:33) n n Î² (cid:98) = (cid:88) X i X i (cid:48) (cid:88) X i Y i . i=1 i=1 Adrien-MarieLegendre The method of least squares was first published in 1805 by the French mathe- maticianAdrien-MarieLegendre(1752-1833). Legendreproposedleastsquares asasolutiontothealgebraicproblemofsolvingasystemofequationswhenthe numberofequationsexceededthenumberofunknowns.Thiswasavexingand commonprobleminastronomicalmeasurement.AsviewedbyLegendre,(3.2)is asetofnequationswithkunknowns.Astheequationscannotbesolvedexactly, LegendreâsgoalwastoselectÎ²tomakethesetoferrorsassmallaspossible. He proposedthesumofsquarederrorcriterionandderivedthealgebraicsolution presented above. As he noted, the first-order conditions (3.9) is a system of k equationswithk unknownswhichcanbesolvedbyâordinaryâmethods. Hence the method became known as OrdinaryLeastSquares and to this day we still usetheabbreviationOLStorefertoLegendreâsestimationmethod.",
    "page": 88,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 69 3.7 Illustration Weillustratetheleastsquaresestimatorinpracticewiththedatasetusedtocalculatetheestimates reportedinChapter2. ThisistheMarch2009CurrentPopulationSurvey,whichhasextensiveinforma- tionontheU.S.population.ThisdatasetisdescribedinmoredetailinSection3.22.Forthisillustration weusethesub-sampleofmarried(spousepresent)Blackfemalewageearnerswith12yearspotential workexperience.Thissub-samplehas20observations2. InTable3.1wedisplaytheobservationsforreference. Eachrowisanindividualobservationwhich arethedataforanindividualperson.Thecolumnscorrespondtothevariables(measurements)forthe individuals. Thesecondcolumnisthereportedwage(totalannualearningsdividedbyhoursworked). The third column is the natural logarithm of the wage. The fourth column is years of education. The fifthandsixcolumnsarefurthertransformations,specificallythesquareofeducationandtheproductof educationandlog(wage).Thebottomrowarethesumsoftheelementsinthatcolumn. Table3.1:ObservationsFromCPSDataSet Observation wage log(wage) education education2 educationÃlog(wage) 1 37.93 3.64 18 324 65.44 2 40.87 3.71 18 324 66.79 3 14.18 2.65 13 169 34.48 4 16.83 2.82 16 256 45.17 5 33.17 3.50 16 256 56.03 6 29.81 3.39 18 324 61.11 7 54.62 4.00 16 256 64.00 8 43.08 3.76 18 324 67.73 9 14.42 2.67 12 144 32.03 10 14.90 2.70 16 256 43.23 11 21.63 3.07 18 324 55.44 12 11.09 2.41 16 256 38.50 13 10.00 2.30 13 169 29.93 14 31.73 3.46 14 196 48.40 15 11.06 2.40 12 144 28.84 16 18.75 2.93 16 256 46.90 17 27.35 3.31 14 196 46.32 18 24.04 3.18 16 256 50.76 19 36.06 3.59 18 324 64.53 20 23.08 3.14 16 256 50.22 Sum 515 62.64 314 5010 995.86 Putting the variables into the standard regression notation, let Y be log(wage) and X be years of i i educationandanintercept.ThenfromthecolumnsumsinTable3.1wehave (cid:88) n (cid:181) 995.86 (cid:182) X Y = i i 62.64 i=1 and (cid:88) n X X (cid:48)= (cid:181) 5010 314 (cid:182) . i i 314 20 i=1 2Thissamplewasselectedspecificallysothatithasasmallnumberofobservations,facilitatingexposition.",
    "page": 89,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 70 Takingtheinverseweobtain (cid:195) (cid:88) n X X (cid:48) (cid:33)â1 = (cid:181) 0.0125 â0.196 (cid:182) . i i â0.196 3.124 i=1 Thusbymatrixmultiplication (cid:181) 0.0125 â0.196 (cid:182)(cid:181) 995.86 (cid:182) (cid:181) 0.155 (cid:182) Î² (cid:98) = = . â0.196 3.124 62.64 0.698 InpracticetheregressionestimatesÎ² (cid:98)arecomputedbycomputersoftwarewithouttheusertaking theexplicitstepslistedabove.However,itisusefultounderstandthattheleastsquaresestimatorcanbe calculatedbysimplealgebraicoperations. IfyourdataisinaspreadsheetsimilartoTable3.1,thenthe listedtransformations(logarithm,squares,cross-products,columnsums)canbecomputedbyspread- sheetoperations. Î² (cid:98)couldthenbecalculatedbymatrixinversionandmultiplication. Onceagain,thisis rarelydonebyappliedeconomistssincecomputersoftwareisavailabletoeasetheprocess. Weoftenwritetheestimatedequationusingtheformat lo(cid:225)g(wage)=0.155education+0.698. (3.12) Aninterpretationoftheestimatedequationisthateachyearofeducationisassociatedwitha16%in- creaseinmeanwages. Equation(3.12)iscalledabivariateregressionastherearetwovariables. Itisalsocalledasimple regressionasthereisasingleregressor. Amultipleregressionhastwoormoreregressorsandallowsa moredetailedinvestigation. Letâstakeanexamplesimilarto(3.12)butincludealllevelsofexperience. Thistimeweusethesub-sampleofsingle(nevermarried)Asianmenwhichhas268observations. In- cluding as regressors years of potential work experience (experience) and its square (experience2/100) (wedivideby100tosimplifyreporting)weobtaintheestimates lo(cid:225)g(wage)=0.143education+0.036experienceâ0.071experience2/100+0.575. (3.13) Theseestimatessuggesta14%increaseinmeanwagesperyearofeducationholdingexperiencecon- stant. 3.8 LeastSquaresResiduals Asaby-productofestimationwedefinethefittedvalueY(cid:98)i =X i (cid:48)Î² (cid:98)andtheresidual e (cid:98)i =Y i âY(cid:98)i =Y i âX i (cid:48)Î² (cid:98). (3.14) SometimesY(cid:98)i iscalledthepredictedvaluebutthisisamisleadinglabel.ThefittedvalueY(cid:98)i isafunction oftheentiresampleincludingY , andthuscannotbeinterpretedasavalidpredictionofY . Itisthus i i moreaccuratetodescribeY(cid:98)i asafittedratherthanapredictedvalue. NotethatY i =Y(cid:98)i +e (cid:98)i and Y i =X i (cid:48)Î² (cid:98) +e (cid:98)i . (3.15) Wemakeadistinctionbetweentheerrore andtheresiduale .Theerrore isunobservablewhilethe i (cid:98)i i residuale isanestimator.Thesetwovariablesarefrequentlymislabeledwhichcancauseconfusion. (cid:98)i Equation(3.9)impliesthat n (cid:88) X e =0. (3.16) i(cid:98)i i=1",
    "page": 90,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 71 Toseethisbyadirectcalculation,using(3.14)and(3.11), n n (cid:88) X i e (cid:98)i = (cid:88) X i (cid:161) Y i âX i (cid:48)Î² (cid:98) (cid:162) i=1 i=1 n n = (cid:88) X i Y i â (cid:88) X i X i (cid:48)Î² (cid:98) i=1 i=1 (cid:195) (cid:33)â1(cid:195) (cid:33) n n n n = (cid:88) X Y â (cid:88) X X (cid:48) (cid:88) X X (cid:48) (cid:88) X Y i i i i i i i i i=1 i=1 i=1 i=1 n n (cid:88) (cid:88) = X Y â X Y =0. i i i i i=1 i=1 WhenX containsaconstantanimplicationof(3.16)is i 1 (cid:88) n e =0. (3.17) (cid:98)i n i=1 Thustheresidualshaveasamplemeanofzeroandthesamplecorrelationbetweentheregressorsand theresidualiszero.Thesearealgebraicresultsandholdtrueforalllinearregressionestimates. 3.9 DemeanedRegressors Sometimesitisusefultoseparatetheconstantfromtheotherregressorsandwritethelinearprojec- tionequationintheformat Y =X (cid:48)Î²+Î±+e i i i whereÎ±istheinterceptand X doesnotcontainaconstant. Theleastsquaresestimatesandresiduals i canbewrittenasY i =X i (cid:48)Î² (cid:98) +Î± (cid:98) +e (cid:98)i . Inthiscase(3.16)canbewrittenastheequationsystem n (cid:88)(cid:161) Y i âX i (cid:48)Î² (cid:98) âÎ± (cid:98) (cid:162)=0 i=1 n (cid:88) X i (cid:161) Y i âX i (cid:48)Î² (cid:98) âÎ± (cid:98) (cid:162)=0. i=1 Thefirstequationimplies (cid:48) Î± (cid:98) =Y âX Î² (cid:98). Subtractingfromthesecondweobtain (cid:88) n (cid:179)(cid:179) (cid:180) (cid:179) (cid:180)(cid:48) (cid:180) X i Y i âY â X i âX Î² (cid:98) =0. i=1 SolvingforÎ² (cid:98)wefind (cid:195) (cid:33)â1(cid:195) (cid:33) (cid:88) n (cid:179) (cid:180)(cid:48) (cid:88) n (cid:179) (cid:180) Î² (cid:98) = X i X i âX X i Y i âY i=1 i=1 (cid:195) (cid:33)â1(cid:195) (cid:33) (cid:88) n (cid:179) (cid:180)(cid:179) (cid:180)(cid:48) (cid:88) n (cid:179) (cid:180)(cid:179) (cid:180) = X âX X âX X âX Y âY . (3.18) i i i i i=1 i=1 ThustheOLSestimatorfortheslopecoefficientsisOLSwithdemeaneddataandnointercept. Therepresentation(3.18)isknownasthedemeanedformulafortheleastsquaresestimator.",
    "page": 91,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 72 3.10 ModelinMatrixNotation Formanypurposes,includingcomputation,itisconvenienttowritethemodelandstatisticsinma- trixnotation. Then linearequationsY =X (cid:48)Î²+e makeasystemofn equations. Wecanstackthesen i i i equationstogetheras Y =X (cid:48)Î²+e 1 1 1 Y =X (cid:48)Î²+e 2 2 2 . . . Y =X (cid:48)Î²+e . n n n Define ï£« ï£¶ ï£« (cid:48) ï£¶ ï£« ï£¶ Y X e 1 1 1 ï£¬ Y ï£· ï£¬ X (cid:48) ï£· ï£¬ e ï£· Y =ï£¬ ï£¬ . 2 ï£· ï£·, X =ï£¬ ï£¬ . 2 ï£· ï£·, e=ï£¬ ï£¬ . 2 ï£· ï£·. ï£¬ . . ï£· ï£¬ . . ï£· ï£¬ . . ï£· ï£­ ï£¸ ï£­ ï£¸ ï£­ ï£¸ (cid:48) Y X e n n n Observe that Y and e are nÃ1 vectors and X is an nÃk matrix. The system of n equations can be compactlywritteninthesingleequation Y =XÎ²+e. (3.19) Samplesumscanbewritteninmatrixnotation.Forexample n (cid:88) X X (cid:48)=X (cid:48) X i i i=1 n (cid:88) X Y =X (cid:48) Y. i i i=1 Thereforetheleastsquaresestimatorcanbewrittenas Î² (cid:98) =(cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48) Y (cid:162) . Thematrixversionof(3.15)andestimatedversionof(3.19)is Y =XÎ² (cid:98) + (cid:98) e. Equivalentlytheresidualvectoris (cid:98) e=Y âXÎ² (cid:98). Usingtheresidualvectorwecanwrite(3.16)as X (cid:48) e=0. (cid:98) Itcanalsobeusefultowritethesumofsquarederrorcriterionas SSE (cid:161)Î²(cid:162)=(cid:161) Y âXÎ²(cid:162)(cid:48)(cid:161) Y âXÎ²(cid:162) . Using matrix notation we have simple expressions for most estimators. This is particularly conve- nientforcomputerprogrammingasmostlanguagesallowmatrixnotationandmanipulation.",
    "page": 92,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 73 Theorem3.2 ImportantMatrixExpressions Î² (cid:98) =(cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48) Y (cid:162) (cid:98) e=Y âXÎ² (cid:98) X (cid:48) e=0. (cid:98) EarlyUseofMatrices The earliest known treatment of the use of matrix methods to solvesimultaneoussystemsisfoundinChapter8oftheChinese textTheNineChaptersontheMathematicalArt,writtenbysev- eralgenerationsofscholarsfromthe10th to2nd centuryBCE. 3.11 ProjectionMatrix Definethematrix P=X (cid:161) X (cid:48) X (cid:162)â1 X (cid:48) . Observethat PX =X (cid:161) X (cid:48) X (cid:162)â1 X (cid:48) X =X. This is a property of a projection matrix. More generally, for any matrix Z which can be written as Z =XÎforsomematrixÎ(wesaythatZ liesintherangespaceofX),then PZ =PXÎ=X (cid:161) X (cid:48) X (cid:162)â1 X (cid:48) XÎ=XÎ=Z. As an important example, if we partition the matrix X into two matrices X and X so that X = 1 2 [X X ]thenPX =X .(SeeExercise3.7.) 1 2 1 1 TheprojectionmatrixP hasthealgebraicpropertythatitisidempotent:PP=P.SeeTheorem3.3.2 below.ForthegeneralpropertiesofprojectionmatricesseeSectionA.11. ThematrixP createsthefittedvaluesinaleastsquaresregression: PY =X (cid:161) X (cid:48) X (cid:162)â1 X (cid:48) Y =XÎ² (cid:98) =Y(cid:98). BecauseofthispropertyP isalsoknownasthehatmatrix. AspecialexampleofaprojectionmatrixoccurswhenX =1 isann-vectorofones.Then n P=1 (cid:161) 1 (cid:48) 1 (cid:162)â1 1 (cid:48) = 1 1 1 (cid:48) . n n n n n n n Notethatinthiscase PY =1 (cid:161) 1 (cid:48) 1 (cid:162)â1 1 (cid:48) Y =1 Y n n n n n createsann-vectorwhoseelementsarethesamplemeanY. TheprojectionmatrixP appearsfrequentlyinalgebraicmanipulationsinleastsquaresregression. Thematrixhasthefollowingimportantproperties.",
    "page": 93,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 74 Theorem3.3 TheprojectionmatrixP=X (cid:161) X (cid:48) X (cid:162)â1 X (cid:48) foranynÃk X withnâ¥ k hasthefollowingalgebraicproperties. 1. P issymmetric(P (cid:48)=P). 2. P isidempotent(PP=P). 3. trP=k. 4. TheeigenvaluesofP are1and0.Therearekeigenvaluesequalling1and nâk equalling0. 5. rank(P)=k. WeclosethissectionbyprovingtheclaimsinTheorem3.3.Part1holdssince P (cid:48)= (cid:179) X (cid:161) X (cid:48) X (cid:162)â1 X (cid:48) (cid:180)(cid:48) =(cid:161) X (cid:48)(cid:162)(cid:48)(cid:179) (cid:161) X (cid:48) X (cid:162)â1 (cid:180)(cid:48) (X) (cid:48) =X (cid:179) (cid:161) X (cid:48) X (cid:162)(cid:48)(cid:180)â1 X (cid:48) =X (cid:179) (X) (cid:48)(cid:161) X (cid:48)(cid:162)(cid:48)(cid:180)â1 X (cid:48)=P. Toestablishpart2,thefactthatPX =X impliesthat PP=PX (cid:161) X (cid:48) X (cid:162)â1 X (cid:48)=X (cid:161) X (cid:48) X (cid:162)â1 X (cid:48)=P asclaimed.Forpart3, trP=tr (cid:179) X (cid:161) X (cid:48) X (cid:162)â1 X (cid:48) (cid:180) =tr (cid:179) (cid:161) X (cid:48) X (cid:162)â1 X (cid:48) X (cid:180) =tr(I )=k. k SeeAppendixA.5fordefinitionandpropertiesofthetraceoperator. For part 4, it is shown in Appendix A.11 that the eigenvalues Î» of an idempotent matrix are all 1 i and0. SincetrP equalsthesumofthen eigenvaluesandtrP =k bypart3, itfollowsthattherearek eigenvaluesequalling1andtheremaindernâk equalling0. For part 5, observe that P is positive semi-definite since its eigenvalues are all non-negative. By TheoremA.4.5itsrankequalsthenumberofpositiveeigenvalues,whichisk asclaimed. 3.12 AnnihilatorMatrix Define M=I âP=I âX (cid:161) X (cid:48) X (cid:162)â1 X (cid:48) n n whereI isthenÃnidentitymatrix.Notethat n MX =(I âP)X =X âPX =X âX =0. (3.21) n ThusM andX areorthogonal.WecallM theannihilatormatrixduetothepropertythatforanymatrix Z intherangespaceofX then MZ =ZâPZ =0.",
    "page": 94,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 75 Forexample,MX =0foranysubcomponentX ofX,andMP=0(seeExercise3.7). 1 1 TheannihilatormatrixM hassimilarpropertieswithP,includingthatM issymmetric(M (cid:48)=M)and idempotent(MM=M).Itisthusaprojectionmatrix.SimilarlytoTheorem3.3.3wecancalculate trM=nâk. (3.22) (SeeExercise3.9.)OneimplicationisthattherankofM isnâk. WhileP createsfittedvalues,M createsleastsquaresresiduals: MY =Y âPY =Y âXÎ² (cid:98) = (cid:98) e. (3.23) Asdiscussedintheprevioussection,aspecialexampleofaprojectionmatrixoccurswhenX =1 is n ann-vectorofones,sothatP=1 (cid:161) 1 (cid:48) 1 (cid:162)â1 1 (cid:48) .Theassociatedannihilatormatrixis n n n n M=I âP=I â1 (cid:161) 1 (cid:48) 1 (cid:162)â1 1 (cid:48) . n n n n n n WhileP createsavectorofsamplemeans,M createsdemeanedvalues: MY =Y â1 Y. n Forsimplicitywewilloftenwritetheright-hand-sideasY âY.Theith elementisY âY,thedemeaned i valueofY . i We can also use (3.23) to write an alternative expression for the residual vector. Substituting Y = XÎ²+e intoe=MY andusingMX =0wefind (cid:98) e=MY =M (cid:161) XÎ²+e (cid:162)=Me (3.24) (cid:98) whichisfreeofdependenceontheregressioncoefficientÎ². 3.13 EstimationofErrorVariance TheerrorvarianceÏ2=(cid:69)(cid:163) e2(cid:164) isamoment,soanaturalestimatorisamomentestimator. Ife were i observedwewouldestimateÏ2by Ï2= 1 (cid:88) n e2. (3.25) (cid:101) n i i=1 However,thisisinfeasiblease isnotobserved.Inthiscaseitiscommontotakeatwo-stepapproachto i estimation.Theresidualse arecalculatedinthefirststep,andthenwesubstitutee fore inexpression (cid:98)i (cid:98)i i (3.25)toobtainthefeasibleestimator Ï2= 1 (cid:88) n e2. (3.26) (cid:98) n (cid:98)i i=1 Inmatrixnotation,wecanwrite(3.25)and(3.26)asÏ2=n â1e (cid:48) e and (cid:101) Ï2=n â1e (cid:48) e. (3.27) (cid:98) (cid:98)(cid:98) Recalltheexpressionse=MY =Me from(3.23)and(3.24).Appliedto(3.27)wefind (cid:98) Ï2=n â1e (cid:48) e=n â1e (cid:48) MMe=n â1e (cid:48) Me (3.28) (cid:98) (cid:98)(cid:98) thethirdequalitysinceMM=M. Aninterestingimplicationisthat Ï2âÏ2=n â1e (cid:48) eân â1e (cid:48) Me=n â1e (cid:48) Peâ¥0. (cid:101) (cid:98) (cid:48) ThefinalinequalityholdsbecauseP ispositivesemi-definiteande Pe isaquadraticform. Thisshows thatthefeasibleestimatorÏ2isnumericallysmallerthantheidealizedestimator(3.25). (cid:98)",
    "page": 95,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 76 3.14 AnalysisofVariance Anotherwayofwriting(3.23)is Y =PY +MY =Y(cid:98) + (cid:98) e. (3.29) Thisdecompositionisorthogonal,thatis Y(cid:98) (cid:48) (cid:98) e=(PY) (cid:48) (MY)=Y (cid:48) PMY =0. (3.30) Itfollowsthat Y (cid:48) Y =Y(cid:98) (cid:48) Y(cid:98) +2Y(cid:98) (cid:48) (cid:98) e+ (cid:98) e (cid:48) (cid:98) e=Y(cid:98) (cid:48) Y(cid:98) + (cid:98) e (cid:48) (cid:98) e or n n n (cid:88) Y i 2= (cid:88) Y(cid:98) i 2+ (cid:88) e (cid:98)i 2. i=1 i=1 i=1 SubtractingY frombothsidesof(3.29)weobtain Y â1 n Y =Y(cid:98) â1 n Y + (cid:98) e. ThisdecompositionisalsoorthogonalwhenX containsaconstant,as (cid:179) Y(cid:98) â1 n Y (cid:180)(cid:48) (cid:98) e=Y(cid:98) (cid:48) (cid:98) eâY1 (cid:48) n(cid:98) e=0 under(3.17).Itfollowsthat (cid:179) (cid:180)(cid:48)(cid:179) (cid:180) (cid:179) (cid:180)(cid:48)(cid:179) (cid:180) Y â1 n Y Y â1 n Y = Y(cid:98) â1 n Y Y(cid:98) â1 n Y + (cid:98) e (cid:48) (cid:98) e or (cid:88) n (cid:179) Y i âY (cid:180)2 = (cid:88) n (cid:179) Y(cid:98)i âY (cid:180)2 + (cid:88) n e (cid:98)i 2. i=1 i=1 i=1 Thisiscommonlycalledtheanalysis-of-varianceformulaforleastsquaresregression. AcommonlyreportedstatisticisthecoefficientofdeterminationorR-squared: (cid:179) (cid:180)2 R2= (cid:80)n i=1 Y(cid:98)i âY =1â (cid:80)n i=1 e (cid:98)i 2 . (cid:179) (cid:180)2 (cid:179) (cid:180)2 (cid:80)n Y âY (cid:80)n Y âY i=1 i i=1 i ItisoftendescribedasâthefractionofthesamplevarianceofY whichisexplainedbytheleastsquares fitâ. R2isacrudemeasureofregressionfit. Wehavebettermeasuresoffit,buttheserequireastatistical (notjustalgebraic)analysisandwewillreturntotheseissueslater. OnedeficiencywithR2 isthatitin- creaseswhenregressorsareaddedtoaregression(seeExercise3.16)sotheâfitâcanbealwaysincreased byincreasingthenumberofregressors. ThecoefficientofdeterminationwasintroducedbyWright(1921). 3.15 Projections Onewaytovisualizeleastsquaresfittingisasaprojectionoperation. Writetheregressormatrixas X =[X X ... X ]where X isthe jth columnof X. Therangespace 1 2 k j R(X) of X is the space consisting of all linear combinations of the columns X ,X ,...,X . R(X) is a k 1 2 k",
    "page": 96,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 77 dimensional surface contained in (cid:82)n. If k =2 then R(X) is a plane. The operator P = X (cid:161) X (cid:48) X (cid:162)â1 X (cid:48) projectsvectorsontoR(X).ThefittedvaluesY(cid:98) =PY aretheprojectionofY ontoR(X). TovisualizeexamineFigure3.3. Thisdisplaysthecasen=3andk =2. Displayedarethreevectors Y,X ,andX ,whichareeachelementsof(cid:82)3.TheplanecreatedbyX andX istherangespaceR(X). 1 2 1 2 Regressionfittedvaluesarelinearcombinationsof X and X andsolieonthisplane. Thefittedvalue 1 2 Y(cid:98) isthevectoronthisplaneclosesttoY. Theresidual (cid:98) e=Y âY(cid:98) isthedifferencebetweenthetwo. The â¦ anglebetweenthevectorsY(cid:98) and (cid:98) e is90 ,andthereforetheyareorthogonalasshown. YYYYYYYY eeeeeee^^^^^^^ XXXXXXXXX 222222222 ^^^^ YYYY XXXXXXXXXX 1111111111 Figure3.3:ProjectionofY ontoX andX 1 2 3.16 RegressionComponents PartitionX =[X X ]andÎ²=(Î² ,Î² ).Theregressionmodelcanbewrittenas 1 2 1 2 Y =X Î² +X Î² +e. (3.31) 1 1 2 2 TheOLSestimatorofÎ²=(Î²(cid:48) ,Î²(cid:48) ) (cid:48) isobtainedbyregressionofY onX =[X X ]andcanbewrittenas 1 2 1 2 Y =XÎ² (cid:98) + (cid:98) e=X 1 Î² (cid:98) 1 +X 2 Î² (cid:98) 2 + (cid:98) e. (3.32) WeareinterestedinalgebraicexpressionsforÎ² (cid:98)1 andÎ² (cid:98)2 . LetâsfirstfocusonÎ² (cid:98)1 .Theleastsquaresestimatorbydefinitionisfoundbythejointminimization (cid:161)Î² (cid:98)1 ,Î² (cid:98)2 (cid:162)=argminSSE (cid:161)Î² 1 ,Î² 2 (cid:162) (3.33) Î² 1,Î² 2 where SSE (cid:161)Î² ,Î² (cid:162)=(cid:161) Y âX Î² âX Î² (cid:162)(cid:48)(cid:161) Y âX Î² âX Î² (cid:162) . 1 2 1 1 2 2 1 1 2 2 AnequivalentexpressionforÎ² (cid:98)1 canbeobtainedbyconcentration(nestedminimization). Thesolution (3.33)canbewrittenas (cid:181) (cid:182) Î² (cid:98)1 =argmin minSSE (cid:161)Î² 1 ,Î² 2 (cid:162) . (3.34) Î² 1 Î² 2",
    "page": 97,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 78 TheinnerexpressionminÎ² SSE (cid:161)Î² 1 ,Î² 2 (cid:162) minimizesoverÎ² 2 whileholdingÎ² 1 fixed. Itisthelowestpos- 2 siblesumofsquarederrorsgivenÎ² 1 . TheouterminimizationargminÎ² findsthecoefficientÎ² 1 which 1 minimizestheâlowestpossiblesumofsquarederrorsgivenÎ² 1 â. ThismeansthatÎ² (cid:98)1 asdefinedin(3.33) and(3.34)arealgebraicallyidentical. Examine the inner minimization problem in (3.34). This is simply the least squares regression of Y âX Î² onX .Thishassolution 1 1 2 argminSSE (cid:161)Î² ,Î² (cid:162)=(cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48) (cid:161) Y âX Î² (cid:162)(cid:162) 1 2 2 2 2 1 1 Î² 2 withresiduals Y âX Î² âX (cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48) (cid:161) Y âX Î² (cid:162)(cid:162)=(cid:161) M Y âM X Î² (cid:162) 1 1 2 2 2 2 1 1 2 2 1 1 =M (cid:161) Y âX Î² (cid:162) 2 1 1 where M =I âX (cid:161) X (cid:48) X (cid:162)â1 X (cid:48) (3.35) 2 n 2 2 2 2 istheannihilatormatrixforX .Thismeansthattheinnerminimizationproblem(3.34)hasminimized 2 value minSSE (cid:161)Î² ,Î² (cid:162)=(cid:161) Y âX Î² (cid:162)(cid:48) M M (cid:161) Y âX Î² (cid:162) 1 2 1 1 2 2 1 1 Î² 2 =(cid:161) Y âX Î² (cid:162)(cid:48) M (cid:161) Y âX Î² (cid:162) 1 1 2 1 1 wherethesecondequalityholdssinceM isidempotent.Substitutingthisinto(3.34)wefind 2 Î² (cid:98)1 =argmin (cid:161) Y âX 1 Î² 1 (cid:162)(cid:48) M 2 (cid:161) Y âX 1 Î² 1 (cid:162) Î² 1 =(cid:161) X (cid:48) M X (cid:162)â1(cid:161) X (cid:48) M Y (cid:162) . 1 2 1 1 2 Byasimilarargumentwefind Î² (cid:98)2 =(cid:161) X (cid:48) 2 M 1 X 2 (cid:162)â1(cid:161) X (cid:48) 2 M 1 Y (cid:162) where M =I âX (cid:161) X (cid:48) X (cid:162)â1 X (cid:48) (3.36) 1 n 1 1 1 1 istheannihilatormatrixforX . 1 Theorem3.4 The least squares estimator (cid:161)Î² (cid:98)1 ,Î² (cid:98)2 (cid:162) for (3.32) has the algebraic solution Î² (cid:98)1 =(cid:161) X (cid:48) 1 M 2 X 1 (cid:162)â1(cid:161) X (cid:48) 1 M 2 Y (cid:162) (3.37) Î² (cid:98)2 =(cid:161) X (cid:48) 2 M 1 X 2 (cid:162)â1(cid:161) X (cid:48) 2 M 1 Y (cid:162) (3.38) whereM andM aredefinedin(3.36)and(3.35),respectively. 1 2",
    "page": 98,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 79 3.17 RegressionComponents(AlternativeDerivation)* AnalternativeproofofTheorem3.4usesanalgebraicargumentbasedonthepopulationcalculations fromSection2.22.Sincethisisaclassicderivationwepresentithereforcompleteness. PartitionQ(cid:98)XX as ï£® 1 1 ï£¹ (cid:48) (cid:48) ï£® ï£¹ X X X X Q(cid:98)11 Q(cid:98)12 ï£¯ n 1 1 n 1 2 ï£º Q(cid:98)XX = ï£° ï£» =ï£¯ ï£¯ ï£º ï£º Q(cid:98)21 Q(cid:98)22 ï£° 1 X (cid:48) X 1 X (cid:48) X ï£» n 2 1 n 2 2 andsimilarlyQ(cid:98)XY as ï£® 1 ï£¹ (cid:48) ï£® ï£¹ X Y Q(cid:98)1Y ï£¯ n 1 ï£º Q(cid:98)XY = ï£° ï£» =ï£¯ ï£¯ ï£º ï£º . Q(cid:98)2Y ï£° 1 X (cid:48) Y ï£» n 2 Bythepartitionedmatrixinversionformula(A.3) Q(cid:98) â X 1 X = ï£® ï£° Q(cid:98)11 Q(cid:98)12 ï£¹ ï£» â1 d=ef ï£® ï£¯ ï£° Q(cid:98) 11 Q(cid:98) 12 ï£¹ ï£º ï£» = ï£® ï£¯ ï£° Q(cid:98) â 11 1 Â·2 âQ(cid:98) â 11 1 Â·2 Q(cid:98)12 Q(cid:98) â 22 1 ï£¹ ï£º ï£» (3.39) Q(cid:98)21 Q(cid:98)22 Q(cid:98) 21 Q(cid:98) 22 âQ(cid:98) â 22 1 Â·1 Q(cid:98)21 Q(cid:98) â 11 1 Q(cid:98) â 22 1 Â·1 whereQ(cid:98)11Â·2 =Q(cid:98)11 âQ(cid:98)12 Q(cid:98) â 22 1 Q(cid:98)21 andQ(cid:98)22Â·1 =Q(cid:98)22 âQ(cid:98)21 Q(cid:98) â 11 1 Q(cid:98)12 .Thus (cid:181) Î² (cid:182) Î²= (cid:98)1 (cid:98) Î² (cid:98)2 = (cid:34) Q(cid:98) â 11 1 Â·2 âQ(cid:98) â 11 1 Â·2 Q(cid:98)12 Q(cid:98) â 22 1 (cid:35) (cid:183) Q(cid:98)1Y (cid:184) âQ(cid:98) â 22 1 Â·1 Q(cid:98)21 Q(cid:98) â 11 1 Q(cid:98) â 22 1 Â·1 Q(cid:98)2Y (cid:195) â1 (cid:33) = Q(cid:98) 1 â 1 1 Â·2 Q(cid:98)1YÂ·2 . Q(cid:98) 22Â·1 Q(cid:98)2YÂ·1 Now Q(cid:98)11Â·2 =Q(cid:98)11 âQ(cid:98)12 Q(cid:98) â 22 1 Q(cid:98)21 = 1 X (cid:48) X â 1 X (cid:48) X (cid:181) 1 X (cid:48) X (cid:182)â1 1 X (cid:48) X n 1 1 n 1 2 n 2 2 n 2 1 = 1 X (cid:48) M X n 1 2 1 and Q(cid:98)1yÂ·2 =Q(cid:98)1Y âQ(cid:98)12 Q(cid:98) â 22 1 Q(cid:98)2Y = 1 X (cid:48) Y â 1 X (cid:48) X (cid:181) 1 X (cid:48) X (cid:182)â1 1 X (cid:48) Y n 1 n 1 2 n 2 2 n 2 = 1 X (cid:48) M Y. n 1 2 Equation(3.38)follows. 1 Similarly to the calculation for Q(cid:98)11Â·2 and Q(cid:98)1YÂ·2 you can show that Q(cid:98)2YÂ·1 = n X (cid:48) 2 M 1 Y and Q(cid:98)22Â·1 = 1 (cid:48) X M X .Thisestablishes(3.37).Together,thisisTheorem3.4. n 2 1 2",
    "page": 99,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 80 3.18 ResidualRegression As first recognized by Frisch and Waugh (1933) and extended by Lovell (1963), expressions (3.37) and(3.38)canbeusedtoshowthattheleastsquaresestimatorsÎ² (cid:98)1 andÎ² (cid:98)2 canbefoundbyatwo-step regressionprocedure. Take(3.38).SinceM isidempotent,M =M M andthus 1 1 1 1 Î² (cid:98)2 =(cid:161) X (cid:48) 2 M 1 X 2 (cid:162)â1(cid:161) X (cid:48) 2 M 1 Y (cid:162) =(cid:161) X (cid:48) M M X (cid:162)â1(cid:161) X (cid:48) M M Y (cid:162) 2 1 1 2 2 1 1 (cid:179) (cid:48) (cid:180)â1(cid:179) (cid:48) (cid:180) = X(cid:101)2 X(cid:101)2 X(cid:101)2(cid:101) e 1 whereX(cid:101)2 =M 1 X 2 and (cid:101) e 1 =M 1 Y. ThusthecoefficientestimatorÎ² (cid:98)2 isalgebraicallyequaltotheleastsquaresregressionof (cid:101) e 1 onX(cid:101)2 .No- ticethatthesetwoareY andX ,respectively,premultipliedbyM .Butweknowthatpre-multiplication 2 1 byM createsleastsquaresresiduals. Thereforee issimplytheleastsquaresresidualfromaregression 1 (cid:101)1 ofY onX 1 ,andthecolumnsofX(cid:101)2 aretheleastsquaresresidualsfromtheregressionsofthecolumnsof X onX . 2 1 Wehaveproventhefollowingtheorem. Theorem3.5 Frisch-Waugh-Lovell(FWL) In the model (3.31), the OLS estimator of Î² and the OLS residuals e may be 2 (cid:98) computedbyeithertheOLSregression(3.32)orviathefollowingalgorithm: 1. RegressY onX ,obtainresidualse ; 1 (cid:101)1 2. RegressX 2 onX 1 ,obtainresidualsX(cid:101)2 ; 3. Regress (cid:101) e 1 onX(cid:101)2 ,obtainOLSestimatesÎ² (cid:98)2 andresiduals (cid:98) e. Insomecontexts(suchaspaneldatamodels,tobeintroducedinChapter17),theFWLtheoremcan beusedtogreatlyspeedcomputation. TheFWLtheoremisadirectanalogofthecoefficientrepresentationobtainedinSection2.23. The resultobtainedinthatsectionconcernedthepopulationprojectioncoefficients;theresultobtainedhere concerntheleastsquaresestimators.Thekeymessageisthesame.Intheleastsquaresregression(3.32) theestimatedcoefficientÎ² (cid:98)2 algebraicallyequalstheregressionofY ontheregressorsX 2 aftertheregres- sors X 1 havebeenlinearlyprojectedout. Similarly,thecoefficientestimateÎ² (cid:98)1 algebraicallyequalsthe regressionofY ontheregressorsX aftertheregressorsX havebeenlinearlyprojectedout.Thisresult 1 2 canbeinsightfulwheninterpretingregressioncoefficients. A common application of the FWL theorem is the demeaning formula for regression obtained in (3.18).PartitionX =[X X ]whereX =1 isavectorofonesandX isamatrixofobservedregressors. 1 2 1 n 2 In this case M 1 = I n â1 n (cid:161) 1 (cid:48) n 1 n (cid:162)â1 1 (cid:48) n . Observe that X(cid:101)2 =M 1 X 2 = X 2 âX 2 and M 1 Y =Y âY are the âdemeanedâvariables.TheFWLtheoremsaysthatÎ² (cid:98)2 istheOLSestimatefromaregressionofY i âY on X âX : 2i 2 (cid:195) (cid:33)â1(cid:195) (cid:33) (cid:88) n (cid:179) (cid:180)(cid:179) (cid:180)(cid:48) (cid:88) n (cid:179) (cid:180)(cid:179) (cid:180) Î² (cid:98)2 = X 2i âX 2 X 2i âX 2 X 2i âX 2 Y i âY . i=1 i=1 Thisis(3.18).",
    "page": 100,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 81 RagnarFrisch Ragnar Frisch (1895-1973) was co-winner with Jan Tinbergen of the first No- bel Memorial Prize in Economic Sciences in 1969 for their work in developing and applying dynamic models for the analysis of economic problems. Frisch madeanumberoffoundationalcontributionstomoderneconomicsbeyondthe Frisch-Waugh-LovellTheorem,includingformalizingconsumertheory,produc- tiontheory,andbusinesscycletheory. 3.19 LeverageValues The leverage values for the regressor matrix X are the diagonal elements of the projection matrix P=X (cid:161) X (cid:48) X (cid:162)â1 X (cid:48) .Therearenleveragevalues,andaretypicallywrittenash fori =1,...,n.Since ii ï£« (cid:48) ï£¶ X 1 ï£¬ X (cid:48) ï£· P=ï£¬ ï£¬ ï£¬ . . . 2 ï£· ï£· ï£· (cid:161) X (cid:48) X (cid:162)â1(cid:161) X 1 X 2 Â·Â·Â· X n (cid:162) ï£­ ï£¸ (cid:48) X n theyare h =X (cid:48)(cid:161) X (cid:48) X (cid:162)â1 X . (3.40) ii i i Theleveragevalueh isanormalizedlengthoftheobservedregressorvector X . Theyappearfre- ii i quentlyinthealgebraicandstatisticalanalysisofleastsquaresregression,includingleave-one-outre- gression,influentialobservations,robustcovariancematrixestimation,andcross-validation. Afewpropertiesoftheleveragevaluesarenowlisted. Theorem3.6 1. 0â¤h â¤1. ii 2. h â¥1/nifX includesanintercept. ii 3. (cid:80)n h =k. i=1 ii WeproveTheorem3.6below. Theleveragevalueh measureshowunusualtheith observationX isrelativetotheotherobserva- ii i tionsinthesample.Alargeh occurswhenX isquitedifferentfromtheothersamplevalues.Ameasure ii i ofoverallunusualnessisthemaximumleveragevalue h= max h . (3.41) ii 1â¤iâ¤n It is common to say that a regression design is balanced when the leverage values are all roughly equaltooneanother.FromTheorem3.6.3wededucethatcompletebalanceoccurswhenh =h=k/n. ii An example of complete balance is when the regressors are all orthogonal dummy variables, each of whichhaveequaloccurranceof0âsand1âs.",
    "page": 101,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 82 Aregressiondesignisunbalancedifsomeleveragevaluesarehighlyunequalfromtheothers. The most extreme case is h =1. An example where this occurs is when there is a dummy regressor which takesthevalue1foronlyoneobservationinthesample. Themaximalleveragevalue(3.41)willchangedependingonthechoiceofregressors. Forexample, considerequation(3.13),thewageregressionforsingleAsianmenwhichhasn=268observations.This regressionhash=0.33.Ifthesquaredexperienceregressorisomittedtheleveragedropstoh=0.10.Ifa cubicinexperienceisaddeditincreasestoh=0.76.Andifafourthandfifthpowerareaddeditincreases toh=0.99. Someinferenceprocedures(suchasrobustcovariancematrixestimationandcross-validation)are sensitivetohighleveragevalues.Wewillreturntotheseissueslater. WenowproveTheorem3.6.Forpart1lets beannÃ1unitvectorwitha1intheith placeandzeros i elsewheresothath =s (cid:48) Ps .ThenapplyingtheQuadraticInequality(B.18)andTheorem3.3.4, ii i i h =s (cid:48) Ps â¤s (cid:48) s Î» (P)=1 ii i i i i max asclaimed. For part 2 partition X =(1,Z (cid:48) ) (cid:48) . Without loss of generality we can replace Z with the demeaned i i i valuesZ â=Z âZ.ThensinceZ â andtheinterceptareorthgonal i i i (cid:183) (cid:184)â1(cid:181) (cid:182) h ii =(1,Z i â(cid:48) ) n 0 Z â(cid:48) 0 Z â Z 1 â i = 1 +Z â(cid:48)(cid:161) Z â(cid:48) Z â(cid:162)â1 Z ââ¥ 1 . n i i n Forpart3, (cid:80)n h =trP=k wherethesecondequalityisTheorem3.3.3. i=1 ii 3.20 Leave-One-OutRegression Thereareanumberofstatisticalproceduresâresidualanalysis,jackknifevarianceestimation,cross- validation,two-stepestimation,hold-outsampleevaluationâwhichmakeuseofestimatorsconstructed onsub-samples. Ofparticularimportanceisthecasewhereweexcludeasingleobservationandthen repeatthisforallobservations.Thisiscalledleave-one-out(LOO)regression. Specifically,theleave-one-outestimatoroftheregressioncoefficientÎ²istheleastsquaresestimator constructedusingthefullsampleexcludingasingleobservationi.Thiscanbewrittenas (cid:195) (cid:33)â1(cid:195) (cid:33) Î² (cid:98)(âi) = (cid:88) X j X j (cid:48) (cid:88) X j Y j j(cid:54)=i j(cid:54)=i =(cid:161) X (cid:48) X âX X (cid:48)(cid:162)â1(cid:161) X (cid:48) Y âX Y (cid:162) i i i i = (cid:179) X (cid:48) (âi) X (âi) (cid:180)â1 X (cid:48) (âi) Y (âi) . (3.42) Here,X (âi) andY (âi) arethedatamatricesomittingtheith row. ThenotationÎ² (cid:98)(âi) orÎ² (cid:98)âi iscommonly used to denote an estimator with the ith observation omitted. There is a leave-one-out estimator for eachobservation,i =1,...,n,sowehavensuchestimators. The leave-one-out predicted value for Y i is Y(cid:101)i = X i (cid:48)Î² (cid:98)(âi) . This is the predicted value obtained by estimatingÎ²onthesamplewithoutobservationi andthenusingthecovariatevector X topredictY . i i NoticethatY(cid:101)i isanauthenticpredictionasY i isnotusedtoconstructY(cid:101)i . Thisisincontrasttothefitted valuesY(cid:98)i whicharefunctionsofY i .",
    "page": 102,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 83 Theleave-one-outresidual,predictionerror,orpredictionresidualise (cid:101)i =Y i âY(cid:101)i . Theprediction errorsmaybeusedasestimatorsoftheerrorsinsteadoftheresiduals. Thepredictionerrorsarebetter estimatorsthantheresidualssincetheformerarebasedonauthenticpredictions. The leave-one-out formula (3.42) gives the unfortunate impression that the leave-one-out coeffi- cientsanderrorsarecomputationallycumbersome,requiringn separateregressions. Inthecontextof linearregressionthisisfortunatelynotthecase.TherearesimplelinearexpressionsforÎ² (cid:98)(âi) ande (cid:101)i . Theorem3.7 Theleave-one-outestimatorandpredictionerrorequal Î² (cid:98)(âi) =Î² (cid:98) â(cid:161) X (cid:48) X (cid:162)â1 X i e (cid:101)i (3.43) and e =(1âh ) â1e (3.44) (cid:101)i ii (cid:98)i whereh aretheleveragevaluesasdefinedin(3.40). ii WeproveTheorem3.7attheendofthesection. Equation (3.43) shows that the leave-one-out coefficients can be calculated by a simple linear op- eration and do not need to be calculated using n separate regressions. Another interesting feature of equation(3.44)isthatthepredictionerrorse areasimplescalingoftheleastsquaresresidualse with (cid:101)i (cid:98)i thescalingdependentontheleveragevaluesh .Ifh issmallthene (cid:39)e .Howeverifh islargethene ii ii (cid:101)i (cid:98)i ii (cid:101)i canbequitedifferentfrome . Thusthedifferencebetweentheresidualsandpredictedvaluesdepends (cid:98)i ontheleveragevalues,thatis,howunusualisX . i Towrite(3.44)invectornotation,define M â=(cid:161) I âdiag{h ,..,h } (cid:162)â1 n 11 nn =diag{(1âh ) â1,..,(1âh ) â1}. 11 nn Then(3.44)isequivalentto e=M â e. (3.45) (cid:101) (cid:98) Oneuseofthepredictionerrorsistoestimatetheout-of-samplemeansquarederror: Ï2= 1 (cid:88) n e2= 1 (cid:88) n (1âh ) â2e2. (3.46) (cid:101) n (cid:101)i n ii (cid:98)i i=1 i=1 (cid:112) Thisisknownasthesamplemeansquaredpredictionerror. ItssquarerootÏ= Ï2 istheprediction (cid:101) (cid:101) standarderror. We complete the section with a proof of Theorem 3.7. The leave-one-out estimator (3.42) can be writtenas Î² (cid:98)(âi) =(cid:161) X (cid:48) X âX i X i (cid:48)(cid:162)â1(cid:161) X (cid:48) Y âX i Y i (cid:162) . (3.47) Multiply(3.47)by (cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48) X âX X (cid:48)(cid:162) .Weobtain i i Î² (cid:98)(âi) â(cid:161) X (cid:48) X (cid:162)â1 X i X i (cid:48)Î² (cid:98)(âi) =(cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48) Y âX i Y i (cid:162)=Î² (cid:98) â(cid:161) X (cid:48) X (cid:162)â1 X i Y i . Rewriting Î² (cid:98)(âi) =Î² (cid:98) â(cid:161) X (cid:48) X (cid:162)â1 X i (cid:161) Y i âX i (cid:48)Î² (cid:98)(âi) (cid:162)=Î² (cid:98) â(cid:161) X (cid:48) X (cid:162)â1 X i e (cid:101)i (cid:48) whichis(3.43).PremultiplyingthisexpressionbyX andusingdefinition(3.40)weobtain i X i (cid:48)Î² (cid:98)(âi) =X i (cid:48)Î² (cid:98) âX i (cid:48)(cid:161) X (cid:48) X (cid:162)â1 X i e (cid:101)i =X i (cid:48)Î² (cid:98) âh ii e (cid:101)i",
    "page": 103,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Rewriting Î² (cid:98)(âi) =Î² (cid:98) â(cid:161) X (cid:48) X (cid:162)â1 X i (cid:161) Y i âX i (cid:48)Î² (cid:98)(âi) (cid:162)=Î² (cid:98) â(cid:161) X (cid:48) X (cid:162)â1 X i e (cid:101)i (cid:48) whichis(3.43).PremultiplyingthisexpressionbyX andusingdefinition(3.40)weobtain i X i (cid:48)Î² (cid:98)(âi) =X i (cid:48)Î² (cid:98) âX i (cid:48)(cid:161) X (cid:48) X (cid:162)â1 X i e (cid:101)i =X i (cid:48)Î² (cid:98) âh ii e (cid:101)i . Usingthedefinitionsfore ande weobtaine =e +h e .Re-writingweobtain(3.44). (cid:98)i (cid:101)i (cid:101)i (cid:98)i ii(cid:101)i",
    "page": 103,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 84 3.21 InfluentialObservations Anotheruseoftheleave-one-outestimatoristoinvestigatetheimpactofinfluentialobservations, sometimescalledoutliers.Wesaythatobservationiisinfluentialifitsomissionfromthesampleinduces asubstantialchangeinaparameterestimateofinterest. ForillustrationconsiderFigure3.4whichshowsascatterplotofrealizations(Y ,X ).The25observa- i i tionsshownwiththeopencirclesaregeneratedbyX â¼U[1,10]andY â¼N(X ,4).The26th observation i i i shownwiththefilledcircleis X =9,Y =0.(ImaginethatY =0wasincorrectlyrecordedduetoa 26 26 26 mistaken key entry.) The figure shows both the least squares fitted line from the full sample and that obtainedafterdeletionofthe26thobservationfromthesample.Inthisexamplewecanseehowthe26th observation(theâoutlierâ)greatlytiltstheleastsquaresfittedlinetowardsthe26th observation. Infact, theslopecoefficientdecreasesfrom0.97(whichisclosetothetruevalueof1.00)to0.56,whichissub- stantiallyreduced. NeitherY norX areunusualvaluesrelativetotheirmarginaldistributionssothis 26 26 outlierwouldnothavebeendetectedfromexaminationofthemarginaldistributionsofthedata. The changeintheslopecoefficientofâ0.41ismeaningfulandshouldraiseconcerntoanappliedeconomist. l l l l l l l l l ll l l l l l l ll l l l l l l l 2 4 6 8 10 01 8 6 4 2 0 x y leaveâoneâout OLS OLS l Figure3.4:ImpactofanInfluentialObservationontheLeast-SquaresEstimator From(3.43)weknowthat Î² (cid:98) âÎ² (cid:98)(âi) =(cid:161) X (cid:48) X (cid:162)â1 X i e (cid:101)i . (3.48) Bydirectcalculationofthisquantityforeachobservationi,wecandirectlydiscoverifaspecificobser- vationi isinfluentialforacoefficientestimateofinterest. For a general assessment, we can focus on the predicted values. The difference between the full- sampleandleave-one-outpredictedvaluesis Y(cid:98)i âY(cid:101)i =X i (cid:48)Î² (cid:98) âX i (cid:48)Î² (cid:98)(âi) =X i (cid:48)(cid:161) X (cid:48) X (cid:162)â1 X i e (cid:101)i =h ii e (cid:101)i whichisasimplefunctionoftheleveragevaluesh andpredictionerrorse .Observationi isinfluential ii (cid:101)i forthepredictedvalueif|h e |islarge,whichrequiresthatbothh and|e |arelarge. ii(cid:101)i ii (cid:101)i Onewaytothinkaboutthisisthatalargeleveragevalueh givesthepotentialforobservationi to ii be influential. A large h means that observation i is unusual in the sense that the regressor X is far ii i",
    "page": 104,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 85 from its sample mean. We call an observation with large h aleveragepoint. A leverage point is not ii necessarilyinfluentialasthelatteralsorequiresthatthepredictionerrore islarge. (cid:101)i Todetermineifanyindividualobservationsareinfluentialinthissenseseveraldiagnosticshavebeen proposed (some names include DFITS, Cookâs Distance, and Welsch Distance). Unfortunately, from a statisticalperspectiveitisdifficulttorecommendthesediagnosticsforapplicationsastheyarenotbased onstatisticaltheory.Probablythemostrelevantmeasureisthechangeinthecoefficientestimatesgiven in (3.48). The ratio of these changes to the coefficientâs standard error is called its DFBETA, and is a postestimationdiagnosticavailableinStata. Whilethereisnomagicthreshold,theconcerniswhether or not an individual observation meaningfully changes an estimated coefficient of interest. A simple diagnosticforinfluentialobservationsistocalculate Influence= max (cid:175) (cid:175)Y(cid:98)i âY(cid:101)i (cid:175) (cid:175) = max |h ii e (cid:101)i |. 1â¤iâ¤n 1â¤iâ¤n Thisisthelargest(absolute)changeinthepredictedvalueduetoasingleobservation. Ifthisdiagnostic islargerelativetothedistributionofY itmayindicatethatthatobservationisinfluential. Ifanobservationisdeterminedtobeinfluentialwhatshouldbedone? Asacommoncauseofinflu- entialobservationsisdataerror,theinfluentialobservationsshouldbeexaminedforevidencethatthe observationwasmis-recorded. Perhapstheobservationfallsoutsideofpermittedranges,orsomeob- servablesareinconsistent(forexample,apersonislistedashavingajobbutreceivesearningsof$0).Ifit isdeterminedthatanobservationisincorrectlyrecorded,thentheobservationistypicallydeletedfrom thesample.Thisprocessisoftencalledâcleaningthedataâ.Thedecisionsmadeinthisprocessinvolvea fairamountofindividualjudgment.[Whenthisisdonetheproperpracticeistoretainthesourcedatain itsoriginalformandcreateaprogramfilewhichexecutesallcleaningoperations(forexampledeletion ofindividualobservations).Thecleaneddatafilecanbesavedatthispoint,andthenusedforthesubse- quentstatisticalanalysis.Thepointofretainingthesourcedataandaspecificprogramfilewhichcleans thedataistwofold: sothatalldecisionsaredocumented,andsothatmodificationscanbemadeinre- visionsandfutureresearch.] Itisalsopossiblethatanobservationiscorrectlymeasured, butunusual andinfluential. Inthiscaseitisunclearhowtoproceed. Someresearcherswilltrytoalterthespecifi- cationtoproperlymodeltheinfluentialobservation. Otherresearcherswilldeletetheobservationfrom the sample. The motivation for this choice is to prevent the results from being skewed or determined byindividualobservations. Thislatterpracticeisviewedskepticallybymanyresearcherswhobelieveit reducestheintegrityofreportedempiricalresults",
    "page": 105,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Inthiscaseitisunclearhowtoproceed. Someresearcherswilltrytoalterthespecifi- cationtoproperlymodeltheinfluentialobservation. Otherresearcherswilldeletetheobservationfrom the sample. The motivation for this choice is to prevent the results from being skewed or determined byindividualobservations. Thislatterpracticeisviewedskepticallybymanyresearcherswhobelieveit reducestheintegrityofreportedempiricalresults. Foranempiricalillustrationconsiderthelogwageregression(3.13)forsingleAsianmen.Thisregres- sion,whichhas268observations,hasInfluence=0.29.Thismeansthatthemostinfluentialobservation, whendeleted,changesthepredicted(fitted)valueofthedependentvariablelog(wage)by0.29,orequiv- alently the average wage by 29%. This is a meaningful change and suggests further investigation. We examinetheinfluentialobservation,andfindthatitsleverageh is0.33.Itisamoderatelylargeleverage ii value, meaning that the regressor X is somewhat unusual. Examining further, we find that this indi- i vidual is 65 years old with 8 years education, so that his potential work experience is 51 years. This is thehighestexperienceinthesubsampleâthenexthighestis41years. Thelargeleverageisduetohis unusual characteristics (very low education and very high experience) within this sample. Essentially, regression(3.13)isattemptingtoestimatetheconditionalmeanatexperience=51withonlyoneobser- vation. Itisnotsurprisingthatthisobservationdeterminesthefitandisthusinfluential. Areasonable conclusionistheregressionfunctioncanonlybeestimatedoverasmallerrangeofexperience. Were- strictthesampletoindividualswithlessthan45yearsexperience,re-estimate,andobtainthefollowing estimates. lo(cid:225)g(wage)=0.144education+0.043experienceâ0.095experience2/100+0.531. (3.49)",
    "page": 105,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 86 Forthisregression,wecalculatethatInfluence=0.11,whichisgreatlyreducedrelativetotheregression (3.13).Comparing(3.49)with(3.13),theslopecoefficientforeducationisessentiallyunchanged,butthe coefficientsonexperienceanditssquarehaveslightlyincreased. Byeliminatingtheinfluentialobservationequation(3.49)canbeviewedasamorerobustestimate oftheconditionalmeanformostlevelsofexperience.Whethertoreport(3.13)or(3.49)inanapplication islargelyamatterofjudgment. 3.22 CPSDataSet Inthissectionwedescribethedatasetusedintheempiricalillustrations. TheCurrentPopulationSurvey(CPS)isamonthlysurveyofabout57,000U.S.householdsconducted bytheBureauoftheCensusoftheBureauofLaborStatistics.TheCPSistheprimarysourceofinforma- tiononthelaborforcecharacteristicsoftheU.S.population. Thesurveycoversemployment,earnings, educationalattainment,income,poverty,healthinsurancecoverage,jobexperience,votingandregistra- tion,computerusage,veteranstatus,andothervariables. Detailscanbefoundatwww.census.gov/cps anddataferrett.census.gov. FromtheMarch2009surveyweextractedtheindividualswithnon-allocatedvariableswhowerefull- timeemployed(definedasthosewhohadworkedatleast36hoursperweekforatleast48weeksthepast year),andexcludedthoseinthemilitary. Thissamplehas50,742individuals. Weextracted14variables fromtheCPSontheseindividualsandcreatedthedatasetcps09mar. Thisdataset,andallothersused inthistextbook,areavailableathttp://www.ssc.wisc.edu/~bhansen/econometrics/. 3.23 NumericalComputation Moderneconometricestimationinvolveslargesamplesandmanycovariates. Consequently,calcu- lationofevensimplestatisticssuchastheleastsquaresestimatorrequiresalargenumber(millions)of arithmetic operations. In practice most economists donât need to think much about this as it is done swiftly and effortlessly on personal computers. Nevertheless it is useful to understand the underlying calculationmethodsaschoicescanoccasionallymakesubstantivedifferences. Whiletodaynearlyallstatisticalcomputationsaremadeusingstatisticalsoftwarerunningonelec- tronic computers, this was not always the case. In the nineteenth and early twentieth centures âcom- puterâwasajoblabelforworkerswhomadecomputationsbyhand. Computerswereemployedbyas- tronomersandstatisticallaboratories.Thisfascinatingjob(andthefactthatmostcomputersemployed in laboratories were women) has entered popular culture. For example the lives of several computers whoworkedfortheearlyU.S.spaceprogramisdescribedinthebookandpopularmovieHiddenFig- ures,afictionalcomputer/astronautistheprotagonistofthenovelTheCalculatingStars,andthelifeof computer/astronomerHenriettaSwanLeavittisdramatizedintheplaySilentSky. Untilprogrammableelectroniccomputersbecameavailableinthe1960seconomicsgraduatestu- dentswereroutinelyemployedascomputers",
    "page": 106,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". For example the lives of several computers whoworkedfortheearlyU.S.spaceprogramisdescribedinthebookandpopularmovieHiddenFig- ures,afictionalcomputer/astronautistheprotagonistofthenovelTheCalculatingStars,andthelifeof computer/astronomerHenriettaSwanLeavittisdramatizedintheplaySilentSky. Untilprogrammableelectroniccomputersbecameavailableinthe1960seconomicsgraduatestu- dentswereroutinelyemployedascomputers. Samplesizeswereconsiderablysmallerthanthoseseen today,butstilltheeffortrequiredtocalculatebyhandaregressionwithevenn=100observationsand k =5variablesisconsiderable! Ifyouareacurrentgraduatestudentyoushouldfeelfortunatethatthe professionhasmovedonfromtheeraofhumancomputers! (Nowresearchassistantsdomoreelevated taskssuchaswritingStataandMATLABcode.) ToobtaintheleastsquaresestimateÎ² (cid:98) =(cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48) Y (cid:162) weneedtoeitherinvertX (cid:48) X orsolveasystem ofequations.Tobespecific,letA=X (cid:48) X andc=X (cid:48) Y sothattheleastsquaresestimatecanbewrittenas eitherthesolutionto AÎ² (cid:98) =c (3.50)",
    "page": 106,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 87 oras Î² (cid:98) =A â1c. (3.51) The equations (3.50) and (3.51) are algebraically identical but they suggest two distinct numerical ap- proachestoobtainÎ² (cid:98). (3.50)suggestssolvingasystemofk equations. (3.51)suggestsfinding A â1 and thenmultiplyingbyc. Whilethetwoexpressionsarealgebraicallyidenticaltheimpliednumericalap- proachesaredifferent. Inanutshell,solvingthesystemofequations(3.50)isnumericallypreferredtothematrixinversion problem(3.51).Directlysolving(3.50)isfasterandproducesasolutionwithahigherdegreeofnumerical accuracy.Thus(3.50)isgenerallyrecommendedover(3.51).However,inmostpracticalapplicationsthe choicewillnotmakeanypracticaldifference.Contextswherethechoicemaymakeadifferenceiswhen thematrix Aisill-conditioned(tobediscussedinSection3.24)orofextremelyhighdimension. Numericalmethodstosolvethesystemofequations(3.50)andcalculate A â1 arediscussedinSec- tionsA.18andA.19,respectively. Statisticalpackagesuseavarietyofmatrixmethodstosolve(3.50). Statausesthesweepalgorithm whichisavariantoftheGauss-JordanalgorithmdiscussedinSectionA.18.(Forthesweepalgorithmsee Goodnight(1979).) InR,solve(A,b)usestheQRdecomposition. InMATLAB,A\\busestheCholesky decompositionwhen AispositivedefiniteandtheQRdecompositionotherwise. 3.24 CollinearityErrors Fortheleastsquaresestimatortobeuniquelydefinedtheregressorscannotbelinearlydependent. However,itisquiteeasytoattempttocalculatearegressionwithlinearlydependentregressors.Thiscan occurformanyreasons,includingthefollowing. 1. Includingthesameregressortwice. 2. Includingregressorswhicharealinearcombinationofoneanother,suchaseducation,experience andageintheCPSdatasetexample(recall,experienceisdefinedasage-education-6). 3. Includingadummyvariableanditssquare. 4. Estimatingaregressiononasub-sampleforwhichadummyvariableiseitherallzerosorallones. 5. Includingadummyvariableinteractionwhichyieldsallzeros. 6. Includingmoreregressorsthanobservations. (cid:48) In any of the above cases the regressors are linearly dependent so X X is singular and the least squaresestimatorisnotunique.Ifyouattempttoestimatetheregression,youarelikelytoencounteran errormessage. (ApossibleexceptionisMATLABusingâA\\bâ,asdiscussedbelow.) Themessagemaybe thatâsystemisexactlysingularâ,âsystemiscomputationallysingularâ, avariableisâomittedbecauseof collinearityâ,oracoefficientislistedasâNAâ.Insomecases(suchasestimationinRusingexplicitmatrix computationorMATLABusingtheregresscommand)theprogramwillstopexecution. Inothercases theprogramwillcontinuetorun. InStata(andinthelmpackageinR),aregressionwillbereportedbut oneormorevariableswillbeomitted. Ifanyofthesewarningsorerrormessagesappear, thecorrectresponseistostopandexaminethe regression coding and data. Did you make an unintended mistake? Have you included a linearly de- pendent regressor? Are you estimating on a subsample for which the variables (in particular dummy",
    "page": 107,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 88 variables)havenovariation? Ifyoucandeterminethatoneofthesescenarioscausedtheerror,theso- lutionisimmediatelyapparent. Youneedtorespecifyyourmodel(eithersampleorregressors)sothat theredundancyiseliminated. Allempiricalresearchersencounterthiserrorinthecourseofempirical work. Youshouldnot,however,simplyacceptoutputifthepackagehasselectedvariablesforomission. Itistheresearcherâsjobtounderstandtheunderlyingcauseandenactasuitableremedy. Thereisalsoapossibilitythatthestatisticalpackagewillnotdetectandreportthematrixsingularity. IfyoucomputeinMATLABusingexplicitmatrixoperationsandusetherecommendedA\\bcommandto computetheleastsquaresestimatorMATLABmayreturnanumericalsolutionwithoutanerrormessage evenwhentheregressorsarealgebraicallydependent. Itisthereforerecommendedthatyouperforma numericalcheckformatrixsingularitywhenusingexplicitmatrixoperationsinMATLAB. How can we numerically check if a matrix A is singular? A standard diagnostic is the reciprocal conditionnumber Î» (A) C = min . Î» (A) max IfC =0then A issingular. IfC =1then A isperfectlybalanced. IfC isextremelysmallwesaythat A is ill-conditioned. The reciprocal condition number can be calculated in MATLAB or R by the rcond command. Unfortunately, there is no accepted tolerance for how smallC should be before regarding A as numerically singular, in part since rcond(A) can return a positive (but small) result even if A is algebraicallysingular.However,indoubleprecision(whichistypicallyusedforcomputation)numerical accuracyisboundedby2 â52(cid:39)2e-16,suggestingtheminimumboundC â¥2e-16. CheckingfornumericalsingularityiscomplicatedbythefactthatlowvaluesofC canalsobecaused byunbalancedorhighlycorrelatedregressors. To illustrate, consider a wage regression using the sample from (3.13) on powers of experience X from 1 through k (e.g. X,X2,X3,...,Xk). We calculated the reciprocal condition numberC for each k, and found that C is decreasing as k increases, indicating increasing ill-conditioning. Indeed, for k = 5, we findC =6e-17, which is lower than double precision accuracy. This means that a regression on (X,X2,X3,X4,X5)isill-conditioned.Theregressormatrix,however,isnotsingular.ThelowvalueofC is notduetoalgebraicsingularitybutratherisduetoalackofbalanceandhighcollinearity. Ill-conditionedregressorshavethepotentialproblemthatthenumericalresults(thereportedcoef- ficientestimates)willbeinaccurate. Itmaynotbeaconcerninmostapplicationsasthisonlyoccursin extremecases.Nevertheless,weshouldtryandavoidill-conditionedregressionswheneverpossible. There are strategies which can reduce or even eliminate ill-conditioning. Often it is sufficient to rescaletheregressors.Asimplerescalingwhichoftenworksfornon-negativeregressorsistodivideeach byitssamplemean,thusreplaceX withX /X . Intheaboveexamplewiththepowersofexperience, ji ji j thismeansreplacingX2withX2/ (cid:161) n â1(cid:80)n X2(cid:162) ,etc.Doingsodramaticallyreducestheill-conditioning",
    "page": 108,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". There are strategies which can reduce or even eliminate ill-conditioning. Often it is sufficient to rescaletheregressors.Asimplerescalingwhichoftenworksfornon-negativeregressorsistodivideeach byitssamplemean,thusreplaceX withX /X . Intheaboveexamplewiththepowersofexperience, ji ji j thismeansreplacingX2withX2/ (cid:161) n â1(cid:80)n X2(cid:162) ,etc.Doingsodramaticallyreducestheill-conditioning. i i i=1 i Withthisscaling,regressionsforkâ¤11satisfyC â¥1e-15. Anotherrescalingspecifictoaregressionwith powersistofirstrescaletheregressortoliein[â1,1]beforetakingpowers.Withthisscaling,regressions forkâ¤16satisfyC â¥1e-15.Asimplerscalingoptionistorescaletheregressortoliein[0,1]beforetaking powers.Withthisscaling,regressionsforkâ¤9satisfyC â¥1e-15.Thisisoftensufficientforapplications. Ill-conditioningcanoftenbecompletelyeliminatedbyorthogonalizationoftheregressors. Thisis achievedbysequentiallyregressingeachvariable(eachcolumninX)onthepreceedingvariables(each preceedingcolumn), takingtheresidual, andthenrescalingtohaveaunitvariance. Thiswillproduce regressorswhichalgebraicallysatisfyX (cid:48) X =nI andhaveaconditionnumberofC =1. Ifweapplythis n methodtotheaboveexample,weobtainaconditionnumbercloseto1forkâ¤20. Whatthisshowsisthatwhenaregressionhasasmallconditionnumberitisimportanttoexamine thespecificationcarefully. Itispossiblethattheregressorsarelinearlydependentinwhichcaseoneor moreregressorswillneedtobeomitted. Itisalsopossiblethattheregressorsarebadlyscaledinwhich case it may be useful to rescale some of the regressors. It is also possible that the variables are highly",
    "page": 108,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 89 collinearinwhichcaseapossiblesolutionisorthogonalization. Thesechoicesshouldbemadebythe researchernotbyanautomatedsoftwareprogram. 3.25 Programming Most packages allow both interactive programming (where you enter commands one-by-one) and batchprogramming(whereyourunapre-writtensequenceofcommandsfromafile). Interactivepro- gramming can be useful for exploratory analysis but eventually all work should be executed in batch mode.Thisisthebestwaytocontrolanddocumentyourwork. Batchprogramsaretextfileswhereeachlineexecutesasinglecommand.ForStata,thisfileneedsto havethefilenameextensionâ.doâ,andforMATLABâ.mâ.ForRthereisnospecificnamingrequirements, thoughitistypicaltousetheextensionâ.râ.Whenwritingbatchfilesitisusefultoincludecommentsfor documentationandreadability.Toexecuteaprogramfileyoutypeacommandwithintheprogram. Stata:do chapter3executesthefilechapter3.do MATLAB:run chapter3executesthefilechapter3.m R:source((cid:16)chapter3.r(cid:17))orsource(âchapter3.râ)executesthefilechapter3.r Therearesimilaritiesanddifferencesbetweenthecommandsusedinthesepackages.Forexample: 1. Differentsymbolsareusedtocreatecomments.*inStata,#inR,and%inMATLAB. 2. MATLABusesthesymbol;toseparatelines.StataandRuseahardreturn. 3. Statausesln()tocomputenaturallogarithms.RandMATLABuselog(). 4. Thesymbol=isusedtodefineavariable.Rprefers<-.Doubleequality==isusedtotestequality. WenowillustrateprogrammingfilesforStata,R,andMATLAB,whichexecuteaportionoftheem- piricalillustrationsfromSections3.7and3.21. FortheRandMATLABcodeweillustrateusingexplicit matrixoperations. Alternatively,RandMATLABhavebuilt-infunctionswhichimplementleastsquares regressionwithouttheneedforexplicitmatrixoperations. InRthestandardfunctionislm. InMATLAB thestandardfunctionisregress. Theadvantageofusingexplicitmatrixoperationsasshownbelowis thatyouknowexactlywhatcomputationsaredoneanditiseasiertogoâoutoftheboxâtoexecutenew procedures.Theadvantageofusingbuilt-infunctionsisthatcodingissimplifiedandyouaremuchless likelytomakeacodingerror.",
    "page": 109,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 90 StatadoFile * Clearmemoryandloadthedata clear usecps09mar.dta * Generatetransformations genwage=ln(earnings/(hours*week)) genexperience=age-education-6 genexp2=(experience^2)/100 * Createindicatorforsubsamples genmbf=(race==2)&(marital<=2)&(female==1) genmbf12=(mbf==1)&(experience==12) gensam=(race==4)&(marital==7)&(female==0) * Regressions regwageeducationifmbf12==1 regwageeducationexperienceexp2ifsam==1 * Leverageandinfluence predictleverage,hat predicte,residual gend=e*leverage/(1-leverage) summarizedifsam==1",
    "page": 110,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 91 RProgramFile # Loadthedataandcreatesubsamples dat<-read.table(\"cps09mar.txt\") experience<-dat[,1]-dat[,4]-6 mbf<-(dat[,11]==2)&(dat[,12]<=2)&(dat[,2]==1)&(experience==12) sam<-(dat[,11]==4)&(dat[,12]==7)&(dat[,2]==0) dat1<-dat[mbf,] dat2<-dat[sam,] # Firstregression y<-as.matrix(log(dat1[,5]/(dat1[,6]*dat1[,7]))) x<-cbind(dat1[,4],matrix(1,nrow(dat1),1)) xx<-t(x)%*%x xy<-t(x)%*%y beta<-solve(xx,xy) print(beta) # Secondregression y<-as.matrix(log(dat2[,5]/(dat2[,6]*dat2[,7]))) experience<-dat2[,1]-dat2[,4]-6 exp2<-(experience^2)/100 x<-cbind(dat2[,4],experience,exp2,matrix(1,nrow(dat2),1)) xx<-t(x)%*%x xy<-t(x)%*%y beta<-solve(xx,xy) print(beta) # Createleverageandinfluence e<-y-x%*%beta xxi<-solve(xx) leverage<-rowSums(x*(x%*%xxi)) r<-e/(1-leverage) d<-leverage*e/(1-leverage) print(max(abs(d)))",
    "page": 111,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 92 MATLABProgramFile %Loadthedataandcreatesubsamples dat=loadcps09mar.txt; #Analternativetoloadthedatafromanexcelfileis #dat=xlsread(âcps09mar.xlsxâ); experience=dat(:,1)-dat(:,4)-6; mbf=(dat(:,11)==2)&(dat(:,12)<=2)&(dat(:,2)==1)&(experience==12); sam=(dat(:,11)==4)&(dat(:,12)==7)&(dat(:,2)==0); dat1=dat(mbf,:); dat2=dat(sam,:); % Firstregression y=log(dat1(:,5)./(dat1(:,6).*dat1(:,7))); x=[dat1(:,4),ones(length(dat1),1)]; xx=xâ*x xy=xâ*y beta=xx\\xy; display(beta); % Secondregression y=log(dat2(:,5)./(dat2(:,6).*dat2(:,7))); experience=dat2(:,1)-dat2(:,4)-6; exp2=(experience.^2)/100; x=[dat2(:,4),experience,exp2,ones(length(dat2),1)]; xx=xâ*x xy=xâ*y beta=xx\\xy;display(beta); % Createleverageandinfluence e=y-x*beta; xxi=inv(xx) leverage=sum((x.*(x*xxi))â)â; d=leverage.*e./(1-leverage); influence=max(abs(d)); display(influence); _____________________________________________________________________________________________ 3.26 Exercises Exercise3.1 LetY bearandomvariablewithÂµ=(cid:69)[Y]andÏ2=var[Y].Define (cid:195) (cid:33) g (cid:161) y,Âµ,Ï2(cid:162)= (cid:161) yâ y Âµ â (cid:162)2 Âµ âÏ2 . Let(Âµ,Ï2)bethevaluessuchthatg (Âµ,Ï2)=0whereg (m,s)=n â1(cid:80)n g (cid:161) y ,m,s (cid:162) .ShowthatÂµandÏ2 (cid:98) (cid:98) n (cid:98) (cid:98) n i=1 i (cid:98) (cid:98) arethesamplemeanandvariance. Exercise3.2 ConsidertheOLSregressionofthenÃ1vectorY onthenÃk matrix X. Consideranal- ternativesetofregressors Z =XC,whereC isakÃk non-singularmatrix. Thus,eachcolumnof Z isa",
    "page": 112,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 93 mixtureofsomeofthecolumnsofX.ComparetheOLSestimatesandresidualsfromtheregressionofY onX totheOLSestimatesfromtheregressionofY onZ. Exercise3.3 Usingmatrixalgebra,showX (cid:48) e=0. (cid:98) Exercise3.4 Lete betheOLSresidualfromaregressionofY onX =[X X ].FindX (cid:48) e. (cid:98) 1 2 2(cid:98) Exercise3.5 Let e be the OLS residual from a regression of Y on X. Find the OLS coefficient from a (cid:98) regressionofe onX. (cid:98) Exercise3.6 LetY(cid:98) =X(X (cid:48) X) â1X (cid:48) Y.FindtheOLScoefficientfromaregressionofY(cid:98) onX. Exercise3.7 ShowthatifX =[X X ]thenPX =X andMX =0. 1 2 1 1 1 Exercise3.8 ShowthatM isidempotent: MM=M. Exercise3.9 ShowthattrM=nâk. Exercise3.10 ShowthatifX =[X X ]andX (cid:48) X =0thenP=P +P . 1 2 1 2 1 2 Exercise3.11 ShowthatwhenX containsaconstant,n â1(cid:80)n i=1 Y(cid:98)i =Y. Exercise3.12 Adummyvariabletakesononlythevalues0and1.Itisusedforcategoricalvariables.Let D andD bevectorsof1âsand0âs,withtheith elementofD equaling1andthatofD equaling0ifthe 1 2 1 2 personisaman,andthereverseifthepersonisawoman.Supposethattherearen menandn women 1 2 inthesample.ConsiderfittingthefollowingthreeequationsbyOLS Y =Âµ+D Î± +D Î± +e (3.52) 1 1 2 2 Y =D Î± +D Î± +e (3.53) 1 1 2 2 Y =Âµ+D Ï+e (3.54) 1 Canallthreeequations(3.52),(3.53),and(3.54)beestimatedbyOLS?Explainifnot. (a) Compareregressions(3.53)and(3.54).Isonemoregeneralthantheother?Explaintherelationship betweentheparametersin(3.53)and(3.54). (b) Compute1 (cid:48) D and1 (cid:48) D ,where1 isannÃ1vectorofones. n 1 n 2 n Exercise3.13 LetD andD bedefinedasinthepreviousexercise. 1 2 (a) IntheOLSregression Y =D Î³ +D Î³ +u, 1(cid:98)1 2(cid:98)2 (cid:98) show that Î³ is the sample mean of the dependent variable among the men of the sample (Y ), (cid:98)1 1 andthatÎ³ isthesamplemeanamongthewomen(Y ). (cid:98)2 2 (b) LetX (nÃk)beanadditionalmatrixofregressors.Describeinwordsthetransformations Y â=Y âD Y âD Y 1 1 2 2 X â=X âD X (cid:48) âD X (cid:48) 1 1 2 2 whereX andX arethekÃ1meansoftheregressorsformenandwomen,respectively. 1 2",
    "page": 113,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 94 (c) CompareÎ² (cid:101)fromtheOLSregression Y â=X âÎ² (cid:101) + (cid:101) e withÎ² (cid:98)fromtheOLSregression Y =D 1 Î± (cid:98)1 +D 2 Î± (cid:98)2 +XÎ² (cid:98) + (cid:98) e. Exercise3.14 LetÎ² (cid:98)n =(cid:161) X (cid:48) n X n (cid:162)â1 X (cid:48) n Y n denotetheOLSestimatewhenY n isnÃ1andX n isnÃk.Anew observation(Y n+1 ,X n+1 )becomesavailable.ProvethattheOLSestimatecomputedusingthisadditional observationis Î² (cid:98)n+1 =Î² (cid:98)n + 1+X n (cid:48) +1 (cid:161) X (cid:48) n 1 X n (cid:162)â1 X n+1 (cid:161) X (cid:48) n X n (cid:162)â1 X n+1 (cid:161) Y n+1 âX n (cid:48) +1 Î² (cid:98)n (cid:162) . Exercise3.15 ProvethatR2isthesquareofthesamplecorrelationbetweenY andY(cid:98). Exercise3.16 Considertwoleastsquaresregressions Y =X 1 Î² (cid:101)1 + (cid:101) e and Y =X 1 Î² (cid:98)1 +X 2 Î² (cid:98)2 + (cid:98) e. LetR2 andR2 betheR-squaredfromthetworegressions. ShowthatR2 â¥R2.Isthereacase(explain) 1 2 2 1 whenthereisequalityR2=R2? 2 1 Exercise3.17 ForÏ2definedin(3.46),showthatÏ2â¥Ï2.Isequalitypossible? (cid:101) (cid:101) (cid:98) Exercise3.18 ForwhichobservationswillÎ² (cid:98)(âi) =Î² (cid:98)? Exercise3.19 Fortheintercept-onlymodelY =Î²+e ,showthattheleave-one-outpredictionerroris i i (cid:179) n (cid:180)(cid:179) (cid:180) e = Y âY . (cid:101)i nâ1 i Exercise3.20 Definetheleave-one-outestimatorofÏ2, Ï (cid:98) 2 (âi) = nâ 1 1 (cid:88)(cid:179) Y j âX j (cid:48)Î² (cid:98)(âi) (cid:180)2 . j(cid:54)=i Thisistheestimatorobtainedfromthesamplewithobservationi omitted.Showthat Ï2 = n Ï2â e (cid:98)i 2 . (cid:98)(âi) nâ1 (cid:98) (nâ1)(1âh ) ii Exercise3.21 Considertheleastsquaresregressionestimators Y i =X 1i Î² (cid:98)1 +X 2i Î² (cid:98)2 +e (cid:98)i andtheâoneregressoratatimeâregressionestimators Y i =X 1i Î² (cid:101)1 +e (cid:101)1i , Y i =X 2i Î² (cid:101)2 +e (cid:101)2i UnderwhatconditiondoesÎ² (cid:101)1 =Î² (cid:98)1 andÎ² (cid:101)2 =Î² (cid:98)2 ?",
    "page": 114,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 95 Exercise3.22 Youestimatealeastsquaresregression Y i =X 1 (cid:48) i Î² (cid:101)1 +u (cid:101)i andthenregresstheresidualsonanothersetofregressors u (cid:101)i =X 2 (cid:48) i Î² (cid:101)2 +e (cid:101)i Doesthissecondregressiongiveyouthesameestimatedcoefficientsasfromestimationofaleastsquares regressiononbothsetofregressors? Y i =X 1 (cid:48) i Î² (cid:98)1 +X 2 (cid:48) i Î² (cid:98)2 +e (cid:98)i Inotherwords,isittruethatÎ² (cid:101)2 =Î² (cid:98)2 ?Explainyourreasoning. Exercise3.23 Thedatamatrixis(Y,X)withX =[X ,X ],andconsiderthetransformedregressormatrix 1 2 Z =[X ,X âX ].SupposeyoudoaleastsquaresregressionofY onX,andaleastsquaresregressionof 1 2 1 Y on Z.LetÏ2 andÏ2 denotetheresidualvarianceestimatesfromthetworegressions. Giveaformula (cid:98) (cid:101) relatingÏ2andÏ2?(Explainyourreasoning.) (cid:98) (cid:101) Exercise3.24 Usethecps09mardatasetdescribedinSection3.22andavailableonthetextbookwebsite. Takethesub-sampleusedforequation(3.49)(seeSection3.25)fordataconstruction) (a) Estimateequation(3.49)andcomputetheequationR2andsumofsquarederrors. (b) Re-estimatetheslopeoneducationusingtheresidualregressionapproach. Regresslog(wage)on experienceanditssquare,regresseducationonexperienceanditssquare,andtheresidualsonthe residuals. Reporttheestimatesfromthisfinalregression,alongwiththeequationR2 andsumof squarederrors.Doestheslopecoefficientequalthevaluein(3.49)?Explain. (c) AretheR2andsum-of-squarederrorsfromparts(a)and(b)equal?Explain. Exercise3.25 Estimateequation(3.49)asinpart(a)ofthepreviousquestion. Lete betheOLSresid- (cid:98)i ual, Y(cid:98)i the predicted value from the regression, X 1i be education and X 2i be experience. Numerically calculatethefollowing: (a) (cid:80)n e i=1(cid:98)i (b) (cid:80)n X e i=1 1i(cid:98)i (c) (cid:80)n X e i=1 2i(cid:98)i (d) (cid:80)n X2e i=1 1i(cid:98)i (e) (cid:80)n X2e i=1 2i(cid:98)i (f) (cid:80)n i=1 Y(cid:98)i e (cid:98)i (g) (cid:80)n e2 i=1(cid:98)i ArethesecalculationsconsistentwiththetheoreticalpropertiesofOLS?Explain. Exercise3.26 Usethecps09mardataset.",
    "page": 115,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER3. THEALGEBRAOFLEASTSQUARES 96 (a) EstimatealogwageregressionforthesubsampleofwhitemaleHispanics. Inadditiontoeduca- tion,experience,anditssquare,includeasetofbinaryvariablesforregionsandmaritalstatus.For regions, createdummyvariablesforNortheast, SouthandWestsothatMidwestistheexcluded group.Formaritalstatus,createvariablesformarried,widowedordivorced,andseparated,sothat single(nevermarried)istheexcludedgroup. (b) Repeatusingadifferenteconometricpackage.Compareyourresults.Dotheyagree?",
    "page": 116,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Chapter 4 Least Squares Regression 4.1 Introduction In this chapter we investigate some finite-sample properties of the least squares estimator in the linearregressionmodel. Inparticularwecalculatethefinite-samplemeanandcovariancematrixand proposestandarderrorsforthecoefficientestimators. 4.2 RandomSampling Assumption 3.1 specified that the observations have identical distributions. To derive the finite- samplepropertiesoftheestimatorswewillneedtoadditionallyspecifythedependencestructureacross theobservations. Thesimplestcontextiswhentheobservationsaremutuallyindependentinwhichcasewesaythat theyareindependentandidenticallydistributedori.i.d. Itisalsocommontodescribei.i.d. observa- tionsasarandomsample. Traditionally, randomsamplinghasbeenthedefaultassumptionincross- section(e.g. survey)contexts. Itisquiteconvenientasi.i.d. samplingleadstostraightforwardexpres- sions for estimation variance. The assumption seems appropriate (meaning that it should be approx- imately valid) when samples are small and relatively dispersed. That is, if you randomly sample 1000 peoplefromalargecountrysuchastheUnitedStatesitseemsreasonabletomodeltheirresponsesas mutuallyindependent. Assumption4.1 The random variables {(Y ,X ),...,(Y ,X ),...,(Y ,X )} are inde- 1 1 i i n n pendentandidenticallydistributed. FormostofthischapterwewilluseAssumption4.1toderivepropertiesoftheOLSestimator. Assumption4.1meansthatifyoutakeanytwoindividualsi (cid:54)=j inasample,thevalues(Y ,X )arein- i i dependentofthevalues(Y ,X )yethavethesamedistribution.Independencemeansthatthedecisions j j andchoicesofindividuali donotaffectthedecisionsofindividual j andconversely. Thisassumptionmaybeviolatedifindividualsinthesampleareconnectedinsomeway,forexample iftheyareneighbors, membersofthesamevillage, classmatesataschool, orevenfirmswithinaspe- cificindustry. Inthiscaseitseemsplausiblethatdecisionsmaybeinter-connectedandthusmutually dependentratherthanindependent. Allowingforsuchinteractionscomplicatesinferenceandrequires specializedtreatment. Acurrentlypopularapproachwhichallowsformutualdependenceisknownas 97",
    "page": 117,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 98 clustereddependencewhichassumesthatthatobservationsaregroupedintoâclustersâ(forexample, schools).WewilldiscussclusteringinmoredetailinSection4.23. 4.3 SampleMean Westartwiththesimplestsettingoftheintercept-onlymodel Y =Âµ+e (cid:69)[e]=0. which is equivalent to the regression model with k =1 and X =1. In the intercept model Âµ=(cid:69)[Y] is i theexpectationofY .(SeeExercise2.15.) TheleastsquaresestimatorÂµ=Y equalsthesamplemeanas i (cid:98) showninequation(3.8). WenowcalculatetheexpectationandvarianceoftheestimatorY.Sincethesamplemeanisalinear functionoftheobservationsitsexpectationissimpletocalculate (cid:34) (cid:35) (cid:104) (cid:105) 1 (cid:88) n 1 (cid:88) n (cid:69) Y =(cid:69) Y = (cid:69)[Y ]=Âµ. i i n n i=1 i=1 Thisshowsthattheexpectedvalueoftheleastsquaresestimator(thesamplemean)equalstheprojec- tioncoefficient(thepopulationexpectation). Anestimatorwiththepropertythatitsexpectationequals theparameteritisestimatingiscalledunbiased. Definition4.1 AnestimatorÎ¸ (cid:98)forÎ¸isunbiasedif(cid:69)(cid:163)Î¸ (cid:98) (cid:164)=Î¸. We next calculate the variance of the estimator Y under Assumption 4.1. Making the substitution Y =Âµ+e wefind i i 1 (cid:88) n Y âÂµ= e . i n i=1 Then (cid:183) (cid:184) (cid:104) (cid:105) (cid:179) (cid:180)2 var Y =(cid:69) Y âÂµ (cid:34)(cid:195) (cid:33)(cid:195) (cid:33)(cid:35) 1 (cid:88) n 1 (cid:88) n =(cid:69) e e i j n n i=1 j=1 = 1 (cid:88) n (cid:88) n (cid:69)(cid:163) e e (cid:164) n2 i j i=1j=1 = 1 (cid:88) n Ï2 n2 i=1 1 = Ï2. n Thesecond-to-lastinequalityisbecause(cid:69)(cid:163) e e (cid:164)=Ï2 fori = j yet(cid:69)(cid:163) e e (cid:164)=0fori (cid:54)= j duetoindepen- i j i j dence. (cid:104) (cid:105) Wehaveshownthatvar Y = 1Ï2.Thisisthefamiliarformulaforthevarianceofthesamplemean. n",
    "page": 118,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 99 4.4 LinearRegressionModel Wenowconsiderthelinearregressionmodel.Throughoutthischapterwemaintainthefollowing. Assumption4.2 LinearRegressionModel Thevariables(Y,X)satisfythelinearregressionequation Y =X (cid:48)Î²+e (4.1) (cid:69)[e|X]=0. (4.2) Thevariableshavefinitesecondmoments (cid:69)(cid:163) Y2(cid:164)<â, (cid:69)(cid:107)X(cid:107)2<â, andaninvertibledesignmatrix Q =(cid:69)(cid:163) XX (cid:48)(cid:164)>0. XX Wewillconsiderboththegeneralcaseofheteroskedasticregressionwheretheconditionalvariance (cid:69)(cid:163) e2|X (cid:164)=Ï2(X)isunrestricted,andthespecializedcaseofhomoskedasticregressionwherethecondi- tionalvarianceisconstant.Inthelattercaseweaddthefollowingassumption. Assumption4.3 HomoskedasticLinearRegressionModel InadditiontoAssumption4.2 (cid:69)(cid:163) e2|X (cid:164)=Ï2(X)=Ï2 (4.3) isindependentofX. 4.5 ExpectationofLeastSquaresEstimator InthissectionweshowthattheOLSestimatorisunbiasedinthelinearregressionmodel. Thiscal- culationcanbedoneusingeithersummationnotationormatrixnotation.Wewilluseboth. Firsttakesummationnotation.Observethatunder(4.1)-(4.2) (cid:69)[Y |X ,...,X ]=(cid:69)[Y |X ]=X (cid:48)Î². (4.4) i 1 n i i i ThefirstequalitystatesthattheconditionalexpectationofY given{X ,...,X }onlydependsonX since i 1 n i theobservationsareindependentacrossi.Thesecondequalityistheassumptionofalinearconditional expectation.",
    "page": 119,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 100 Usingdefinition(3.11),theconditioningtheorem(Theorem2.3),thelinearityofexpectations,(4.4), andpropertiesofthematrixinverse, (cid:34)(cid:195) (cid:33)â1(cid:195) (cid:33)(cid:175) (cid:35) (cid:69)(cid:163)Î² (cid:98) |X 1 ,...,X n (cid:164)=(cid:69) (cid:88) n X i X i (cid:48) (cid:88) n X i Y i (cid:175) (cid:175) (cid:175) X 1 ,...,X n i=1 i=1 (cid:175) (cid:195) (cid:33)â1 (cid:34)(cid:195) (cid:33)(cid:175) (cid:35) = (cid:88) n X X (cid:48) (cid:69) (cid:88) n X Y (cid:175) (cid:175)X ,...,X i i i i (cid:175) 1 n i=1 i=1 (cid:175) (cid:195) (cid:33)â1 n n = (cid:88) X X (cid:48) (cid:88) (cid:69)[X Y |X ,...,X ] i i i i 1 n i=1 i=1 (cid:195) (cid:33)â1 n n = (cid:88) X X (cid:48) (cid:88) X (cid:69)[Y |X ] i i i i i i=1 i=1 (cid:195) (cid:33)â1 n n = (cid:88) X X (cid:48) (cid:88) X X (cid:48)Î² i i i i i=1 i=1 =Î². Nowletâsshowthesameresultusingmatrixnotation.(4.4)implies ï£« . ï£¶ ï£« . ï£¶ . . . . (cid:69)[Y |X]=ï£¬ ï£¬ ï£­ (cid:69)[Y i |X] ï£· ï£· ï£¸ =ï£¬ ï£¬ ï£­ X i (cid:48)Î² ï£· ï£· ï£¸ =XÎ². (4.5) . . . . . . Similarly ï£« . ï£¶ ï£« . ï£¶ . . . . (cid:69)[e|X]=ï£¬ ï£¬ (cid:69)[e i |X] ï£· ï£· =ï£¬ ï£¬ (cid:69)[e i |X i ] ï£· ï£· =0. ï£­ ï£¸ ï£­ ï£¸ . . . . . . UsingÎ² (cid:98) =(cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48) Y (cid:162) ,theconditioningtheorem,thelinearityofexpectations,(4.5),andtheprop- ertiesofthematrixinverse, (cid:69)(cid:163)Î² (cid:98) |X (cid:164)=(cid:69) (cid:104) (cid:161) X (cid:48) X (cid:162)â1 X (cid:48) Y |X (cid:105) =(cid:161) X (cid:48) X (cid:162)â1 X (cid:48)(cid:69)[Y |X] =(cid:161) X (cid:48) X (cid:162)â1 X (cid:48) XÎ² =Î². Attheriskofbelaboringthederivation,anotherwaytocalculatethesameresultisasfollows. Insert Y =XÎ²+e intotheformulaforÎ² (cid:98)toobtain Î² (cid:98) =(cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48)(cid:161) XÎ²+e (cid:162)(cid:162) =(cid:161) X (cid:48) X (cid:162)â1 X (cid:48) XÎ²+(cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48) e (cid:162) =Î²+(cid:161) X (cid:48) X (cid:162)â1 X (cid:48) e. (4.6) This is a useful linear decomposition of the estimator Î² (cid:98)into the true parameter Î² and the stochastic (cid:161) (cid:48) (cid:162)â1 (cid:48) component X X X e.Onceagain,wecancalculatethat (cid:69)(cid:163)Î² (cid:98) âÎ²|X (cid:164)=(cid:69) (cid:104) (cid:161) X (cid:48) X (cid:162)â1 X (cid:48) e|X (cid:105) =(cid:161) X (cid:48) X (cid:162)â1 X (cid:48)(cid:69)[e|X]=0.",
    "page": 120,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 101 Regardlessofthemethodwehaveshownthat(cid:69)(cid:163)Î² (cid:98) |X (cid:164)=Î².Wehaveshownthefollowingtheorem. Theorem4.1 ExpectationofLeastSquaresEstimator In the linear regression model (Assumption 4.2) with i.i.d. sampling (Assumption4.1) (cid:69)(cid:163)Î² (cid:98) |X (cid:164)=Î². (4.7) Equation (4.7) says that the estimator Î² (cid:98)is unbiased for Î², conditional on X. This means that the conditionaldistributionofÎ² (cid:98)iscenteredatÎ². Byâconditionalon Xâthismeansthatthedistributionis unbiased(centeredatÎ²)foranyrealizationoftheregressormatrixX. Inconditionalmodelswesimply refertothisassayingâÎ² (cid:98)isunbiasedforÎ²â. ItisworthmentioningthatTheorem4.1,andallfinitesampleresultsinthischapter,maketheim- (cid:48) plicitassumptionthatX X isfullrankwithprobabilityone. 4.6 VarianceofLeastSquaresEstimator InthissectionwecalculatetheconditionalvarianceoftheOLSestimator. ForanyrÃ1randomvectorZ definetherÃr covariancematrix var[Z]=(cid:69)(cid:163) (Zâ(cid:69)[Z])(Zâ(cid:69)[Z]) (cid:48)(cid:164)=(cid:69)(cid:163) ZZ (cid:48)(cid:164)â((cid:69)[Z])((cid:69)[Z]) (cid:48) andforanypair(Z,X)definetheconditionalcovariancematrix var[Z |X]=(cid:69)(cid:163) (Zâ(cid:69)[Z |X])(Zâ(cid:69)[Z |X]) (cid:48)|X (cid:164) . WedefineV Î²(cid:98) d=ef var (cid:163)Î² (cid:98) |X (cid:164) astheconditionalcovariancematrixoftheregressioncoefficientestimators. Wenowderiveitsform. TheconditionalcovariancematrixofthenÃ1regressionerrore isthenÃnmatrix var[e|X]=(cid:69)(cid:163) ee (cid:48)|X (cid:164)d=ef D. Theith diagonalelementofD is (cid:69)(cid:163) e2|X (cid:164)=(cid:69)(cid:163) e2|X (cid:164)=Ï2 i i i i whiletheijth off-diagonalelementofD is (cid:69)(cid:163) e e |X (cid:164)=(cid:69)(e |X )(cid:69)(cid:163) e |X (cid:164)=0 i j i i j j wherethefirstequalityusesindependenceoftheobservations(Assumption4.1)andthesecondis(4.2). ThusD isadiagonalmatrixwithith diagonalelementÏ2: i ï£« ï£¶ Ï2 0 Â·Â·Â· 0 1 ï£¬ 0 Ï2 Â·Â·Â· 0 ï£· D=diag (cid:161)Ï2 1 ,...,Ï2 n (cid:162)=ï£¬ ï£¬ ï£¬ . . . . . . 2 ... . . . ï£· ï£· ï£· . (4.8) ï£­ ï£¸ 0 0 Â·Â·Â· Ï2 n",
    "page": 121,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 102 Inthespecialcaseofthelinearhomoskedasticregressionmodel(4.3),then(cid:69)(cid:163) e2|X (cid:164)=Ï2=Ï2 andwe i i i havethesimplificationD=I Ï2.Ingeneral,however,D neednotnecessarilytakethissimplifiedform. n ForanynÃr matrix A=A(X), var (cid:163) A (cid:48) Y |X (cid:164)=var (cid:163) A (cid:48) e|X (cid:164)=A (cid:48) DA. (4.9) Inparticular,wecanwriteÎ² (cid:98) =A (cid:48) Y where A=X (cid:161) X (cid:48) X (cid:162)â1 andthus V Î²(cid:98) =var (cid:163)Î² (cid:98) |X (cid:164)=A (cid:48) DA=(cid:161) X (cid:48) X (cid:162)â1 X (cid:48) DX (cid:161) X (cid:48) X (cid:162)â1 . Itisusefultonotethat n X (cid:48) DX = (cid:88) X X (cid:48)Ï2, i i i i=1 (cid:48) aweightedversionofX X. Inthespecialcaseofthelinearhomoskedasticregressionmodel,D=I Ï2,soX (cid:48) DX =X (cid:48) XÏ2,and n thecovariancematrixsimplifiestoV =(cid:161) X (cid:48) X (cid:162)â1Ï2. Î²(cid:98) Theorem4.2 VarianceofLeastSquaresEstimator Inthelinearregressionmodel(Assumption4.2)withi.i.d. sampling(Assump- tion4.1) V Î²(cid:98) =var (cid:163)Î² (cid:98) |X (cid:164)=(cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48) DX (cid:162)(cid:161) X (cid:48) X (cid:162)â1 (4.10) whereD isdefinedin(4.8). Ifinadditiontheerrorishomoskedastic(Assump- tion4.3)then(4.10)simplifiestoV =Ï2(cid:161) X (cid:48) X (cid:162)â1 . Î²(cid:98) 4.7 UnconditionalMoments The previous sections derived the form of the conditional mean and variance of the least squares estimator where we conditioned on the regressor matrix X. What about the unconditional mean and variance? Indeed,itisnotobviousifÎ² (cid:98)hasafinitemeanorvariance. Takethecaseofasingledummyvariable regressorD withnointercept.Assume(cid:80)[D =1]=p<1.Then i i (cid:80)n D Y Î²= i=1 i i (cid:98) (cid:80)n D i=1 i iswelldefinedif (cid:80)n D >0. However,(cid:80)(cid:163)(cid:80)n D =0 (cid:164)=(cid:161) 1âp (cid:162)n>0. Thismeansthatwithpositive(but i=1 i i=1 i small)probabilityÎ² (cid:98)doesnotexist.ConsequentlyÎ² (cid:98)hasnofinitemoments!Weignorethiscomplication inpracticebutitdoesposeaconundrumfortheory. Thisexistenceproblemariseswheneverthereare discreteregressors. Thisdilemmaisavoidedwhentheregressorshavecontinuousdistributions. Acleanstatementwas obtainedbyKinal(1980)undertheassumptionofnormalregressorsanderrors.",
    "page": 122,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 103 Theorem4.3 Kinal(1980) Inthelinearregressionmodelwithi.i.d. sampling,ifinaddition(X,e)havea jointnormaldistributionthenforanyr,(cid:69)(cid:176) (cid:176) Î² (cid:98) (cid:176) (cid:176) r <âifandonlyifr <nâk+1. Thisshowsthatwhentheerrorsandregressorsarenormallydistributedthattheleastsquaresesti- matorpossessesallmomentsuptonâkwhichincludesallmomentsofpracticalinterest.Thenormality assumptionisnotcriticalforthisresult. Whatiskeyistheassumptionthattheregressorsarecontinu- ouslydistributed. The law of iterated expectations (Theorem 2.1) combined with Theorems 4.1 and 4.3 allow us to deducethattheleastsquaresestimatorisunconditionallyunbiased. Underthenormalityassumption Theorem4.3allowsustoapplythelawofiteratedexpectations,andthususingTheorems4.1wededuce thatifn>k (cid:69)(cid:163)Î² (cid:98) (cid:164)=(cid:69)(cid:163)(cid:69)(cid:163)Î² (cid:98) |X (cid:164)(cid:164)=Î². HenceÎ² (cid:98)isunconditionallyunbiasedasasserted. Furthermore,ifnâk>1then(cid:69)(cid:176) (cid:176) Î² (cid:98) (cid:176) (cid:176) 2<âandÎ² (cid:98)hasafiniteunconditionalvariance. UsingTheorem 2.8wecancalculateexplicitlythat var (cid:163)Î² (cid:98) (cid:164)=(cid:69)(cid:163) var (cid:163)Î² (cid:98) |X (cid:164)(cid:164)+var (cid:163)(cid:69)(cid:163)Î² (cid:98) |X (cid:164)(cid:164)=(cid:69) (cid:104) (cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48) DX (cid:162)(cid:161) X (cid:48) X (cid:162)â1 (cid:105) thesecondequalitysince(cid:69)(cid:163)Î² (cid:98) |X (cid:164)=Î²haszerovariance.Inthehomoskedasticcasethissimplifiesto var (cid:163)Î² (cid:98) (cid:164)=Ï2(cid:69) (cid:104) (cid:161) X (cid:48) X (cid:162)â1 (cid:105) . Inbothcasestheexpectationcannotpassthroughthematrixinversesincethisisanonlinearfunction. Thus there is not a simple expression for the unconditional variance, other than stating that is it the meanoftheconditionalvariance. 4.8 Gauss-MarkovTheorem Consider the class of estimators of Î² which are linear functions of the vector Y and thus can be written as Î² (cid:101) = A (cid:48) Y where A is an nÃk function of X. As noted before, the least squares estimator is the special case obtained by setting A = X(X (cid:48) X) â1. What is the best choice of A? The Gauss-Markov theorem1 which we now present says that the least squares estimator is the best choice among linear unbiased estimators when the errors are homoskedastic, in the sense that the least squares estimator hasthesmallestvarianceamongallunbiasedlinearestimators",
    "page": 123,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". As noted before, the least squares estimator is the special case obtained by setting A = X(X (cid:48) X) â1. What is the best choice of A? The Gauss-Markov theorem1 which we now present says that the least squares estimator is the best choice among linear unbiased estimators when the errors are homoskedastic, in the sense that the least squares estimator hasthesmallestvarianceamongallunbiasedlinearestimators. Toseethis,since(cid:69)[Y |X]=XÎ²thenforanylinearestimatorÎ² (cid:101) =A (cid:48) Y wehave (cid:69)(cid:163)Î² (cid:101) |X (cid:164)=A (cid:48)(cid:69)[Y |X]=A (cid:48) XÎ², soÎ² (cid:101)isunbiasedif(andonlyif) A (cid:48) X =I k .Furthermore,wesawin(4.9)that var (cid:163)Î² (cid:101) |X (cid:164)=var (cid:163) A (cid:48) Y |X (cid:164)=A (cid:48) DA=A (cid:48) AÏ2 thelastequalityusingthehomoskedasticityassumptionD=I Ï2.Theâbestâunbiasedlinearestimator n is obtained by finding the matrix A satisfying A (cid:48) X = I such that A (cid:48) A is minimized in the positive 0 0 k 0 0 1NamedafterthemathematiciansCarlFriedrichGaussandAndreyMarkov.",
    "page": 123,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 104 definitesense,whichmeansthatforanyothermatrix Asatisfying A (cid:48) X =I then A (cid:48) AâA (cid:48) A ispositive k 0 0 semi-definite. Theorem4.4 Gauss-Markov Inthehomoskedasticlinearregressionmodel(Assumption4.3)withi.i.d.sam- pling(Assumption4.1),ifÎ² (cid:101)isalinearunbiasedestimatorofÎ²then var (cid:163)Î² (cid:101) |X (cid:164)â¥Ï2(cid:161) X (cid:48) X (cid:162)â1 . TheGauss-Markovtheoremprovidesalowerboundonthecovariancematrixofunbiasedlineares- timatorsundertheassumptionofhomoskedasticity. Itsaysthatnounbiasedlinearestimatorcanhave avariancematrixsmaller(inthepositivedefinitesense)thanÏ2(cid:161) X (cid:48) X (cid:162)â1 . SincethevarianceoftheOLS estimator is exactly equal to this bound this means that the OLS estimator is efficient in the class of linearunbiasedestimators. ThisgivesrisetothedescriptionofOLSasBLUE,standingforâbestlinear unbiasedestimatorâ.Thisisanefficiencyjustificationfortheleastsquaresestimator.Thejustificationis limitedbecausetheclassofmodelsisrestrictedtohomoskedasticregressionsandtheclassofpotential estimatorsisrestrictedtolinearunbiasedestimators.Thislatterrestrictionisparticularlyunsatisfactory asthereisnosensiblemotivationforfocusingonlinearestimators. WecompletethissectionwithaproofoftheGauss-Markovtheorem. Let A beanynÃk functionof X suchthat A (cid:48) X =I .Theestimator A (cid:48) Y isunbiasedforÎ²andhas k varianceA (cid:48) AÏ2.Sincetheleastsquaresestimatorisunbiasedandhasvariance (cid:161) X (cid:48) X (cid:162)â1Ï2,itissufficient toshowthatthedifferenceinthetwovariancematricesispositivesemi-definite,or C =A (cid:48) Aâ(cid:161) X (cid:48) X (cid:162)â1â¥0. (4.11) NotethatX (cid:48) C =0.Wecalculatethat A (cid:48) Aâ(cid:161) X (cid:48) X (cid:162)â1= (cid:179) C+X (cid:161) X (cid:48) X (cid:162)â1 (cid:180)(cid:48)(cid:179) C+X (cid:161) X (cid:48) X (cid:162)â1 (cid:180) â(cid:161) X (cid:48) X (cid:162)â1 =C (cid:48) C+C (cid:48) X (cid:161) X (cid:48) X (cid:162)â1+(cid:161) X (cid:48) X (cid:162)â1 X (cid:48) C +(cid:161) X (cid:48) X (cid:162)â1 X (cid:48) X (cid:161) X (cid:48) X (cid:162)â1â(cid:161) X (cid:48) X (cid:162)â1 =C (cid:48) C â¥0. (cid:48) ThefinalinequalitystatesthatthematrixC C ispositivesemi-definitewhichisapropertyofquadratic forms(seeAppendixA.10).Wehaveshown(4.11)asrequred. 4.9 ModernGauss-MarkovTheorem InthissectionweestablishanimprovedversionoftheGauss-MarkovTheorem. Whatisimportant aboutthisresultisthatitremovestherestrictiontolinearestimators. Theorem4.5 ModernGauss-Markov Inthelinearregressionmodelwithi.i.d.sampling,if(cid:69)(cid:163)Î² (cid:101) |X (cid:164)=Î²andAssump- tion4.3holdsthenvar (cid:163)Î² (cid:101) |X (cid:164)â¥Ï2(cid:161) X (cid:48) X (cid:162)â1 .",
    "page": 124,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 105 TheproofofTheorem4.5istechnicallyadvancedsoweleaveittoSection4.26.Itisageneralizationof Theorem11.1fromIntroductiontoEconometricsforthebestunbiasedestimationofthemean.Theorem 4.5isdifferentfrombothclassicalGauss-MarkovandCramÃ©r-Raoefficiencyresultsinthatitdoesnot restrictattentiontolinearestimators(asintheGauss-MarkovTheorem)norrestrictsamplingtonormal variables (as in the CramÃ©r-Rao Theorem). Theorem 4.7 dominates both these classical results as the latterholdasspecialcases. TheinterpretationofTheorem4.5issimilartoTheorem4.4. Theorem4.5showsthatthecovariance matrixÏ2(cid:161) X (cid:48) X (cid:162)â1 isthebestpossibleamongallunbiasedestimators. 4.10 GeneralizedLeastSquares Takethelinearregressionmodelinmatrixformat Y =XÎ²+e. (4.12) Considerageneralizedsituationwheretheobservationerrorsarepossiblycorrelatedand/orheteroskedas- tic.Specifically,supposethat (cid:69)[e|X]=0 (4.13) var[e|X]=â¦ (4.14) forsomenÃncovariancematrixâ¦,possiblyafunctionofX.Thisincludesthei.i.d.samplingframework whereâ¦=D asdefinedin(4.8)butallowsfornon-diagonalcovariancematricesaswell.Asacovariance matrix,â¦isnecessarilysymmetricandpositivesemi-definite. Undertheseassumptions,byargumentssimilartotheprevioussectiondwecancalculatethemean andvarianceoftheOLSestimator: (cid:69)(cid:163)Î² (cid:98) |X (cid:164)=Î² (4.15) var (cid:163)Î² (cid:98) |X (cid:164)=(cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48)â¦X (cid:162)(cid:161) X (cid:48) X (cid:162)â1 (4.16) (seeExercise4.5). WehaveananalogoftheGauss-MarkovTheorem. Theorem4.6 GeneralizedGauss-Markov Inthelinearregressionmodel(Assumption4.2)andâ¦>0,ifÎ² (cid:101)isalinearunbi- asedestimatorofÎ²then var (cid:163)Î² (cid:101) |X (cid:164)â¥(cid:161) X (cid:48)â¦â1X (cid:162)â1 . WeleavetheproofforExercise4.6. The theorem provides a lower bound on the covariance matrix of unbiased linear estimators. The bound is different from the variance matrix of the OLS estimator as stated in (4.16) except when â¦= I Ï2. Inthei.i.d. samplingcasethevariancelowerboundis (cid:161) X (cid:48) D â1X (cid:162)â1 sinceâ¦=D. Thefactthatthe n varianceboundisdifferent(andlower)thantheleastsquaresvariancesuggeststhatwecanimproveon theOLSestimator. Thisisindeedthecasewhenâ¦isknownuptoscale. Thatis,supposethatâ¦=c2Î£wherec2>0is realandÎ£isnÃnandknown.Takethelinearmodel(4.12)andpre-multiplybyÎ£â1/2.Thisproducesthe",
    "page": 125,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 106 equationY(cid:101) =X(cid:101) Î²+ (cid:101) e whereY(cid:101) =Î£â1/2Y, X(cid:101) =Î£â1/2X,and (cid:101) e =Î£â1/2e. ConsiderOLSestimationofÎ²in thisequation. (cid:179) (cid:48) (cid:180)â1 (cid:48) Î² (cid:101)gls = X(cid:101) X(cid:101) X(cid:101) Y(cid:101) = (cid:179) (cid:161)Î£â1/2X (cid:162)(cid:48)(cid:161)Î£â1/2X (cid:162) (cid:180)â1(cid:161)Î£â1/2X (cid:162)(cid:48)(cid:161)Î£â1/2Y (cid:162) =(cid:161) X (cid:48)Î£â1X (cid:162)â1 X (cid:48)Î£â1Y. (4.17) ThisiscalledtheGeneralizedLeastSquares(GLS)estimatorofÎ²andwasintroducedbyAitken(1935). Youcancalculatethat (cid:69)(cid:163)Î² (cid:101)gls |X (cid:164)=Î² (4.18) var (cid:163)Î² (cid:101)gls |X (cid:164)=(cid:161) X (cid:48)â¦â1X (cid:162)â1 . (4.19) ThisshowsthattheGLSestimatorisunbiasedandhasacovariancematrixwhichequalsthelowerbound fromTheorem4.6. ThisshowsthatthelowerboundissharpwhenÎ£isknown. GLSisthusefficientin theclassoflinearunbiasedestimators. Inthelinearregressionmodelwithindependentobservationsandknownconditionalvariances,so thatâ¦=Î£=D=diag (cid:161)Ï2,...,Ï2(cid:162) ,theGLSestimatortakestheform 1 n Î² (cid:101)gls =(cid:161) X (cid:48) D â1X (cid:162)â1 X (cid:48) D â1Y (cid:195) (cid:33)â1(cid:195) (cid:33) n n = (cid:88) Ïâ2X X (cid:48) (cid:88) Ïâ2X Y . i i i i i i i=1 i=1 Theassumptionâ¦>0inthiscasereducestoÏ2>0fori =1,...n. i Inpractice,thecovariancematrixâ¦isunknownsotheGLSestimatoraspresentedhereisnotfeasi- ble. However,theformoftheGLSestimatormotivatesfeasibleversions,effectivelybyreplacingâ¦with anestimator.Wedonotpursuethishereasitisnotcommonincurrentappliedeconometricpractice. 4.11 ModernGeneralizedGaussMarkovTheorem In this section we establish a version of the Generalized Gauss-Markov Theorem which does not requirelinearestimators. Theorem4.7 ModernGeneralizedGauss-Markov In the linear regression model with i.i.d. sampling, if (cid:69)(cid:163)Î² (cid:101) |X (cid:164) = Î² then var (cid:163)Î² (cid:101) |X (cid:164)â¥(cid:161) X (cid:48) D â1X (cid:162)â1 . TheproofofTheorem4.7istechnicallyadvancedsoweleaveittoSection4.26. TheinterpretationofTheorem4.7issimilartoTheorem4.6underi.i.d.sampling.Theorem4.7shows thattheGLScovariancematrix (cid:161) X (cid:48) D â1X (cid:162)â1 isthebestpossibleamongallunbiasedestimators.",
    "page": 126,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 107 4.12 Residuals Whataresomepropertiesoftheresidualse (cid:98)i =Y i âX i (cid:48)Î² (cid:98)andpredictionerrorse (cid:101)i =Y i âX i (cid:48)Î² (cid:98)(âi) inthe contextofthelinearregressionmodel? Recall from (3.24) that we can write the residuals in vector notation as e = Me where M = I â (cid:98) n (cid:161) (cid:48) (cid:162)â1 (cid:48) X X X X istheorthogonalprojectionmatrix.Usingthepropertiesofconditionalexpectation (cid:69)[e|X]=(cid:69)[Me|X]=M(cid:69)[e|X]=0 (cid:98) and var[e|X]=var[Me|X]=Mvar[e|X]M=MDM (4.20) (cid:98) whereD isdefinedin(4.8). Wecansimplifythisexpressionundertheassumptionofconditionalhomoskedasticity (cid:69)(cid:163) e2|X (cid:164)=Ï2. Inthiscase(4.20)simplifiesto var[e|X]=MÏ2. (4.21) (cid:98) Inparticular,forasingleobservationi wecanfindthevarianceofe bytakingtheith diagonalelement (cid:98)i of(4.21).Sincetheith diagonalelementofM is1âh asdefinedin(3.40)weobtain ii var[e |X]=(cid:69)(cid:163) e2|X (cid:164)=(1âh )Ï2. (4.22) (cid:98)i (cid:98)i ii Asthisvarianceisafunctionofh andhenceX theresidualse areheteroskedasticeveniftheerrorse ii i (cid:98)i i arehomoskedastic.Noticeaswellthat(4.22)impliese2isabiasedestimatorofÏ2. (cid:98)i Similarly,recallfrom(3.45)thatthepredictionerrorse =(1âh ) â1e canbewritteninvectornota- (cid:101)i ii (cid:98)i tionase =M â e whereM â isadiagonalmatrixwithith diagonalelement(1âh ) â1.Thuse =M â Me. (cid:101) (cid:98) ii (cid:101) Wecancalculatethat (cid:69)[e|X]=M â M(cid:69)[e|X]=0 (cid:101) and var[e|X]=M â Mvar[e|X]MM â=M â MDMM â (cid:101) whichsimplifiesunderhomoskedasticityto var[e|X]=M â MMM âÏ2=M â MM âÏ2. (cid:101) Thevarianceoftheith predictionerroristhen var[e |X]=(cid:69)(cid:163) e2|X (cid:164) (cid:101)i (cid:101)i =(1âh ) â1(1âh )(1âh ) â1Ï2 ii ii ii =(1âh ) â1Ï2. ii Aresidualwithconstantconditionalvariancecanbeobtainedbyrescaling.Thestandardizedresid- ualsare e =(1âh ) â1/2e , (4.23) i ii (cid:98)i andinvectornotation e=(cid:161) e ,...,e (cid:162)(cid:48) =M â1/2Me. (4.24) 1 n",
    "page": 127,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 108 Fromtheabovecalculations,underhomoskedasticity, var (cid:163) e|X (cid:164)=M â1/2MM â1/2Ï2 and var (cid:163) e |X (cid:164)=(cid:69)(cid:163) e2|X (cid:164)=Ï2 i i andthusthesestandardizedresidualshavethesamebiasandvarianceastheoriginalerrorswhenthe latterarehomoskedastic. 4.13 EstimationofErrorVariance TheerrorvarianceÏ2=(cid:69)(cid:163) e2(cid:164) canbeaparameterofinteresteveninaheteroskedasticregressionor aprojectionmodel.Ï2measuresthevariationintheâunexplainedâpartoftheregression.Itsmethodof momentsestimator(MME)isthesampleaverageofthesquaredresiduals: Ï2= 1 (cid:88) n e2. (cid:98) n (cid:98)i i=1 InthelinearregressionmodelwecancalculatethemeanofÏ2.From(3.28)andthepropertiesofthe (cid:98) traceoperatorobservethat Ï2= 1 e (cid:48) Me= 1 tr (cid:161) e (cid:48) Me (cid:162)= 1 tr (cid:161) Mee (cid:48)(cid:162) . (cid:98) n n n Then (cid:69)(cid:163)Ï2|X (cid:164)= 1 tr (cid:161)(cid:69)(cid:163) Mee (cid:48)|X (cid:164)(cid:162) (cid:98) n = 1 tr (cid:161) M(cid:69)(cid:163) ee (cid:48)|X (cid:164)(cid:162) n 1 = tr(MD) (4.25) n = 1 (cid:88) n (1âh )Ï2. n ii i i=1 ThefinalequalityholdssincethetraceisthesumofthediagonalelementsofMD,andsinceD isdiago- nalthediagonalelementsofMD aretheproductofthediagonalelementsofM andD whichare1âh ii andÏ2,respectively. i Addingtheassumptionofconditionalhomoskedasticity(cid:69)(cid:163) e2|X (cid:164)=Ï2sothatD=I Ï2,then(4.25) n simplifiesto (cid:69)(cid:163)Ï2|X (cid:164)= 1 tr (cid:161) MÏ2(cid:162)=Ï2 (cid:181) nâk (cid:182) (cid:98) n n thefinalequalityby(3.22). ThiscalculationshowsthatÏ2 isbiasedtowardszero. Theorderofthebias (cid:98) dependsonk/n,theratioofthenumberofestimatedcoefficientstothesamplesize. Anotherwaytoseethisistouse(4.22).Notethat (cid:69)(cid:163)Ï2|X (cid:164)= 1 (cid:88) n (cid:69)(cid:163) e2|X (cid:164)= 1 (cid:88) n (1âh )Ï2= (cid:181) nâk (cid:182) Ï2 (cid:98) n (cid:98)i n ii n i=1 i=1 thelastequalityusingTheorem3.6.",
    "page": 128,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 109 Since the bias takes a scale form a classic method to obtain an unbiased estimator is by rescaling. Define s2= 1 (cid:88) n e2. (4.26) nâk (cid:98)i i=1 Bytheabovecalculation(cid:69)(cid:163) s2|X (cid:164)=Ï2 and(cid:69)(cid:163) s2(cid:164)=Ï2. Hencetheestimators2 isunbiasedforÏ2.Con- sequently, s2 isknownastheâbias-correctedestimatorâforÏ2 andinempiricalpractices2 isthemost widelyusedestimatorforÏ2. Interestingly, this is not the only method to construct an unbiased estimator for Ï2. An estimator constructedwiththestandardizedresidualse from(4.23)is i Ï2= 1 (cid:88) n e2= 1 (cid:88) n (1âh ) â1e2. n i n ii (cid:98)i i=1 i=1 Youcanshow(seeExercise4.9)that (cid:69)(cid:163)Ï2|X (cid:164)=Ï2 (4.27) andthusÏ2isunbiasedforÏ2(inthehomoskedasticlinearregressionmodel). Whenk/n issmalltheestimatorsÏ2, s2 andÏ2 arelikelytobesimilartooneanother. However, if (cid:98) k/n islargethens2 andÏ2 aregenerallypreferredtoÏ2.Consequentlyitisbesttouseoneofthebias- (cid:98) correctedvarianceestimatorsinapplications. 4.14 Mean-SquareForecastError Oneuseofanestimatedregressionistopredictout-of-sample. Consideranout-of-samplerealiza- tion(Y n+1 ,X n+1 )where X n+1 isobservedbutnotY n+1 . GiventhecoefficientestimatorÎ² (cid:98)thestandard pointestimatorof(cid:69)[Y n+1 |X n+1 ]=X n (cid:48) +1 Î²isY(cid:101)n+1 =X n (cid:48) +1 Î² (cid:98).Theforecasterroristhedifferencebetween theactualvalueY n+1 andthepointforecastY(cid:101)n+1 .Thisistheforecasterrore (cid:101)n+1 =Y n+1 âY(cid:101)n+1 .Themean- squaredforecasterror(MSFE)isitsexpectedsquaredvalueMSFE =(cid:69)(cid:163) e2 (cid:164) . Inthelinearregression n (cid:101)n+1 modele (cid:101)n+1 =e n+1 âX n (cid:48) +1 (cid:161)Î² (cid:98) âÎ²(cid:162) so MSFE n =(cid:69)(cid:163) e n 2 +1 (cid:164)â2(cid:69)(cid:163) e n+1 X n (cid:48) +1 (cid:161)Î² (cid:98) âÎ²(cid:162)(cid:164)+(cid:69) (cid:104) X n (cid:48) +1 (cid:161)Î² (cid:98) âÎ²(cid:162)(cid:161)Î² (cid:98) âÎ²(cid:162)(cid:48) X n+1 (cid:105)",
    "page": 129,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Inthelinearregression n (cid:101)n+1 modele (cid:101)n+1 =e n+1 âX n (cid:48) +1 (cid:161)Î² (cid:98) âÎ²(cid:162) so MSFE n =(cid:69)(cid:163) e n 2 +1 (cid:164)â2(cid:69)(cid:163) e n+1 X n (cid:48) +1 (cid:161)Î² (cid:98) âÎ²(cid:162)(cid:164)+(cid:69) (cid:104) X n (cid:48) +1 (cid:161)Î² (cid:98) âÎ²(cid:162)(cid:161)Î² (cid:98) âÎ²(cid:162)(cid:48) X n+1 (cid:105) . (4.28) Thefirsttermin(4.28)isÏ2.Thesecondtermin(4.28)iszerosincee n+1 X n (cid:48) +1 isindependentofÎ² (cid:98) âÎ² andbotharemeanzero.Usingthepropertiesofthetraceoperatorthethirdtermin(4.28)is tr (cid:179) (cid:69)(cid:163) X n+1 X n (cid:48) +1 (cid:164)(cid:69) (cid:104) (cid:161)Î² (cid:98) âÎ²(cid:162)(cid:161)Î² (cid:98) âÎ²(cid:162)(cid:48)(cid:105)(cid:180) =tr (cid:179) (cid:69)(cid:163) X n+1 X n (cid:48) +1 (cid:164)(cid:69) (cid:104) (cid:69) (cid:104) (cid:161)Î² (cid:98) âÎ²(cid:162)(cid:161)Î² (cid:98) âÎ²(cid:162)(cid:48) |X (cid:105)(cid:105)(cid:180) (cid:179) (cid:104) (cid:105)(cid:180) =tr (cid:69)(cid:163) X n+1 X n (cid:48) +1 (cid:164)(cid:69) V Î²(cid:98) (cid:104) (cid:179) (cid:180)(cid:105) =(cid:69) tr (cid:161) X n+1 X n (cid:48) +1 (cid:162) V Î²(cid:98) (cid:104) (cid:105) =(cid:69) X n (cid:48) +1 V Î²(cid:98) X n+1 (4.29) whereweusethefactthat X n+1 isindependentofÎ² (cid:98),thedefinitionV Î²(cid:98) =(cid:69) (cid:104) (cid:161)Î² (cid:98) âÎ²(cid:162)(cid:161)Î² (cid:98) âÎ²(cid:162)(cid:48) |X (cid:105) ,andthe factthatX n+1 isindependentofV Î²(cid:98) .Thus (cid:104) (cid:105) MSFE n =Ï2+(cid:69) X n (cid:48) +1 V Î²(cid:98) X n+1 .",
    "page": 129,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 110 Underconditionalhomoskedasticitythissimplifiesto MSFE n =Ï2 (cid:179) 1+(cid:69) (cid:104) X n (cid:48) +1 (cid:161) X (cid:48) X (cid:162)â1 X n+1 (cid:105)(cid:180) . AsimpleestimatorfortheMSFEisobtainedbyaveragingthesquaredpredictionerrors(3.46) Ï2= 1 (cid:88) n e2 (cid:101) n (cid:101)i i=1 wheree (cid:101)i =Y i âX i (cid:48)Î² (cid:98)(âi) =e (cid:98)i (1âh ii ) â1.Indeed,wecancalculatethat (cid:69)(cid:163)Ï2(cid:164)=(cid:69)(cid:163) e2(cid:164) (cid:101) (cid:101)i =(cid:69) (cid:104) (cid:161) e i âX i (cid:48)(cid:161)Î² (cid:98)(âi) âÎ²(cid:162)(cid:162)2(cid:105) =Ï2+(cid:69) (cid:104) X i (cid:48)(cid:161)Î² (cid:98)(âi) âÎ²(cid:162)(cid:161)Î² (cid:98)(âi) âÎ²(cid:162)(cid:48) X i (cid:105) . Byasimilarcalculationasin(4.29)wefind (cid:104) (cid:105) (cid:69)(cid:163)Ï (cid:101) 2(cid:164)=Ï2+(cid:69) X i (cid:48) V Î²(cid:98)(âi) X i =MSFE nâ1 . This is the MSFE based on a sample of size nâ1 rather than size n. The difference arises because the in-samplepredictionerrorse fori â¤n arecalculatedusinganeffectivesamplesizeofnâ1,whilethe (cid:101)i out-ofsamplepredictionerrore (cid:101)n+1 iscalculatedfromasamplewiththefullnobservations. Unlessnis verysmallweshouldexpectMSFE nâ1 (theMSFEbasedonnâ1observations)tobeclosetoMSFE n (the MSFEbasedonnobservations).ThusÏ2isareasonableestimatorforMSFE . (cid:101) n Theorem4.8 MSFE Inthelinearregressionmodel(Assumption4.2)andi.i.d. sampling(Assump- tion4.1) (cid:104) (cid:105) MSFE n =(cid:69)(cid:163) e (cid:101)n 2 +1 (cid:164)=Ï2+(cid:69) X n (cid:48) +1 V Î²(cid:98) X n+1 whereV Î²(cid:98) =var (cid:163)Î² (cid:98) |X (cid:164) .Furthermore,Ï (cid:101) 2 definedin(3.46)isanunbiasedesti- matorofMSFE nâ1 ,since(cid:69)(cid:163)Ï (cid:101) 2(cid:164)=MSFE nâ1 . 4.15 CovarianceMatrixEstimationUnderHomoskedasticity ForinferenceweneedanestimatorofthecovariancematrixV oftheleastsquaresestimator.Inthis Î²(cid:98) sectionweconsiderthehomoskedasticregressionmodel(Assumption4.3). Underhomoskedasticitythecovariancematrixtakesthesimpleform V0 =(cid:161) X (cid:48) X (cid:162)â1Ï2 Î²(cid:98) whichisknownuptothescaleÏ2. InSection4.13wediscussedthreeestimatorsofÏ2.Themostcom- monlyusedchoiceiss2leadingtotheclassiccovariancematrixestimator V(cid:98) 0 Î²(cid:98) =(cid:161) X (cid:48) X (cid:162)â1 s2. (4.30)",
    "page": 130,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 111 Sinces2isconditionallyunbiasedforÏ2itissimpletocalculatethatV(cid:98) 0 Î²(cid:98) isconditionallyunbiasedfor V undertheassumptionofhomoskedasticity: Î²(cid:98) (cid:69) (cid:104) V(cid:98) 0 Î²(cid:98) |X (cid:105) =(cid:161) X (cid:48) X (cid:162)â1(cid:69)(cid:163) s2|X (cid:164)=(cid:161) X (cid:48) X (cid:162)â1Ï2=V Î²(cid:98) . This was the dominant covariance matrix estimator in applied econometrics for many years and isstillthedefaultmethodinmostregressionpackages. Forexample, Statausesthecovariancematrix estimator(4.30)bydefaultinlinearregressionunlessanalternativeisspecified. 0 If the estimator (4.30) is used but the regression error is heteroskedastic it is possible forV(cid:98)Î²(cid:98) to be quite biased for the correct covariance matrix V = (cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48) DX (cid:162)(cid:161) X (cid:48) X (cid:162)â1 . For example, suppose Î²(cid:98) k = 1 and Ï2 = X2 with (cid:69)[X] = 0. The ratio of the true variance of the least squares estimator to the i i expectationofthevarianceestimatoris V Î²(cid:98) = (cid:80)n i=1 X i 4 (cid:39) (cid:69)(cid:163) X4(cid:164) d=efÎº. (cid:69) (cid:104) V(cid:98) 0 Î²(cid:98) |X (cid:105) Ï2(cid:80)n i=1 X i 2 (cid:161)(cid:69)(cid:163) X2 (cid:164)(cid:162)2 (NoticethatweusethefactthatÏ2=X2impliesÏ2=(cid:69)(cid:163)Ï2(cid:164)=(cid:69)(cid:163) X2(cid:164) .)TheconstantÎºisthestandardized i i i fourthmoment(orkurtosis)oftheregressorX andcanbeanynumbergreaterthanone.Forexample,if X â¼N (cid:161) 0,Ï2(cid:162) thenÎº=3,sothetruevarianceV isthreetimeslargerthantheexpectedhomoskedastic Î²(cid:98) estimatorV(cid:98) 0 Î²(cid:98) . But Îº can be much larger. Take, for example, the variable wage in the CPS data set. It satisfiesÎº=30sothatiftheconditionalvarianceequalsÏ2=X2 thenthetruevarianceV is30times i i Î²(cid:98) 0 largerthantheexpectedhomoskedasticestimatorV(cid:98)Î²(cid:98) .Whilethisisanextremecasethepointisthatthe classiccovariancematrixestimator(4.30)maybequitebiasedwhenthehomoskedasticityassumption fails. 4.16 CovarianceMatrixEstimationUnderHeteroskedasticity In the previous section we showed that that the classic covariance matrix estimator can be highly biasedifhomoskedasticityfails. Inthissectionweshowhowtoconstructcovariancematrixestimators whichdonotrequirehomoskedasticity. Recallthatthegeneralformforthecovariancematrixis V =(cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48) DX (cid:162)(cid:161) X (cid:48) X (cid:162)â1",
    "page": 131,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". 4.16 CovarianceMatrixEstimationUnderHeteroskedasticity In the previous section we showed that that the classic covariance matrix estimator can be highly biasedifhomoskedasticityfails. Inthissectionweshowhowtoconstructcovariancematrixestimators whichdonotrequirehomoskedasticity. Recallthatthegeneralformforthecovariancematrixis V =(cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48) DX (cid:162)(cid:161) X (cid:48) X (cid:162)â1 . Î²(cid:98) withD definedin(4.8).ThisdependsontheunknownmatrixD whichwecanwriteas D=diag (cid:161)Ï2 1 ,...,Ï2 n (cid:162)=(cid:69)(cid:163) ee (cid:48)|X (cid:164)=(cid:69)(cid:163) D(cid:101) |X (cid:164) whereD(cid:101) =diag (cid:161) e 1 2,...,e n 2(cid:162) .ThusD(cid:101) isaconditionallyunbiasedestimatorforD.Ifthesquarederrorse i 2 wereobservable,wecouldconstructanunbiasedestimatorforV as Î²(cid:98) V(cid:98) i Î²(cid:98) deal=(cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48) D(cid:101)X (cid:162)(cid:161) X (cid:48) X (cid:162)â1 (cid:195) (cid:33) n =(cid:161) X (cid:48) X (cid:162)â1 (cid:88) X X (cid:48) e2 (cid:161) X (cid:48) X (cid:162)â1 . i i i i=1",
    "page": 131,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 112 Indeed, (cid:195) (cid:33) (cid:69) (cid:104) V(cid:98) i Î²(cid:98) deal|X (cid:105) =(cid:161) X (cid:48) X (cid:162)â1 (cid:88) n X i X i (cid:48)(cid:69)(cid:163) e i 2|X (cid:164) (cid:161) X (cid:48) X (cid:162)â1 i=1 (cid:195) (cid:33) n =(cid:161) X (cid:48) X (cid:162)â1 (cid:88) X X (cid:48)Ï2 (cid:161) X (cid:48) X (cid:162)â1 i i i i=1 =(cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48) DX (cid:162)(cid:161) X (cid:48) X (cid:162)â1=V Î²(cid:98) ideal verifyingthatV(cid:98)Î²(cid:98) isunbiasedforV Î²(cid:98) . Sincetheerrorse i 2areunobservedV(cid:98) i Î²(cid:98) deal isnotafeasibleestimator.However,wecanreplacee i 2with thesquaredresidualse2.Makingthissubstitutionweobtaintheestimator (cid:98)i (cid:195) (cid:33) n V(cid:98) H Î²(cid:98) C0=(cid:161) X (cid:48) X (cid:162)â1 (cid:88) X i X i (cid:48) e (cid:98)i 2 (cid:161) X (cid:48) X (cid:162)â1 . (4.31) i=1 ThelabelâHCâreferstoâheteroskedasticity-consistentâ.ThelabelâHC0âreferstothisbeingthebaseline heteroskedasticity-consistentcovariancematrixestimator. Weknow,however,thate2isbiasedtowardszero(recallequation(4.22)).ToestimatethevarianceÏ2 (cid:98)i theunbiasedestimators2 scalesthemomentestimatorÏ2 byn/(nâk). Makingthesameadjustment (cid:98) weobtaintheestimator (cid:195) (cid:33) V(cid:98) H Î²(cid:98) C1= (cid:179) n n âk (cid:180) (cid:161) X (cid:48) X (cid:162)â1 (cid:88) n X i X i (cid:48) e (cid:98)i 2 (cid:161) X (cid:48) X (cid:162)â1 . (4.32) i=1 Whilethescalingbyn/(nâk)isadhoc,HC1isoftenrecommendedovertheunscaledHC0estimator. Alternatively, we could use the standardized residuals e or the prediction errors e , yielding the i (cid:101)i âHC2âandâHC3âestimators (cid:195) (cid:33) n V(cid:98) H Î²(cid:98) C2=(cid:161) X (cid:48) X (cid:162)â1 (cid:88) X i X i (cid:48) e2 i (cid:161) X (cid:48) X (cid:162)â1 i=1 (cid:195) (cid:33) n =(cid:161) X (cid:48) X (cid:162)â1 (cid:88) (1âh ) â1X X (cid:48) e2 (cid:161) X (cid:48) X (cid:162)â1 (4.33) ii i i(cid:98)i i=1 and (cid:195) (cid:33) n V(cid:98) H Î²(cid:98) C3=(cid:161) X (cid:48) X (cid:162)â1 (cid:88) X i X i (cid:48) e (cid:101)i 2 (cid:161) X (cid:48) X (cid:162)â1 i=1 (cid:195) (cid:33) n =(cid:161) X (cid:48) X (cid:162)â1 (cid:88) (1âh ) â2X X (cid:48) e2 (cid:161) X (cid:48) X (cid:162)â1 . (4.34) ii i i(cid:98)i i=1 The four estimators HC0, HC1, HC2 and HC3 are collectively called robust, heteroskedasticity- consistent, or heteroskedasticity-robust covariance matrix estimators. The HC0 estimator was first developed by Eicker (1963) and introduced to econometrics by White (1980) and is sometimes called theEicker-WhiteorWhitecovariancematrixestimator.Thedegree-of-freedomadjustmentinHC1was recommendedbyHinkley(1977)andisthedefaultrobustcovariancematrixestimatorimplementedin Stata. Itisimplementbytheâ, râoption. Incurrentappliedeconometricpracticethisisthemostpop- ularcovariancematrixestimator",
    "page": 132,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". The HC0 estimator was first developed by Eicker (1963) and introduced to econometrics by White (1980) and is sometimes called theEicker-WhiteorWhitecovariancematrixestimator.Thedegree-of-freedomadjustmentinHC1was recommendedbyHinkley(1977)andisthedefaultrobustcovariancematrixestimatorimplementedin Stata. Itisimplementbytheâ, râoption. Incurrentappliedeconometricpracticethisisthemostpop- ularcovariancematrixestimator. TheHC2estimatorwasintroducedbyHorn,HornandDuncan(1975) andisimplementedusingthevce(hc2)optioninStata.TheHC3estimatorwasderivedbyMacKinnon",
    "page": 132,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 113 andWhite(1985)fromthejackknifeprinciple(seeSection10.3),andbyAndrews(1991a)basedonthe principleofleave-one-outcross-validation,andisimplementedusingthevce(hc3)optioninStata. Since(1âh ) â2>(1âh ) â1>1itisstraightforwardtoshowthat ii ii V(cid:98) H Î²(cid:98) C0<V(cid:98) H Î²(cid:98) C2<V(cid:98) H Î²(cid:98) C3 . (4.35) (SeeExercise4.10.)TheinequalityA<BwhenappliedtomatricesmeansthatthematrixBâAispositive definite. In general, the bias of the covariance matrix estimators is complicated but simplify under the as- sumptionofhomoskedasticity(4.3).Forexample,using(4.22), (cid:195) (cid:33) (cid:69) (cid:104) V(cid:98) H Î²(cid:98) C0|X (cid:105) =(cid:161) X (cid:48) X (cid:162)â1 (cid:88) n X i X i (cid:48)(cid:69)(cid:163) e (cid:98)i 2|X (cid:164) (cid:161) X (cid:48) X (cid:162)â1 i=1 (cid:195) (cid:33) n =(cid:161) X (cid:48) X (cid:162)â1 (cid:88) X X (cid:48) (1âh )Ï2 (cid:161) X (cid:48) X (cid:162)â1 i i ii i=1 (cid:195) (cid:33) n =(cid:161) X (cid:48) X (cid:162)â1Ï2â(cid:161) X (cid:48) X (cid:162)â1 (cid:88) X X (cid:48) h (cid:161) X (cid:48) X (cid:162)â1Ï2 i i ii i=1 <(cid:161) X (cid:48) X (cid:162)â1Ï2=V . Î²(cid:98) HC0 ThiscalculationshowsthatV(cid:98)Î²(cid:98) isbiasedtowardszero. Byasimilarcalculation(againunderhomoskedasticity)wecancalculatethattheHC2estimatoris unbiased (cid:69) (cid:104) V(cid:98) H Î²(cid:98) C2|X (cid:105) =(cid:161) X (cid:48) X (cid:162)â1Ï2. (4.36) (SeeExercise4.11.) Itmightseemratheroddtocomparethebiasofheteroskedasticity-robustestimatorsundertheas- sumptionofhomoskedasticitybutitdoesgiveusabaselineforcomparison. Anotherinterestingcalculationshowsthatingeneral(thatis,withoutassuminghomoskedasticity) theHC3estimatorisbiasedawayfromzero.Indeed,usingthedefinitionofthepredictionerrors(3.44) e (cid:101)i =Y i âX i (cid:48)Î² (cid:98)(âi) =e i âX i (cid:48)(cid:161)Î² (cid:98)(âi) âÎ²(cid:162) so e (cid:101)i 2=e i 2â2X i (cid:48)(cid:161)Î² (cid:98)(âi) âÎ²(cid:162) e i +(cid:161) X i (cid:48)(cid:161)Î² (cid:98)(âi) âÎ²(cid:162)(cid:162)2 . Notethate i andÎ² (cid:98)(âi) arefunctionsofnon-overlappingobservationsandarethusindependent. Hence (cid:69)(cid:163)(cid:161)Î² (cid:98)(âi) âÎ²(cid:162) e i |X (cid:164)=0and (cid:69)(cid:163) e (cid:101)i 2|X (cid:164)=(cid:69)(cid:163) e i 2|X (cid:164)â2X i (cid:48)(cid:69)(cid:163)(cid:161)Î² (cid:98)(âi) âÎ²(cid:162) e i |X (cid:164)+(cid:69) (cid:104) (cid:161) X i (cid:48)(cid:161)Î² (cid:98)(âi) âÎ²(cid:162)(cid:162)2|X (cid:105) =Ï2 i +(cid:69) (cid:104) (cid:161) X i (cid:48)(cid:161)Î² (cid:98)(âi) âÎ²(cid:162)(cid:162)2|X (cid:105) â¥Ï2. i Itfollowsthat (cid:195) (cid:33) (cid:69) (cid:104) V(cid:98) H Î²(cid:98) C3|X (cid:105) =(cid:161) X (cid:48) X (cid:162)â1 (cid:88) n X i X i (cid:48)(cid:69)(cid:163) e (cid:101)i 2|X (cid:164) (cid:161) X (cid:48) X (cid:162)â1 i=1 (cid:195) (cid:33) n â¥(cid:161) X (cid:48) X (cid:162)â1 (cid:88) X X (cid:48)Ï2 (cid:161) X (cid:48) X (cid:162)â1=V . i i i Î²(cid:98) i=1",
    "page": 133,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 114 ThismeansthattheHC3estimatorisconservativeinthesensethatitisweaklylarger(inexpectation) thanthecorrectvarianceforanyrealizationofX. 0 We have introduced five covariance matrix estimators, including the homoskedastic estimator V(cid:98)Î²(cid:98) 0 andthefourHCestimators. Whichshouldyouuse? TheclassicestimatorV(cid:98)Î²(cid:98) istypicallyapoorchoice asitisonlyvalidundertheunlikelyhomoskedasticityrestriction. Forthisreasonitisnottypicallyused in contemporary econometric research. Unfortunately, standard regression packages set their default 0 choiceasV(cid:98)Î²(cid:98) sousersmustintentionallyselectarobustcovariancematrixestimator. OfthefourrobustestimatorsHC1isthemostcommonlyusedasitisthedefaultrobustcovariance matrixoptioninStata.However,HC2andHC3arepreferred.HC2isunbiased(underhomoskedasticity) andHC3isconservativeforanyX.InmostapplicationsHC1,HC2andHC3willbesimilarsothischoice willnotmatter.Thecontextwheretheestimatorscandiffersubstantiallyiswhenthesamplehasalarge leveragevalueh foratleastoneobservation. Youcanseethisbycomparingtheformulas(4.32),(4.33) ii and (4.34) and noting that the only difference is the scaling by the leverage values h . If there is an ii observation with h close to one, then (1âh ) â1 and (1âh ) â2 will be large, giving this observation ii ii ii muchgreaterweightinthecovariancematrixformula. HalbertL.White HalWhite(1950-2012)oftheUnitedStateswasaninfluentialeconometricianof recentyears.His1980paperonheteroskedasticity-consistentcovariancematrix estimationisoneofthemostcitedpapersineconomics. Hisresearchwascen- traltothemovementtovieweconometricmodelsasapproximations,andtothe driveforincreasedmathematicalrigorinthediscipline. Inadditiontobeinga highlyprolificandinfluentialscholar,healsoco-foundedtheeconomicconsult- ingfirmBatesWhite. 4.17 StandardErrors A variance estimator such as V(cid:98)Î²(cid:98) is an estimator of the variance of the distribution of Î² (cid:98). A more easilyinterpretablemeasureofspreadisitssquarerootâthestandarddeviation. Thisissoimportant whendiscussingthedistributionofparameterestimatorswehaveaspecialnameforestimatesoftheir standarddeviation. Definition4.2 A standarderror s(Î² (cid:98)) for a real-valued estimator Î² (cid:98)is an esti- matorofthestandarddeviationofthedistributionofÎ² (cid:98). WhenÎ²isavectorwithestimatorÎ² (cid:98)andcovariancematrixestimatorV(cid:98)Î²(cid:98) ,standarderrorsforindivid- ualelementsarethesquarerootsofthediagonalelementsofV(cid:98)Î²(cid:98) .Thatis, (cid:113) (cid:114)(cid:104) (cid:105) s(Î² (cid:98)j )= V(cid:98)Î²(cid:98)j = V(cid:98)Î²(cid:98) jj .",
    "page": 134,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 115 Whentheclassicalcovariancematrixestimator(4.30)isusedthestandarderrortakesthesimpleform s(Î² (cid:98)j )=s (cid:114)(cid:104) (cid:161) X (cid:48) X (cid:162)â1 (cid:105) . (4.37) jj Aswediscussedintheprevioussectiontherearemultiplepossiblecovariancematrixestimatorsso standarderrorsarenotunique.Itisthereforeimportanttounderstandwhatformulaandmethodisused byanauthorwhenstudyingtheirwork.Itisalsoimportanttounderstandthataparticularstandarderror mayberelevantunderonesetofmodelassumptionsbutnotunderanothersetofassumptions. Toillustrate,wereturntothelogwageregression(3.12)ofSection3.7. Wecalculatethats2=0.160. Thereforethehomoskedasticcovariancematrixestimateis V(cid:98) 0 Î²(cid:98) = (cid:181) 5 3 0 1 1 4 0 3 2 1 0 4 (cid:182)â1 0.160= (cid:181) â 0 0 .0 .0 0 3 2 1 â 0 0 .4 .0 9 3 9 1 (cid:182) . Wealsocalculatethat (cid:88) n (1âh ) â1X X (cid:48) e2= (cid:181) 763.26 48.513 (cid:182) . ii i i(cid:98)i 48.513 3.1078 i=1 ThereforetheHC2covariancematrixestimateis (cid:181) (cid:182)â1(cid:181) (cid:182)(cid:181) (cid:182)â1 V(cid:98) H Î²(cid:98) C2= 5 3 0 1 1 4 0 3 2 1 0 4 7 4 6 8 3 .5 .2 1 6 3 4 3 8 .1 .5 0 1 7 3 8 5 3 0 1 1 4 0 3 2 1 0 4 (cid:181) 0.001 â0.015 (cid:182) = . (4.38) â0.015 0.243 The standard errors are the square roots of the diagonal elements of these matrices. A conventional formattowritetheestimatedequationwithstandarderrorsis lo(cid:225)g(wage)= 0.155 education+ 0.698 . (4.39) (0.031) (0.493) Alternatively,standarderrorscouldbecalculatedusingtheotherformulae. Wereportthedifferent standarderrorsinthefollowingtable. Table4.1:StandardErrors Education Intercept Homoskedastic(4.30) 0.045 0.707 HC0(4.31) 0.029 0.461 HC1(4.32) 0.030 0.486 HC2(4.33) 0.031 0.493 HC3(4.34) 0.033 0.527 Thehomoskedasticstandarderrorsarenoticeablydifferent(largerinthiscase)thantheothers. The robust standard errors are reasonably close to one another though the HC3 standard errors are larger thantheothers.",
    "page": 135,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 116 4.18 EstimationwithSparseDummyVariables Theheteroskedasticity-robustcovariancematrixestimatorscanbequiteimpreciseinsomecontexts. Oneisinthepresenceofsparsedummyvariablesâwhenadummyvariableonlytakesthevalue1or0 forveryfewobservations.Inthesecontextsonecomponentofthecovariancematrixisestimatedonjust thosefewobservationsandwillbeimprecise.Thisiseffectivelyhiddenfromtheuser. Toseetheproblem,letDbeadummyvariable(takesonthevalues1and0)andconsiderthedummy variableregression Y =Î² D+Î² +e. (4.40) 1 2 The number of observations for which D =1 is n =(cid:80)n D . The number of observations for which i 1 i=1 i D =0isn =nân .Wesaythedesignissparseifn orn issmall. i 2 1 1 2 Tosimplifyouranalysis,wetaketheextremecasen =1. Theideasextendtothecaseofn >1but 1 1 small,thoughwithlessdramaticeffects. Intheregressionmodel(4.40)wecancalculatethatthetruecovariancematrixoftheleastsquares estimatorforthecoefficientsunderthesimplifyingassumptionofconditionalhomoskedasticityis V =Ï2(cid:161) X (cid:48) X (cid:162)â1=Ï2 (cid:181) 1 1 (cid:182)â1 = Ï2 (cid:181) n â1 (cid:182) . Î²(cid:98) 1 n nâ1 â1 1 Inparticular,thevarianceoftheestimatorforthecoefficientonthedummyvariableis n V =Ï2 . Î²(cid:98)1 nâ1 Essentially,thecoefficientÎ² isestimatedfromasingleobservationsoitsvarianceisroughlyunaffected 1 by sample size. An important message is that certain coefficient estimators in the presence of sparse dummyvariableswillbeimprecise,regardlessofthesamplesize. Alargesamplealoneisnotsufficient toensurepreciseestimation. NowletâsexaminethestandardHC1covariancematrixestimator(4.32). Theregressionhasperfect fitfortheobservationforwhichD =1sothecorrespondingresidualise =0.ItfollowsthatD e =0for i (cid:98)i i(cid:98)i alli (eitherD =0ore =0).Hence i (cid:98)i (cid:88) n X X (cid:48) e2= (cid:181) 0 0 (cid:182) = (cid:181) 0 0 (cid:182) i i(cid:98)i 0 (cid:80)n e2 0 (nâ2)s2 i=1 i=1(cid:98)i wheres2=(nâ2) â1(cid:80)n e2isthebias-correctedestimatorofÏ2.Togetherwefindthat i=1(cid:98)i V(cid:98) H Î²(cid:98) C1= (cid:179) n n â2 (cid:180) (nâ 1 1)2 (cid:181) â n 1 â 1 1 (cid:182)(cid:181) 0 0 (nâ 0 2)s2 (cid:182)(cid:181) â n 1 â 1 1 (cid:182) n (cid:181) 1 â1 (cid:182) =s2 . (nâ1)2 â1 1 Inparticular,theestimatorforV is Î²(cid:98)1 n V(cid:98) HC1=s2 . Î²(cid:98)1 (nâ1)2 Ithasexpectation (cid:69) (cid:104) V(cid:98) Î²(cid:98) H 1 C1 (cid:105) =Ï2 (nâ n 1)2 = n V â Î²(cid:98)1 1 <<V Î²(cid:98)1 .",
    "page": 136,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 117 ThevarianceestimatorV(cid:98) Î²(cid:98) H 1 C1isextremelybiasedforV Î²(cid:98)1 . Itistoosmallbyamultipleofn! Thereported varianceâandstandarderrorâismisleadinglysmall. Thevarianceestimateerroneouslymis-statesthe precisionofÎ² (cid:98)1 . ThefactthatV(cid:98) HC1isbiasedisunlikelytobenoticedbyanappliedresearcher.Nothinginthereported Î²(cid:98)1 outputwillalertaresearchertotheproblem. AnotherwaytoseetheissueistoconsidertheestimatorÎ¸ (cid:98) =Î² (cid:98)1 +Î² (cid:98)2 forthesumofthecoefficients Î¸=Î² 1 +Î² 2 . ThisestimatorhastruevarianceÏ2. Thevarianceestimator,howeverisV(cid:98) H Î¸(cid:98) C1=0! (Itequals thesumofthefourelementsinV(cid:98) H Î²(cid:98) C1 ).Clearly,theestimatorâ0âisbiasedforthetruevalueÏ2. Anotherinsightistoexaminetheleveragevalues.The(single)observationwithD =1has i h = 1 (cid:161) 1 1 (cid:162) (cid:181) n â1 (cid:182)(cid:181) 1 (cid:182) =1. ii nâ1 â1 1 1 Thisisanextremeleveragevalue. ApossiblesolutionistoreplacethebiasedcovariancematrixestimatorV(cid:98) HC1withtheunbiasedesti- Î²(cid:98)1 matorV(cid:98) HC2 (unbiasedunderhomoskedasticity)ortheconservativeestimatorV(cid:98) HC3. Neitherapproach Î²(cid:98)1 Î²(cid:98)1 can be done in the extreme sparse case n 1 =1 (forV(cid:98) Î²(cid:98) H 1 C2 andV(cid:98) Î²(cid:98) H 1 C3 cannot be calculated if h ii =1 for anyobservation)butappliesotherwise.Whenh ii =1foranobservationthenV(cid:98) Î²(cid:98) H 1 C2andV(cid:98) Î²(cid:98) H 1 C3cannotbe calculated.Inthiscaseunbiasedcovariancematrixestimationappearstobeimpossible. It is unclear if there is a best practice to avoid this situation. Once possibility is to calculate the maximumleveragevalue. Ifitisverylargecalculatethestandarderrorsusingseveralmethodstoseeif variationoccurs. 4.19 Computation Weillustratemethodstocomputestandarderrorsforequation(3.13)extendingthecodeofSection 3.25. StatadoFile(continued) * Homoskedasticformula(4.30): regwageeducationexperienceexp2if(mnwf==1) * HC1formula(4.32): regwageeducationexperienceexp2if(mnwf==1),r * HC2formula(4.33): regwageeducationexperienceexp2if(mnwf==1),vce(hc2) * HC3formula(4.34): regwageeducationexperienceexp2if(mnwf==1),vce(hc3)",
    "page": 137,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 118 RProgramFile(continued) n<-nrow(y) k<-ncol(x) a<-n/(n-k) sig2<-(t(e)%*%e)/(n-k) u1<-x*(e%*%matrix(1,1,k)) u2<-x*((e/sqrt(1-leverage))%*%matrix(1,1,k)) u3<-x*((e/(1-leverage))%*%matrix(1,1,k)) xx<-solve(t(x)%*%x) v0<-xx*sig2 v1<-xx%*%(t(u1)%*%u1)%*%xx v1a<-a*xx%*%(t(u1)%*%u1)%*%xx v2<-xx%*%(t(u2)%*%u2)%*%xx v3<-xx%*%(t(u3)%*%u3)%*%xx s0<-sqrt(diag(v0)) #Homoskedasticformula s1<-sqrt(diag(v1)) #HC0 s1a<-sqrt(diag(v1a)) #HC1 s2<-sqrt(diag(v2)) #HC2 s3<-sqrt(diag(v3)) #HC3 MATLABProgramFile(continued) [n,k]=size(x); a=n/(n-k); sig2=(eâ*e)/(n-k); u1=x.*(e*ones(1,k));u2=x.*((e./sqrt(1-leverage))*ones(1,k));u3=x.*((e./(1- leverage))*ones(1,k));xx=inv(xâ*x); v0=xx*sig2; v1=xx*(u1â*u1)*xx; v1a=a*xx*(u1â*u1)*xx; v2=xx*(u2â*u2)*xx; v3=xx*(u3â*u3)*xx; s0=sqrt(diag(v0)); #Homoskedasticformula s1=sqrt(diag(v1)); #HC0formula s1a=sqrt(diag(v1a)); #HC1formula s2=sqrt(diag(v2)); #HC2formula s3=sqrt(diag(v3)); #HC3formula",
    "page": 138,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 119 4.20 MeasuresofFit Aswedescribedinthepreviouschapteracommonlyreportedmeasureofregressionfitistheregres- sionR2definedas (cid:80)n e2 Ï2 R2=1â i=1(cid:98)i =1â (cid:98) . (cid:179) (cid:180)2 Ï2 (cid:80)n i=1 Y i âY (cid:98)Y whereÏ2 =n â1(cid:80)n (cid:179) Y âY (cid:180)2 .R2isanestimatorofthepopulationparameter (cid:98)Y i=1 i var (cid:163) X (cid:48)Î²(cid:164) Ï2 Ï2= =1â . var[Y] Ï2 Y However,Ï2 andÏ2 arebiased. Theil(1961)proposedreplacingthesebytheunbiasedversions s2 (cid:98) (cid:98)Y andÏ2 =(nâ1) â1(cid:80)n (cid:179) Y âY (cid:180)2 yieldingwhatisknownasR-bar-squaredoradjustedR-squared: (cid:101)Y i=1 i R 2=1â s2 =1â (nâ1) â1(cid:80)n i=1 e (cid:98)i 2 . Ï (cid:101) 2 Y (nâk) â1(cid:80)n i=1 (cid:179) Y i âY (cid:180)2 WhileR 2 isanimprovementonR2amuchbetterimprovementis (cid:80)n e2 Ï2 R(cid:101) 2=1â i=1(cid:101)i =1â (cid:101) (cid:179) (cid:180)2 Ï2 (cid:80)n i=1 Y i âY (cid:98)Y wheree arethepredictionerrors(3.44)andÏ2 istheMSPEfrom(3.46). AsdescribedinSection(4.14) (cid:101)i (cid:101) Ï (cid:101) 2isagoodestimatoroftheout-of-samplemean-squaredforecasterrorsoR(cid:101) 2isagoodestimatorofthe percentage of the forecast variance whichis explained by the regression forecast. Inthis sense R(cid:101) 2 is a goodmeasureoffit. OneproblemwithR2whichispartiallycorrectedbyR 2 andfullycorrectedbyR(cid:101) 2isthatR2necessarily increaseswhenregressorsareaddedtoaregressionmodel.ThisoccursbecauseR2isanegativefunction 2 ofthesumofsquaredresidualswhichcannotincreasewhenaregressorisadded. Incontrast, R and R(cid:101) 2 are non-monotonic in the number of regressors. R(cid:101) 2 can even be negative, which occurs when an estimatedmodelpredictsworsethanaconstant-onlymodel. In the statistical literature the MSPE Ï2 is known as the leave-one-out cross validation criterion (cid:101) andispopularformodelcomparisonandselection,especiallyinhigh-dimensionalandnon-parametric contexts. ItisequivalenttouseR(cid:101) 2orÏ (cid:101) 2tocompareandselectmodels. ModelswithhighR(cid:101) 2(orlowÏ (cid:101) 2) arebettermodelsintermsofexpectedoutofsamplesquarederror. Incontrast,R2 cannotbeusedfor 2 modelselectionasitnecessarilyincreaseswhenregressorsareaddedtoaregressionmodel.R isalsoan inappropriatechoiceformodelselection(ittendstoselectmodelswithtoomanyparameters)though 2 a justification of this assertion requires a study of the theory of model selection. Unfortunately, R is routinelyusedbysomeeconomists,possiblyasahold-overfrompreviousgenerations. Insummary,itisrecommendedtoomitR2andR 2 .Ifameasureoffitisdesired,reportR(cid:101) 2orÏ (cid:101) 2. HenriTheil 2 Henri Theil (1924-2000) of the Netherlands invented R and two-stage least squares,bothofwhichareroutinelyseeninappliedeconometrics.Healsowrote anearlyinfluentialadvancedtextbookoneconometrics(Theil,1971).",
    "page": 139,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 120 4.21 EmpiricalExample Weagainreturntoourwageequationbutuseamuchlargersampleofallindividualswithatleast12 yearsofeducation. Forregressorsweincludeyearsofeducation,potentialworkexperience,experience squared, anddummyvariableindicatorsforthefollowing: female, femaleunionmember, maleunion member, marriedfemale2, marriedmale, formerlymarriedfemale3, formerlymarriedmale, Hispanic, Black,AmericanIndian,Asian,andmixedrace4 . Theavailablesampleis46,943sotheparameteresti- matesarequitepreciseandreportedinTable4.2.ForstandarderrorsweusetheunbiasedHC2formula. Table 4.2 displays the parameter estimates in a standard tabular format. Parameter estimates and standard errors are reported for all coefficients. In addition to the coefficient estimates the table also reportstheestimatederrorstandarddeviationandthesamplesize.Theseareusefulsummarymeasures offitwhichaidreaders. Table4.2:OLSEstimatesofLinearEquationforlog(wage) Î² (cid:98) s(Î² (cid:98)) Education 0.117 0.001 Experience 0.033 0.001 Experience2/100 -0.056 0.002 Female -0.098 0.011 FemaleUnionMember 0.023 0.020 MaleUnionMember 0.095 0.020 MarriedFemale 0.016 0.010 MarriedMale 0.211 0.010 FormerlyMarriedFemale -0.006 0.012 FormerlyMarriedMale 0.083 0.015 Hispanic -0.108 0.008 Black -0.096 0.008 AmericanIndian -0.137 0.027 Asian -0.038 0.013 MixedRace -0.041 0.021 Intercept 0.909 0.021 Ï 0.565 (cid:98) SampleSize 46,943 Standarderrorsareheteroskedasticity-consistent(Horn-Horn-Duncanformula). Asageneralruleitisadvisabletoalwaysreportstandarderrorsalongwithparameterestimates.This allowsreaderstoassesstheprecisionoftheparameterestimates,andaswewilldiscussinlaterchapters, formconfidenceintervalsandt-testsforindividualcoefficientsifdesired. TheresultsinTable4.2confirmourearlierfindingsthatthereturntoayearofeducationisapproxi- mately12%,thereturntoexperienceisconcave,singlewomenearnapproximately10%lessthensingle men,andBlacksearnabout10%lessthanwhites.Inaddition,weseethatHispanicsearnabout11%less thanwhites, AmericanIndians14%less, andAsiansandMixedracesabout4%less. Wealsoseethere 2Definingâmarriedâasmaritalcode1,2,or3. 3Definingâformerlymarriedâasmaritalcode4,5,or6. 4Racecode6orhigher.",
    "page": 140,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 121 are wage premiums for men who are members of a labor union (about 10%), married (about 21%) or formerlymarried(about8%),butnosimilarpremiumsareapparentforwomen. 4.22 Multicollinearity AsdiscussedinSection3.24, if X (cid:48) X issingularthen (cid:161) X (cid:48) X (cid:162)â1 andÎ² (cid:98)arenotdefined. Thissituation is called strictmulticollinearity as the columns of X are linearly dependent, i.e., there is some Î±(cid:54)=0 suchthatXÎ±=0.Mostcommonlythisariseswhensetsofregressorsareincludedwhichareidentically related.InSection3.24wediscussedpossiblecausesofstrictmulticollinearityanddiscussedtherelated problemofill-conditioningwhichcancausenumericalinaccuraciesinseverecases. A related common situation is near multicollinearity which is often called âmulticollinearityâ for brevity. This is the situation when the regressors are highly correlated. An implication of near multi- collinearityisthatindividualcoefficientestimateswillbeimprecise. Thisisnotnecessarilyaproblem foreconometricanalysisifthereportedstandarderrorsareaccurate. However, robuststandarderrors canbesensitivetolargeleveragevalueswhichcanoccurundernearmulticollinearity. Thisleadstothe undesirablesituationwherethecoefficientestimatesareimpreciseyetthestandarderrorsaremislead- inglysmall. Wecanseetheimpactofnearmulticollinearityonprecisioninasimplehomoskedasticlinearregres- sionmodelwithtworegressors Y =X Î² +X Î² +e, 1 1 2 2 and 1 X (cid:48) X = (cid:181) 1 Ï (cid:182) . n Ï 1 Inthiscase var (cid:163)Î² (cid:98) |X (cid:164)= Ï n 2(cid:181) Ï 1 Ï 1 (cid:182)â1 = n (cid:161) 1 Ï â 2 Ï2 (cid:162) (cid:181) â 1 Ï â 1 Ï (cid:182) . ThecorrelationÏindexescollinearitysinceasÏapproaches1thematrixbecomessingular. Wecansee theeffectofcollinearityonprecisionbyobservingthatthevarianceofacoefficientestimateÏ2(cid:163) n (cid:161) 1âÏ2(cid:162)(cid:164)â1 approachesinfinityasÏapproaches1. Thusthemoreâcollinearâaretheregressorstheworsethepreci- sionoftheindividualcoefficientestimates. ArthurS.Goldberger ArtGoldberger(1930-2009)wasoneofthemostdistinguishedmembersofthe DepartmentofEconomicsattheUniversityofWisconsin.HisPh.D.thesisdevel- opedapioneeringmacroeconometricforecastingmodel(theKlein-Goldberger model). Mostofhisremainingcareerfocusedonmicroeconometricissues. He wastheleadingpioneerofwhathasbeencalledtheWisconsinTraditionofem- piricalworkâacombinationofformaleconometrictheorywithacarefulcritical analysisofempiricalwork. Goldbergerwroteaseriesofhighlyregardedandin- fluentialgraduateeconometrictextbooks,includingEconometricTheory(1964), TopicsinRegressionAnalysis(1968),andACourseinEconometrics(1991).",
    "page": 141,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 122 Whatishappeningisthatwhentheregressorsarehighlydependentitisstatisticallydifficulttodis- entangle the impact of Î² from that of Î² . As a consequence the precision of individual estimates are 1 2 reduced. Many early-generation textbooks overemphasized multicollinearity. An amusing parody of these textsappearedinChapter23.3ofGoldbergerâsACourseinEconometrics(1991),partofwhichisreprinted below.TounderstandhisbasicpointyoushouldnoticehowtheestimationvarianceÏ2(cid:163) n (cid:161) 1âÏ2(cid:162)(cid:164)â1 de- pendsequallyandsymmetricallyonthecorrelationÏ andthesamplesizen. Goldbergerwaspointing outthattheonlystatisticalimplicationofmulticollinearityinthehomoskedasticmodelisalackofpreci- sion. Smallsamplesizeshavetheexactsameimplication. (WhatbothGoldbergerandtheseotherearly textsmissed,however,isthatmulticollinearityincreasesthebiasofrobuststandarderrorsasdiscussed inSection4.18.) MicronumerositybyArthurS.Goldberger ACourseinEconometrics(1991),Chapter23.3 Econometricstextsdevotemanypagestotheproblemofmulticollinearityinmultipleregression, but they say little about the closely analogous problem of small sample size in estimating a univari- atemean. Perhapsthatimbalanceisattributabletothelackofanexoticpolysyllabicnameforâsmall samplesize.âIfso,wecanremovethatimpedimentbyintroducingthetermmicronumerosity. 1. Micronumerosity Theextremecase,âexactmicronumerosity,âariseswhenn=0,inwhichcasethesampleestimate of Âµ is not unique. (Technically, there is a violation of the rank condition n >0: the matrix 0 is singular.) The extreme case is easy enough to recognize. âNear micronumerosityâ is more subtle, and yet very serious. It arises when the rank condition n > 0 is barely satisfied. Near micronumerosityisveryprevalentinempiricaleconomics. 2. Consequencesofmicronumerosity Theconsequencesofmicronumerosityareserious.Precisionofestimationisreduced.Thereare twoaspectsofthisreduction:estimatesofÂµmayhavelargeerrors,andnotonlythat,butV will yÂ¯ belarge. InvestigatorswillsometimesbeledtoacceptthehypothesisÂµ=0because yÂ¯/Ï issmall, even (cid:98)yÂ¯ thoughthetruesituationmaybenotthatÂµ=0butsimplythatthesampledatahavenotenabled ustopickÂµup. 3. Testingformicronumerosity Tests for the presence of micronumerosity require the judicious use of various fingers. Some researcherspreferasinglefinger,othersusetheirtoes,stillotherslettheirthumbsrule. Agenerallyreliableguidemaybeobtainedbycountingthenumberofobservations. Mostofthe timeineconometricanalysis,whennisclosetozero,itisalsofarfrominfinity. 4. Remediesformicronumerosity IfmicronumerosityprovesseriousinthesensethattheestimateofÂµhasanunsatisfactorilylow degree of precision, we are in the statistical position of not being able to make bricks without straw.",
    "page": 142,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 123 4.23 ClusteredSampling InSection4.2webrieflymentionedclusteredsamplingasanalternativetotheassumptionofran- domsampling. Wenowintroducetheframeworkinmoredetailandextendtheprimaryresultsofthis chaptertoencompassclustereddependence. It might be easiest to understand the idea of clusters by considering a concrete example. Duflo, Dupas and Kremer (2011) investigate the impact of tracking (assigning students based on initial test score)oneducationalattainmentinarandomizedexperiment. Anextractoftheirdatasetisavailable onthetextbookwebpageinthefileDDK2011. In2005,140primaryschoolsinKenyareceivedfundingtohireanextrafirstgradeteachertoreduce class sizes. In half of the schools (selected randomly) students were assigned to classrooms based on aninitialtestscore(âtrackingâ);intheremainingschoolsthestudentswererandomlyassignedtoclass- rooms. Fortheiranalysistheauthorsrestrictedattentiontothe121schoolswhichinitiallyhadasingle first-gradeclass. Thekeyregression5inthepaperis TestScore =â0.071+0.138Tracking +e (4.41) ig g ig whereTestScore isthestandardizedtestscore(normalizedtohavemean0andvariance1)ofstudenti ig inschoolg,andTracking isadummyequalto1ifschoolg wastracking. TheOLSestimatesindicate g that schools which tracked the students had an overall increase in test scores by about 0.14 standard deviations,whichismeaningful. Moregeneralversionsofthisregressionareestimated,manyofwhich taketheform TestScore =Î±+Î³Tracking +X (cid:48) Î²+e (4.42) ig g ig ig whereX isasetofcontrolsspecifictothestudent(includingage,gender,andinitialtestscore). ig A difficulty with applying the classical regression framework is that student achievement is likely correlatedwithinagivenschool. Studentachievementmaybeaffectedbylocaldemographics,individ- ualteachers,andclassmates,allofwhichimplydependence. Theseconcerns,however,donotsuggest thatachievementwillbecorrelatedacrossschools,soitseemsreasonabletomodelachievementacross schoolsasmutuallyindependent.Wecallsuchdependenceclustered. Inclusteringcontextsitisconvenienttodoubleindextheobservationsas(Y ,X )whereg =1,...,G ig ig indexes the cluster and i =1,...,n indexes the individual within the gth cluster. The number of ob- g servations per cluster n may vary across clusters. The number of clusters isG. The total number of g observationsisn =(cid:80)G n . IntheKenyanschoolingexamplethenumberofclusters(schools)inthe g=1 g estimation sample is G = 121, the number of students per school varies from 19 to 62, and the total numberofobservationsisn=5795. Whileitistypicaltowritetheobservationsusingthedoubleindexnotation(Y ,X )itisalsouseful ig ig tousecluster-levelnotation.LetY =(Y ,...,Y ) (cid:48) andX =(X ,...,X ) (cid:48) denotethen Ã1vectorof g 1g ngg g 1g ngg g dependentvariablesandn Ãkmatrixofregressorsforthegth cluster.Alinearregressionmodelcanbe g writtenbyindividualas Y =X (cid:48) Î²+e ig ig ig andusingclusternotationas Y =X Î²+e (4.43) g g g where e =(e ,...,e ) (cid:48) is a n Ã1 error vector",
    "page": 143,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Whileitistypicaltowritetheobservationsusingthedoubleindexnotation(Y ,X )itisalsouseful ig ig tousecluster-levelnotation.LetY =(Y ,...,Y ) (cid:48) andX =(X ,...,X ) (cid:48) denotethen Ã1vectorof g 1g ngg g 1g ngg g dependentvariablesandn Ãkmatrixofregressorsforthegth cluster.Alinearregressionmodelcanbe g writtenbyindividualas Y =X (cid:48) Î²+e ig ig ig andusingclusternotationas Y =X Î²+e (4.43) g g g where e =(e ,...,e ) (cid:48) is a n Ã1 error vector. We can also stack the observations into full sample g 1g ngg g matricesandwritethemodelas Y =XÎ²+e. 5Table2,column(1).Duflo,DupasandKremer(2011)reportacoefficientestimateof0.139,perhapsduetoaslightlydifferent calculationtostandardizethetestscore.",
    "page": 143,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 124 Using this notation we can write the sums over the observations using the double sum (cid:80)G (cid:80)ng . g=1 i=1 Thisisthesumacrossclustersofthesumacrossobservationswithineachcluster. TheOLSestimator canbewrittenas Î² (cid:98) = (cid:195) (cid:88) G (cid:88) ng X ig X i (cid:48) g (cid:33)â1(cid:195) (cid:88) G (cid:88) ng X ig Y ig (cid:33) g=1i=1 g=1i=1 (cid:195) (cid:33)â1(cid:195) (cid:33) G G = (cid:88) X (cid:48) X (cid:88) X (cid:48) Y (4.44) g g g g g=1 g=1 =(cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48) Y (cid:162) . Theresidualsaree (cid:98)ig =Y ig âX i (cid:48) g Î² (cid:98)inindividuallevelnotationand (cid:98) e g =Y g âX g Î² (cid:98)inclusterlevelnotation. The standard clustering assumption is that the clusters are known to the researcher and that the observationsareindependentacrossclusters. Assumption4.4 Theclusters(Y ,X )aremutuallyindependentacrossclustersg. g g In our example clusters are schools. In other common applications cluster dependence has been assumedwithinindividualclassrooms,families,villages,regions,andwithinlargerunitssuchasindus- triesandstates. Thischoiceisuptotheresearcherthoughthejustificationwilldependonthecontext, thenatureofthedata,andwillreflectinformationandassumptionsonthedependencestructureacross observations. Themodelisalinearregressionundertheassumption (cid:69)(cid:163) e |X (cid:164)=0. (4.45) g g Thisisthesameasassumingthattheindividualerrorsareconditionallymeanzero (cid:69)(cid:163) e |X (cid:164)=0 ig g orthattheconditionalmeanofY given X islinear.Asintheindependentcaseequation(4.45)means g g that the linear regression model is correctly specified. In the clustered regression model this requires thatallinteractioneffectswithinclustershavebeenaccountedforinthespecificationoftheindividual regressorsX . ig Intheregression(4.41)theconditionalmeanisnecessarilylinearandsatisfies(4.45)sincetheregres- sorTracking isadummyvariableattheclusterlevel. Intheregression(4.42)withindividualcontrols, g (4.45) requires that the achievement of any student is unaffected by the individual controls (e.g. age, genderandinitialtestscore)ofotherstudentswithinthesameschool. Given(4.45)wecancalculatethemeanoftheOLSestimator.Substituting(4.43)into(4.44)wefind (cid:195) (cid:33)â1(cid:195) (cid:33) G G Î² (cid:98) âÎ²= (cid:88) X (cid:48) g X g (cid:88) X (cid:48) g e g . g=1 g=1",
    "page": 144,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 125 ThemeanofÎ² (cid:98) âÎ²conditioningonalltheregressorsis (cid:195) (cid:33)â1(cid:195) (cid:33) G G (cid:69)(cid:163)Î² (cid:98) âÎ²|X (cid:164)= (cid:88) X (cid:48) g X g (cid:88) X (cid:48) g (cid:69)(cid:163) e g |X (cid:164) g=1 g=1 (cid:195) (cid:33)â1(cid:195) (cid:33) G G = (cid:88) X (cid:48) X (cid:88) X (cid:48) (cid:69)(cid:163) e |X (cid:164) g g g g g g=1 g=1 =0. Thefirstequalityholdsbylinearity,thesecondbyAssumption4.4,andthethirdby(4.45). ThisshowsthatOLSisunbiasedunderclusteringiftheconditionalmeanislinear. Theorem4.9 Intheclusteredlinearregressionmodel(Assumption4.4 and(4.45))(cid:69)(cid:163)Î² (cid:98) |X (cid:164)=Î². (cid:104) (cid:105) NowconsiderthecovariancematrixofÎ² (cid:98). LetÎ£ g =(cid:69) e g e (cid:48) g |X g denotethen g Ãn g conditionalco- variancematrixoftheerrorswithinthegth cluster.Sincetheobservationsareindependentacrossclus- ters, (cid:34)(cid:195) (cid:33)(cid:175) (cid:35) var (cid:88) G X (cid:48) e (cid:175) (cid:175)X = (cid:88) G var (cid:104) X (cid:48) e |X (cid:105) g g (cid:175) g g g g=1 (cid:175) g=1 = (cid:88) G X (cid:48) (cid:69) (cid:104) e e (cid:48) |X (cid:105) X g g g g g g=1 G = (cid:88) X (cid:48) Î£ X g g g g=1 d=efâ¦ . (4.46) n Itfollowsthat V Î²(cid:98) =var (cid:163)Î² (cid:98) |X (cid:164)=(cid:161) X (cid:48) X (cid:162)â1â¦ n (cid:161) X (cid:48) X (cid:162)â1 . (4.47) Thisdiffersfromtheformulaintheindependentcaseduetothecorrelationbetweenobservations withinclusters.Themagnitudeofthedifferencedependsonthedegreeofcorrelationbetweenobserva- tionswithinclustersandthenumberofobservationswithinclusters.Toseethis,supposethatallclusters (cid:104) (cid:105) havethesamenumberofobservationsn g =N,(cid:69) e i 2 g |X g =Ï2,(cid:69)(cid:163) e ig e(cid:96)g |X g (cid:164)=Ï2Ïfori (cid:54)=(cid:96),andthe regressors X donotvarywithinacluster. InthiscasetheexactvarianceoftheOLSestimatorequals6 ig (aftersomecalculations) V =(cid:161) X (cid:48) X (cid:162)â1Ï2(cid:161) 1+Ï(Nâ1) (cid:162) . (4.48) Î²(cid:98) If Ï >0 the exact variance is appropriately a multiple ÏN of the conventional formula. In the Kenyan schoolexampletheaverageclustersizeis48. IfÏ=0.25thismeanstheexactvarianceexceedsthecon- ventionalformulabyafactorofabouttwelve.Inthiscasethecorrectstandarderrors(thesquarerootof thevariance)areamultipleofaboutthreetimestheconventionalformula. Thisisasubstantialdiffer- enceandshould notbeneglected. 6ThisformulaisduetoMoulton(1990).",
    "page": 145,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 126 Arellano(1987)proposedacluster-robustcovariancematrixestimatorwhichisanextensionofthe White estimator. Recall that the insight of the White covariance estimator is that the squared error e2 is unbiased for (cid:69)(cid:163) e2|X (cid:164)=Ï2. Similarly with cluster dependence the matrix e e (cid:48) is unbiased for i i i i g g (cid:104) (cid:105) (cid:69) e g e (cid:48) g |X g =Î£ g .Thismeansthatanunbiasedestimatorfor(4.46)isâ¦ (cid:101)n =(cid:80)G g=1 X (cid:48) g e g e (cid:48) g X g .Thisisnot feasible,butwecanreplacetheunknownerrorsbytheOLSresidualstoobtainArellanoâsestimator G â¦ (cid:98)n = (cid:88) X (cid:48) g(cid:98) e g(cid:98) e (cid:48) g X g g=1 = (cid:88) G (cid:88) ng (cid:88) ng X ig X (cid:96) (cid:48) g e (cid:98)ig e (cid:98)(cid:96)g g=1i=1(cid:96)=1 (cid:88) G (cid:195) (cid:88) ng (cid:33)(cid:195) (cid:88) ng (cid:33)(cid:48) = X ig e (cid:98)ig X(cid:96)g e (cid:98)(cid:96)g . (4.49) g=1 i=1 (cid:96)=1 Thethreeexpressionsin(4.49)givethreeequivalentformulaewhichcouldbeusedtocalculateâ¦ (cid:98)n .The finalexpressionwritesâ¦ (cid:98)n intermsoftheclustersums (cid:80)n (cid:96)= g 1 X(cid:96)g e (cid:98)(cid:96)g whichisthebasisforourexampleR andMATLABcodesshownbelow. Giventheexpressions(4.46)-(4.47)anaturalclustercovariancematrixestimatortakestheform V(cid:98)Î²(cid:98) =a n (cid:161) X (cid:48) X (cid:162)â1â¦ (cid:98)n (cid:161) X (cid:48) X (cid:162)â1 (4.50) wherea isapossiblefinite-sampleadjustment.TheStataclustercommanduses n (cid:181) nâ1 (cid:182)(cid:181) G (cid:182) a = . (4.51) n nâk Gâ1 ThefactorG/(Gâ1)wasderivedbyChrisHansen(2007)inthecontextofequal-sizedclusterstoimprove performancewhenthenumberofclustersGissmall.Thefactor(nâ1)/(nâk)isanadhocgeneralization whichneststheadjustmentusedin(4.32)sinceG=nimpliesthesimplificationa =n/(nâk). n Alternativecluster-robustcovariancematrixestimatorscanbeconstructedusingcluster-levelpre- dictionerrorssuchas (cid:101) e g =Y g âX g Î² (cid:98)(âg) whereÎ² (cid:98)(âg) istheleastsquaresestimatoromittingclusterg.As inSection3.20,wecanshowthat e = (cid:179) I âX (cid:161) X (cid:48) X (cid:162)â1 X (cid:48) (cid:180)â1 e (4.52) (cid:101)g ng g g (cid:98)g and Î² (cid:98)(âg) =Î² (cid:98) â(cid:161) X (cid:48) X (cid:162)â1 X (cid:48) g(cid:101) e g . (4.53) Wethenhavetherobustcovariancematrixestimator (cid:195) (cid:33) G V(cid:98) C Î²(cid:98) R3=(cid:161) X (cid:48) X (cid:162)â1 (cid:88) X (cid:48) g(cid:101) e g(cid:101) e (cid:48) g X g (cid:161) X (cid:48) X (cid:162)â1 . (4.54) g=1 ThelabelâCRâreferstoâcluster-robustâandâCR3âreferstotheanalogousformulafortheHC3estimator. Similarly to the heteroskedastic-robust case you can show that CR3 is a conservative estimator for CR3 V Î²(cid:98) inthesensethattheconditionalexpectationofV(cid:98)Î²(cid:98) exceedsV Î²(cid:98)",
    "page": 146,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". (4.54) g=1 ThelabelâCRâreferstoâcluster-robustâandâCR3âreferstotheanalogousformulafortheHC3estimator. Similarly to the heteroskedastic-robust case you can show that CR3 is a conservative estimator for CR3 V Î²(cid:98) inthesensethattheconditionalexpectationofV(cid:98)Î²(cid:98) exceedsV Î²(cid:98) . Thiscovariancematrixestimator maybemorecumbersometoimplement,however,asthecluster-levelpredictionerrors(4.52)cannotbe calculatedinasimplelinearoperationandappeartorequirealoop(acrossclusters)tocalculate. ToillustrateinthecontextoftheKenyanschoolingexamplewepresenttheregressionofstudenttest scoresontheschool-leveltrackingdummywithtwostandarderrorsdisplayed.Thefirst(inparenthesis)",
    "page": 146,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 127 istheconventionalrobuststandarderror.Thesecond[insquarebrackets]istheclusteredstandarderror from(4.50)-(4.4)whereclusteringisattheleveloftheschool. TestScore =â 0.071 + 0.138 Tracking +e . (4.55) ig g ig (0.019) (0.026) [0.054] [0.078] Wecanseethatthecluster-robuststandarderrorsareroughlythreetimestheconventionalrobust standarderrors.Consequently,confidenceintervalsforthecoefficientsaregreatlyaffectedbythechoice. Forillustration,welistherethecommandsneededtoproducetheregressionresultswithclustered standarderrorsinStata,R,andMATLAB. StatadoFile * Loaddata: use\"DDK2011.dta\" * Standardthetestscorevariabletohavemeanzeroandunitvariance: egentestscore=std(totalscore) * Regressionwithstandarderrorsclusteredattheschoollevel: regtestscoretracking,cluster(schoolid) YoucanseethatclusteredstandarderrorsaresimpletocalculateinStata. RProgramFile #Loadthedataandcreatevariables data<-read.table(\"DDK2011.txt\",header=TRUE,sep=\"\\t\") y<-scale(as.matrix(data$totalscore)) n<-nrow(y) x<-cbind(as.matrix(data$tracking),matrix(1,n,1)) schoolid<-as.matrix(data$schoolid) k<-ncol(x) xx<-t(x)%*%x invx<-solve(xx) beta<-solve(xx,t(x)%*%y) xe<-x*rep(y-x%*%beta,times=k) #Clusteredrobuststandarderror xe_sum<-rowsum(xe,schoolid) G<-nrow(xe_sum) omega<-t(xe_sum)%*%xe_sum scale<-G/(G-1)*(n-1)/(n-k) V_clustered<-scale*invx%*%omega%*%invx se_clustered<-sqrt(diag(V_clustered)) print(beta) print(se_clustered)",
    "page": 147,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 128 ProgrammingclusteredstandarderrorsinRisalsorelativelyeasyduetotheconvenientrowsumcom- mandwhichsumsvariableswithinclusters. MATLABProgramFile %Loadthedataandcreatevariables data=xlsread(âDDK2011.xlsxâ); schoolid=data(:,2); tracking=data(:,7); totalscore=data(:,62); y=(totalscore-mean(totalscore))./std(totalscore); x=[tracking,ones(size(y,1),1)]; [n,k]=size(x); xx=xâ*x; invx=inv(xx); beta=xx\\(xâ*y) e=y-x*beta; %Clusteredrobuststandarderror [schools,~,schoolidx]=unique(schoolid); G=size(schools,1); cluster_sums=zeros(G,k); forj=1:k cluster_sums(:,j)=accumarray(schoolidx,x(:,j).*e); end omega=cluster_sumsâ*cluster_sums; scale=G/(G-1)*(n-1)/(n-k); V_clustered=scale*invx*omega*invx; se_clustered=sqrt(diag(V_clustered)); display(beta); display(se_clustered); HereweseethatprogrammingclusteredstandarderrorsinMATLABislessconvenientthantheother packages but still can be executed with just a few lines of code. This example uses the accumarray commandwhichissimilartotherowsumcommandinRbutonlycanbeappliedtovectors(hencethe loopacrosstheregressors)andworksbestiftheclusteridvariableareindices(whichiswhytheoriginal schoolidvariableistransformedintoindicesinschoolidx. Applicationofthesecommandsrequirescare andattention. 4.24 InferencewithClusteredSamples Inthissectionwegivesomecautionaryremarksandgeneraladviceaboutcluster-robustinference in econometricpractice. There has beenremarkably littletheoretical researchaboutthe properties of cluster-robustmethodsâuntilquiterecentlyâsotheseremarksmaybecomedatedratherquickly. Inmanyrespectscluster-robustinferenceshouldbeviewedsimilarlytoheteroskedaticity-robustin- ference where a âclusterâ in the cluster-robust case is interpreted similarly to an âobservationâ in the heteroskedasticity-robustcase. Inparticular,theeffectivesamplesizeshouldbeviewedasthenumber",
    "page": 148,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 129 of clusters not the âsample sizeâ n. This is because the cluster-robust covariance matrix estimator ef- fectively treats each cluster as a single observation and estimates the covariance matrix based on the variation across cluster means. Hence if there are onlyG =50 clusters inference should be viewed as (atbest)similartoheteroskedasticity-robustinferencewithn=50observations. Thisisabitunsettling when the number of regressors is large (say k = 20) for then the covariance matrix will be estimated imprecisely. Furthermore, most cluster-robust theory (for example, the work of Chris Hansen (2007)) assumes thattheclustersarehomogeneousincludingtheassumptionthattheclustersizesareallidentical. This turnsouttobeaveryimportantsimplication.Whenthisisviolatedâwhen,forexample,clustersizesare highlyheterogeneousâtheregressionshouldbeviewedasroughlyequivalenttotheheteroskedasticcase withanextremelyhighdegreeofheteroskedasticity.Clustersumshavevarianceswhichareproportional to the cluster sizes so if the latter is heterogeneous so will be the variances of the cluster sums. This alsohasalargeeffectonfinitesampleinference. Whenclustersareheterogeneousthencluster-robust inferenceissimilartoheteroskedasticity-robustinferencewithhighlyheteroskedasticobservations. Puttogether,ifthenumberofclustersGissmallandthenumberofobservationsperclusterishighly varied then we should interpret inferential statements with a great degree of caution. Unfortunately, smallG withheterogeneousclustersizesiscommonplace. ManyempiricalstudiesonU.S.datacluster attheâstateâlevelmeaningthatthereare50or51clusters(theDistrictofColumbiaistypicallytreated asastate). Thenumberofobservationsvaryconsiderablyacrossstatessincethepopulationsarehighly unequal. Thus when you read empirical papers with individual-level data but clustered at the âstateâ levelyoushouldbecautiousandrecognizethatthisisequivalenttoinferencewithasmallnumberof extremelyheterogeneousobservations. Afurthercomplicationoccurswhenweareinterestedintreatmentasinthetrackingexamplegiven intheprevioussection. Inmanycases(includingDuflo,DupasandKremer(2011))theinterestisinthe effectofatreatmentappliedattheclusterlevel(e.g.,schools).Inmanycases(not,however,Duflo,Dupas andKremer(2011)),thenumberoftreatedclustersissmallrelativetothetotalnumberofclusters;inan extremecasethereisjustasingletreatedcluster.Basedonthereasoninggivenabovetheseapplications shouldbeinterpretedasequivalenttoheteroskedasticity-robustinferencewithasparsedummyvariable as discussed in Section 4.18. As discussed there, standard error estimates can be erroneously small. Intheextremeofasingletreatedcluster(intheexample, ifonlyasingleschoolwastracked)thenthe estimatedcoefficientontracking willbeveryimpreciselyestimatedyetwillhaveamisleadinglysmall cluster standard error. In general, reported standard errors will greatly understate the imprecision of parameterestimates",
    "page": 149,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". As discussed there, standard error estimates can be erroneously small. Intheextremeofasingletreatedcluster(intheexample, ifonlyasingleschoolwastracked)thenthe estimatedcoefficientontracking willbeveryimpreciselyestimatedyetwillhaveamisleadinglysmall cluster standard error. In general, reported standard errors will greatly understate the imprecision of parameterestimates. 4.25 AtWhatLeveltoCluster? Apracticalquestionwhicharisesinthecontextofcluster-robustinferenceisâAtwhatlevelshould wecluster?âInsomeexamplesyoucouldclusterataveryfinelevel,suchasfamiliesorclassrooms,or athigherlevelsofaggregation,suchasneighborhoods,schools,towns,counties,orstates. Whatisthe correct level at which to cluster? Rules of thumb have been advocated by practitioners but at present thereislittleformalanalysistoprovideusefulguidance.Whatdoweknow? First,supposeclusterdependenceisignoredorimposedattoofinealevel(e.g.clusteringbyhouse- holdsinsteadofvillages).Thenvarianceestimatorswillbebiasedastheywillomitcovarianceterms.As correlationistypicallypositive,thissuggeststhatstandarderrorswillbetoosmallgivingrisetospurious indicationsofsignificanceandprecision. Second,supposeclusterdependenceisimposedattooaggregateameasure(e.g.clusteringbystates rather than villages). This does not cause bias. But the variance estimators will contain many extra",
    "page": 149,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 130 componentssotheprecisionofthecovariancematrixestimatorwillbepoor. Thismeansthatreported standarderrorswillbeimpreciseâmorerandomâthanifclusteringhadbeenlessaggregate. Theseconsiderationsshowthatthereisatrade-offbetweenbiasandvarianceintheestimationofthe covariancematrixbycluster-robustmethods.Itisnotatallclearâbasedoncurrenttheoryâwhattodo. Istatethisemphatically. Wereallydonotknowwhatistheâcorrectâlevelatwhichtodocluster-robust inference.Thisisaveryinterestingquestionandshouldcertainlybeexploredbyeconometricresearch. One challenge is that in empirical practice many people have observed: âClustering is important. Standarderrorschangealotwhetherornotwecluster. Thereforeweshouldonlyreportclusteredstan- darderrors.âTheflawinthisreasoningisthatwedonotknowwhyinaspecificempiricalexamplethe standarderrorschangeunderclustering.Onepossibilityisthatclusteringreducesbiasandthusismore accurate.Theotherpossibilityisthatclusteringaddssamplingnoiseandisthuslessaccurate.Inreality itislikelythatbothfactorsarepresent. Inanyeventaresearchershouldbeawareofthenumberofclustersusedinthereportedcalculations andshouldtreatthenumberofclustersastheeffectivesamplesizeforassessinginference.Ifthenumber ofclustersis,say,G=20,thisshouldbetreatedasaverysmallsample. To illustrate the thought experiment consider the empirical example of Duflo, Dupas and Kremer (2011).Theyreportedstandarderrorsclusteredattheschoollevelandtheapplicationuses111schools. ThusG =111istheeffectivesamplesize. Thenumberofobservations(students)rangesfrom19to62, which is reasonably homogeneous. This seems like a well balanced application of clustered variance estimation.However,onecouldimagineclusteringatadifferentlevelofaggregation.Wemightconsider clusteringatalessaggregatelevelsuchastheclassroomlevel,butthiscannotbedoneinthisparticular applicationastherewasonlyoneclassroomperschool. Clusteringatamoreaggregatelevelcouldbe doneinthisapplicationattheleveloftheâzoneâ. However,thereareonly9zones. Thusifweclusterby zone,G=9istheeffectivesamplesizewhichwouldleadtoimprecisestandarderrors. Inthisparticular exampleclusteringattheschoollevel(asdonebytheauthors)isindeedtheprudentchoice. 4.26 TechnicalProofs* ProofofTheorems4.5and4.7Theorem4.5isaspecialcasesowefocusonTheorem4.7. Let F be the class of distributions on (Y,X) which satisfy the linear regression model with finite conditionalvariance. LetÎ² bethetruevalueoftheregressioncoefficient. Let f (e|x)denotethetrue 0 conditionaldensity(orprobabilitymassfunctioninthediscretecase)oftheregressionerrore=YâX (cid:48)Î² . 0 Forsome0<c<âdefine Ï2(x)=(cid:69)(cid:163) e21 {|e|â¤c}|X =x (cid:164) . c Noticethatascââ,Ï2(x)âÏ2(x). Pickc sufficientlylargesothatÏ2(X )>0forall1â¤i â¤n whichis c c i feasiblesinceÏ2(X i )>0forall1â¤i â¤n.SetÎ´=min 1â¤iâ¤n Ï2 c (X i )andM=max 1â¤iâ¤n (cid:107)X i (cid:107). Definethefunction Ï (e)=e 1 {|e|â¤c}â(cid:69)[e 1 {|e|â¤c}|X]",
    "page": 150,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Let f (e|x)denotethetrue 0 conditionaldensity(orprobabilitymassfunctioninthediscretecase)oftheregressionerrore=YâX (cid:48)Î² . 0 Forsome0<c<âdefine Ï2(x)=(cid:69)(cid:163) e21 {|e|â¤c}|X =x (cid:164) . c Noticethatascââ,Ï2(x)âÏ2(x). Pickc sufficientlylargesothatÏ2(X )>0forall1â¤i â¤n whichis c c i feasiblesinceÏ2(X i )>0forall1â¤i â¤n.SetÎ´=min 1â¤iâ¤n Ï2 c (X i )andM=max 1â¤iâ¤n (cid:107)X i (cid:107). Definethefunction Ï (e)=e 1 {|e|â¤c}â(cid:69)[e 1 {|e|â¤c}|X]. (4.56) c Noticethatitsatisfies (cid:175) (cid:175) Ï c (e) (cid:175) (cid:175) â¤2c,(cid:69)(cid:163)Ï c (e)|X (cid:164)=0,and(cid:69)(cid:163) eÏ c (e)|X (cid:164)=Ï2(X). Definetheparametricconditionalmodelfore (cid:195) (cid:161)Î²âÎ² (cid:162)(cid:48) xÏ (e) (cid:33) f (cid:161) e|x,Î²(cid:162)=f (e|x) 1+ 0 c c Ï2(x) c wheretheparameterÎ²takesvaluesintheset (cid:189) Î´ (cid:190) B c = Î²â(cid:82)m: (cid:176) (cid:176) Î²âÎ² 0 (cid:176) (cid:176) â¤ 2cM",
    "page": 150,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 131 and the regressor X takes values in the set {(cid:107)X(cid:107)â¤M}. This model implies that Y has the parametric density f (cid:161) yâx (cid:48)Î² |x,Î²(cid:162) . c 0 Theboundsimplythat (cid:175) (cid:175) (cid:175) (cid:161)Î²âÎ² 0 (cid:162)(cid:48) XÏ c (e) (cid:175) (cid:175) (cid:175)â¤ (cid:176) (cid:176) Î²âÎ² 0 (cid:176) (cid:176) (cid:107)X(cid:107)(cid:175) (cid:175) Ï c (e) (cid:175) (cid:175) <1. (cid:175) (cid:175) Ï2 c (X) (cid:175) (cid:175) Î´ Thisimpliesthat f (cid:161) e|x,Î²(cid:162) hasthesamesupportas f (e|x)andsatisfiesthebounds0â¤ f (cid:161) e|x,Î²(cid:162)â¤ c c 2f (e|x).Wecalculatethat (cid:90) (cid:90) (cid:90) (cid:161)Î²âÎ² (cid:162)(cid:48) xÏ (e) f (cid:161) e|x,Î²(cid:162) de= f (e|x)de+ f (e|x) 0 c de c Ï2(x) c (cid:161)Î²âÎ² (cid:162)(cid:48) x =1+ 0 (cid:69)(cid:163)Ï (e)|X =x (cid:164) Ï2(x) c c =1 (4.57) thelastequalitysince(cid:69)(cid:163)Ï (e)|X =x (cid:164)=0.Together,thesefactsimplythatf (cid:161) e|x,Î²(cid:162) andf (cid:161) yâx (cid:48)Î² |x,Î²(cid:162) c c c 0 arevalidconditionaldensityfunctions. Let (cid:69) [Â·|X =x] denote expectation under the conditional density f (cid:161) e|x,Î²(cid:162) . The conditional ex- c c pectationofY inthismodelis (cid:90) (cid:69) [Y |X =x]= yf (cid:161) yâx (cid:48)Î² |x,Î²(cid:162) dy c c 0 (cid:90) (cid:161)Î²âÎ² (cid:162)(cid:48) x(cid:90) =x (cid:48)Î² + ef (e|x)de+ 0 Ï (e)ef (e|x)de 0 Ï2(x) c c =x (cid:48)Î². (4.58) ThustheconditionalmeanislinearinxandÎ²correspondstotheregressioncoefficient. Thebound f (cid:161) e|x,Î²(cid:162)â¤2f (e|x)meansthatanymomentwhichisfiniteunder f (e|x)isalsofinite c under f (cid:161) e|x,Î²(cid:162) .Thisimpliesthat c (cid:69) (cid:163) e2|X =x (cid:164)â¤2(cid:69)(cid:163) e2|X =x (cid:164)=2Ï2(x)<â c sothemodel f (cid:161) e|x,Î²(cid:162) hasafinitesecondconditionalmoment.Wededucethat c f (cid:161) yâx (cid:48)Î² |x,Î²(cid:162)âF. c 0 When Î² = Î² , f (cid:161) yâx (cid:48)Î² |x,Î² (cid:162) = f (cid:161) yâx (cid:48)Î² |x (cid:162) equals the true conditional density of Y. Thus 0 c 0 0 0 f (cid:161) yâx (cid:48)Î² |x,Î²(cid:162) isacorrectlyspecifiedmodelwithtrueparametervalueÎ²=Î² . c 0 0 ThescoreofthemodelatthetruevalueÎ²=Î² is 0 â (cid:175) S= âÎ² logf c (cid:161) yâx (cid:48)Î² 0 |x,Î²(cid:162)(cid:175) (cid:175) (cid:175) Î²=Î² 0 (cid:175) xÏ (e) (cid:175) = c (cid:175) Ï2 c (x)+(cid:161)Î²âÎ² 0 (cid:162)(cid:48) xÏ c (e) (cid:175) (cid:175)Î²=Î² 0 xÏ (e) = c . Ï2(x) c",
    "page": 151,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 132 Theinformationmatrixfortheith observationis I(X i )=var[S i |X i ]=var (cid:183) X Ï i Ï 2( c X (e ) i ) (cid:175) (cid:175) (cid:175) (cid:175) X i (cid:184) =X i X i (cid:48) Ï v c 2 4 ( ( X X i ) ) c i c i where v c 2(X)=var (cid:163)Ï c (e) (cid:175) (cid:175) X (cid:164) â¤(cid:69)(cid:163) e21 {|e|â¤c}|X (cid:164) =Ï2(X). (4.59) c Theinformationmatrixforthefullsampleis n (cid:88) I = I(X ) c i i=1 = (cid:88) n X X (cid:48) v c 2(X i ) i=1 i iÏ4 c (X i ) â¤ (cid:88) n X X (cid:48) 1 i=1 i iÏ2 c (X i ) =X (cid:48) D â1X (4.60) c wheretheinequalityis(4.59)andD =diag (cid:169)Ï2(X ),...,Ï2(X ) (cid:170) . c c 1 c n By assumption the estimator Î² (cid:101) is unbiased for Î² for all F â F. Since f c (cid:161) yâx (cid:48)Î² 0 |x,Î²(cid:162) â F this meansthatÎ² (cid:101)isunbiasedforÎ²inthemodel f c (cid:161) yâx (cid:48)Î² 0 |x,Î²(cid:162) . Themodel f c (cid:161) yâx (cid:48)Î² 0 |x,Î²(cid:162) iscorrectly- specified,thesupportofY doesnotdependonÎ²,I >0,andthetruevalueÎ² liesintheinteriorofthe c 0 parameterspaceB . TheconditionsfortheCramÃ©r-RaoTheorem(e.g. Theorem10.6ofIntroductionto c Econometrics)arethussatisfied,whichstatesthat var (cid:163)Î² (cid:101) |X (cid:164)â¥Iâ1â¥(cid:161) X (cid:48) D â1X (cid:162)â1 c c wherethesecondinequalityis(4.60).Sincethisholdsforallc var (cid:163)Î² (cid:101) |X (cid:164)â¥limsup (cid:161) X (cid:48) D â1X (cid:162)â1=(cid:161) X (cid:48) D â1X (cid:162)â1 . c cââ ThisisthestatementofTheorem4.7. Theorem4.5obtainsasthespecialcaseD =I Ï2 sothebound n simplifiestoÏ2(cid:161) X (cid:48) X (cid:162)â1 . â  _____________________________________________________________________________________________ 4.27 Exercises Exercise4.1 Forsomeintegerk,setÂµ =(cid:69)[Yk]. k (a) ConstructanestimatorÂµ forÂµ . (cid:98)k k (b) ShowthatÂµ isunbiasedforÂµ . (cid:98)k k (c) CalculatethevarianceofÂµ ,sayvar (cid:163)Âµ (cid:164) .Whatassumptionisneededforvar (cid:163)Âµ (cid:164) tobefinite? (cid:98)k (cid:98)k (cid:98)k (d) Proposeanestimatorofvar (cid:163)Âµ (cid:164) . (cid:98)k",
    "page": 152,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 133 (cid:183) (cid:184) (cid:179) (cid:180)3 Exercise4.2 Calculate(cid:69) Y âÂµ ,theskewnessofY.Underwhatconditionisitzero? Exercise4.3 ExplainthedifferencebetweenY andÂµ.Explainthedifferencebetweenn â1(cid:80)n X X (cid:48) and i=1 i i (cid:69)(cid:163) X X (cid:48)(cid:164) . i i Exercise4.4 TrueorFalse. IfY =X (cid:48)Î²+e,X â(cid:82),(cid:69)[e|X]=0,ande istheOLSresidualfromtheregres- (cid:98)i sionofY onX ,then (cid:80)n X2e =0. i i i=1 i (cid:98)i Exercise4.5 Prove(4.15)and(4.16) Exercise4.6 ProveTheorem4.6. Exercise4.7 Let Î² (cid:101)be the GLS estimator (4.17) under the assumptions (4.13) and (4.14). Assume that â¦=c2Î£withÎ£knownandc2unknown.Definetheresidualvector (cid:101) e=Y âXÎ² (cid:101),andanestimatorforc2 c2= 1 e (cid:48)Î£â1e. (cid:101) nâk (cid:101) (cid:101) (a) Show(4.18). (b) Show(4.19). (c) Provethate=M e,whereM =IâX (cid:161) X (cid:48)Î£â1X (cid:162)â1 X (cid:48)Î£â1. (cid:101) 1 1 (d) ProvethatM (cid:48)Î£â1M =Î£â1âÎ£â1X (cid:161) X (cid:48)Î£â1X (cid:162)â1 X (cid:48)Î£â1. 1 1 (e) Find(cid:69)(cid:163) c2|X (cid:164) . (cid:101) (f) Isc2areasonableestimatorforc2? (cid:101) Exercise4.8 Let(Y ,X )bearandomsamplewith(cid:69)[Y |X]=X (cid:48)Î².ConsidertheWeightedLeastSquares i i (WLS)estimatorÎ² (cid:101)wls =(cid:161) X (cid:48) WX (cid:162)â1(cid:161) X (cid:48) WY (cid:162) whereW =diag(w 1 ,...,w n )andw i =X j â i 2 ,whereX ji isone oftheX . i (a) InwhichcontextswouldÎ² (cid:101)wls beagoodestimator? (b) Usingyourintuition,inwhichsituationsdoyouexpectÎ² (cid:101)wls toperformbetterthanOLS? Exercise4.9 Show(4.27)inthehomoskedasticregressionmodel. Exercise4.10 Prove(4.35). Exercise4.11 Show(4.36)inthehomoskedasticregressionmodel. (cid:104) (cid:105) (cid:104) (cid:105) Exercise4.12 LetÂµ=(cid:69)[Y],Ï2=(cid:69) (cid:161) Y âÂµ(cid:162)2 andÂµ =(cid:69) (cid:161) Y âÂµ(cid:162)3 andconsiderthesamplemeanY = 3 (cid:183) (cid:184) (cid:179) (cid:180)3 1(cid:80)n Y .Find(cid:69) Y âÂµ asafunctionofÂµ,Ï2,Âµ andn. n i=1 i 3 Exercise4.13 TakethesimpleregressionmodelY =XÎ²+e, X â(cid:82),(cid:69)[e|X]=0. DefineÏ2=(cid:69)(cid:163) e2|X (cid:164) i i i andÂµ 3i =(cid:69)(cid:163) e i 3|X i (cid:164) andconsidertheOLScoefficientÎ² (cid:98).Find(cid:69) (cid:104) (cid:161)Î² (cid:98) âÎ²(cid:162)3|X (cid:105) .",
    "page": 153,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 134 Exercise4.14 TakearegressionmodelY =XÎ²+e with(cid:69)[e|X]=0andi.i.d. observations(Y ,X )and i i scalarX.TheparameterofinterestisÎ¸=Î²2.ConsidertheOLSestimatorsÎ² (cid:98)andÎ¸ (cid:98) =Î² (cid:98) 2. (a) Find(cid:69)(cid:163)Î¸ (cid:98) |X (cid:164) usingourknowledgeof(cid:69)(cid:163)Î² (cid:98) |X (cid:164) andV Î²(cid:98) =var (cid:163)Î² (cid:98) |X (cid:164) .IsÎ¸ (cid:98)biasedforÎ¸? (b) Suggestan(approximate)biased-correctedestimatorÎ¸ (cid:98) â usinganestimatorV(cid:98)Î²(cid:98) forV Î²(cid:98) . (c) ForÎ¸ (cid:98) â tobepotentiallyunbiased,whichestimatorofV Î²(cid:98) ismostappropriate? UnderwhichconditionsisÎ¸ (cid:98) â unbiased? Exercise4.15 Considerani.i.d.sample{Y ,X }i =1,...,n whereX iskÃ1.Assumethelinearconditional i i expectation model Y = X (cid:48)Î²+e with (cid:69)[e|X]=0. Assume that n â1X (cid:48) X =I (orthonormal regressors). k ConsidertheOLSestimatorÎ² (cid:98). (a) FindV Î²(cid:98) =var (cid:163)Î² (cid:98) (cid:164) (b) Ingeneral,areÎ² (cid:98)j andÎ² (cid:98)(cid:96)for j (cid:54)=(cid:96)correlatedoruncorrelated? (c) FindasufficientconditionsothatÎ² (cid:98)j andÎ² (cid:98)(cid:96)for j (cid:54)=(cid:96)areuncorrelated. Exercise4.16 TakethelinearhomoskedasticCEF Y â=X (cid:48)Î²+e (4.61) (cid:69)[e|X]=0 (cid:69)(cid:163) e2|X (cid:164)=Ï2 andsupposethatY â ismeasuredwitherror. InsteadofY â ,weobserveY =Y â+uwhereuismeasure- menterror.Supposethateanduareindependentand (cid:69)[u|X]=0 (cid:69)(cid:163) u2|X (cid:164)=Ï2(X) u (a) DeriveanequationforY asafunctionofX. Beexplicittowritetheerrortermasafunctionofthe structuralerrorseandu.Whatistheeffectofthismeasurementerroronthemodel(4.61)? (b) DescribetheeffectofthismeasurementerroronOLSestimationofÎ²inthefeasibleregressionof theobservedY onX. (c) Describetheeffect(ifany)ofthismeasurementerroronstandarderrorcalculationforÎ² (cid:98). Exercise4.17 Supposethatfortherandomvariables(Y,X)withX >0aneconomicmodelimplies (cid:69)[Y |X]=(cid:161)Î³+Î¸X (cid:162)1/2 . (4.62) AfriendsuggeststhatyouestimateÎ³andÎ¸ bythelinearregressionofY2 on X,thatis,toestimatethe equation Y2=Î±+Î²X+e. (4.63) (a) Investigateyourfriendâssuggestion. Defineu=Y â(cid:161)Î³+Î¸X (cid:162)1/2 .Showthat(cid:69)[u|X]=0isimplied by(4.62).",
    "page": 154,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 135 (b) UseY =(cid:161)Î³+Î¸X (cid:162)1/2+utocalculate(cid:69)(cid:163) Y2|X (cid:164) . Whatdoesthistellyouabouttheimpliedequation (4.63)? (c) CanyourecovereitherÎ³and/orÎ¸fromestimationof(4.63)?Areadditionalassumptionsrequired? (d) Isthisareasonablesuggestion? Exercise4.18 Takethemodel Y =X (cid:48)Î² +X (cid:48)Î² +e 1 1 2 2 (cid:69)[e|X]=0 (cid:69)(cid:163) e2|X (cid:164)=Ï2 whereX =(X 1 ,X 2 ),withX 1 k 1 Ã1andX 2 k 2 Ã1.ConsidertheshortregressionY i =X 1 (cid:48) i Î² (cid:98)1 +e (cid:98)i anddefine theerrorvarianceestimators2=(nâk ) â1(cid:80)n e2.Find(cid:69)(cid:163) s2|X (cid:164) . 1 i=1(cid:98)i Exercise4.19 LetY benÃ1,X benÃk,andX â=XC whereC iskÃk andfull-rank. LetÎ² (cid:98)betheleast squaresestimatorfromtheregressionofY onX,andletV(cid:98) betheestimateofitsasymptoticcovariance matrix.LetÎ² (cid:98) â andV(cid:98) â bethosefromtheregressionofY onX â .DeriveanexpressionforV(cid:98) â asafunction ofV(cid:98). Exercise4.20 Takethemodelinvectornotation Y =XÎ²+e (cid:69)[e|X]=0 (cid:69)(cid:163) ee (cid:48)|X (cid:164)=â¦. Assume for simplicity that â¦ is known. Consider the OLS and GLS estimators Î² (cid:98) =(cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48) Y (cid:162) and Î² (cid:101) =(cid:161) X (cid:48)â¦â1X (cid:162)â1(cid:161) X (cid:48)â¦â1Y (cid:162) .Computethe(conditional)covariancebetweenÎ² (cid:98)andÎ² (cid:101): (cid:69) (cid:104) (cid:161)Î² (cid:98) âÎ²(cid:162)(cid:161)Î² (cid:101) âÎ²(cid:162)(cid:48) |X (cid:105) Findthe(conditional)covariancematrixforÎ² (cid:98) âÎ² (cid:101): (cid:69) (cid:104) (cid:161)Î² (cid:98) âÎ² (cid:101) (cid:162)(cid:161)Î² (cid:98) âÎ²(cid:162)(cid:48) |X (cid:105) . Exercise4.21 Themodelis Y =X (cid:48)Î²+e i i i (cid:69)[e |X ]=0 i i (cid:69)(cid:163) e2|X (cid:164)=Ï2 i i i â¦=diag{Ï2,...,Ï2}. 1 n TheparameterÎ²isestimatedbyOLSÎ² (cid:98) =(cid:161) X (cid:48) X (cid:162)â1 X (cid:48) Y andGLSÎ² (cid:101) =(cid:161) X (cid:48)â¦â1X (cid:162)â1 X (cid:48)â¦â1Y.Let (cid:98) e=YâXÎ² (cid:98) and (cid:101) e = Y âXÎ² (cid:101)denote the residuals. Let R(cid:98) 2 = 1â (cid:98) e (cid:48) (cid:98) e/(Y â(cid:48) Y â ) and R(cid:101) 2 = 1â (cid:101) e (cid:48) (cid:101) e/(Y â(cid:48) Y â ) denote the equationR2whereY â=Y âY.Iftheerrore i istrulyheteroskedasticwillR(cid:98) 2orR(cid:101) 2besmaller? Exercise4.22 Aneconomistfriendtellsyouthattheassumptionthattheobservations(Y ,X )arei.i.d. i i impliesthattheregressionY =X (cid:48)Î²+eishomoskedastic.Doyouagreewithyourfriend?Howwouldyou explainyourposition?",
    "page": 155,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER4. LEASTSQUARESREGRESSION 136 Exercise4.23 Takethelinearregressionmodelwith(cid:69)[Y |X]=XÎ².Definetheridgeregressionestimator Î² (cid:98) =(cid:161) X (cid:48) X +I k Î»(cid:162)â1 X (cid:48) Y whereÎ»>0isafixedconstant.FindE (cid:163)Î² (cid:98) |X (cid:164) .IsÎ² (cid:98)biasedforÎ²? Exercise4.24 ContinuetheempiricalanalysisinExercise3.24. (a) Calculatestandarderrorsusingthehomoskedasticityformulaandusingthefourcovariancema- tricesfromSection4.16. (b) Repeatinyoursecondprogramminglanguage.Aretheyidentical? Exercise4.25 ContinuetheempiricalanalysisinExercise3.26.CalculatestandarderrorsusingtheHC3 method.Repeatinyoursecondprogramminglanguage.Aretheyidentical? Exercise4.26 ExtendtheempiricalanalysisreportedinSection4.23usingtheDDK2011datasetonthe textbookwebsite.. Doaregressionofstandardizedtestscore(totalscorenormalizedtohavezeromean andvariance1)ontracking,age,gender,beingassignedtothecontractteacher,andstudentâspercentile in the initial distribution. (The sample size will be smaller as some observations have missing vari- ables.) Calculatestandarderrorsusingboththeconventionalrobustformula,andclusteringbasedon theschool. (a) Compare thetwo setsofstandard errors. Whichstandarderrorchangesthemostby clustering? Whichchangestheleast? (b) Howdoesthecoefficientontracking changebyinclusion oftheindividualcontrols(incompari- sontotheresultsfrom(4.55))?",
    "page": 156,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Chapter 5 Normal Regression 5.1 Introduction Thischapterintroducesthenormalregressionmodel,whichisaspecialcaseofthelinearregression model.Itisimportantasnormalityallowsprecisedistributionalcharacterizationsandsharpinferences. Italso provides a baseline for comparison withalternative inference methods, suchas asymptotic ap- proximationsandthebootstrap. The normal regression model is a fully parametric setting where maximum likelihood estimation is appropriate. Therefore in this chapter we introduce likelihood methods. The method of maximum likelihoodisapowerfulstatisticalmethodforparametricmodels(suchasthenormalregressionmodel) andiswidelyusedineconometricpractice. We start the chapter with a review of the definition and properties of the normal distribution. For detailandmathematicalproofsseeChapter5ofIntroductiontoEconometrics. 5.2 TheNormalDistribution We say that a random variable Z has the standardnormaldistribution, or Gaussian, written Z â¼ N(0,1),ifithasthedensity 1 (cid:181) x2(cid:182) Ï(x)= (cid:112) exp â , ââ<x<â. 2Ï 2 ThestandardnormaldensityistypicallywrittenwiththesymbolÏ(x)andthecorrespondingdistribution functionbyÎ¦(x). PlotsofthestandardnormaldensityfunctionÏ(z)anddistributionfunctionÎ¦(x)are displayedinFigure5.1. 137",
    "page": 157,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER5. NORMALREGRESSION 138 â4 â2 0 2 4 4.0 3.0 2.0 1.0 0.0 â4 â3 â2 â1 0 1 2 3 4 (a)NormalDensity 0.1 8.0 6.0 4.0 2.0 0.0 â4 â3 â2 â1 0 1 2 3 4 (b)NormalDistribution Figure5.1:StandardNormalDensityandDistribution Theorem5.1 IfZ â¼N(0,1)then 1. AllintegermomentsofZ arefinite. 2. AlloddmomentsofZ equal0. 3. Foranypositiveintegerm (cid:69)(cid:163) Z2m(cid:164)=(2mâ1)!!=(2mâ1)Ã(2mâ3)ÃÂ·Â·Â·Ã1. 4. Foranyr >0 2r/2 (cid:181) r+1 (cid:182) (cid:69)|Z|r = (cid:112) Î Ï 2 whereÎ(t)=(cid:82)â utâ1e âuduisthegammafunction. 0 If Z â¼N(0,1) and X =Âµ+ÏZ for Âµâ(cid:82) and Ïâ¥0 then X has the univariatenormaldistribution, writtenX â¼N (cid:161)Âµ,Ï2(cid:162) .Bychange-of-variablesX hasthedensity 1 (cid:195) (cid:161) xâÂµ(cid:162)2(cid:33) f(x)= (cid:112) exp â , ââ<x<â. 2ÏÏ2 2Ï2 ThemeanandvarianceofX areÂµandÏ2,respectively. Thenormaldistributionanditsrelatives(thechi-square,studentt,F,non-centralchi-squareandF) arefrequentlyusedforinferencetocalculatecriticalvaluesandp-values. Thisinvolvesevaluatingthe normalcdfÎ¦(x)anditsinverse. SincethecdfÎ¦(x)isnotavailableinclosedformstatisticaltextbooks havetraditionallyprovidedtablesforthispurpose. Suchtablesarenotusedcurrentlyasthesecalcula- tionsareembeddedinmodernstatisticalsoftware. Forconvenience,welisttheappropriatecommands",
    "page": 158,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER5. NORMALREGRESSION 139 inMATLAB,R,andStatatocomputethecumulativedistributionfunctionofcommonlyusedstatistical distributions. NumericalCumulativeDistributionFunction Tocalculate(cid:80)(Z â¤x)forgivenx MATLAB R Stata N(0,1) normcdf(x) pnorm(x) normal(x) Ï2 chi2cdf(x,r) pchisq(x,r) chi2(r,x) r t tcdf(x,r) pt(x,r) 1-ttail(r,x) r F fcdf(x,r,k) pf(x,r,k) F(r,k,x) r,k Ï2(d) ncx2cdf(x,r,d) pchisq(x,r,d) nchi2(r,d,x) r F (d) ncfcdf(x,r,k,d) pf(x,r,k,d) 1-nFtail(r,k,d,x) r,k Herewelisttheappropriatecommandstocomputetheinverseprobabilities(quantiles)ofthesame distributions. NumericalQuantileFunction Tocalculatexwhichsolvesp=(cid:80)(Z â¤x)forgivenp MATLAB R Stata N(0,1) norminv(p) qnorm(p) invnormal(p) Ï2 chi2inv(p,r) qchisq(p,r) invchi2(r,p) r t tinv(p,r) qt(p,r) invttail(r,1-p) r F finv(p,r,k) qf(p,r,k) invF(r,k,p) r,k Ï2(d) ncx2inv(p,r,d) qchisq(p,r,d) invnchi2(r,d,p) r F (d) ncfinv(p,r,k,d) qf(p,r,k,d) invnFtail(r,k,d,1-p) r,k 5.3 MultivariateNormalDistribution Wesaythatthek-vectorZ hasamultivariatestandardnormaldistribution,writtenZ â¼N(0,I ),if k ithasthejointdensity (cid:181) (cid:48) (cid:182) 1 x x f(x)= exp â , xâ(cid:82)k. (2Ï)k/2 2 Themeanandcovariancematrixof Z are0and I ,respectively. Themultivariatejointdensityfactors k intotheproductofunivariatenormaldensities,sotheelementsofZ aremutuallyindependentstandard normals. IfZ â¼N(0,I )andX =Âµ+BZ thenthek-vectorX hasamultivariatenormaldistribution,written k X â¼N (cid:161)Âµ,Î£(cid:162) whereÎ£=BB (cid:48)â¥0.IfÎ£>0thenbychange-of-variablesX hasthejointdensityfunction 1 (cid:195) (cid:161) xâÂµ(cid:162)(cid:48) Î£â1(cid:161) xâÂµ(cid:162)(cid:33) f(x)= exp â , xâ(cid:82)k. (2Ï)k/2det(Î£)1/2 2",
    "page": 159,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER5. NORMALREGRESSION 140 ThemeanandcovariancematrixofX areÂµandÎ£,respectively. Bysettingk=1youcancheckthatthe multivariatenormalsimplifiestotheunivariatenormal. Animportantpropertyofnormalrandomvectorsisthataffinefunctionsaremultivariatenormal. Theorem5.2 IfX â¼N (cid:161)Âµ,Î£(cid:162) andY =a+BX,thenY â¼N (cid:161) a+BÂµ,BÎ£B (cid:48)(cid:162) . OnesimpleimplicationofTheorem5.2isthatifX ismultivariatenormaltheneachcomponentofX isunivariatenormal. Anotherusefulpropertyofthemultivariatenormaldistributionisthatuncorrelatednessisthesame asindependence. Thatis,ifavectorismultivariatenormal,subsetsofvariablesareindependentifand onlyiftheyareuncorrelated. Theorem5.3 PropertiesoftheMultivariateNormalDistribution 1. The mean and covariance matrix of X â¼ N (cid:161)Âµ,Î£(cid:162) are (cid:69)[X] = Âµ and var[X]=Î£. 2. If(X,Y)aremultivariatenormal,X andY areuncorrelatedifandonlyif theyareindependent. 3. IfX â¼N (cid:161)Âµ,Î£(cid:162) andY =a+BX,thenY â¼N (cid:161) a+BÂµ,BÎ£B (cid:48)(cid:162) . 4. IfX â¼N(0,I )thenX (cid:48) X â¼Ï2,chi-squarewithk degreesoffreedom. k k 5. IfX â¼N(0,Î£)withÎ£>0thenX (cid:48)Î£â1X â¼Ï2 wherek=dim(X). k 6. IfX â¼N(Âµ,Î£)withÎ£>0,rÃr,thenX (cid:48)Î£â1X â¼Ï2(Î»)whereÎ»=Âµ(cid:48)Î£â1Âµ. r 7. If Z â¼N(0,1)andQ â¼Ï2 areindependentthen Z/ (cid:112) Q/kâ¼t ,studentt k k withk degreesoffreedom. 8. If(Y X)aremultivariatenormal , (cid:181) Y (cid:182) (cid:181)(cid:181) Âµ (cid:182) (cid:181) Î£ Î£ (cid:182)(cid:182) â¼N Y , YY YX X Âµ Î£ Î£ X XY XX withÎ£ >0andÎ£ >0thentheconditionaldistributionsare YY XX Y |X â¼N (cid:161)Âµ +Î£ Î£â1 (cid:161) XâÂµ (cid:162) ,Î£ âÎ£ Î£â1 Î£ (cid:162) Y YX XX X YY YX XX XY X |Y â¼N (cid:161)Âµ +Î£ Î£â1 (cid:161) Y âÂµ (cid:162) ,Î£ âÎ£ Î£â1 Î£ (cid:162) . X XY YY Y XX XY YY YX",
    "page": 160,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER5. NORMALREGRESSION 141 5.4 JointNormalityandLinearRegression Supposethevariables(Y,X)arejointlynormallydistributed.ConsiderthebestlinearpredictorofY givenX Y =X (cid:48)Î²+Î±+e. Bythepropertiesofthebestlinearpredictor,(cid:69)[Xe]=0and(cid:69)[e]=0,soX ande areuncorrelated. Since (e,X)isanaffinetransformationofthenormalvector(Y,X)itfollowsthat(e,X)isjointlynormal(Theo- rem5.2). Since(e,X)isjointlynormalanduncorrelatedtheyareindependent(Theorem5.3). Indepen- denceimpliesthat (cid:69)[e|X]=(cid:69)[e]=0 and (cid:69)(cid:163) e2|X (cid:164)=(cid:69)(cid:163) e2(cid:164)=Ï2 whicharepropertiesofahomoskedasticlinearCEF. Wehaveshownthatwhen(Y,X)arejointlynormallydistributedtheysatisfyanormallinearCEF Y =X (cid:48)Î²+Î±+e where eâ¼N(0,Ï2) isindependentofX.ThisresultcanalsobededucedfromTheorem5.3.7. Thisisaclassicalmotivationforthelinearregressionmodel. 5.5 NormalRegressionModel Thenormalregressionmodelisthelinearregressionmodelwithanindependentnormalerror Y =X (cid:48)Î²+e (5.1) eâ¼N(0,Ï2). As we learned in Section 5.4 the normal regression model holds when (Y,X) are jointly normally dis- tributed. Normal regression, however, does not require joint normality. All that is required is that the conditionaldistributionofY given X isnormal(themarginaldistributionof X isunrestricted). Inthis sense the normal regression model is broader than joint normality. Notice that for notational conve- niencewehavewritten(5.1)sothatX containstheintercept. Normalregressionisaparametricmodelwherelikelihoodmethodscanbeusedforestimation,test- ing, and distribution theory. The likelihood is the name for the joint probability density of the data, evaluatedattheobservedsample,andviewedasafunctionoftheparameters.Themaximumlikelihood estimatoristhevaluewhichmaximizesthislikelihoodfunction. Letusnowderivethelikelihoodofthe normalregressionmodel. First,observethatmodel(5.1)isequivalenttothestatementthattheconditionaldensityofY given X takestheform (cid:181) (cid:182) f (cid:161) y|x (cid:162)= 1 exp â 1 (cid:161) yâx (cid:48)Î²(cid:162)2 . (cid:161) 2ÏÏ2 (cid:162)1/2 2Ï2",
    "page": 161,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER5. NORMALREGRESSION 142 Undertheassumptionthattheobservationsaremutuallyindependentthisimpliesthattheconditional densityof(Y ,...,Y )given(X ,...,X )is 1 n 1 n n f (cid:161) y ,...,y |x ,...,x (cid:162)= (cid:89) f (cid:161) y |x (cid:162) 1 n 1 n i i i=1 = (cid:89) n 1 exp (cid:181) â 1 (cid:161) y âx (cid:48)Î²(cid:162)2 (cid:182) i=1 (cid:161) 2ÏÏ2 (cid:162)1/2 2Ï2 i i (cid:195) (cid:33) = 1 exp â 1 (cid:88) n (cid:161) y âx (cid:48)Î²(cid:162)2 (cid:161) 2ÏÏ2 (cid:162)n/2 2Ï2 i=1 i i d=ef L (Î²,Ï2). n Thisiscalledthelikelihoodfunctionwhenevaluatedatthesampledata. Forconvenienceitistypicaltoworkwiththenaturallogarithm logL (Î²,Ï2)=â n log(2ÏÏ2)â 1 (cid:88) n (cid:161) Y âX (cid:48)Î²(cid:162)2d=ef(cid:96) (Î²,Ï2) (5.2) n 2 2Ï2 i i n i=1 whichiscalledthelog-likelihoodfunction. Themaximumlikelihoodestimator(MLE)(Î² (cid:98)mle ,Ï (cid:98) 2 mle )isthevaluewhichmaximizesthelog-likelihood. Wecanwritethemaximizationproblemas (Î² (cid:98)mle ,Ï (cid:98) 2 mle )= argmax (cid:96) n (Î²,Ï2). (5.3) Î²â(cid:82)k,Ï2>0 InmostapplicationsofmaximumlikelihoodtheMLEmustbefoundbynumericalmethods. However inthecaseofthenormalregressionmodelwecanfindanexplicitexpressionforÎ² (cid:98)mle andÏ (cid:98) 2 mle . Themaximizers(Î² (cid:98)mle ,Ï (cid:98) 2 mle )of(5.3)jointlysolvethefirst-orderconditions(FOC) 0= â â Î² (cid:96) n (Î²,Ï2) (cid:175) (cid:175) (cid:175) (cid:175) = Ï2 1 (cid:88) n X i (cid:161) Y i âX i (cid:48)Î² (cid:98)mle (cid:162) (5.4) Î²=Î²(cid:98)mle,Ï2=Ï (cid:98) 2 mle (cid:98)mlei=1 0= âÏ â 2 (cid:96) n (Î²,Ï2) (cid:175) (cid:175) (cid:175) (cid:175)Î²=Î²(cid:98)mle,Ï2=Ï (cid:98) 2 mle =â 2Ï (cid:98) n 2 mle + 2Ï (cid:98) 1 4 mlei (cid:88) = n 1 (cid:161) Y i âX i (cid:48)Î² (cid:98)mle (cid:162)2 . (5.5) ThefirstFOC(5.4)isproportionaltothefirst-orderconditionsfortheleastsquaresminimizationprob- lemofSection3.6.ItfollowsthattheMLEsatisfies (cid:195) (cid:33)â1(cid:195) (cid:33) n n Î² (cid:98)mle = (cid:88) X i X i (cid:48) (cid:88) X i Y i =Î² (cid:98)ols . i=1 i=1 Thatis,theMLEforÎ²isalgebraicallyidenticaltotheOLSestimator. SolvingthesecondFOC(5.5)forÏ2 wefind (cid:98)mle Ï (cid:98) 2 mle = n 1 (cid:88) n (cid:161) Y i âX i (cid:48)Î² (cid:98)mle (cid:162)2= n 1 (cid:88) n (cid:161) Y i âX i (cid:48)Î² (cid:98)ols (cid:162)2= n 1 (cid:88) n e (cid:98)i 2=Ï (cid:98) 2 ols . i=1 i=1 i=1 ThustheMLEforÏ2isidenticaltotheOLS/momentestimatorfrom(3.26). SincetheOLSestimatorandMLEundernormalityareequivalent,Î² (cid:98)isdescribedbysomeauthorsas themaximumlikelihoodestimator,andbyotherauthorsastheleastsquaresestimator. Itisimportant",
    "page": 162,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER5. NORMALREGRESSION 143 toremember,however,thatÎ² (cid:98)isonlytheMLEwhentheerrorehasaknownnormaldistributionandnot otherwise. Pluggingtheestimatorsinto(5.2)weobtainthemaximizedlog-likelihood (cid:96) n (cid:161)Î² (cid:98)mle ,Ï (cid:98) 2 mle (cid:162)=â n 2 log (cid:161) 2ÏÏ (cid:98) 2 mle (cid:162)â n 2 . (5.6) Thelog-likelihoodistypicallyreportedasameasureoffit. ItmayseemsurprisingthattheMLEÎ² (cid:98)mle isnumericallyequaltotheOLSestimatordespiteemerging fromquitedifferentmotivations. Itisnotcompletelyaccidental. Theleastsquaresestimatorminimizes aparticularsamplelossfunctionâthesumofsquarederrorcriterionâandmostlossfunctionsareequiv- alenttothelikelihoodofaspecificparametricdistribution,inthiscasethenormalregressionmodel. In thissenseitisnotsurprisingthattheleastsquaresestimatorcanbemotivatedaseithertheminimizer ofasamplelossfunctionorasthemaximizerofalikelihoodfunction. CarlFriedrichGauss The mathematician Carl Friedrich Gauss (1777-1855) proposed the normal re- gression model, and derived the least squares estimator as the maximum like- lihoodestimatorforthismodel. Heclaimedtohavediscoveredthemethodin 1795 at the age of eighteen but did not publish the result until 1809. Interest in Gaussâs approach was reinforced by Laplaceâs simultaneous discovery of the centrallimittheorem,whichprovidedajustificationforviewingrandomdistur- bancesasapproximatelynormal. 5.6 DistributionofOLSCoefficientVector InthenormallinearregressionmodelwecanderiveexactsamplingdistributionsfortheOLS/MLE estimator,residuals,andvarianceestimator. InthissectionwederivethedistributionoftheOLScoeffi- cientestimator. Thenormalityassumptione|X â¼N (cid:161) 0,Ï2(cid:162) combinedwithindependenceoftheobservationshasthe multivariateimplication e|X â¼N (cid:161) 0,I Ï2(cid:162) . n Thatis,theerrorvectore isindependentofX andisnormallydistributed. RecallthattheOLSestimatorsatisfies Î² (cid:98) âÎ²=(cid:161) X (cid:48) X (cid:162)â1 X (cid:48) e which is a linear function of e. Since linear functions of normals are also normal (Theorem 5.2) this impliesthatconditionalonX, Î² (cid:98) âÎ²|X â¼(cid:161) X (cid:48) X (cid:162)â1 X (cid:48) N (cid:161) 0,I n Ï2(cid:162) â¼N (cid:179) 0,Ï2(cid:161) X (cid:48) X (cid:162)â1 X (cid:48) X (cid:161) X (cid:48) X (cid:162)â1 (cid:180) =N (cid:179) 0,Ï2(cid:161) X (cid:48) X (cid:162)â1 (cid:180) . ThisshowsthatundertheassumptionofnormalerrorstheOLSestimatorhasanexactnormaldis- tribution.",
    "page": 163,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER5. NORMALREGRESSION 144 Theorem5.4 Inthenormalregressionmodel, Î² (cid:98) |X â¼N (cid:179) Î²,Ï2(cid:161) X (cid:48) X (cid:162)â1 (cid:180) . Theorems5.2and5.4implythatanyaffinefunctionoftheOLSestimatorisalsonormallydistributed includingindividualcomponents.LettingÎ² j andÎ² (cid:98)j denotethe jth elementsofÎ²andÎ² (cid:98),wehave (cid:181) (cid:182) Î² (cid:98)j |X â¼N Î² j ,Ï2 (cid:104) (cid:161) X (cid:48) X (cid:162)â1 (cid:105) . (5.7) jj Theorem5.4isastatementabouttheconditionaldistribution.Whatabouttheunconditionaldistri- bution?InSection4.7wepresentedKinalâstheoremabouttheexistenceofmomentsforthejointnormal regressionmodel.Were-statetheresulthere. Theorem5.5 Kinal(1980)If(Y,X)arejointlynormal,thenforanyr,(cid:69)(cid:176) (cid:176) Î² (cid:98) (cid:176) (cid:176) r < âifandonlyifr <nâk+1. 5.7 DistributionofOLSResidualVector ConsidertheOLSresidualvector. Recallfrom(3.24)thate=Me whereM =I âX (cid:161) X (cid:48) X (cid:162)â1 X (cid:48) . This (cid:98) n showsthate islinearine.SoconditionalonX (cid:98) e=Me|X â¼N (cid:161) 0,Ï2MM (cid:162)=N (cid:161) 0,Ï2M (cid:162) (cid:98) thefinalequalitysince M isidempotent(seeSection3.12). Thisshowsthattheresidualvectorhasan exactnormaldistribution. Furthermore,itisusefultofindthejointdistributionofÎ² (cid:98)and (cid:98) e. Thisiseasiestdonebywritingthe twoasastackedlinearfunctionoftheerrore.Indeed, (cid:181) Î² (cid:98) âÎ² (cid:182) (cid:195) (cid:161) X (cid:48) X (cid:162)â1 X (cid:48) e (cid:33) (cid:195) (cid:161) X (cid:48) X (cid:162)â1 X (cid:48) (cid:33) = = e e Me M (cid:98) whichisalinearfunctionofe.Thevectorthushasajointnormaldistributionwithcovariancematrix (cid:195) Ï2(cid:161) X (cid:48) X (cid:162)â1 0 (cid:33) . 0 Ï2M Theoff-diagonalblockiszerobecauseX (cid:48) M =0from(3.21). SincethisiszeroitfollowsthatÎ² (cid:98)and (cid:98) e are statisticallyindependent(Theorem5.3.2). Theorem5.6 Inthenormalregressionmodel,e |X â¼N (cid:161) 0,Ï2M (cid:162) andisinde- (cid:98) pendentofÎ² (cid:98). ThefactthatÎ² (cid:98) and (cid:98) e areindependentimpliesthatÎ² (cid:98) isindependentofanyfunctionoftheresidual vectorincludingindividualresidualse andthevarianceestimatorss2andÏ2. (cid:98)i (cid:98)",
    "page": 164,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER5. NORMALREGRESSION 145 5.8 DistributionofVarianceEstimator Next,considerthevarianceestimators2 from(4.26). Using(3.28)itsatisfies(nâk)s2=e (cid:48) e=e (cid:48) Me. (cid:98)(cid:98) ThespectraldecompositionofM (equation(A.4))isM=HÎH (cid:48) whereH (cid:48) H =I andÎisdiagonalwith n theeigenvaluesof M onthediagonal. Since M isidempotentwithranknâk (seeSection3.12)ithas nâk eigenvaluesequalling1andk eigenvaluesequalling0,so (cid:183) (cid:184) Î= I nâk 0 . 0 0 k Letu=H (cid:48) eâ¼N (cid:161) 0,I n Ï2(cid:162) (seeExercise5.2)andpartitionu=(cid:161) u (cid:48) 1 ,u (cid:48) 2 (cid:162)(cid:48) whereu 1 â¼N (cid:161) 0,I nâk Ï2(cid:162) .Then (nâk)s2=e (cid:48) Me (cid:183) (cid:184) =e (cid:48) H I nâk 0 H (cid:48) e 0 0 (cid:183) (cid:184) =u (cid:48) I nâk 0 u 0 0 =u (cid:48) u 1 1 â¼Ï2Ï2 . nâk Weseethatinthenormalregressionmodeltheexactdistributionofs2isascaledchi-square. Since (cid:98) e isindependentofÎ² (cid:98)itfollowsthats2isindependentofÎ² (cid:98)aswell. Theorem5.7 Inthenormalregressionmodel, (nâk)s2 â¼Ï2 Ï2 nâk andisindependentofÎ² (cid:98). 5.9 t-statistic Analternativewayofwriting(5.7)is Î² âÎ² (cid:98)j j â¼N(0,1). (cid:114) Ï2 (cid:104) (cid:161) X (cid:48) X (cid:162)â1 (cid:105) jj Thisissometimescalledastandardizedstatisticasthedistributionisthestandardnormal. NowtakethestandardizedstatisticandreplacetheunknownvarianceÏ2 withitsestimators2. We callthisat-ratioort-statistic Î² âÎ² Î² âÎ² T = (cid:98)j j = (cid:98)j j (cid:114) s2 (cid:104) (cid:161) X (cid:48) X (cid:162)â1 (cid:105) s(Î² (cid:98)j ) jj",
    "page": 165,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER5. NORMALREGRESSION 146 wheres(Î² (cid:98)j )istheclassical(homoskedastic)standarderrorforÎ² (cid:98)j from(4.37). Wewillsometimeswrite thet-statisticasT(Î² )toexplicitlyindicateitsdependenceontheparametervalueÎ² ,andsometimes j j willsimplifynotationandwritethet-statisticasT whenthedependenceisclearfromthecontext. Withalgebraicre-scalingwecanwritethet-statisticastheratioofthestandardizedstatisticandthe squarerootofthescaledvarianceestimator.Sincethedistributionsofthesetwocomponentsarenormal andchi-square,respectively,andindependent,wededucethatthet-statistichasthedistribution (cid:44)(cid:115) T = Î² (cid:98)j âÎ² j (nâk)s2(cid:193) (nâk) (cid:114) Ï2 (cid:104) (cid:161) X (cid:48) X (cid:162)â1 (cid:105) Ï2 jj N(0,1) â¼ (cid:113) Ï2 (cid:177) (nâk) nâk â¼t nâk astudentt distributionwithnâk degreesoffreedom. Thisderivationshowsthatthet-ratiohasasamplingdistributionwhichdependsonlyonthequantity nâk.Thedistributiondoesnotdependonanyotherfeaturesofthedata.Inthiscontext,wesaythatthe distributionofthet-ratioispivotal,meaningthatitdoesnotdependonunknowns. Thetrickbehindthisresultisscalingthecenteredcoefficientbyitsstandarderror,andrecognizing thateachdependsontheunknownÏonlythroughscale. Thustheratioofthetwodoesnotdependon Ï.Thistrick(scalingtoeliminatedependenceonunknowns)isknownasstudentization. Theorem5.8 Inthenormalregressionmodel,T â¼t nâk . AnimportantcaveataboutTheorem5.8isthatitonlyappliestothet-statisticconstructedwiththe homoskedastic (old-fashioned) standard error. It does not apply to a t-statistic constructed with any oftherobuststandarderrors. Infact,therobustt-statisticscanhavefinitesampledistributionswhich deviateconsiderablyfromt nâk evenwhentheregressionerrorsareindependentN(0,Ï2). Thusthedis- tributional result in Theorem 5.8 and the use of the t distribution in finite samples is only exact when appliedtoclassicalt-statisticsunderthenormalityassumption. 5.10 ConfidenceIntervalsforRegressionCoefficients TheOLSestimatorÎ² (cid:98)isapointestimatorforacoefficientÎ². Abroaderconceptisasetorinterval estimator which takes the formC(cid:98) =[L(cid:98),U(cid:98)]. The goal of an interval estimatorC(cid:98) is to contain the true value,e.g.Î²âC(cid:98),withhighprobability. TheintervalestimatorC(cid:98)isafunctionofthedataandhenceisrandom. AnintervalestimatorC(cid:98)iscalleda1âÎ±confidenceintervalwhen(cid:80)(cid:163)Î²âC(cid:98) (cid:164)=1âÎ±foraselectedvalue ofÎ±.Thevalue1âÎ±iscalledthecoverageprobability.Typicalchoicesforthecoverageprobability1âÎ± are0.95or0.90. Theprobabilitycalculation(cid:80)(cid:163)Î²âC(cid:98) (cid:164) iseasilymis-interpretedastreatingÎ²asrandomandC(cid:98)asfixed. (The probability that Î² is inC(cid:98).) This is not the appropriate interpretation. Instead, the correct inter- pretation is that the probability (cid:80)(cid:163)Î²âC(cid:98) (cid:164) treats the point Î² as fixed and the setC(cid:98)as random. It is the probabilitythattherandomsetC(cid:98)covers(orcontains)thefixedtruecoefficientÎ².",
    "page": 166,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER5. NORMALREGRESSION 147 Thereisnotauniquemethodtoconstructconfidenceintervals. Forexample,onesimple(yetsilly) intervalis (cid:189) (cid:82) withprobability1âÎ± C(cid:98) = (cid:169)Î² (cid:98) (cid:170) withprobabilityÎ±. IfÎ² (cid:98)hasacontinuousdistribution,thenbyconstruction(cid:80)(cid:163)Î²âC(cid:98) (cid:164)=1âÎ±,sothisconfidenceintervalhas perfectcoverage.However,C(cid:98)isuninformativeaboutÎ² (cid:98)andisthereforenotuseful. Instead,agoodchoiceforaconfidenceintervalfortheregressioncoefficientÎ²isobtainedbyadding andsubtractingfromtheestimatorÎ² (cid:98)afixedmultipleofitsstandarderror: C(cid:98) =(cid:163)Î² (cid:98) âcÃs(Î² (cid:98)), Î² (cid:98) +cÃs(Î² (cid:98)) (cid:164) (5.8) wherec>0isapre-specifiedconstant. Thisconfidenceintervalissymmetricaboutthepointestimator Î² (cid:98)anditslengthisproportionaltothestandarderrors(Î² (cid:98)). Equivalently,C(cid:98)isthesetofparametervaluesforÎ²suchthatthet-statisticT(Î²)issmaller(inabsolute value)thanc,thatis (cid:40) (cid:41) Î²âÎ² C(cid:98) =(cid:169)Î²: (cid:175) (cid:175)T(Î²) (cid:175) (cid:175) â¤c (cid:170)= Î²:âcâ¤ (cid:98) â¤c . s(Î² (cid:98)) Thecoverageprobabilityofthisconfidenceintervalis (cid:80)(cid:163)Î²âC(cid:98) (cid:164)=(cid:80)(cid:163)(cid:175) (cid:175)T(Î²) (cid:175) (cid:175) â¤c (cid:164) =(cid:80)(cid:163)âcâ¤T(Î²)â¤c (cid:164) . (5.9) Sincethet-statisticT(Î²)hasthet nâk distribution(5.9)equalsF(c)âF(âc),whereF(u)isthestudentt distributionfunctionwithnâkdegreesoffreedom.SinceF(âc)=1âF(c)(seeExercise5.8)wecanwrite (5.9)as (cid:80)(cid:163)Î²âC(cid:98) (cid:164)=2F(c)â1. ThisisthecoverageprobabilityoftheintervalC(cid:98),andonlydependsontheconstantc. Aswementionedbefore,aconfidenceintervalhasthecoverageprobability1âÎ±. Thisrequiresse- lectingtheconstantc sothatF(c)=1âÎ±/2. Thisholdsifc equalsthe1âÎ±/2quantileofthet nâk distri- bution. Asthereisnoclosedformexpressionforthesequantileswecomputetheirvaluesnumerically. Forexample, bytinv(1-alpha/2,n-k)inMATLAB.Withthischoicetheconfidenceinterval(5.8)has exactcoverageprobability1âÎ±.Bydefault,Statareports95%confidenceintervalsC(cid:98)foreachestimated regressioncoefficientusingthesameformula. Theorem5.9 Inthenormalregressionmodel,(5.8)withc =F â1(1âÎ±/2)has coverageprobability(cid:80)(cid:163)Î²âC(cid:98) (cid:164)=1âÎ±. Whenthedegreeoffreedomislargethedistinctionbetweenthestudentt andthenormaldistribu- tionisnegligible. Inparticular, fornâk â¥61wehavec â¤2.00fora95%interval. Usingthisvaluewe obtainthemostcommonlyusedconfidenceintervalinappliedeconometricpractice: C(cid:98) =(cid:163)Î² (cid:98) â2s(Î² (cid:98)), Î² (cid:98) +2s(Î² (cid:98)) (cid:164) . (5.10) Thisisausefulrule-of-thumb. This95%confidenceintervalC(cid:98)issimpletocomputeandcanbeeasily calculatedfromcoefficientestimatesandstandarderrors.",
    "page": 167,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER5. NORMALREGRESSION 148 Theorem5.10 In the normal regression model, if nâk â¥ 61 then (5.10) has coverageprobability(cid:80)(cid:163)Î²âC(cid:98) (cid:164)â¥0.95. Confidenceintervalsareasimpleyeteffectivetooltoassessestimationuncertainty. Whenreading asetofempiricalresultslookattheestimatedcoefficientestimatesandthestandarderrors. Forapa- rameter of interest compute the confidence intervalC(cid:98) and consider the meaning of the spread of the suggestedvalues. IftherangeofvaluesintheconfidenceintervalaretoowidetolearnaboutÎ²thendo notjumptoaconclusionaboutÎ²basedonthepointestimatealone. 5.11 ConfidenceIntervalsforErrorVariance WecanalsoconstructaconfidenceintervalfortheregressionerrorvarianceÏ2 usingthesampling distributionofs2fromTheorem5.7.Thisstatesthatinthenormalregressionmodel (nâk)s2 â¼Ï2 . (5.11) Ï2 nâk LetF(u)denotetheÏ2 distributionfunctionandforsomeÎ±setc =F â1(Î±/2)andc =F â1(1âÎ±/2) nâk 1 2 (theÎ±/2and1âÎ±/2quantilesoftheÏ2 distribution).Equation(5.11)impliesthat nâk (cid:183) (nâk)s2 (cid:184) (cid:80) c â¤ â¤c =F(c )âF(c )=1âÎ±. 1 Ï2 2 2 1 Rewritingtheinequalitieswefind (cid:183) (nâk)s2 (nâk)s2(cid:184) (cid:80) â¤Ï2â¤ =1âÎ±. c c 2 1 Thisshowsthatanexact1âÎ±confidenceintervalforÏ2is (cid:183) (nâk)s2 (nâk)s2(cid:184) C(cid:98) = , . (5.12) c c 2 1 Theorem5.11 Inthenormalregressionmodel(5.12)hascoverageprobability (cid:80)(cid:163)Ï2âC(cid:98) (cid:164)=1âÎ±. The confidence interval (5.12) for Ï2 is asymmetric about the point estimate s2 due to the latterâs asymmetricsamplingdistribution.",
    "page": 168,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER5. NORMALREGRESSION 149 5.12 tTest AtypicalgoalinaneconometricexerciseistoassesswhetherornotacoefficientÎ²equalsaspecific valueÎ² . OftenthespecificvaluetobetestedisÎ² =0butthisisnotessential.Thisiscalledhypothesis 0 0 testing,asubjectwhichwillbeexploredindetailinChapter9. Inthissectionandthefollowingwegive ashortintroductionspecifictothenormalregressionmodel. ForsimplicitywritethecoefficienttobetestedasÎ².Thenullhypothesisis (cid:72) :Î²=Î² . (5.13) 0 0 ThisstatesthatthehypothesisisthatthetruevalueofÎ²equalsthehypothesizedvalueÎ² . 0 Thealternativehypothesisisthecomplementof(cid:72) ,andiswrittenas 0 (cid:72) :Î²(cid:54)=Î² . 1 0 ThisstatesthatthetruevalueofÎ²doesnotequalthehypothesizedvalue. Weareinterestedintesting(cid:72) against(cid:72) . Themethodistodesignastatisticwhichisinformative 0 1 about(cid:72) .Iftheobservedvalueofthestatisticisconsistentwithrandomvariationundertheassumption 1 that(cid:72) istrue,thenwededucethatthereisnoevidenceagainst(cid:72) andconsequentlydonotreject(cid:72) . 0 0 0 However, if the statistic takes a value which is unlikely to occur under the assumption that (cid:72) is true, 0 thenwededucethatthereisevidenceagainst(cid:72) andconsequentlywereject(cid:72) infavorof(cid:72) .Themain 0 0 1 stepsaretodesignateststatisticandtocharacterizeitssamplingdistribution. Thestandardstatistictotest(cid:72) against(cid:72) istheabsolutevalueofthet-statistic 0 1 (cid:175) (cid:175) |T|= (cid:175) (cid:175) Î² (cid:98) âÎ² 0 (cid:175) (cid:175). (5.14) (cid:175) (cid:175) s(Î² (cid:98)) (cid:175) (cid:175) If(cid:72) istruethenweexpect|T|tobesmall,butif(cid:72) istruethenwewouldexpect|T|tobelarge. Hence 0 1 thestandardruleistoreject(cid:72) infavorof(cid:72) forlargevaluesofthet-statistic|T|andotherwisefailto 0 1 reject(cid:72) .Thusthehypothesistesttakestheform 0 Reject(cid:72) if|T|>c. 0 The constant c which appears in the statement of the test is called the critical value. Its value is selected to control the probability of false rejections. When the null hypothesis is true T has an exact t nâk distribution in the normal regression model. Thus for a given value of c the probability of false rejectionis (cid:80)(cid:163) Reject(cid:72) |(cid:72) (cid:164)=(cid:80)[|T|>c|(cid:72) ] 0 0 0 =(cid:80)[T >c|(cid:72) ]+(cid:80)[T <âc|(cid:72) ] 0 0 =1âF(c)+F(âc) =2(1âF(c)) whereF(u)isthet nâk distributionfunction. Thisistheprobabilityoffalserejectionandisdecreasing inthecriticalvaluec.Weselectthevaluec sothatthisprobabilityequalsapre-selectedvaluecalledthe significancelevelwhichistypicallywrittenasÎ±. ItisconventionaltosetÎ±=0.05,thoughthisisnota hard rule. We then select c so that F(c)=1âÎ±/2, which means that c is the 1âÎ±/2 quantile (inverse CDF)ofthe t nâk distribution,thesameasusedforconfidenceintervals. Withthischoicethedecision ruleâReject(cid:72) if|T|>câhasasignificancelevel(falserejectionprobability)ofÎ±. 0",
    "page": 169,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER5. NORMALREGRESSION 150 Theorem5.12 Inthenormalregressionmodelifthenullhypothesis(5.13)is true, then for |T| defined in (5.14) T â¼t nâk . If c is set so that (cid:80)[|t nâk |â¥c]= Î±thenthetestâReject(cid:72) infavorof(cid:72) if|T|>câhassignificancelevelÎ±. 0 1 Toreporttheresultofahypothesistestweneedtopre-determinethesignificancelevelÎ±inorderto calculatethecriticalvaluec. Thiscanbeinconvenientandarbitrary. Asimplificationistoreportwhat isknownasthep-valueofthetest. Ingeneral,whenatesttakestheformâReject(cid:72) ifS>câandS has 0 null distributionG(u) then the p-value of the test is p =1âG(S). A test with significance level Î± can berestatedasâReject(cid:72) ifp<Î±â. Itissufficienttoreportthep-valuep andwecaninterpretthevalue 0 of p asindexingthetestâsstrengthofrejectionofthenullhypothesis. Thusap-valueof0.07mightbe interpretedasânearlysignificantâ, 0.05asâborderlinesignificantâ, and0.001asâhighlysignificantâ. In thecontextofthenormalregressionmodelthep-valueofat-statistic|T|is p =2(1âF nâk (|T|))where F nâk isthe t nâk CDF.Forexample, inMATLABthecalculationis2*(1-tcdf(abs(t),n-k)). InStata, the default is that for any estimated regression, t-statistics for each estimated coefficient are reported alongwiththeirp-valuescalculatedusingthissameformula. Theset-statisticstestthehypothesesthat eachcoefficientiszero. Ap-valuereportsthestrengthofevidenceagainst(cid:72) butisnotitselfaprobability.Acommonmis- 0 understandingisthatthep-valueistheâprobabilitythatthenullhypothesisistrueâ.Thisisanincorrect interpretation.Itisastatistic,israndom,andisameasureoftheevidenceagainst(cid:72) .Nothingmore. 0 5.13 LikelihoodRatioTest In the previous section we described the t-test as the standard method to test a hypothesis on a singlecoefficientinaregression. Inmanycontexts,however,wewanttosimultaneouslyassessasetof coefficients. Inthenormalregressionmodel,thiscanbedonebyanF testwhichcanbederivedfrom thelikelihoodratiotest. Partitiontheregressorsas X =(X (cid:48) ,X (cid:48) )andsimilarlypartitionthecoefficientvectorasÎ²=(Î²(cid:48) ,Î²(cid:48) ) (cid:48) . 1 2 1 2 Theregressionmodelcanbewrittenas Y =X (cid:48)Î² +X (cid:48)Î² +e. (5.15) 1 1 2 2 Let k =dim(X), k =dim(X ), and q =dim(X ), so that k =k +q. Partition the variables so that the 1 1 2 1 hypothesisisthatthesecondsetofcoefficientsarezero,or (cid:72) :Î² =0. (5.16) 0 2 If(cid:72) istruethentheregressorsX canbeomittedfromtheregression.Inthiscasewecanwrite(5.15)as 0 2 Y =X (cid:48)Î² +e. (5.17) 1 1 Wecall(5.17)thenullmodel. ThealternativehypothesisisthatatleastoneelementofÎ² isnon-zero 2 andiswrittenas(cid:72) :Î² (cid:54)=0. 1 2 When models are estimated by maximum likelihood a well-accepted testing procedure is to reject (cid:72) infavorof(cid:72) forlargevaluesoftheLikelihoodRatioâtheratioofthemaximizedlikelihoodfunction 0 1 under (cid:72) and (cid:72) , respectively. We now construct this statistic inthe normal regression model",
    "page": 170,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". (5.17) 1 1 Wecall(5.17)thenullmodel. ThealternativehypothesisisthatatleastoneelementofÎ² isnon-zero 2 andiswrittenas(cid:72) :Î² (cid:54)=0. 1 2 When models are estimated by maximum likelihood a well-accepted testing procedure is to reject (cid:72) infavorof(cid:72) forlargevaluesoftheLikelihoodRatioâtheratioofthemaximizedlikelihoodfunction 0 1 under (cid:72) and (cid:72) , respectively. We now construct this statistic inthe normal regression model. Recall 1 0 from(5.6)thatthemaximizedlog-likelihoodequals (cid:96) n (cid:161)Î² (cid:98),Ï (cid:98) 2(cid:162)=â n log (cid:161) 2ÏÏ (cid:98) 2(cid:162)â n . 2 2",
    "page": 170,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER5. NORMALREGRESSION 151 Wesimilarlycalculatethemaximizedlog-likelihoodfortheconstrainedmodel(5.17).Bythesamesteps forderivationoftheunconstrainedMLEwefindthattheMLEfor(5.17)isOLSofY onX . Wecanwrite 1 thisestimatoras Î² (cid:101)1 =(cid:161) X (cid:48) 1 X 1 (cid:162)â1 X (cid:48) 1 Y withresiduale (cid:101)i =Y i âX 1 (cid:48) i Î² (cid:101)1 anderrorvarianceestimateÏ (cid:101) 2 = n 1(cid:80)n i=1 e (cid:101)i 2. Weusetildesâ~âratherthan hatsâ^âabovetheconstrainedestimatestodistinguishthemfromtheunconstrainedestimates.Youcan calculatesimilarto(5.6)thatthemaximizedconstrainedlog-likelihoodis (cid:96) n (cid:161)Î² (cid:101)1 ,Ï (cid:101) 2(cid:162)=â n log (cid:161) 2ÏÏ (cid:101) 2(cid:162)â n . 2 2 Aclassictestingprocedureistoreject(cid:72) forlargevaluesoftheratioofthemaximizedlikelihoods. 0 Equivalently the test rejects (cid:72) for large values of twice the difference in the log-likelihood functions. 0 (Multiplyingthelikelihooddifferencebytwoturnsouttobeausefulscaling.)Thisequals LR=2 (cid:161)(cid:96) n (cid:161)Î² (cid:98),Ï (cid:98) 2(cid:162)â(cid:96) n (cid:161)Î² (cid:101)1 ,Ï (cid:101) 2(cid:162)(cid:162) =2 (cid:179)(cid:179) â n log (cid:161) 2ÏÏ2(cid:162)â n(cid:180) â (cid:179) â n log (cid:161) 2ÏÏ2(cid:162)â n(cid:180)(cid:180) (cid:98) (cid:101) 2 2 2 2 (cid:181)Ï2(cid:182) =nlog (cid:101) . (5.18) Ï2 (cid:98) The likelihood ratio test rejects (cid:72) for large values of LR, or equivalently (see Exercise 5.10) for large 0 valuesof (cid:161)Ï2âÏ2(cid:162) /q F= (cid:101) (cid:98) . (5.19) Ï2/(nâk) (cid:98) ThisisknownastheF statisticforthetestofhypothesis(cid:72) against(cid:72) . 0 1 To develop an appropriate critical value we need the null distribution of F. Recall from (3.28) that nÏ2 =e (cid:48) Me where M = I âP with P = X (cid:161) X (cid:48) X (cid:162)â1 X (cid:48) . Similarly, under (cid:72) , nÏ2 =e (cid:48) M e where M = (cid:98) n 0 (cid:101) 1 I âP withP =X (cid:161) X (cid:48) X (cid:162)â1 X (cid:48) . YoucancalculatethatM âM =PâP isidempotentwithrankq. n 1 1 1 1 1 1 1 1 Furthermore,(M âM)M=0.Itfollowsthate (cid:48) (M âM)eâ¼Ï2 andisindependentofe (cid:48) Me.Hence 1 1 q F= e e (cid:48) ( (cid:48) M M 1 e â /(n M â )e k / ) q â¼ Ï2 Ï / 2 q ( / n q âk) â¼F q,nâk nâk anexactF distributionwithdegreesoffreedomq andnâk,respectively. Thusunder(cid:72) ,theF statistic 0 hasanexactF distribution. ThecriticalvaluesareselectedfromtheuppertailoftheF distribution.Foragivensignificancelevel Î±(typicallyÎ±=0.05)weselectthecriticalvaluec sothat(cid:80)(cid:163) F q,nâk â¥c (cid:164)=Î±. Forexample, inMATLAB the expression is finv(1-Î±,q,n-k). The test rejects (cid:72) in favor of (cid:72) if F >c and does not reject (cid:72) 0 1 0 otherwise.Thep-valueofthetestisp=1âG q,nâk (F)whereG q,nâk (u)istheF q,nâk distributionfunction. InMATLAB,thep-valueiscomputedas1-fcdf(f,q,n-k).Itisequivalenttoreject(cid:72) ifF >c orp<Î±",
    "page": 171,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Forexample, inMATLAB the expression is finv(1-Î±,q,n-k). The test rejects (cid:72) in favor of (cid:72) if F >c and does not reject (cid:72) 0 1 0 otherwise.Thep-valueofthetestisp=1âG q,nâk (F)whereG q,nâk (u)istheF q,nâk distributionfunction. InMATLAB,thep-valueiscomputedas1-fcdf(f,q,n-k).Itisequivalenttoreject(cid:72) ifF >c orp<Î±. 0 In Stata, the command to test multiple coefficients takes the form âtest X1 X2â where X1 and X2 are the names of the variables whose coefficients are tested. Stata then reports the F statistic for the hypothesisthatthecoefficientsarejointlyzeroalongwiththep-valuecalculatedusingtheF distribution. Theorem5.13 Inthenormalregressionmodelifthenullhypothesis(5.16)is truethenforF definedin(5.19)F â¼F q,nâk . Ifc issetsothat(cid:80)(cid:163) F q,nâk â¥c (cid:164)=Î± thenthetestâReject(cid:72) infavorof(cid:72) ifF >câhassignificancelevelÎ±. 0 1",
    "page": 171,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER5. NORMALREGRESSION 152 Theorem5.13justifiestheF testinthenormalregressionmodelwithcriticalvaluesfromtheF dis- tribution. 5.14 InformationBoundforNormalRegression ThissectionrequiresafamiliaritywiththetheoryoftheCramÃ©r-RaoLowerBound. SeeChapter10 ofIntroductiontoEconometrics. Thelikelihoodscoresforthenormalregressionmodelare â (cid:96) (Î²,Ï2)= 1 (cid:88) n X (cid:161) Y âX (cid:48)Î²(cid:162)= 1 (cid:88) n X e âÎ² n Ï2 i i i Ï2 i i i=1 i=1 and â (cid:96) (Î²,Ï2)=â n + 1 (cid:88) n (cid:161) Y âX (cid:48)Î²(cid:162)2= 1 (cid:88) n (cid:161) e2âÏ2(cid:162) . âÏ2 n 2Ï2 2Ï4 i i 2Ï4 i i=1 i=1 Itfollowsthattheinformationmatrixis (cid:34) â (cid:96)(Î²,Ï2) (cid:175) (cid:175) (cid:35) ï£« Ï 1 2 X (cid:48) X 0 ï£¶ I =var â â Ï â Î² 2 (cid:96)(Î²,Ï2) (cid:175) (cid:175) (cid:175) X = ï£­ 0 2 n Ï4 ï£¸ (5.20) (seeExercise5.11).TheCramÃ©r-RaoLowerBoundis ï£« Ï2(cid:161) X (cid:48) X (cid:162)â1 0 ï£¶ Iâ1= ï£­ 2Ï4 ï£¸. 0 n ThisshowsthatthelowerboundforestimationofÎ²isÏ2(cid:161) X (cid:48) X (cid:162)â1 andthelowerboundforÏ2is2Ï4/n. SinceinthehomoskedasticlinearregressionmodeltheOLSestimatorisunbiasedandhasvariance Ï2(cid:161) X (cid:48) X (cid:162)â1 itfollowsthattheOLScoefficientestimatorÎ² (cid:98)isCramÃ©r-Raoefficientinthenormalregres- sion model. CramÃ©r-Rao efficiency means that no unbiased estimator has a lower covariance matrix. ThisexpandsontheGauss-Markovtheoremwhichstatedthatnolinearunbiasedestimatorhasalower variancematrixinthehomoskedasticregressionmodel.Noticethatthattheresultsarecomplementary. Gauss-Markovefficiencyconcernsamorenarrowclassofestimators(linear)butallowsabroadermodel class (linear homoskedastic rather than normal regression). The CramÃ©r-Rao efficiency result is more powerfulinthatitdoesnotrestricttheclassofestimators(beyondunbiasedness)butismorerestrictive intheclassofmodelsallowed(normalregression). However, theresultisnotaspowerfulastheMod- ernGauss-MarkovTheorem(Theorem4.7)asthelatterdoesnotrequiretheobservationstobenormally distributed. Theunbiasedvarianceestimators2ofÏ2hasvariance2Ï4/(nâk)(seeExercise5.12)whichislarger than the CramÃ©r-Rao lower bound 2Ï4/n. Thus in contrast to the coefficient estimator, the variance estimatorisnotCramÃ©r-Raoefficient. _____________________________________________________________________________________________ 5.15 Exercises Exercise5.1 ShowthatifQâ¼Ï2,then(cid:69)[Q]=r andvar[Q]=2r. r Hint:UsetherepresentationQ=(cid:80)n Z2withZ independentN(0,1). i=1 i i",
    "page": 172,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER5. NORMALREGRESSION 153 Exercise5.2 Showthatifeâ¼N (cid:161) 0,I Ï2(cid:162) andH (cid:48) H=I thenu=H (cid:48) eâ¼N (cid:161) 0,I Ï2(cid:162) . n n n Exercise5.3 Showthatifeâ¼N(0,Î£)andÎ£=AA (cid:48) thenu=A â1eâ¼N(0,I ). n Exercise5.4 ShowthatargmaxÎ¸âÎ (cid:96) n (Î¸)=argmaxÎ¸âÎL n (Î¸). Exercise5.5 For the regression in-sample predicted values Y(cid:98)i show that Y(cid:98)i | X â¼N (cid:161) X i (cid:48)Î²,Ï2h ii (cid:162) where h aretheleveragevalues(3.40). ii Exercise5.6 In the normal regression model show that the leave-one out prediction errors e and the (cid:101)i standardizedresidualse i areindependentofÎ² (cid:98),conditionalonX. Hint:Use(3.45)and(4.24). HC0 HC1 Exercise5.7 In the normal regression model show that the robust covariance matrices V(cid:98)Î²(cid:98) , V(cid:98)Î²(cid:98) , V(cid:98) H Î²(cid:98) C2 ,andV(cid:98) H Î²(cid:98) C3 areindependentoftheOLSestimatorÎ² (cid:98),conditionalonX. Exercise5.8 LetF(u)bethedistributionfunctionofarandomvariable X whosedensityissymmetric aboutzero.(Thisincludesthestandardnormalandthestudentt.)ShowthatF(âu)=1âF(u). Exercise5.9 LetC(cid:98)Î² =[L,U]bea1âÎ±confidenceintervalforÎ²,andconsiderthetransformationÎ¸=g(Î²) where g(Â·)ismonotonicallyincreasing. ConsidertheconfidenceintervalC(cid:98)Î¸ =[g(L),g(U)]forÎ¸. Show that(cid:80)(cid:163)Î¸âC(cid:98)Î¸ (cid:164)=(cid:80)(cid:163)Î²âC(cid:98)Î² (cid:164) .UsethisresulttodevelopaconfidenceintervalforÏ. Exercise5.10 ShowthatthetestâReject(cid:72) ifLRâ¥c âforLRdefinedin(5.18),andthetestâReject(cid:72) if 0 1 0 Fâ¥c âforFdefinedin(5.19),yieldthesamedecisionsifc =(cid:161) exp(c /n)â1 (cid:162) (nâk)/q. Doesthismean 2 2 1 thatthetwotestsareequivalent? Exercise5.11 Show(5.20). Exercise5.12 Inthenormalregressionmodellets2 betheunbiasedestimatoroftheerrorvarianceÏ2 from(4.26). (a) Showthatvar (cid:163) s2(cid:164)=2Ï4/(nâk). (b) Showthatvar (cid:163) s2(cid:164) isstrictlylargerthantheCramÃ©r-RaoLowerBoundforÏ2.",
    "page": 173,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Part II Large Sample Methods 154",
    "page": 174,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Chapter 6 A Review of Large Sample Asymptotics 6.1 Introduction The most widely-used tool in sampling theory is large sample asymptotics. By âasymptoticsâ we meanapproximatingafinite-samplesamplingdistributionbytakingitslimitasthesamplesizediverges toinfinity. Inthischapterweprovideabriefreviewofthemainresultsoflargesampleasymptotics. It ismeantasareference,notasateachingguide. AsymptotictheoryiscoveredindetailinChapters7-9 ofIntroductiontoEconometrics.Ifyouhavenotpreviousstudiedasymptotictheoryindetailyoushould studythesechaptersbeforeproceeding. 6.2 ModesofConvergence Definition6.1 Asequenceofrandomvectors Z â(cid:82)k convergesinprobabil- n itytoZ asnââ,denotedZ n ââZ oralternativelyplim nââZ n =Z,ifforall p Î´>0, lim (cid:80)[(cid:107)Z âZ(cid:107)â¤Î´]=1. (6.1) nââ n WecallZ theprobabilitylimit(orplim)ofZ . n Theabovedefinitiontreatsrandomvariablesandrandomvectorssimultaneouslyusingthevector norm. Itisusefultoknowthatforarandomvector,(6.1)holdsifandonlyifeachelementinthevector convergesinprobabilitytoitslimit. Definition6.2 Let Z be a sequence of random vectors with distributions n F (u)=(cid:80)[Z â¤u]. We say that Z convergesindistributionto Z as n ââ, n n n denotedZ ââZ,ifforallu atwhichF(u)=(cid:80)[Z â¤u]iscontinuous,F (u)â n n d F(u)asnââ. Wereferto Z anditsdistributionF(u)astheasymptoticdis- tribution,largesampledistribution,orlimitdistributionofZ . n 155",
    "page": 175,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER6. AREVIEWOFLARGESAMPLEASYMPTOTICS 156 6.3 WeakLawofLargeNumbers Theorem6.1 WeakLawofLargeNumbers(WLLN) IfY â(cid:82)k arei.i.d.and(cid:69)(cid:107)Y(cid:107)<â,thenasnââ, i 1 (cid:88) n Y = Y ââ(cid:69)[Y]. i n i=1 p TheWLLNshowsthatthesamplemeanY convergesinprobabilitytothetruepopulationexpecta- tionÂµ.Theresultappliestoanytransformationofarandomvectorwithafinitemean. Theorem6.2 IfY â(cid:82)k arei.i.d., h(y):(cid:82)k â(cid:82)q, and(cid:69)(cid:107)h(Y)(cid:107)<â,thenÂµ= i (cid:98) 1(cid:80)n h(Y )ââÂµ=(cid:69)[h(Y)]asnââ. n i=1 i p Anestimatorwhichconvergesinprobabilitytothepopulationvalueiscalledconsistent. Definition6.3 AnestimatorÎ¸ (cid:98)ofÎ¸isconsistentifÎ¸ (cid:98) ââÎ¸asnââ. p 6.4 CentralLimitTheorem Theorem6.3 Multivariate Lindeberg-LÃ©vy Central Limit Theorem (CLT). If Y â(cid:82)k arei.i.d.and(cid:69)(cid:107)Y(cid:107)2<â,thenasnââ i (cid:112) (cid:179) (cid:180) n Y âÂµ ââN(0,V) d whereÂµ=(cid:69)[Y]andV =(cid:69) (cid:104) (cid:161) Y âÂµ(cid:162)(cid:161) Y âÂµ(cid:162)(cid:48)(cid:105) . Thecentrallimittheoremshowsthatthedistributionofthesamplemeanisapproximatelynormal inlargesamples.ForsomeapplicationsitmaybeusefultonoticethatTheorem6.3doesnotimposeany restrictionsonV otherthanthattheelementsarefinite.Thereforethisresultallowsforthepossibilityof singularV. Thefollowingtwogeneralizationsallowforheterogeneousrandomvariables.",
    "page": 176,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER6. AREVIEWOFLARGESAMPLEASYMPTOTICS 157 Theorem6.4 MultivariateLindebergCLT.Supposethatforalln,Y â(cid:82)k,i = ni 1,...,r ,areindependentbutnotnecessarilyidenticallydistributedwithexpec- n tations(cid:69)[Y ]=0andvariancematricesV =(cid:69)(cid:163) Y Y (cid:48) (cid:164) .SetV =(cid:80)n V . ni ni ni ni n i=1 ni SupposeÎ½2 =Î» (V )>0andforall(cid:178)>0 n min n lim 1 (cid:88) rn (cid:69)(cid:163)(cid:107)Y (cid:107)21(cid:169)(cid:107)Y (cid:107)2â¥(cid:178)Î½2(cid:170)(cid:164)=0. (6.2) nââÎ½2 n i=1 ni ni n Thenasnââ V â1/2(cid:88) rn Y ââN(0,I ). n ni k i=1 d Theorem6.5 Suppose Y â (cid:82)k are independent but not necessarily identi- ni cally distributed with expectations (cid:69)[Y ] = 0 and variance matrices V = ni ni (cid:69)(cid:163) Y Y (cid:48) (cid:164) .Suppose ni ni 1 (cid:88) n V âV >0 ni n i=1 andforsomeÎ´>0 sup(cid:69)(cid:107)Y (cid:107)2+Î´<â. (6.3) ni n,i Thenasnââ (cid:112) nY ââN(0,V). d 6.5 ContinuousMappingTheoremandDeltaMethod Continuousfunctionsarelimit-preserving.Therearetwoformsofthecontinuousmappingtheorem, forconvergenceinprobabilityandconvergenceindistribution. Theorem6.6 Continuous Mapping Theorem (CMT). Let Z â(cid:82)k and g(u): n (cid:82)k â(cid:82)q. IfZ ââc asnââandg(u)iscontinuousatc theng(Z )ââg(c) n n p p asnââ. Theorem6.7 ContinuousMappingTheorem. If Z ââ Z as n ââ and g : n d (cid:82)m â(cid:82)k hasthesetofdiscontinuitypointsD suchthat(cid:80)(cid:163) Z âD (cid:164)=0,then g g g(Z )ââg(Z)asnââ. n d",
    "page": 177,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER6. AREVIEWOFLARGESAMPLEASYMPTOTICS 158 Differentiablefunctionsofasymptoticallynormalrandomestimatorsareasymptoticallynormal. (cid:112) Theorem6.8 DeltaMethod.LetÂµâ(cid:82)k andg(u):(cid:82)kâ(cid:82)q.If n (cid:161)ÂµâÂµ(cid:162)ââÎ¾, (cid:98) d whereg(u)iscontinuouslydifferentiableinaneighborhoodofÂµ,thenasnâ â (cid:112) n (cid:161) g (cid:161)Âµ(cid:162)âg(Âµ) (cid:162)ââG (cid:48)Î¾ (6.4) (cid:98) d whereG(u)= â g(u) (cid:48) andG=G(Âµ).Inparticular,ifÎ¾â¼N(0,V)thenasnââ âu (cid:112) n (cid:161) g (cid:161)Âµ(cid:162)âg(Âµ) (cid:162)ââN (cid:161) 0,G (cid:48) VG (cid:162) . (6.5) (cid:98) d 6.6 SmoothFunctionModel ThesmoothfunctionmodelisÎ¸=g (cid:161)Âµ(cid:162) whereÂµ=(cid:69)[h(Y)]andg (cid:161)Âµ(cid:162) issmoothinasuitablesense. The parameter Î¸ = g (cid:161)Âµ(cid:162) is not a population moment so it does not have a direct moment esti- mator. Instead, it is common to use a plug-inestimator formed by replacing the unknown Âµ with its pointestimatorÂµandthenâpluggingâthisintotheexpressionforÎ¸. Thefirststepisthesamplemean (cid:98) Âµ (cid:98) =n â1(cid:80)n i=1 h(Y i ).ThesecondstepisthetransformationÎ¸ (cid:98) =g (cid:161)Î¸ (cid:98) (cid:162) .Thehatâ^âindicatesthatÎ¸ (cid:98)isasam- ple estimator of Î¸. The smoothfunction model includes a broad class of estimators including sample variancesandtheleastsquaresestimator. Theorem6.9 If Y â (cid:82)m are i.i.d., h(u) : (cid:82)m â (cid:82)k, (cid:69)(cid:107)h(Y)(cid:107) < â, and g(u) : i (cid:82)kâ(cid:82)q iscontinuousatÂµ,thenÎ¸ (cid:98) ââÎ¸asnââ. p Theorem6.10 IfY â(cid:82)m arei.i.d.,h(u):(cid:82)mâ(cid:82)k,(cid:69)(cid:107)h(Y)(cid:107)2<â,g(u):(cid:82)k â i â (cid:82)q,andG(u)= g(u) (cid:48) iscontinuousinaneighborhoodofÂµ,thenasnââ âu (cid:112) n (cid:161)Î¸ (cid:98) âÎ¸(cid:162)ââN(0,VÎ¸) d whereVÎ¸ =G (cid:48) VG,V =(cid:69) (cid:104) (cid:161) h(Y)âÂµ(cid:162)(cid:161) h(Y)âÂµ(cid:162)(cid:48)(cid:105) ,andG=G (cid:161)Âµ(cid:162) . Theorem6.9establishestheconsistencyofÎ¸ (cid:98)forÎ¸andTheorem6.10establishesitsasymptoticnor- mality. Itisinstructivetocomparetheconditions. Consistencyrequiresthath(Y)hasafiniteexpecta- tion; asymptotic normality requires that h(Y) has a finite variance. Consistency requires that g(u) be continuous;asymptoticnormalityrequiresthatg(u)iscontinuouslydifferentiable.",
    "page": 178,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER6. AREVIEWOFLARGESAMPLEASYMPTOTICS 159 6.7 BestUnbiasedEstimation Thissectionpresentsanefficiencyboundforestimationofthemean. Theresultisarefinite-sample ratherthanasymptotic, butisconvenienttointroduceatthispointsincetheboundisidenticaltothe asymptoticvariance. Theorem6.11 Suppose Y are i.i.d., Âµ = (cid:69)[h(Y)], and (cid:69)(cid:107)h(Y)(cid:107)2 < â. If Âµ is i (cid:101) unbiasedforÂµthenvar (cid:163)Âµ(cid:164)â¥n â1V whereV =(cid:69) (cid:104) (cid:161) h(Y)âÂµ(cid:162)(cid:161) h(Y)âÂµ(cid:162)(cid:48)(cid:105) . (cid:101) FordetailsandaproofseeSection11.6ofIntroductiontoEconometrics.Theorem6.11isananalogof theCramÃ©r-Raolowerboundforsemiparametricestimation. Theresultshowsthattheasymptoticvari- ancefromTheorems6.3isthebestpossibleinanyfinitesampleamongunbiasedestimators. Theorem 6.11issharp,sincethesamplemeanhasthefinitesamplevariancen â1V. 6.8 StochasticOrderSymbols Itisconvenienttohavesimplesymbolsforrandomvariablesandvectorswhichconvergeinprob- ability to zero or are stochastically bounded. In this section we introduce some of the most common notation. LetZ anda ,n=1,2,...besequencesofrandomvariablesandconstants.Thenotation n n Z =o (1) n p (âsmalloh-P-oneâ)meansthatZ ââ0asnââ.Wealsowrite n p Z =o (a ) n p n ifa â1Z =o (1). n n p Similarly, the notation Z =O (1) (âbig oh-P-oneâ) means that Z is bounded in probability. Pre- n p n cisely,forany(cid:178)>0thereisaconstantM(cid:178) <âsuchthat limsup(cid:80)[|Z n |>M(cid:178)]â¤(cid:178). nââ Furthermore,wewrite Z =O (a ) n p n ifa â1Z =O (1). n n p O (1)isweakerthano (1)inthesensethatZ =o (1)impliesZ =O (1)butnotthereverse.How- p p n p n p ever,ifZ =O (a )thenZ =o (b )foranyb suchthata /b â0. n p n n p n n n n Arandomsequencewithaboundedmomentisstochasticallybounded.",
    "page": 179,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER6. AREVIEWOFLARGESAMPLEASYMPTOTICS 160 Theorem6.12 IfZ isarandomvectorwhichsatisfies n (cid:69)(cid:107)Z (cid:107)Î´=O(a ) n n forsomesequencea andÎ´>0,then n Z =O (a1/Î´ ). n p n Similarly,(cid:69)(cid:107)Z (cid:107)Î´=o(a )impliesZ =o (a1/Î´ ). n n n p n There are many simple rules for manipulating o (1) and O (1) sequences which can be deduced p p fromthecontinuousmappingtheorem.Forexample, o (1)+o (1)=o (1) p p p o (1)+O (1)=O (1) p p p O (1)+O (1)=O (1) p p p o (1)o (1)=o (1) p p p o (1)O (1)=o (1) p p p O (1)O (1)=O (1). p p p 6.9 ConvergenceofMoments We give a sufficient condition for the existence of the mean of the asymptotic distribution, define uniform integrability, provide a primitive condition for uniform integrability, and show that uniform integrabilityisthekeyconditionunderwhich(cid:69)[Z ]convergesto(cid:69)[Z]. n Theorem6.13 IfZ ââZ and(cid:69)(cid:107)Z (cid:107)â¤C then(cid:69)(cid:107)Z(cid:107)â¤C. n n d Definition6.4 TherandomvectorZ isuniformlyintegrableasnââif n lim limsup(cid:69)[(cid:107)Z (cid:107)1 {(cid:107)Z (cid:107)>M}]=0. n n Mââ nââ Theorem6.14 IfforsomeÎ´>0,(cid:69)(cid:107)Z (cid:107)1+Î´â¤C <â,thenZ isuniformlyinte- n n grable.",
    "page": 180,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER6. AREVIEWOFLARGESAMPLEASYMPTOTICS 161 Theorem6.15 IfZ ââZ andZ isuniformlyintegrablethen(cid:69)[Z ]ââ(cid:69)[Z]. n n n d 6.10 UniformStochasticBounds Theorem6.16 If|Y |r isuniformlyintegrable,thenasnââ i n â1/r max |Y |ââ0. (6.6) i 1â¤iâ¤n p Equation (6.6) implies that if Y has r finite moments then the largest observation will diverge at a rateslowerthann1/r.Thehigherthemoments,theslowertherateofdivergence.",
    "page": 181,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Chapter 7 Asymptotic Theory for Least Squares 7.1 Introduction Itturnsoutthattheasymptotictheoryofleastsquaresestimationappliesequallytotheprojection model and the linear CEF model. Therefore the results in this chapter will be stated for the broader projectionmodeldescribedinSection2.18.RecallthatthemodelisY =X (cid:48)Î²+ewiththelinearprojection coefficientÎ²=(cid:161)(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:162)â1(cid:69)[XY]. Maintainedassumptionsinthischapterwillberandomsampling(Assumption1.2)andfinitesecond moments(Assumption2.1).Werestatethesehereforclarity. Assumption7.1 1. Thevariables(Y ,X ),i =1,...,n,arei.i.d. i i 2. (cid:69)(cid:163) Y2(cid:164)<â. 3. (cid:69)(cid:107)X(cid:107)2<â. 4. Q =(cid:69)(cid:163) XX (cid:48)(cid:164) ispositivedefinite. XX Thedistributionalresultswillrequireastrengtheningoftheseassumptionstofinitefourthmoments. WediscussthespecificconditionsinSection7.3. 7.2 ConsistencyofLeastSquaresEstimator Inthissectionweusetheweaklawoflargenumbers(WLLN,Theorem6.1andTheorem6.2)andcon- tinuousmappingtheorem(CMT,Theorem6.6)toshowthattheleastsquaresestimatorÎ² (cid:98)isconsistent fortheprojectioncoefficientÎ². Thisderivationisbasedonthreekeycomponents. First,theOLSestimatorcanbewrittenasacon- tinuousfunctionofasetofsamplemoments. Second,theWLLNshowsthatsamplemomentsconverge in probability to population moments. And third, the CMT states that continuous functions preserve convergenceinprobability.Wenowexplaineachstepinbriefandtheningreaterdetail. 162",
    "page": 182,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 163 First,observethattheOLSestimator (cid:195) (cid:33)â1(cid:195) (cid:33) Î² (cid:98) = n 1 (cid:88) n X i X i (cid:48) n 1 (cid:88) n X i Y i =Q(cid:98) â X 1 X Q(cid:98)XY i=1 i=1 isafunctionofthesamplemomentsQ(cid:98)XX = n 1(cid:80)n i=1 X i X i (cid:48) andQ(cid:98)XY = n 1(cid:80)n i=1 X i Y i . Second,byanapplicationoftheWLLNthesesamplemomentsconvergeinprobabilitytotheirpop- ulation expectations. Specifically, the fact that (Y ,X ) are mutually i.i.d. implies that any function of i i (cid:48) (Y ,X )is i.i.d., including X X and X Y . These variablesalso have finite expectationsunder Assump- i i i i i i tion7.1.Undertheseconditions,theWLLN(Theorem6.2)impliesthatasnââ, Q(cid:98)XX = n 1 i (cid:88) = n 1 X i X i (cid:48)â p â(cid:69)(cid:163) XX (cid:48)(cid:164)=Q XX (7.1) and 1 (cid:88) n Q(cid:98)XY = X i Y i ââ(cid:69)[XY]=Q XY . n i=1 p Third, the CMT (Theorem 6.6) allows us to combine these equations to show that Î² (cid:98)converges in probabilitytoÎ².Specifically,asnââ, Î² (cid:98) =Q(cid:98) â X 1 X Q(cid:98)XY â p âQ â X 1 X Q XY =Î². (7.2) We have shown that Î² (cid:98) ââ Î² as n â â. In words, the OLS estimator converges in probability to the p projectioncoefficientvectorÎ²asthesamplesizengetslarge. TofullyunderstandtheapplicationoftheCMTwewalkthroughitindetail.Wecanwrite Î² (cid:98) =g (cid:161) Q(cid:98)XX ,Q(cid:98)XY (cid:162) whereg(A,b)=A â1bisafunctionof Aandb.Thefunctiong(A,b)isacontinuousfunctionof Aandb atallvaluesoftheargumentssuchthatA â1exists.Assumption7.1specifiesthatQ ispositivedefinite, XX whichmeansthatQ â1 exists. Thus g(A,b)iscontinuousat A=Q .Thisjustifiestheapplicationof XX XX theCMTin(7.2). Foraslightlydifferentdemonstrationof(7.2)recallthat(4.6)impliesthat Î² (cid:98) âÎ²=Q(cid:98) â X 1 X Q(cid:98)Xe (7.3) where 1 (cid:88) n Q(cid:98)Xe = X i e i . n i=1 TheWLLNand(2.25)imply Q(cid:98)Xe ââ(cid:69)[Xe]=0. p Therefore Î² (cid:98) âÎ²=Q(cid:98) â X 1 X Q(cid:98)Xe â p âQ â X 1 X 0=0 whichisthesameasÎ² (cid:98) ââÎ². p",
    "page": 183,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 164 Theorem7.1 ConsistencyofLeastSquares. UnderAssumption7.1,Q(cid:98)XX ââ p Q XX ,Q(cid:98)XY â p âQ XY ,Q(cid:98) â X 1 X â p âQ â X 1 X ,Q(cid:98)Xe â p â0,andÎ² (cid:98) â p âÎ²asnââ. Theorem7.1statesthattheOLSestimatorÎ² (cid:98)convergesinprobabilitytoÎ²asn increasesandthusÎ² (cid:98) isconsistentforÎ².Inthestochasticordernotation,Theorem7.1canbeequivalentlywrittenas Î² (cid:98) =Î²+o p (1). (7.4) Toillustratetheeffectofsamplesizeontheleastsquaresestimatorconsidertheleastsquaresregres- sion log(wage)=Î² education+Î² experience+Î² experience2+Î² +e. 1 2 3 4 Weusethesampleof24,344whitemenfromtheMarch2009CPS.Werandomlysortedtheobservations andsequentiallyestimatedthemodelbyleastsquaresstartingwiththefirst5observationsandcontin- uing until the full sample is used. The sequence of estimates are displayed in Figure 7.1. You can see howtheleastsquaresestimatechangeswiththesamplesize.Asthenumberofobservationsincreasesit settlesdowntothefull-sampleestimateÎ² (cid:98)1 =0.114. Number of Observations ^b 1 5000 10000 15000 20000 521.0 021.0 511.0 011.0 Figure7.1:TheLeast-SquaresEstimatorÎ² (cid:98)1 asaFunctionofSampleSizen 7.3 AsymptoticNormality WestartedthischapterdiscussingtheneedforanapproximationtothedistributionoftheOLSesti- matorÎ² (cid:98).InSection7.2weshowedthatÎ² (cid:98)convergesinprobabilitytoÎ². Consistencyisagoodfirststep, butinitselfdoesnotdescribethedistributionoftheestimator. Inthissectionwederiveanapproxima- tiontypicallycalledtheasymptoticdistribution.",
    "page": 184,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 165 Thederivationstartsbywritingtheestimatorasafunctionofsamplemoments.Oneofthemoments mustbewrittenasasumofzero-meanrandomvectorsandnormalizedsothatthecentrallimittheorem canbeapplied.Thestepsareasfollows. (cid:112) Takeequation(7.3)andmultiplyitby n.Thisyieldstheexpression (cid:112) n (cid:161)Î² (cid:98) âÎ²(cid:162)= (cid:195) n 1 i (cid:88) = n 1 X i X i (cid:48) (cid:33)â1(cid:195) (cid:112) 1 n i (cid:88) = n 1 X i e i (cid:33) . (7.5) (cid:112) Thisshowsthatthenormalizedandcenteredestimator n (cid:161)Î² (cid:98) âÎ²(cid:162) isafunctionofthesampleaverage n â1(cid:80)n X X (cid:48) andthenormalizedsampleaveragen â1/2(cid:80)n X e . i=1 i i i=1 i i Therandompairs(Y ,X )arei.i.d.,meaningthattheyareindependentacrossi andidenticallydis- i i tributed.Anyfunctionof(Y ,X )isalsoi.i.d.Thisincludese =Y âX (cid:48)Î²andtheproductX e .Thelatter i i i i i i i ismean-zero((cid:69)[Xe]=0)andhaskÃk covariancematrix â¦=(cid:69)(cid:163) (Xe)(Xe) (cid:48)(cid:164)=(cid:69)(cid:163) XX (cid:48) e2(cid:164) . Weshowbelowthatâ¦hasfiniteelementsunderastrengtheningofAssumption7.1. Since X e isi.i.d., i i meanzero,andfinitevariance,thecentrallimittheorem(Theorem6.3)implies 1 (cid:88) n (cid:112) X e ââN(0,â¦). i i n i=1 d Westatetherequiredconditionshere. Assumption7.2 1. Thevariables(Y ,X ),i =1,...,n,arei.i.d.. i i 2. (cid:69)(cid:163) Y4(cid:164)<â. 3. (cid:69)(cid:107)X(cid:107)4<â. 4. Q =(cid:69)(cid:163) XX (cid:48)(cid:164) ispositivedefinite. XX Assumption7.2impliesthatâ¦<â. Toseethis,takethe j(cid:96)th elementofâ¦,(cid:69)(cid:163) X j X(cid:96)e2(cid:164) . First,Theo- rem2.9.6showsthat(cid:69)(cid:163) e4(cid:164)<â. Bytheexpectationinequality(B.30)the j(cid:96)th elementofâ¦isbounded by (cid:175) (cid:175) (cid:69)(cid:163) X j X(cid:96)e2(cid:164)(cid:175) (cid:175) â¤(cid:69)(cid:175) (cid:175)X j X(cid:96)e2(cid:175) (cid:175) =(cid:69)(cid:163)(cid:175) (cid:175)X j (cid:175) (cid:175) |X(cid:96) |e2(cid:164) . BytwoapplicationsoftheCauchy-Schwarzinequality(B.32)thisissmallerthan (cid:179) (cid:69) (cid:104) X2X2 (cid:105)(cid:180)1/2(cid:161)(cid:69)(cid:163) e4(cid:164)(cid:162)1/2â¤ (cid:179) (cid:69) (cid:104) X4 (cid:105)(cid:180)1/4(cid:161)(cid:69)(cid:163) X4(cid:164)(cid:162)1/4(cid:161)(cid:69)(cid:163) e4(cid:164)(cid:162)1/2<â j (cid:96) j (cid:96) wherethefinitenessholdsunderAssumption7.2.2and7.2.3.Thusâ¦<â. Analternativewaytoshowthattheelementsofâ¦arefiniteisbyusingamatrixnorm(cid:107)Â·(cid:107)(SeeAp- pendixA.23).Thenbytheexpectationinequality,theCauchy-Schwarzinequality,Assumption7.2.3,and (cid:69)(cid:163) e4(cid:164)<â, (cid:107)â¦(cid:107)â¤(cid:69)(cid:176) (cid:176)XX (cid:48) e2(cid:176) (cid:176) =(cid:69)(cid:163)(cid:107)X(cid:107)2e2(cid:164)â¤(cid:161)(cid:69)(cid:107)X(cid:107)4(cid:162)1/2(cid:161)(cid:69)(cid:163) e4(cid:164)(cid:162)1/2<â.",
    "page": 185,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 166 Thisisamorecompactargument(oftendescribedasmoreelegant)butsuchmanipulationsshouldnot bedonewithoutunderstandingthenotationandtheapplicabilityofeachstepoftheargument. Regardless, the finiteness of the covariance matrix means that we can then apply the multivariate CLT(Theorem6.3). Theorem7.2 Assumption7.2impliesthat â¦<â (7.6) and 1 (cid:88) n (cid:112) X e ââN(0,â¦) (7.7) i i n i=1 d asnââ. Puttingtogether(7.1),(7.5),and(7.7), (cid:112) n (cid:161)Î² (cid:98) âÎ²(cid:162)ââQ â1 N(0,â¦)=N (cid:161) 0,Q â1 â¦Q â1 (cid:162) XX XX XX d asnââ. Thefinalequalityfollowsfromthepropertythatlinearcombinationsofnormalvectorsare alsonormal(Theorem5.2). Wehavederivedtheasymptoticnormalapproximationtothedistributionoftheleastsquaresesti- mator. Theorem7.3 AsymptoticNormalityofLeastSquaresEstimator UnderAssumption7.2,asnââ (cid:112) n (cid:161)Î² (cid:98) âÎ²(cid:162)ââN (cid:161) 0,VÎ² (cid:162) d whereQ =(cid:69)(cid:163) XX (cid:48)(cid:164) ,â¦=(cid:69)(cid:163) XX (cid:48) e2(cid:164) ,and XX VÎ² =Q â1 â¦Q â1 . (7.8) XX XX Inthestochasticordernotation,Theorem7.3impliesthatÎ² (cid:98) =Î²+O p (n â1/2)whichisstrongerthan (7.4). (cid:112) The matrix VÎ² =Q â1 â¦Q â1 is the variance of the asymptotic distribution of n (cid:161)Î² (cid:98) âÎ²(cid:162) . Conse- XX XX quently,VÎ²isoftenreferredtoastheasymptoticcovariancematrixofÎ² (cid:98).TheexpressionVÎ² =Q â1 â¦Q â1 XX XX iscalledasandwichformasthematrixâ¦issandwichedbetweentwocopiesofQ â1 . XX Itisusefultocomparethevarianceoftheasymptoticdistributiongivenin(7.8)andthefinite-sample conditionalvarianceintheCEFmodelasgivenin(4.10): V Î²(cid:98) =var (cid:163)Î² (cid:98) |X (cid:164)=(cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48) DX (cid:162)(cid:161) X (cid:48) X (cid:162)â1 . (7.9)",
    "page": 186,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 167 (cid:112) Notice thatV Î²(cid:98) is the exact conditional variance of Î² (cid:98)andVÎ² is the asymptotic variance of n (cid:161)Î² (cid:98) âÎ²(cid:162) . ThusVÎ² shouldbe(roughly)n timesaslargeasV Î²(cid:98) , orVÎ² ânV Î²(cid:98) . Indeed, multiplying(7.9)byn and distributingwefind nV = (cid:181) 1 X (cid:48) X (cid:182)â1(cid:181) 1 X (cid:48) DX (cid:182)(cid:181) 1 X (cid:48) X (cid:182)â1 Î²(cid:98) n n n which looks like an estimator ofVÎ². Indeed, as n ââ, nV Î²(cid:98) â p âVÎ². The expressionV Î²(cid:98) is useful for practicalinference(suchascomputationofstandarderrorsandtests)sinceitisthevarianceofthees- timatorÎ² (cid:98),whileVÎ² isusefulforasymptotictheoryasitiswelldefinedinthelimitasn goestoinfinity. Wewillmakeuseofbothsymbolsanditwillbeadvisabletoadheretothisconvention. Thereisaspecialcasewhereâ¦andVÎ²simplify.Supposethat cov(XX (cid:48) ,e2)=0. (7.10) Condition(7.10)holdsinthehomoskedasticlinearregressionmodelbutissomewhatbroader. Under (7.10)theasymptoticvarianceformulaesimplifyas â¦=(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:69)(cid:163) e2(cid:164)=Q Ï2 XX VÎ² =Q â X 1 X â¦Q â X 1 X =Q â X 1 X Ï2â¡V0 Î² . (7.11) In(7.11)wedefineV0 Î² =Q â X 1 X Ï2whether(7.10)istrueorfalse. When(7.10)istruethenVÎ² =V0 Î² ,other- wiseVÎ² (cid:54)=V0 Î² .WecallV0 Î² thehomoskedasticasymptoticcovariancematrix. Theorem 7.3 states that the sampling distribution of the least squares estimator, after rescaling, is approximatelynormalwhenthesamplesizen issufficientlylarge. Thisholdstrueforalljointdistribu- tions of (Y,X) which satisfy the conditions of Assumption 7.2. Consequently, asymptotic normality is (cid:112) routinelyusedtoapproximatethefinitesampledistributionof n (cid:161)Î² (cid:98) âÎ²(cid:162) . AdifficultyisthatforanyfixednthesamplingdistributionofÎ² (cid:98)canbearbitrarilyfarfromthenormal distribution.Thenormalapproximationimprovesasnincreases,buthowlargeshouldnbeinorderfor theapproximationtobeuseful? Unfortunately, thereisnosimpleanswertothisreasonablequestion. Thetroubleisthatnomatterhowlargeisthesamplesizethenormalapproximationisarbitrarilypoorfor somedatadistributionsatisfyingtheassumptions.Weillustratethisproblemusingasimulation.LetY = Î² X+Î² +ewhereX isN(0,1)andeisindependentofX withtheDoubleParetodensity f(e)= Î±|e|âÎ±â1, 1 2 2 |e|â¥1.IfÎ±>2theerrorehaszeromeanandvarianceÎ±/(Î±â2).AsÎ±approaches2,however,itsvariance (cid:113) diverges to infinity. In this context the normalized least squares slope estimator n Î± Î± â2(cid:161)Î² (cid:98)1 âÎ² 1 (cid:162) has theN(0,1)asymptoticdistributionforanyÎ±>2. InFigure7.2(a)wedisplaythefinitesampledensities (cid:113) of the normalized estimator n Î± Î± â2(cid:161)Î² (cid:98)1 âÎ² 1 (cid:162) , setting n = 100 and varying the parameter Î±. For Î± = 3.0 the density is very close to the N(0,1) density. As Î± diminishes the density changes significantly, concentratingmostoftheprobabilitymassaroundzero",
    "page": 187,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". In this context the normalized least squares slope estimator n Î± Î± â2(cid:161)Î² (cid:98)1 âÎ² 1 (cid:162) has theN(0,1)asymptoticdistributionforanyÎ±>2. InFigure7.2(a)wedisplaythefinitesampledensities (cid:113) of the normalized estimator n Î± Î± â2(cid:161)Î² (cid:98)1 âÎ² 1 (cid:162) , setting n = 100 and varying the parameter Î±. For Î± = 3.0 the density is very close to the N(0,1) density. As Î± diminishes the density changes significantly, concentratingmostoftheprobabilitymassaroundzero. AnotherexampleisshowninFigure7.2(b).HerethemodelisY =Î²+ewhere urâ(cid:69)[ur] e= (7.12) (cid:161)(cid:69)(cid:163) u2r (cid:164)â((cid:69)[ur])2(cid:162)1/2 (cid:112) anduâ¼N(0,1).Weshowthesamplingdistributionof n (cid:161)Î² (cid:98) âÎ²(cid:162) forn=100,varyingr =1,4,6and8.As r increases,thesamplingdistributionbecomeshighlyskewedandnon-normal. ThelessonfromFigure 7.2isthattheN(0,1)asymptoticapproximationisneverguaranteedtobeaccurate.",
    "page": 187,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 168 â2 â1 0 1 2 0.1 8.0 6.0 4.0 2.0 0.0 N(0, 1) N(0,1) a=3 N(0,1)4 a=2.4 N(0,1)6 a=2.2 N(0,1)8 a=2.1 a=2.05 â2 â1 0 1 2 (a)DoubleParetoError (b)ErrorProcess(7.12) Figure7.2:DensityofNormalizedOLSEstimator 7.4 JointDistribution Theorem 7.3 gives the joint asymptotic distribution of the coefficient estimators. We can use the result to study the covariance between the coefficient estimators. For simplicity, take the case of two regressors,nointercept,andhomoskedasticerror. Assumetheregressorsaremeanzero,varianceone, withcorrelationÏ.Thenusingtheformulaforinversionofa2Ã2matrix, V0 =Ï2Q â1 = Ï2 (cid:183) 1 âÏ (cid:184) . Î² XX 1âÏ2 âÏ 1 Thus if X 1 and X 2 are positively correlated (Ï >0) then Î² (cid:98)1 and Î² (cid:98)2 are negatively correlated (and vice- versa). Forillustration,Figure7.3(a)displaystheprobabilitycontoursofthejointasymptoticdistributionof Î² (cid:98)1 âÎ² 1 andÎ² (cid:98)2 âÎ² 2 whenÎ² 1 =Î² 2 =0andÏ =0.5.Thecoefficientestimatorsarenegativelycorrelated sincetheregressorsarepositivelycorrelated. ThismeansthatifÎ² (cid:98)1 isunusuallynegative,itislikelythat Î² (cid:98)2 isunusuallypositive,orconversely. ItisalsounlikelythatwewillobservebothÎ² (cid:98)1 andÎ² (cid:98)2 unusually largeandofthesamesign. Thisfindingthatthecorrelationoftheregressorsisofoppositesignofthecorrelationofthecoeffi- cientestimatesissensitivetotheassumptionofhomoskedasticity.Iftheerrorsareheteroskedasticthen thisrelationshipisnotguaranteed. This can be seen through a simple constructed example. Suppose that X and X only take the 1 2 values {â1,+1}, symmetrically, with (cid:80)[X =X =1] = (cid:80)[X =X =â1] = 3/8, and (cid:80)[X =1,X =â1] = 1 2 1 2 1 2 (cid:80)[X =â1,X =1]=1/8.Youcancheckthattheregressorsaremeanzero,unitvarianceandcorrelation 1 2 0.5,whichisidenticalwiththesettingdisplayedinFigure7.3(a). Now suppose that the error is heteroskedastic. Specifically, suppose that (cid:69)(cid:163) e2|X =X (cid:164) = 5 and 1 2 4",
    "page": 188,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 169 b 1 b 2 â3 â2 â1 0 1 2 3 3 2 1 0 1â 2â 3â b 1 (a)HomoskedasticCase b 2 â3 â2 â1 0 1 2 3 3 2 1 0 1â 2â 3â (b)HeteroskedasticCase Figure7.3:ContoursofJointDistributionof(Î² (cid:98)1 ,Î² (cid:98)2 ) (cid:69)(cid:163) e2|X (cid:54)=X (cid:164)= 1 .Youcancheckthat(cid:69)(cid:163) e2(cid:164)=1,(cid:69)(cid:163) X2e2(cid:164)=(cid:69)(cid:163) X2e2(cid:164)=1and(cid:69)(cid:163) X X e2(cid:164)= 7 .Therefore 1 2 4 1 2 1 2 i 8 VÎ² =Q â1 â¦Q â1 XX XX ï£® 1 ï£¹ï£® 7 ï£¹ï£® 1 ï£¹ 9 1 â 1 1 â = ï£¯ 2 ï£ºï£¯ 8 ï£ºï£¯ 2 ï£º ï£° 1 ï£»ï£° 7 ï£»ï£° 1 ï£» 16 â 1 1 â 1 2 8 2 ï£® 1 ï£¹ 4 1 = ï£¯ 4 ï£º. ï£° 1 ï£» 3 1 4 Thus the coefficient estimators Î² (cid:98)1 and Î² (cid:98)2 are positively correlated (their correlation is 1/4.) The joint probabilitycontoursoftheirasymptoticdistributionisdisplayedinFigure7.3(b). Wecanseehowthe twoestimatorsarepositivelyassociated. Whatwefoundthroughthisexampleisthatinthepresenceofheteroskedasticitythereisnosimple relationshipbetweenthecorrelationoftheregressorsandthecorrelationoftheparameterestimators. Wecanextendtheaboveanalysistostudythecovariancebetweencoefficientsub-vectors. Forex- ample,partitioningX (cid:48)=(cid:161) X (cid:48) ,X (cid:48)(cid:162) andÎ²(cid:48)=(cid:161)Î²(cid:48) ,Î²(cid:48)(cid:162) ,wecanwritethegeneralmodelas 1 2 1 2 Y =X (cid:48)Î² +X (cid:48)Î² +e 1 1 2 2 andthecoefficientestimatesasÎ² (cid:98) (cid:48)=(cid:161)Î² (cid:98) (cid:48) ,Î² (cid:98) (cid:48)(cid:162) .Makethepartitions 1 2 (cid:183) Q Q (cid:184) (cid:183) â¦ â¦ (cid:184) Q = 11 12 , â¦= 11 12 . XX Q Q â¦ â¦ 21 22 21 22 From(2.43) Q â X 1 X = (cid:183) âQ â Q 1 â 1 Q 1 1 Â·2 Q â1 âQ â 11 Q 1 Â·2 â Q 1 12 Q â 22 1 (cid:184) 22Â·1 21 11 22Â·1",
    "page": 189,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 170 whereQ 11Â·2 =Q 11 âQ 12 Q â 22 1Q 21 andQ 22Â·1 =Q 22 âQ 21 Q â 11 1Q 12 .Thuswhentheerrorishomoskedastic cov (cid:161)Î² (cid:98)1 ,Î² (cid:98)2 (cid:162)=âÏ2Q â 11 1 Â·2 Q 12 Q â 22 1 whichisamatrixgeneralizationofthetwo-regressorcase. Ingeneralyoucanshowthat(Exercise7.5) (cid:183) (cid:184) V V VÎ² = 11 12 (7.13) V V 21 22 where V =Q â1 (cid:161)â¦ âQ Q â1â¦ ââ¦ Q â1Q +Q Q â1â¦ Q â1Q (cid:162) Q â1 (7.14) 11 11Â·2 11 12 22 21 12 22 21 12 22 22 22 21 11Â·2 V =Q â1 (cid:161)â¦ âQ Q â1â¦ ââ¦ Q â1Q +Q Q â1â¦ Q â1Q (cid:162) Q â1 (7.15) 21 22Â·1 21 21 11 11 22 22 21 21 11 12 22 21 11Â·2 V =Q â1 (cid:161)â¦ âQ Q â1â¦ ââ¦ Q â1Q +Q Q â1â¦ Q â1Q (cid:162) Q â1 . (7.16) 22 22Â·1 22 21 11 12 21 11 12 21 11 11 11 12 22Â·1 Unfortunately,theseexpressionsarenoteasilyinterpretable. 7.5 ConsistencyofErrorVarianceEstimators UsingthemethodsofSection7.2wecanshowthattheestimatorsÏ2= 1(cid:80)n e2ands2= 1 (cid:80)n e2 (cid:98) n i=1(cid:98)i nâk i=1(cid:98)i areconsistentforÏ2. Thetrickistowritetheresiduale asequaltotheerrore plusadeviation (cid:98)i i e (cid:98)i =Y i âX i (cid:48)Î² (cid:98) =e i âX i (cid:48)(cid:161)Î² (cid:98) âÎ²(cid:162) . Thusthesquaredresidualequalsthesquarederrorplusadeviation e (cid:98)i 2=e i 2â2e i X i (cid:48)(cid:161)Î² (cid:98) âÎ²(cid:162)+(cid:161)Î² (cid:98) âÎ²(cid:162)(cid:48) X i X i (cid:48)(cid:161)Î² (cid:98) âÎ²(cid:162) . (7.17) Sowhenwetaketheaverageofthesquaredresidualsweobtaintheaverageofthesquarederrors,plus twotermswhichare(hopefully)asymptoticallynegligible. (cid:195) (cid:33) (cid:195) (cid:33) Ï (cid:98) 2= n 1 (cid:88) n e i 2â2 n 1 (cid:88) n e i X i (cid:48) (cid:161)Î² (cid:98) âÎ²(cid:162)+(cid:161)Î² (cid:98) âÎ²(cid:162)(cid:48) n 1 (cid:88) n X i X i (cid:48) (cid:161)Î² (cid:98) âÎ²(cid:162) . (7.18) i=1 i=1 i=1 Indeed,theWLLNshowsthat 1 (cid:88) n e2ââÏ2 n i=1 i p 1 (cid:88) n e X (cid:48)ââ(cid:69)(cid:163) eX (cid:48)(cid:164)=0 n i=1 i i p 1 (cid:88) n X X (cid:48)ââ(cid:69)(cid:163) XX (cid:48)(cid:164)=Q . n i=1 i i p XX Theorem7.1showsthatÎ² (cid:98) ââÎ².Hence(7.18)convergesinprobabilitytoÏ2asdesired. p Finally,sincen/(nâk)â1asnââitfollowsthats2=(cid:161) n (cid:162)Ï2ââÏ2. Thusbothestimatorsare nâk (cid:98) p consistent. Theorem7.4 UnderAssumption7.1,Ï2ââÏ2ands2ââÏ2asnââ. (cid:98) p p",
    "page": 190,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 171 7.6 HomoskedasticCovarianceMatrixEstimation (cid:112) Theorem7.3showsthat n (cid:161)Î² (cid:98) âÎ²(cid:162) isasymptoticallynormalwithasymptoticcovariancematrixVÎ². Forasymptoticinference(confidenceintervalsandtests)weneedaconsistentestimatorofVÎ². Under homoskedasticityVÎ² simplifiestoV0 Î² =Q â X 1 X Ï2 andinthissectionweconsiderthesimplifiedproblem ofestimatingV0. Î² The standard moment estimator ofQ XX isQ(cid:98)XX defined in (7.1) and thus an estimator forQ â X 1 X is Q(cid:98) â1 .ThestandardestimatorofÏ2istheunbiasedestimators2definedin(4.26).Thusanaturalplug-in XX estimatorforV0 Î² =Q â X 1 X Ï2isV(cid:98) 0 Î² =Q(cid:98) â X 1 X s2. Consistency ofV(cid:98) 0 Î² forV0 Î² follows from consistency of the moment estimatorsQ(cid:98)XX and s2 and an applicationofthecontinuousmappingtheorem. Specifically, Theorem7.1establishedQ(cid:98)XX ââQ XX , p andTheorem7.4establisheds2ââÏ2.ThefunctionV0 =Q â1 Ï2isacontinuousfunctionofQ and p Î² XX XX Ï2solongasQ >0,whichholdstrueunderAssumption7.1.4.ItfollowsbytheCMTthat XX V(cid:98) 0 Î² =Q(cid:98) â X 1 X s2â p âQ â X 1 X Ï2=V0 Î² sothatV(cid:98) 0 Î²isconsistentforV0 Î² . Theorem7.5 UnderAssumption7.1,V(cid:98) 0 Î² ââV0 Î² asnââ. p ItisinstructivetonoticethatTheorem7.5doesnotrequiretheassumptionofhomoskedasticity.That is, V(cid:98) 0 Î² is consistent for V0 Î² regardless if the regression is homoskedastic or heteroskedastic. However, V0 Î² =VÎ² =avar (cid:163)Î² (cid:98) (cid:164) onlyunderhomoskedasticity. Thus,inthegeneralcaseV(cid:98) 0 Î² isconsistentforawell- definedbutnon-usefulobject. 7.7 HeteroskedasticCovarianceMatrixEstimation (cid:112) Theorems 7.3 established that the asymptotic covariance matrix of n (cid:161)Î² (cid:98) âÎ²(cid:162) isVÎ² =Q â1 â¦Q â1 . XX XX Wenowconsiderestimationofthiscovariancematrixwithoutimposinghomoskedasticity.Thestandard approachistouseaplug-inestimatorwhichreplacestheunknownswithsamplemoments. AsdescribedintheprevioussectionanaturalestimatorforQ â X 1 X isQ(cid:98) â X 1 X whereQ(cid:98)XX definedin(7.1). Themomentestimatorforâ¦is â¦ (cid:98) = n 1 (cid:88) n X i X i (cid:48) e (cid:98)i 2, i=1 leadingtotheplug-incovariancematrixestimator V(cid:98) H Î² C0=Q(cid:98) â X 1 X â¦ (cid:98)Q(cid:98) â X 1 X . (7.19) YoucancheckthatV(cid:98) H Î² C0=nV(cid:98) H Î²(cid:98) C0 whereV(cid:98) H Î²(cid:98) C0 istheHC0covariancematrixestimatorfrom(4.31). AsshowninTheorem7.1,Q(cid:98) â X 1 X â p âQ â X 1 X ,sowejustneedtoverifytheconsistencyofâ¦ (cid:98). Thekeyisto replacethesquaredresiduale2withthesquarederrore2,andthenshowthatthedifferenceisasymptot- (cid:98)i i icallynegligible.",
    "page": 191,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 172 Specifically,observethat â¦ (cid:98) = n 1 (cid:88) n X i X i (cid:48) e (cid:98)i 2 i=1 = 1 (cid:88) n X X (cid:48) e2+ 1 (cid:88) n X X (cid:48)(cid:161) e2âe2(cid:162) . n i i i n i i (cid:98)i i i=1 i=1 Thefirsttermisanaverageofthei.i.d. randomvariablesX X (cid:48) e2,andthereforebytheWLLNconverges i i i inprobabilitytoitsexpectation,namely, 1 (cid:88) n X X (cid:48) e2ââ(cid:69)(cid:163) XX (cid:48) e2(cid:164)=â¦. n i=1 i i i p Technically,thisrequiresthatâ¦hasfiniteelements,whichwasshownin(7.6). Toestablishthatâ¦ (cid:98) isconsistentforâ¦itremainstoshowthat 1 (cid:88) n X X (cid:48)(cid:161) e2âe2(cid:162)ââ0. (7.20) n i=1 i i (cid:98)i i p Therearemultiplewaystodothis.Areasonablestraightforwardyetslightlytediousderivationistostart byapplyingthetriangleinequality(B.16)usingamatrixnorm: (cid:176) (cid:176) (cid:176) (cid:176) (cid:176) (cid:176)n 1 i (cid:88) = n 1 X i X i (cid:48)(cid:161) e (cid:98)i 2âe i 2(cid:162) (cid:176) (cid:176) (cid:176) (cid:176) â¤ n 1 i (cid:88) = n 1 (cid:176) (cid:176)X i X i (cid:48)(cid:161) e (cid:98)i 2âe i 2(cid:162)(cid:176) (cid:176) = n 1 (cid:88) n (cid:107)X i (cid:107)2(cid:175) (cid:175)e (cid:98)i 2âe i 2(cid:175) (cid:175). (7.21) i=1 Thenrecallingtheexpressionforthesquaredresidual(7.17),applythetriangleinequality(B.1)andthen theSchwarzinequality(B.12)twice (cid:175) (cid:175)e (cid:98)i 2âe i 2(cid:175) (cid:175) â¤2 (cid:175) (cid:175)e i X i (cid:48)(cid:161)Î² (cid:98) âÎ²(cid:162)(cid:175) (cid:175) +(cid:161)Î² (cid:98) âÎ²(cid:162)(cid:48) X i X i (cid:48)(cid:161)Î² (cid:98) âÎ²(cid:162) =2|e i |(cid:175) (cid:175)X i (cid:48)(cid:161)Î² (cid:98) âÎ²(cid:162)(cid:175) (cid:175) + (cid:175) (cid:175) (cid:175) (cid:161)Î² (cid:98) âÎ²(cid:162)(cid:48) X i (cid:175) (cid:175) (cid:175) 2 â¤2|e i |(cid:107)X i (cid:107)(cid:176) (cid:176) Î² (cid:98) âÎ²(cid:176) (cid:176) +(cid:107)X i (cid:107)2(cid:176) (cid:176) Î² (cid:98) âÎ²(cid:176) (cid:176) 2 . (7.22) Combining(7.21)and(7.22),wefind (cid:176) (cid:176) (cid:195) (cid:33) (cid:195) (cid:33) (cid:176) (cid:176) (cid:176) (cid:176)n 1 i (cid:88) = n 1 X i X i (cid:48)(cid:161) e (cid:98)i 2âe i 2(cid:162) (cid:176) (cid:176) (cid:176) (cid:176) â¤2 n 1 i (cid:88) = n 1 (cid:107)X i (cid:107)3|e i | (cid:176) (cid:176) Î² (cid:98) âÎ²(cid:176) (cid:176) + n 1 i (cid:88) = n 1 (cid:107)X i (cid:107)4 (cid:176) (cid:176) Î² (cid:98) âÎ²(cid:176) (cid:176) 2 =o (1). (7.23) p Theexpressioniso p (1)because (cid:176) (cid:176) Î² (cid:98) âÎ²(cid:176) (cid:176) ââ0andbothaveragesinparenthesisareaveragesofrandom p variableswithfiniteexpectationunderAssumption7.2(andarethusO (1)).Indeed,byHÃ¶lderâsinequal- p ity(B.31) (cid:69)(cid:163)(cid:107)X(cid:107)3|e|(cid:164)â¤ (cid:179) (cid:69) (cid:104) (cid:161)(cid:107)X(cid:107)3(cid:162)4/3 (cid:105)(cid:180)3/4(cid:161)(cid:69)(cid:163) e4(cid:164)(cid:162)1/4=(cid:161)(cid:69)(cid:107)X(cid:107)4(cid:162)3/4(cid:161)(cid:69)(cid:163) e4(cid:164)(cid:162)1/4<â",
    "page": 192,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Wehaveestablished(7.20)asdesired. Theorem7.6 UnderAssumption7.2,asnââ,â¦ (cid:98) âââ¦andV(cid:98) H Î² C0ââVÎ². p p Foranalternativeproofofthisresult,seeSection7.20.",
    "page": 192,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 173 7.8 SummaryofCovarianceMatrixNotation Thenotationwehaveintroducedmaybesomewhatconfusingsoitishelpfultowriteitdowninone place. Theexa (cid:112) ctvarianceofÎ² (cid:98)(undertheassumptionsofthelinearregressionmodel)andtheasymptotic varianceof n (cid:161)Î² (cid:98) âÎ²(cid:162) (underthemoregeneralassumptionsofthelinearprojectionmodel)are V Î²(cid:98) =var (cid:163)Î² (cid:98) (cid:112) |X (cid:164)=(cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48) DX (cid:162)(cid:161) X (cid:48) X (cid:162)â1 VÎ² =avar (cid:163) n (cid:161)Î² (cid:98) âÎ²(cid:162)(cid:164)=Q â1 â¦Q â1 . XX XX TheHC0estimatorsofthesetwocovariancematricesare (cid:195) (cid:33) n V(cid:98) H Î²(cid:98) C0=(cid:161) X (cid:48) X (cid:162)â1 (cid:88) X i X i (cid:48) e (cid:98)i 2 (cid:161) X (cid:48) X (cid:162)â1 i=1 V(cid:98) H Î² C0=Q(cid:98) â X 1 X â¦ (cid:98)Q(cid:98) â X 1 X andsatisfythesimplerelationshipV(cid:98) H Î² C0=nV(cid:98) H Î²(cid:98) C0 . Similarly,undertheassumptionofhomoskedasticitytheexactandasymptoticvariancessimplifyto V0 =(cid:161) X (cid:48) X (cid:162)â1Ï2 Î²(cid:98) V0 =Q â1 Ï2. Î² XX Theirstandardestimatorsare V(cid:98) 0 Î²(cid:98) =(cid:161) X (cid:48) X (cid:162)â1 s2 V(cid:98) 0 Î² =Q(cid:98) â X 1 X s2 whichalsosatisfytherelationshipV(cid:98) 0 Î² =nV(cid:98) 0 Î²(cid:98) . The exact formula and estimators are useful when constructing test statistics and standard errors. However,fortheoreticalpurposestheasymptoticformula(variancesandtheirestimates)aremoreuse- fulastheseretainnon-generatelimitsasthesamplesizesdiverge. Thatiswhybothsetsofnotationare useful. 7.9 AlternativeCovarianceMatrixEstimators* HC0 HC0 HC0 In Section 7.7 we introduced V(cid:98)Î² as an estimator of VÎ². V(cid:98)Î² is a scaled version of V(cid:98)Î²(cid:98) from Section 4.16, where we also introduced the alternative HC1, HC2 and HC3 heteroskedasticity-robust covariancematrixestimators.Wenowdiscusstheconsistencypropertiesoftheseestimators. Todosoweintroducetheirscaledversions,e.g. V(cid:98) H Î² C1=nV(cid:98) H Î²(cid:98) C1 ,V(cid:98) H Î² C2=nV(cid:98) H Î²(cid:98) C2 ,andV(cid:98) H Î² C3=nV(cid:98) H Î²(cid:98) C3 . Theseare(alternative)estimatorsoftheasymptoticcovariancematrixVÎ². First, considerV(cid:98) H Î² C1 . NoticethatV(cid:98) H Î² C1 =nV(cid:98) H Î²(cid:98) C1 = n n âk V(cid:98) H Î² C0 whereV(cid:98) H Î² C0 wasdefinedin(7.19)and shownconsistentforVÎ²inTheorem7.6.Ifk isfixedasnââ,then n n âk â1 andthus V(cid:98) H Î² C1=(1+o(1))V(cid:98) H Î² C0ââVÎ². p HC1 ThusV(cid:98)Î² isconsistentforVÎ².",
    "page": 193,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 174 ThealternativeestimatorsV(cid:98) H Î² C2 andV(cid:98) H Î² C3 taketheform(7.19)butwithâ¦ (cid:98) replacedby â¦ (cid:101) = n 1 (cid:88) n (1âh ii ) â2X i X i (cid:48) e (cid:98)i 2 i=1 and â¦= 1 (cid:88) n (1âh ) â1X X (cid:48) e2, n ii i i(cid:98)i i=1 respectively. ToshowthattheseestimatorsalsoconsistentforVÎ² givenâ¦ (cid:98) âââ¦itissufficienttoshow p thatthedifferencesâ¦ (cid:101) ââ¦ (cid:98) andâ¦ââ¦ (cid:98) convergeinprobabilitytozeroasnââ. Thetrickisthefactthattheleveragevaluesareasymptoticallynegligible: h â= max h =o (1). (7.24) n 1â¤iâ¤n ii p (SeeTheorem7.17inSection7.21.)Thenusingthetriangleinequality(B.16) (cid:176) (cid:176) (cid:176) â¦ââ¦ (cid:98) (cid:176) (cid:176) (cid:176) â¤ n 1 (cid:88) n (cid:176) (cid:176)X i X i (cid:48)(cid:176) (cid:176)e (cid:98)i 2(cid:175) (cid:175)(1âh ii ) â1â1 (cid:175) (cid:175) i=1 (cid:195) (cid:33) â¤ 1 (cid:88) n (cid:107)X (cid:107)2e2 (cid:175) (cid:175)(cid:161) 1âh â(cid:162)â1â1 (cid:175) (cid:175). n i (cid:98)i (cid:175) n (cid:175) i=1 ThesuminparenthesiscanbeshowntobeO (1)underAssumption7.2bythesameargumentasinin p theproofofTheorem7.6.(Infact,itcanbeshowntoconvergeinprobabilityto(cid:69)(cid:163)(cid:107)X(cid:107)2e2(cid:164) .)Thetermin absolutevaluesiso p (1)by(7.24).Thustheproductiso p (1)whichmeansthatâ¦=â¦ (cid:98) +o p (1)âââ¦. p Similarly, (cid:176) (cid:176) â¦ (cid:101) ââ¦ (cid:98) (cid:176) (cid:176) â¤ n 1 (cid:88) n (cid:176) (cid:176)X i X i (cid:48)(cid:176) (cid:176)e (cid:98)i 2(cid:175) (cid:175)(1âh ii ) â2â1 (cid:175) (cid:175) i=1 (cid:195) (cid:33) â¤ 1 (cid:88) n (cid:107)X (cid:107)2e2 (cid:175) (cid:175)(cid:161) 1âh â(cid:162)â2â1 (cid:175) (cid:175) n i (cid:98)i (cid:175) n (cid:175) i=1 =o (1). p Theorem7.7 Under Assumption 7.2, as n ââ, â¦ (cid:101) âââ¦, â¦âââ¦,V(cid:98) H Î² C1 ââ p p p VÎ²,V(cid:98) H Î² C2ââVÎ²,andV(cid:98) H Î² C3ââVÎ². p p Theorem7.7showsthatthealternativecovariancematrixestimatorsarealsoconsistentfortheasymp- toticcovariancematrix. To simplify notation, for the remainder of the chapter we will use the notationV(cid:98)Î² andV(cid:98)Î²(cid:98) to refer toanyoftheheteroskedasticity-consistentcovariancematrixestimatorsHC0,HC1,HC2andHC3,since theyallhavethesameasymptoticlimits.",
    "page": 194,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 175 7.10 FunctionsofParameters In most serious applications a researcher is actually interested in a specific transformation of the coefficientvectorÎ²=(Î² ,...,Î² ).Forexample,theresearchermaybeinterestedinasinglecoefficientÎ² 1 k j oraratioÎ² /Î² .Moregenerally,interestmayfocusonaquantitysuchasconsumersurpluswhichcould j l beacomplicatedfunctionofthecoefficients.Inanyofthesecaseswecanwritetheparameterofinterest Î¸asafunctionofthecoefficients,e.g.Î¸=r(Î²)forsomefunctionr :(cid:82)kâ(cid:82)q.TheestimateofÎ¸is Î¸ (cid:98) =r(Î² (cid:98)). By the continuous mapping theorem (Theorem 6.6) and the fact Î² (cid:98) ââÎ² we can deduce that Î¸ (cid:98)is p consistentforÎ¸ifthefunctionr(Â·)iscontinuous. Theorem7.8 UnderAssumption7.1,ifr(Î²)iscontinuousatthetruevalueof Î²thenasnââ,Î¸ (cid:98) ââÎ¸. p Furthermore,ifthetransformationissufficientlysmooth,bytheDeltaMethod(Theorem6.8)wecan showthatÎ¸ (cid:98)isasymptoticallynormal. Assumption7.3 r(Î²):(cid:82)k â(cid:82)q iscontinuouslydifferentiableatthetruevalue ofÎ²andR= â r(Î²) (cid:48) hasrankq. âÎ² Theorem7.9 AsymptoticDistributionofFunctionsofParameters UnderAssumptions7.2and7.3,asnââ, (cid:112) n (cid:161)Î¸ (cid:98) âÎ¸(cid:162)ââN(0,VÎ¸) (7.25) d whereVÎ¸ =R (cid:48) VÎ²R. Inmanycasesthefunctionr(Î²)islinear: r(Î²)=R (cid:48)Î² forsomekÃq matrixR.InparticularifR isaâselectormatrixâ (cid:181) (cid:182) I R= 0 thenwecanpartitionÎ²=(Î²(cid:48) ,Î²(cid:48) ) (cid:48) sothatR (cid:48)Î²=Î² .Then 1 2 1 (cid:181) (cid:182) VÎ¸ =(cid:161) I 0 (cid:162) VÎ² I =V 11 , 0",
    "page": 195,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 176 theupper-leftsub-matrixofV givenin(7.14).Inthiscase(7.25)statesthat 11 (cid:112) n (cid:161)Î² (cid:98)1 âÎ² 1 (cid:162)ââN(0,V 11 ). d Thatis,subsetsofÎ² (cid:98)areapproximatelynormalwithvariancesgivenbytheconformablesubcomponents ofV. ToillustratethecaseofanonlineartransformationtaketheexampleÎ¸=Î² /Î² for j (cid:54)=l.Then j l ï£« â (cid:161)Î² /Î² (cid:162) ï£¶ ï£« ï£¶ âÎ² j l 0 1 ï£¬ ï£¬ ï£¬ . . . ï£· ï£· ï£· ï£¬ ï£¬ . . . ï£· ï£· â ï£¬ ï£¬ ï£¬ âÎ² â j (cid:161)Î² j /Î² l (cid:162) ï£· ï£· ï£· ï£¬ ï£¬ ï£¬ 1/Î² l ï£· ï£· ï£· R= âÎ² r(Î²)=ï£¬ ï£¬ ï£¬ . . . ï£· ï£· ï£· =ï£¬ ï£¬ ï£¬ . . . ï£· ï£· ï£· (7.26) ï£¬ ï£¬ ï£¬ ï£¬ ï£¬ âÎ² â (cid:96) (cid:161)Î² . . . j /Î² l (cid:162) ï£· ï£· ï£· ï£· ï£· ï£¬ ï£¬ ï£¬ ï£¬ ï£­ âÎ² j . . . /Î²2 l ï£· ï£· ï£· ï£· ï£¸ ï£­ ï£¸ â (cid:161)Î² /Î² (cid:162) 0 âÎ² j l k so VÎ¸ =V jj /Î²2 l +V ll Î²2 j /Î²4 l â2V jl Î² j /Î²3 l whereV ab denotestheabth elementofVÎ². ForinferenceweneedanestimatoroftheasymptoticcovariancematrixVÎ¸ =R (cid:48) VÎ²R. Forthisitis typicaltousetheplug-inestimator â R(cid:98) = r(Î² (cid:98)) (cid:48) . (7.27) âÎ² Thederivativein(7.27)maybecalculatedanalyticallyornumerically. Byanalytically,wemeanworking outfortheformulaforthederivativeandreplacingtheunknownsbypointestimates.Forexample,ifÎ¸= Î² /Î² then â r(Î²)is(7.26).Howeverinsomecasesthefunctionr(Î²)maybeextremelycomplicatedand j l âÎ² aformulafortheanalyticderivativemaynotbeeasilyavailable. Inthiscasenumericaldifferentiation maybepreferable.LetÎ´ =(0Â·Â·Â· 1Â·Â·Â· 0) (cid:48) betheunitvectorwiththeâ1âinthelthplace.The jlthelement l ofanumericalderivativeR(cid:98) is R(cid:98)jl = r j (Î² (cid:98) +Î´ l (cid:178) (cid:178))âr j (Î² (cid:98)) forsomesmall(cid:178). TheestimatorofVÎ¸ is (cid:48) V(cid:98)Î¸ =R(cid:98) V(cid:98)Î²R(cid:98). (7.28) Alternatively,thehomoskedasticcovariancematrixestimatorcouldbeusedleadingtoahomoskedastic covariancematrixestimatorforÎ¸. V(cid:98) 0 Î¸ =R(cid:98) (cid:48) V(cid:98) 0 Î²R(cid:98) =R(cid:98) (cid:48) Q(cid:98) â X 1 X R(cid:98)s2. (7.29) Given(7.27),(7.28)and(7.29)aresimpletocalculateusingmatrixoperations. As the primary justification for V(cid:98)Î¸ is the asymptotic approximation (7.25), V(cid:98)Î¸ is often called an asymptoticcovariancematrixestimator. TheestimatorV(cid:98)Î¸ isconsistentforVÎ¸ undertheconditionsofTheorem7.9sinceV(cid:98)Î² ââVÎ²byThe- p orem7.6and â â R(cid:98) = r(Î² (cid:98)) (cid:48)ââ r(Î²) (cid:48)=R âÎ² p âÎ² sinceÎ² (cid:98) ââÎ²andthefunction â â Î² r(Î²) (cid:48) iscontinuousinÎ². p",
    "page": 196,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 177 Theorem7.10 UnderAssumptions7.2and7.3,asnââ,V(cid:98)Î¸ ââVÎ¸. p Theorem7.10showsthatV(cid:98)Î¸ isconsistentforVÎ¸ andthusmaybeusedforasymptoticinference. In practicewemayset V(cid:98)Î¸(cid:98) =R(cid:98) (cid:48) V(cid:98)Î²(cid:98) R(cid:98) =n â1R(cid:98) (cid:48) V(cid:98)Î²R(cid:98) (7.30) asanestimatorofthevarianceofÎ¸ (cid:98). 7.11 AsymptoticStandardErrors AsdescribedinSection4.17astandarderrorisanestimatorofthestandarddeviationofthedistri- butionofanestimator. ThusifV(cid:98)Î²(cid:98) isanestimatorofthecovariancematrixofÎ² (cid:98)thenstandarderrorsare thesquarerootsofthediagonalelementsofthismatrix.Thesetaketheform (cid:113) (cid:114)(cid:104) (cid:105) s(Î² (cid:98)j )= V(cid:98)Î²(cid:98)j = V(cid:98)Î²(cid:98) jj . StandarderrorsforÎ¸ (cid:98)areconstructedsimilarly. SupposingthatÎ¸=h(Î²)isreal-valuedthenthestandard errorforÎ¸ (cid:98)isthesquarerootof(7.30) (cid:114) (cid:113) s(Î¸ (cid:98))= R(cid:98) (cid:48) V(cid:98)Î²(cid:98) R(cid:98) = n â1R(cid:98) (cid:48) V(cid:98)Î²R(cid:98). Whenthejustificationisbasedonasymptotictheorywecalls(Î² (cid:98)j )ors(Î¸ (cid:98))anasymptoticstandarderror forÎ² (cid:98)j orÎ¸ (cid:98). Whenreportingyourresultsitisgoodpracticetoreportstandarderrorsforeachreported estimateandthisincludesfunctionsandtransformationsofyourparameterestimates. Thishelpsusers ofthework(includingyourself)assesstheestimationprecision. Weillustrateusingthelogwageregression log(wage)=Î² education+Î² experience+Î² experience2/100+Î² +e. 1 2 3 4 Considerthefollowingthreeparametersofinterest. 1. Percentagereturntoeducation: Î¸ =100Î² 1 1 (100timesthepartialderivativeoftheconditionalexpectationoflog(wage)withrespecttoeduca- tion.) 2. Percentagereturntoexperienceforindividualswith10yearsofexperience: Î¸ =100Î² +20Î² 2 2 3 (100timesthepartialderivativeoftheconditionalexpectationoflogwageswithrespecttoexperi- ence,evaluatedatexperience=10.)",
    "page": 197,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 178 3. Experiencelevelwhichmaximizesexpectedlogwages: Î¸ =â50Î² /Î² 3 2 3 (Thelevelofexperienceatwhichthepartialderivativeoftheconditionalexpectationoflog(wage) withrespecttoexperienceequals0.) The4Ã1vectorR forthesethreeparametersis ï£« ï£¶ ï£« ï£¶ ï£« ï£¶ 100 0 0 ï£¬ 0 ï£· ï£¬ 100 ï£· ï£¬ â50/Î² ï£· R=ï£¬ ï£·, ï£¬ ï£·, ï£¬ 3 ï£·, ï£¬ 0 ï£· ï£¬ 20 ï£· ï£¬ 50Î² /Î²2 ï£· ï£­ ï£¸ ï£­ ï£¸ ï£­ 2 3 ï£¸ 0 0 0 respectively. WeusethesubsampleofmarriedBlackwomen(allexperiencelevels)whichhas982observations. Thepointestimatesandstandarderrorsare lo(cid:225)g(wage)= 0.118 education+ 0.016 experienceâ 0.022 experience2/100+ 0.947 . (7.31) (0.008) (0.006) (0.012) (0.157) ThestandarderrorsarethesquarerootsoftheHC2covariancematrixestimate ï£« ï£¶ 0.632 0.131 â0.143 â11.1 V = ï£¬ ï£¬ 0.131 0.390 â0.731 â6.25 ï£· ï£·Ã10 â4. (7.32) Î²(cid:98) ï£¬ â0.143 â0.731 1.48 9.43 ï£· ï£­ ï£¸ â11.1 â6.25 9.43 246 Wecalculatethat Î¸ (cid:98)1 =100Î² (cid:98)1 =100Ã0.118=11.8 (cid:112) s(Î¸ (cid:98)1 )= 1002Ã0.632Ã10 â4=0.8 Î¸ (cid:98)2 =100Î² (cid:98)2 +20Î² (cid:98)3 =100Ã0.016â20Ã0.022=1.16 (cid:115) s(Î¸ (cid:98)2 )= (cid:161) 100 20 (cid:162) (cid:181) â 0 0 .3 .7 9 3 0 1 â 1 0 . . 4 7 8 31 (cid:182)(cid:181) 1 2 0 0 0 (cid:182) Ã10 â4=0.55 Î¸ (cid:98)3 =â50Î² (cid:98)2 /Î² (cid:98)3 =50Ã0.016/0.022=35.2 (cid:115) s(Î¸ (cid:98)3 )= (cid:161) â50/Î² (cid:98)3 50Î² (cid:98)2 /Î² (cid:98) 2 3 (cid:162) (cid:181) â 0 0 .3 .7 9 3 0 1 â 1 0 . . 4 7 8 31 (cid:182)(cid:181) 5 â 0 5 Î² (cid:98) 0 2 / / Î² (cid:98) Î² (cid:98) 3 2 3 (cid:182) Ã10 â4=7.0. The calculations show that the estimate of the percentage return to education (for married Black women)is12%peryearwithastandarderrorof0.8.Theestimateofthepercentagereturntoexperience forthosewith10yearsofexperienceis1.2%peryearwithastandarderrorof0.6. Theestimateofthe experiencelevelwhichmaximizesexpectedlogwagesis35yearswithastandarderrorof7. InStatathenlcomcommandcanbeusedafterestimationtoperformthesamecalculations.Toillus- trate,afterestimationof(7.31)usethecommandsgivenbelow.Ineachcase,Statareportsthecoefficient estimate,asymptoticstandarderror,and95%confidenceinterval.",
    "page": 198,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 179 StataCommands nlcom100*_b[education] nlcom100*_b[experience]+20*_b[exp2] nlcom-50*_b[experience]/_b[exp2] 7.12 t-statistic LetÎ¸=r(Î²):(cid:82)kâ(cid:82)beaparameterofinterest,Î¸ (cid:98)itsestimator,ands(Î¸ (cid:98))itsasymptoticstandarderror. Considerthestatistic Î¸âÎ¸ (cid:98) T(Î¸)= . (7.33) s(Î¸ (cid:98)) Differentwritershavecalled(7.33)at-statistic,at-ratio,az-statisticorastudentizedstatistic,some- timesusingthedifferentlabelstodistinguishbetweenfinite-sampleandasymptoticinference. Asthe statisticsthemselvesarealways(7.33)wewonâtmakethisdistinctionandwillsimplyrefertoT(Î¸)asa t-statisticorat-ratio.Wealsooftensuppresstheparameterdependence,writingitasT.Thet-statisticis afunctionoftheestimator,itsstandarderror,andtheparameter. (cid:112) ByTheorems7.9and7.10, n (cid:161)Î¸ (cid:98) âÎ¸(cid:162)ââN(0,VÎ¸)andV(cid:98)Î¸ ââVÎ¸.Thus d p Î¸âÎ¸ (cid:98) T(Î¸)= s(Î¸ (cid:98)) (cid:112) n (cid:161)Î¸ (cid:98) âÎ¸(cid:162) = (cid:113) V(cid:98)Î¸ N(0,VÎ¸) ââ (cid:112) d VÎ¸ =Z â¼N(0,1). Thelastequalityisthepropertythataffinefunctionsofnormalvariablesarenormal(Theorem5.2). This calculation requires that VÎ¸ >0, otherwise the continuous mapping theorem cannot be em- ployed. Inpracticethisisaninnocuousrequirementasitonlyexcludesdegeneratesamplingdistribu- tions.Formallyweaddthefollowingassumption. Assumption7.4 VÎ¸ =R (cid:48) VÎ²R>0. Assumption7.4statesthatVÎ¸ ispositivedefinite. SinceR isfullrankunderAssumption7.3asuffi- cientconditionisthatVÎ² >0. SinceQ XX >0asufficientconditionisâ¦>0. ThusAssumption7.4could bereplacedbytheassumptionâ¦>0.Assumption7.4isweakersothisiswhatweuse. Thustheasymptoticdistributionofthet-ratioT(Î¸)isstandardnormal. Sincethisdistributiondoes notdependontheparameterswesaythatT(Î¸)isasymptoticallypivotal. InfinitesamplesT(Î¸)isnot necessarilypivotalbutthepropertymeansthatthedependenceonunknownsdiminishesasnincreases.",
    "page": 199,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 180 Itisalsousefultoconsiderthedistributionoftheabsolutet-ratio|T(Î¸)|.SinceT(Î¸)ââZ thecon- d tinuous mapping theorem yields |T(Î¸)| ââ |Z|. Letting Î¦(u) = (cid:80)[Z â¤u] denote the standard normal d distributionfunctionwecalculatethatthedistributionof|Z|is (cid:80)[|Z|â¤u]=(cid:80)[âuâ¤Z â¤u] =(cid:80)[Z â¤u]â(cid:80)[Z <âu] =Î¦(u)âÎ¦(âu) =2Î¦(u)â1. (7.34) Theorem7.11 UnderAssumptions7.2, 7.3, and7.4, T(Î¸)ââZ â¼N(0,1)and d |T(Î¸)|ââ|Z|. d The asymptotic normality of Theorem 7.11 is used to justify confidence intervals and tests for the parameters. 7.13 ConfidenceIntervals TheestimatorÎ¸ (cid:98)isapointestimatorforÎ¸meaningthatÎ¸ (cid:98)isasinglevaluein(cid:82)q. Abroaderconcept isasetestimatorC(cid:98)whichisacollectionofvaluesin(cid:82)q.WhentheparameterÎ¸isreal-valuedthenitis commontofocusonsetsoftheformC(cid:98) =[L(cid:98),U(cid:98)]whichiscalledanintervalestimatorforÎ¸. AnintervalestimateC(cid:98)isafunctionofthedataandhenceisrandom.Thecoverageprobabilityofthe intervalC(cid:98) =[L(cid:98),U(cid:98)]is(cid:80)(cid:163)Î¸âC(cid:98) (cid:164) .TherandomnesscomesfromC(cid:98)astheparameterÎ¸istreatedasfixed. In Section5.10weintroducedconfidenceintervalsforthenormalregressionmodelwhichusedthefinite sampledistributionofthet-statistic. Whenweareoutsidethenormalregressionmodelwecannotrely ontheexactnormaldistributiontheorybutinsteaduseasymptoticapproximations.Abenefitisthatwe canconstructconfidenceintervalsforgeneralparametersofinterestÎ¸notjustregressioncoefficients. An interval estimatorC(cid:98) is called a confidence interval when the goal is to set the coverage prob- ability to equal a pre-specified target such as 90% or 95%. C(cid:98) is called a 1âÎ± confidence interval if infÎ¸ (cid:80) Î¸ (cid:163)Î¸âC(cid:98) (cid:164)=1âÎ±. WhenÎ¸ (cid:98)isasymptoticallynormalwithstandarderrors(Î¸ (cid:98))theconventionalconfidenceintervalforÎ¸ takestheform C(cid:98) =(cid:163)Î¸ (cid:98) âcÃs(Î¸ (cid:98)), Î¸ (cid:98) +cÃs(Î¸ (cid:98)) (cid:164) (7.35) wherecequalsthe1âÎ±quantileofthedistributionof|X|.Using(7.34)wecalculatethatcisequivalently the1âÎ±/2quantileofthestandardnormaldistribution.Thus,c solves 2Î¦(c)â1=1âÎ±. Thiscanbecomputedby, forexample, norminv(1-Î±/2)inMATLAB.Theconfidenceinterval(7.35)is symmetricaboutthepointestimatorÎ¸ (cid:98)anditslengthisproportionaltothestandarderrors(Î¸ (cid:98)). Equivalently, (7.35) is the set of parameter values for Î¸ such that the t-statistic T(Î¸) is smaller (in absolutevalue)thanc,thatis (cid:40) (cid:41) Î¸âÎ¸ (cid:98) C(cid:98) ={Î¸:|T(Î¸)|â¤c}= Î¸:âcâ¤ â¤c . s(Î¸ (cid:98))",
    "page": 200,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 181 Thecoverageprobabilityofthisconfidenceintervalis (cid:80)(cid:163)Î¸âC(cid:98) (cid:164)=(cid:80)[|T(Î¸)|â¤c]â(cid:80)[|Z|â¤c]=1âÎ± wherethelimitistakenasnââ,andholdssinceT(Î¸)isasymptotically|X|byTheorem7.11. Wecall thelimittheasymptoticcoverageprobabilityandcallC(cid:98)anasymptotic1âÎ±%confidenceintervalfor Î¸. Sincethet-ratioisasymptoticallypivotaltheasymptoticcoverageprobabilityisindependentofthe parameterÎ¸. Itisusefultocontrasttheconfidenceinterval(7.35)with(5.8)forthenormalregressionmodel.They aresimilarbuttherearedifferences. Thenormalregressioninterval(5.8)onlyappliestoregressionco- efficientsÎ²nottofunctionsÎ¸ ofthecoefficients. Thenormalinterval(5.8)alsoisconstructedwiththe homoskedasticstandarderror, while(7.35)canbeconstructedwithaheteroskedastic-robuststandard error. Furthermore, the constants c in (5.8) are calculated using the student t distribution, while c in (7.35) are calculated using the normal distribution. The difference between the student t and normal values are typically small in practice (since sample sizes are large in typical economic applications). However, since the student t values are larger it results in slightly larger confidence intervals which is reasonable. (A practical rule of thumb is that if the sample sizes are sufficiently small that it makes a differencethenneither(5.8)nor(7.35)shouldbetrusted.) Despitethesedifferencesthecoincidenceof theintervalsmeansthatinferenceonregressioncoefficientsisgenerallyrobusttousingeithertheexact normalsamplingassumptionortheasymptoticlargesampleapproximation,atleastinlargesamples. Statabydefaultreports95%confidenceintervalsforeachcoefficientwherethecriticalvaluesc are calculatedusingthet nâk distribution. Thisisdoneforallstandarderrormethodseventhoughitisonly exactforhomoskedasticstandarderrorsandundernormality. Thestandardcoverageprobabilityforconfidenceintervalsis95%,leadingtothechoicec=1.96for the constantin(7.35). Rounding 1.96 to 2, we obtainthe most commonly used confidence interval in appliedeconometricpractice C(cid:98) =(cid:163)Î¸ (cid:98) â2s(Î¸ (cid:98)), Î¸ (cid:98) +2s(Î¸ (cid:98)) (cid:164) . This is a useful rule-of thumb. This asymptotic 95% confidence interval C(cid:98) is simple to compute and canberoughlycalculatedfromtablesofcoefficientestimatesandstandarderrors. (Technically,itisan asymptotic95.4%intervalduetothesubstitutionof2.0for1.96butthisdistinctionisoverlyprecise.) Theorem7.12 UnderAssumptions7.2,7.3and7.4,forC(cid:98)definedin(7.35)with c=Î¦â1(1âÎ±/2),(cid:80)(cid:163)Î¸âC(cid:98) (cid:164)â1âÎ±.Forc=1.96,(cid:80)(cid:163)Î¸âC(cid:98) (cid:164)â0.95. Confidenceintervalsareasimpleyeteffectivetooltoassessestimationuncertainty. Whenreading asetofempiricalresultslookattheestimatedcoefficientestimatesandthestandarderrors. Forapa- rameter of interest compute the confidence intervalC(cid:98) and consider the meaning of the spread of the suggestedvalues",
    "page": 201,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Confidenceintervalsareasimpleyeteffectivetooltoassessestimationuncertainty. Whenreading asetofempiricalresultslookattheestimatedcoefficientestimatesandthestandarderrors. Forapa- rameter of interest compute the confidence intervalC(cid:98) and consider the meaning of the spread of the suggestedvalues. IftherangeofvaluesintheconfidenceintervalaretoowidetolearnaboutÎ¸thendo notjumptoaconclusionaboutÎ¸basedonthepointestimatealone. Forillustration,considerthethreeexamplespresentedinSection7.11basedonthelogwageregres- sionformarriedBlackwomen. Percentage return to education. A 95% asymptotic confidence interval is 11.8Â±1.96Ã0.8=[10.2, 13.3]. Percentagereturntoexperienceforindividualswith10yearsexperience. A90%asymptoticconfi- denceintervalis1.1Â±1.645Ã0.4=[0.5,1.8]. Experience level which maximizes expected log wages. An 80% asymptotic confidence interval is 35Â±1.28Ã7=[26,44].",
    "page": 201,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 182 7.14 RegressionIntervals InthelinearregressionmodeltheconditionalexpectationofY givenX =xis m(x)=(cid:69)[Y |X =x]=x (cid:48)Î². Insomecaseswewanttoestimate m(x)ataparticularpoint x.Noticethatthisisalinearfunctionof (cid:113) Î². Letting r(Î²)= x (cid:48)Î² and Î¸ =r(Î²) we see that m (cid:98) (x)=Î¸ (cid:98) = x (cid:48)Î² (cid:98)and R = x so s(Î¸ (cid:98))= x (cid:48) V(cid:98)Î²(cid:98) x. Thus an asymptotic95%confidenceintervalform(x)is (cid:104) (cid:113) (cid:105) x (cid:48)Î² (cid:98) Â±1.96 x (cid:48) V(cid:98)Î²(cid:98) x . Itisinterestingtoobservethatifthisisviewedasafunctionofx thewidthoftheconfidenceintervalis dependentonx. To illustrate we return to the log wage regression (3.12) of Section 3.7. The estimated regression equationis lo(cid:225)g(wage)=x (cid:48)Î² (cid:98) =0.155x+0.698 wherex=education.Thecovariancematrixestimatefrom(4.38)is (cid:181) 0.001 â0.015 (cid:182) V(cid:98)Î²(cid:98) = â0.015 0.243 . Thusthe95%confidenceintervalfortheregressionis (cid:112) 0.155x+0.698Â±1.96 0.001x2â0.030x+0.243. Theestimatedregressionand95%intervalsareshowninFigure7.4(a). Noticethattheconfidence bandstakeahyperbolicshape. Thismeansthattheregressionlineislesspreciselyestimatedforlarge andsmallvaluesofeducation. 0.4 5.3 0.3 5.2 0.2 5.1 0.1 Education )egaw(gol 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 (a)WageonEducation 8.2 7.2 6.2 5.2 4.2 3.2 2.2 Experience )egaw(gol 0 5 10 15 20 25 30 35 40 45 50 55 60 (b)WageonExperience Figure7.4:WageonEducationRegressionIntervals",
    "page": 202,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 183 Plotsoftheestimatedregressionlineandconfidenceintervalsareespeciallyusefulwhentheregres- sionincludesnonlinearterms.Toillustrateconsiderthelogwageregression(7.31)whichincludesexpe- rienceanditssquareandcovariancematrixestimate(7.32). Weareinterestedinplottingtheregression estimateandregressionintervalsasafunctionofexperience. Sincetheregressionalsoincludeseduca- tion,toplottheestimatesinasimplegraphwefixeducationataspecificvalue. Weselecteducation=12. Thisonlyaffectstheleveloftheestimatedregressionsinceeducationenterswithoutaninteraction.De- finethepointsofevaluation ï£« ï£¶ 12 ï£¬ x ï£· z(x)=ï£¬ ï£· ï£¬ x2/100 ï£· ï£­ ï£¸ 1 wherex=experience. The95%regressionintervalforeducation=12asafunctionofx=experienceis 0.118Ã12 +0.016xâ0.022x2/100+0.947 (cid:118) (cid:117) (cid:117) ï£« 0.632 0.131 â0.143 â11.1 ï£¶ (cid:117) (cid:117) ï£¬ 0.131 0.390 â0.731 â6.25 ï£· Â±1.96(cid:117)z(x) (cid:48)ï£¬ ï£·z(x)Ã10 â4 (cid:117) ï£¬ â0.143 â0.731 1.48 9.43 ï£· (cid:116) ï£­ ï£¸ â11.1 â6.25 9.43 246 =0.016xâ.00022x2+2.36 (cid:112) Â±0.0196 70.608â9.356x+0.54428x2â0.01462x3+0.000148x4. Theestimatedregressionand95%intervalsareshowninFigure7.4(b).Theregressionintervalwidens greatlyforsmallandlargevaluesofexperienceindicatingconsiderableuncertaintyabouttheeffectof experienceonmeanwagesforthispopulation. Theconfidencebandstakeamorecomplicatedshape thaninFigure7.4(a)duetothenonlinearspecification. 7.15 ForecastIntervals Supposewearegivenavalueoftheregressorvector X n+1 foranindividualoutsidethesampleand wewanttoforecast(guess)Y n+1 forthisindividual.ThisisequivalenttoforecastingY n+1 givenX n+1 =x whichwillgenerallybeafunctionofx.Areasonableforecastingruleistheconditionalexpectationm(x) asitisthemean-squareminimizingforecast. Apointforecastistheestimatedconditionalexpectation m (cid:98) (x)=x (cid:48)Î² (cid:98).Wewouldalsolikeameasureofuncertaintyfortheforecast. Theforecasterrorise (cid:98)n+1 =Y n+1 âm (cid:98) (x)=e n+1 âx (cid:48)(cid:161)Î² (cid:98) âÎ²(cid:162) . Astheout-of-sampleerrore n+1 isinde- pendentofthein-sampleestimatorÎ² (cid:98)thishasconditionalvariance (cid:69)(cid:163) e (cid:98)n 2 +1 |X n+1 =x (cid:164)=(cid:69) (cid:104) e n 2 +1 â2x (cid:48)(cid:161)Î² (cid:98) âÎ²(cid:162) e n+1 +x (cid:48)(cid:161)Î² (cid:98) âÎ²(cid:162)(cid:161)Î² (cid:98) âÎ²(cid:162)(cid:48) x|X n+1 =x (cid:105) =(cid:69)(cid:163) e n 2 +1 |X n+1 =x (cid:164)+x (cid:48)(cid:69) (cid:104) (cid:161)Î² (cid:98) âÎ²(cid:162)(cid:161)Î² (cid:98) âÎ²(cid:162)(cid:48)(cid:105) x =Ï2(x)+x (cid:48) V x (7.36) Î²(cid:98) Underhomoskedasticity(cid:69)(cid:163) e n 2 +1 |X n+1 (cid:164)=Ï2. Inthiscaseasimpleestimatorof(7.36)isÏ (cid:98) 2+x (cid:48) V Î²(cid:98) x soa (cid:113) standarderrorfortheforecastiss(x)= Ï2+x (cid:48) V x.Noticethatthisisdifferentfromthestandarderror (cid:98) (cid:98) Î²(cid:98) fortheconditionalexpectation.",
    "page": 203,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 184 Theconventional95%forecastintervalforY n+1 usesanormalapproximationandequals (cid:163) x (cid:48)Î² (cid:98) Â±2s (cid:98) (x) (cid:164) . Itisdifficult,however,tofullyjustifythischoice. Itwouldbecorrectifwehaveanormalapproximation totheratio e n+1 âx (cid:48)(cid:161)Î² (cid:98) âÎ²(cid:162) . s(x) (cid:98) Thedifficultyisthattheequationerrore n+1 isgenerallynon-normalandasymptotictheorycannotbe appliedtoasingleobservation.Theonlyspecialexceptionisthecasewheree n+1 hastheexactdistribu- tionN(0,Ï2)whichisgenerallyinvalid. Anaccurateforecastintervalwouldusetheconditionaldistributionofe n+1 givenX n+1 =x,whichis morechallengingtoestimate.Duetothisdifficultymanyappliedforecastersusethesimpleapproximate interval (cid:163) x (cid:48)Î² (cid:98) Â±2s (cid:98) (x) (cid:164) despitethelackofaconvincingjustification. 7.16 WaldStatistic Let Î¸ =r(Î²):(cid:82)k â(cid:82)q be any parameter vector of interest, Î¸ (cid:98)its estimator, and V(cid:98)Î¸(cid:98) its covariance matrixestimator.Considerthequadraticform W(Î¸)=(cid:161)Î¸ (cid:98) âÎ¸(cid:162)(cid:48) V(cid:98) â Î¸(cid:98) 1(cid:161)Î¸ (cid:98) âÎ¸(cid:162)=n (cid:161)Î¸ (cid:98) âÎ¸(cid:162)(cid:48) V(cid:98) â Î¸ 1(cid:161)Î¸ (cid:98) âÎ¸(cid:162) . (7.37) whereV(cid:98)Î¸ =nV(cid:98)Î¸(cid:98) .Whenq=1,thenW(Î¸)=T(Î¸)2isthesquareofthet-ratio.Whenq>1,W(Î¸)istypically calledaWaldstatisticasitwasproposedbyWald(1943).Weareinterestedinitssamplingdistribution. TheasymptoticdistributionofW(Î¸)issimpletoderivegivenTheorem7.9andTheorem7.10. They (cid:112) showthat n (cid:161)Î¸ (cid:98) âÎ¸(cid:162)ââZ â¼N(0,VÎ¸)andV(cid:98)Î¸ ââVÎ¸.Itfollowsthat d p (cid:112) (cid:112) W(Î¸)= n (cid:161)Î¸ (cid:98) âÎ¸(cid:162)(cid:48) V(cid:98) â Î¸ 1 n (cid:161)Î¸ (cid:98) âÎ¸(cid:162)ââZ (cid:48) V â Î¸ 1Z d aquadraticinthenormalrandomvectorZ.AsshowninTheorem5.3.5thedistributionofthisquadratic formisÏ2,achi-squarerandomvariablewithq degreesoffreedom. q Theorem7.13 UnderAssumptions7.2,7.3and7.4,asnââ,W(Î¸)ââÏ2. q d Theorem7.13isusedtojustifymultivariateconfidenceregionsandmultivariatehypothesistests. 7.17 HomoskedasticWaldStatistic Undertheconditionalhomoskedasticityassumption(cid:69)(cid:163) e2|X (cid:164)=Ï2wecanconstructtheWaldstatis- 0 ticusingthehomoskedasticcovariancematrixestimatorV(cid:98)Î¸definedin(7.29).Thisyieldsahomoskedas- ticWaldstatistic W0(Î¸)=(cid:161)Î¸ (cid:98) âÎ¸(cid:162)(cid:48)(cid:179) V(cid:98) 0 Î¸(cid:98) (cid:180)â1(cid:161)Î¸ (cid:98) âÎ¸(cid:162)=n (cid:161)Î¸ (cid:98) âÎ¸(cid:162)(cid:48)(cid:179) V(cid:98) 0 Î¸ (cid:180)â1(cid:161)Î¸ (cid:98) âÎ¸(cid:162) . (7.38) Undertheadditionalassumptionofconditionalhomoskedasticityithasthesameasymptoticdistri- butionasW(Î¸).",
    "page": 204,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 185 Theorem7.14 UnderAssumptions7.2,7.3,and(cid:69)(cid:163) e2|X (cid:164)=Ï2>0,asnââ, W0(Î¸)ââÏ2. q d 7.18 ConfidenceRegions AconfidenceregionC(cid:98)isasetestimatorforÎ¸â(cid:82)q whenq >1.AconfidenceregionC(cid:98)isasetin(cid:82)q intendedtocoverthetrueparametervaluewithapre-selectedprobability1âÎ±.Thusanidealconfidence regionhasthecoverageprobability(cid:80)(cid:163)Î¸âC(cid:98) (cid:164)=1âÎ±.Inpracticeitistypicallynotpossibletoconstructa regionwithexactcoveragebutwecancalculateitsasymptoticcoverage. When the parameter estimator satisfies the conditions of Theorem 7.13 a good choice for a confi- denceregionistheellipse C(cid:98) ={Î¸:W(Î¸)â¤c 1âÎ±} withc 1âÎ± the1âÎ±quantileoftheÏ2 q distribution. (ThusF q (c 1âÎ±)=1âÎ±.)Itcanbecomputedby,for example,chi2inv(1-Î±,q)inMATLAB. Theorem7.13implies (cid:104) (cid:105) (cid:80)(cid:163)Î¸âC(cid:98) (cid:164)â(cid:80) Ï2 q â¤c 1âÎ± =1âÎ± whichshowsthatC(cid:98)hasasymptoticcoverage1âÎ±. Toillustratetheconstructionofaconfidenceregion,considertheestimatedregression(7.31)of lo(cid:225)g(wage)=Î² 1 education+Î² 2 experience+Î² 3 experience2/100+Î² 4 . SupposethatthetwoparametersofinterestarethepercentagereturntoeducationÎ¸ =100Î² andthe 1 1 percentagereturntoexperienceforindividualswith10yearsexperienceÎ¸ =100Î² +20Î² . Thesetwo 2 2 3 parametersarealineartransformationoftheregressionparameterswithpointestimates (cid:181) (cid:182) (cid:181) (cid:182) 100 0 0 0 11.8 Î¸ (cid:98) = Î² (cid:98) = , 0 100 20 0 1.2 andhavethecovariancematrixestimate ï£« ï£¶ 0 0 (cid:181) 0 100 0 0 (cid:182) ï£¬ 100 0 ï£· (cid:181) 0.632 0.103 (cid:182) V(cid:98)Î¸(cid:98) = 0 0 100 20 V(cid:98)Î²(cid:98) ï£¬ ï£¬ 0 100 ï£· ï£· = 0.103 0.157 ï£­ ï£¸ 0 20 withinverse V(cid:98) â Î¸(cid:98) 1= (cid:181) â 1 1 .7 .1 7 6 â 7 1 .1 .1 3 6 (cid:182) . ThustheWaldstatisticis W(Î¸)=(cid:161)Î¸ (cid:98) âÎ¸(cid:162)(cid:48) V(cid:98) â Î¸(cid:98) 1(cid:161)Î¸ (cid:98) âÎ¸(cid:162) (cid:181) 11.8âÎ¸ (cid:182)(cid:48)(cid:181) 1.77 â1.16 (cid:182)(cid:181) 11.8âÎ¸ (cid:182) = 1 1 1.2âÎ¸ â1.16 7.13 1.2âÎ¸ 2 2 =1.77(11.8âÎ¸ )2â2.32(11.8âÎ¸ )(1.2âÎ¸ )+7.13(1.2âÎ¸ )2. 1 1 2 2",
    "page": 205,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 186 The90%quantileoftheÏ2 distributionis4.605(weusetheÏ2 distributionasthedimensionofÎ¸is 2 2 two)soanasymptotic90%confidenceregionforthetwoparametersistheinterioroftheellipseW(Î¸)= 4.605whichisdisplayedinFigure7.5. Sincetheestimatedcorrelationofthetwocoefficientestimatesis modest(about0.3)theregionismodestlyelliptical. l 5.2 0.2 5.1 0.1 5.0 0.0 Return to Education (%) )%( ecneirepxE ot nruteR ^ b l 10 11 12 13 14 Figure7.5:ConfidenceRegionforReturntoExperienceandReturntoEducation 7.19 EdgeworthExpansion* Theorem 7.11 showed that the t-ratio T(Î¸) is asymptotically normal. In practice this means that we use the normal distribution to approximate the finite sample distribution of T. How good is this approximation? Some insight into the accuracy of the normal approximation can be obtained by an Edgeworth expansion which is a higher-order approximation to the distribution of T. The following resultisanapplicationofTheorem9.11ofIntroductiontoEconometrics. Theorem7.15 Under Assumptions 7.2, 7.3, â¦ > 0, (cid:69)(cid:107)e(cid:107)16 < â, (cid:69)(cid:107)X(cid:107)16 < â, g (cid:161)Î²(cid:162) has five continuous derivatives in a neighborhood of Î², and (cid:69)(cid:163) exp (cid:161) t (cid:161)(cid:107)e(cid:107)4+(cid:107)X(cid:107)4(cid:162)(cid:162)(cid:164)â¤B<1,asnââ (cid:80)[T(Î¸)â¤x]=Î¦(x)+n â1/2p (x)Ï(x)+n â1p (x)Ï(x)+o (cid:161) n â1(cid:162) 1 2 uniformly in x, where p (x) is an even polynomial of order 2 and p (x) is an 1 2 oddpolynomialofdegree5withcoefficientsdependingonthemomentsofe andX uptoorder16. Theorem 7.15 shows that the finite sample distribution of the t-ratio can be approximated up to o(n â1)bythesumofthreeterms,thefirstbeingthestandardnormaldistribution,thesecondaO (cid:161) n â1/2(cid:162) adjustment,andthethirdaO (cid:161) n â1(cid:162) adjustment.",
    "page": 206,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 187 Consideraone-sidedconfidenceintervalC(cid:98) =(cid:163)Î¸ (cid:98) âz 1âÎ±s(Î¸ (cid:98)),â(cid:162) wherez 1âÎ±isthe1âÎ±th quantileof Z â¼N(0,1),thusÎ¦(z 1âÎ±)â1âÎ±.Then (cid:80)(cid:163)Î¸âC(cid:98) (cid:164)=(cid:80)[T(Î¸)â¤z 1âÎ±] =Î¦(z 1âÎ±)+n â1/2p 1 (z 1âÎ±)Ï(z 1âÎ±)+O (cid:161) n â1(cid:162) =1âÎ±+O (cid:161) n â1/2(cid:162) . ThismeansthattheactualcoverageiswithinO (cid:161) n â1/2(cid:162) ofthedesired1âÎ±level. Nowconsideratwo-sidedintervalC(cid:98) =(cid:163)Î¸ (cid:98) âz 1âÎ±/2 s(Î¸ (cid:98)),Î¸ (cid:98) +z 1âÎ±/2 s(Î¸ (cid:98)) (cid:164) .Ithascoverage (cid:80)(cid:163)Î¸âC(cid:98) (cid:164)=(cid:80)[|T(Î¸)|â¤z 1âÎ±/2 ] =2Î¦(z 1âÎ±/2 )â1+n â12p 2 (z 1âÎ±/2 )Ï(z 1âÎ±/2 )+o (cid:161) n â1(cid:162) =1âÎ±+O (cid:161) n â1(cid:162) . ThismeansthattheactualcoverageiswithinO (cid:161) n â1(cid:162) ofthedesired1âÎ±level. Theaccuracyisbetter thantheone-sidedintervalbecausetheO (cid:161) n â1/2(cid:162) termintheEdgeworthexpansionhasoffsettingeffects inthetwotailsofthedistribution. 7.20 UniformlyConsistentResiduals* Itseemsnaturaltoviewtheresidualse asestimatorsoftheunknownerrorse .Aretheyconsistent? (cid:98)i i Inthissectionwedevelopaconvergenceresult. Wecanwritetheresidualas e (cid:98)i =Y i âX i (cid:48)Î² (cid:98) =e i âX i (cid:48)(cid:161)Î² (cid:98) âÎ²(cid:162) . (7.39) SinceÎ² (cid:98) âÎ²ââ0itseemsreasonabletoguessthate (cid:98)i willbeclosetoe i ifnislarge. p Wecanboundthedifferencein(7.39)usingtheSchwarzinequality(B.12)tofind |e (cid:98)i âe i |=(cid:175) (cid:175)X i (cid:48)(cid:161)Î² (cid:98) âÎ²(cid:162)(cid:175) (cid:175) â¤(cid:107)X i (cid:107)(cid:176) (cid:176) Î² (cid:98) âÎ²(cid:176) (cid:176). (7.40) To bound (7.40) we can use (cid:176) (cid:176) Î² (cid:98) âÎ²(cid:176) (cid:176) = O p (cid:161) n â1/2(cid:162) from Theorem 7.3 but we also need to bound the random variable (cid:107)X i (cid:107). If the regressor is bounded, that is, (cid:107)X i (cid:107)â¤B <â, then |e (cid:98)i âe i |â¤B (cid:176) (cid:176) Î² (cid:98) âÎ²(cid:176) (cid:176) = O (cid:161) n â1/2(cid:162) .Howeveriftheregressordoesnothaveboundedsupportthenwehavetobemorecareful. p ThekeyisTheorem6.16whichshowsthat(cid:69)(cid:107)X(cid:107)r <âimpliesX =o (cid:161) n1/r(cid:162) uniformlyini,or i p n â1/r max (cid:107)X (cid:107)ââ0. i 1â¤iâ¤n p Appliedto(7.40)weobtain max |e (cid:98)i âe i |â¤ max (cid:107)X i (cid:107)(cid:176) (cid:176) Î² (cid:98) âÎ²(cid:176) (cid:176) =o p (cid:161) n â1/2+1/r(cid:162) . 1â¤iâ¤n 1â¤iâ¤n Wehaveshownthefollowing. Theorem7.16 UnderAssumption7.2and(cid:69)(cid:107)X(cid:107)r <â,then max |e âe |=o (cid:161) n â1/2+1/r(cid:162) . (7.41) (cid:98)i i p 1â¤iâ¤n",
    "page": 207,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 188 Therateofconvergencein(7.41)dependsonr.Assumption7.2requiresr â¥4sotherateofconver- genceisatleasto (cid:161) n â1/4(cid:162) .Asr increasestherateimproves. p WementionedinSection7.7thattherearemultiplewaystoprovetheconsistencyofthecovariance matrixestimatorâ¦ (cid:98).WenowshowthatTheorem7.16providesonesimplemethodtoestablish(7.23)and thusTheorem7.6.Letq n =max 1â¤iâ¤n |e (cid:98)i âe i |=o p (cid:161) n â1/4(cid:162) .Sincee (cid:98)i 2âe i 2=2e i (e (cid:98)i âe i )+(e (cid:98)i âe i )2,then (cid:176) (cid:176) (cid:176) (cid:176) (cid:176) (cid:176)n 1 i (cid:88) = n 1 X i X i (cid:48)(cid:161) e (cid:98)i 2âe i 2(cid:162) (cid:176) (cid:176) (cid:176) (cid:176) â¤ n 1 i (cid:88) = n 1 (cid:176) (cid:176)X i X i (cid:48)(cid:176) (cid:176) (cid:175) (cid:175)e (cid:98)i 2âe i 2(cid:175) (cid:175) â¤ 2 (cid:88) n (cid:107)X (cid:107)2|e ||e âe |+ 1 (cid:88) n (cid:107)X (cid:107)2|e âe |2 i i (cid:98)i i i (cid:98)i i n n i=1 i=1 â¤ 2 (cid:88) n (cid:107)X (cid:107)2|e |q + 1 (cid:88) n (cid:107)X (cid:107)2q2 n i i n n i n i=1 i=1 â¤o (cid:161) n â1/4(cid:162) . p 7.21 AsymptoticLeverage* Recallthedefinitionofleveragefrom(3.40)h =X (cid:48)(cid:161) X (cid:48) X (cid:162)â1 X . Thesearethediagonalelementsof ii i i theprojectionmatrixP andappearintheformulaforleave-one-outpredictionerrorsandHC2andHC3 covariancematrixestimators. Wecanshowthatunderi.i.d. samplingtheleveragevaluesareuniformly asymptoticallysmall. LetÎ» (A)andÎ» (A)denotethesmallestandlargesteigenvaluesofasymmetricsquarematrixA min max andnotethatÎ» (A â1)=(Î» (A)) â1. max min Since 1X (cid:48) X ââQ >0,bytheCMTÎ» (cid:161)1X (cid:48) X (cid:162)ââÎ» (cid:161) Q (cid:162)>0.(Thelatterispositivesince n p XX min n p min XX Q ispositivedefiniteandthusallitseigenvaluesarepositive.)ThenbytheQuadraticInequality(B.18) XX h =X (cid:48)(cid:161) X (cid:48) X (cid:162)â1 X ii i i â¤Î» (cid:179) (cid:161) X (cid:48) X (cid:162)â1 (cid:180) (cid:161) X (cid:48) X (cid:162) max i i = (cid:181) Î» (cid:181) 1 X (cid:48) X (cid:182)(cid:182)â1 1 (cid:107)X (cid:107)2 min i n n â¤(cid:161)Î» (cid:161) Q (cid:162)+o (1) (cid:162)â1 1 max (cid:107)X (cid:107)2. (7.42) min XX p i n1â¤iâ¤n (cid:181) (cid:182)2 Theorem 6.16 shows that (cid:69)(cid:107)X(cid:107)r <â implies max (cid:107)X (cid:107)2 = max (cid:107)X (cid:107) =o (cid:161) n2/r(cid:162) and thus (7.42) is i i p 1â¤iâ¤n 1â¤iâ¤n o (cid:161) n2/râ1(cid:162) . p Theorem7.17 If X is i.i.d., Q > 0, and (cid:69)(cid:107)X(cid:107)r < â for some r â¥ 2, then i XX max h =o (cid:161) n2/râ1(cid:162) . ii p 1â¤iâ¤n Foranyr â¥2thenh =o (1)(uniformlyini â¤n).Largerr impliesastrongerrateofconvergence. ii p Forexampler =4impliesh =o (cid:161) n â1/2(cid:162) . ii p",
    "page": 208,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 189 Theorem(7.17)impliesthatunderrandomsamplingwithfinitevariancesandlargesamplesnoindi- vidualobservationshouldhavealargeleveragevalue.Consequentlyindividualobservationsshouldnot beinfluentialunlessoneoftheseconditionsisviolated. _____________________________________________________________________________________________ 7.22 Exercises Exercise7.1 TakethemodelY =X (cid:48)Î² +X (cid:48)Î² +ewith(cid:69)[Xe]=0.SupposethatÎ² isestimatedbyregress- 1 1 2 2 1 ingY on X only. Findtheprobabilitylimitofthisestimator. Ingeneral, isitconsistentforÎ² ? Ifnot, 1 1 underwhatconditionsisthisestimatorconsistentforÎ² ? 1 Exercise7.2 TakethemodelY =X (cid:48)Î²+ewith(cid:69)[Xe]=0.Definetheridgeregressionestimator (cid:195) (cid:33)â1(cid:195) (cid:33) n n Î² (cid:98) = (cid:88) X i X i (cid:48)+Î»I k (cid:88) X i Y i (7.43) i=1 i=1 hereÎ»>0isafixedconstant.FindtheprobabilitylimitofÎ² (cid:98)asnââ.IsÎ² (cid:98)consistentforÎ²? Exercise7.3 Fortheridgeregressionestimator(7.43),setÎ»=cnwherec>0isfixedasnââ.Findthe probabilitylimitofÎ² (cid:98)asnââ. Exercise7.4 VerifysomeofthecalculationsreportedinSection7.4.Specifically,supposethatX andX 1 2 onlytakethevalues{â1,+1},symmetrically,with (cid:80)[X =X =1]=(cid:80)[X =X =â1]=3/8 1 2 1 2 (cid:80)[X =1,X =â1]=(cid:80)[X =â1,X =1]=1/8 1 2 1 2i (cid:69)(cid:163) e2|X =X (cid:164)= 5 i 1 2 4 (cid:69)(cid:163) e2|X (cid:54)=X (cid:164)= 1 . i 1 2 4 Verifythefollowing: (a) (cid:69)[X ]=0 1 (b) (cid:69)(cid:163) X2(cid:164)=1 1 1 (c) (cid:69)[X X ]= 1 2 2 (d) (cid:69)(cid:163) e2(cid:164)=1 (e) (cid:69)(cid:163) X2e2(cid:164)=1 1 (f) (cid:69)(cid:163) X X e2(cid:164)= 7 . 1 2 8 Exercise7.5 Show(7.13)-(7.16).",
    "page": 209,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 190 Exercise7.6 Themodelis Y =X (cid:48)Î²+e (cid:69)[Xe]=0 â¦=(cid:69)(cid:163) XX (cid:48) e2(cid:164) . Findthemethodofmomentsestimators(Î² (cid:98),â¦ (cid:98))for (cid:161)Î²,â¦(cid:162) . â â Exercise7.7 Ofthevariables(Y ,Y,X)onlythepair(Y,X)areobserved.InthiscasewesaythatY isa latentvariable.Suppose Y â=X (cid:48)Î²+e (cid:69)[Xe]=0 Y =Y â+u whereuisameasurementerrorsatisfying (cid:69)[Xu]=0 (cid:69)(cid:163) Y â u (cid:164)=0. LetÎ² (cid:98)denotetheOLScoefficientfromtheregressionofY onX. (a) IsÎ²thecoefficientfromthelinearprojectionofY onX? (b) IsÎ² (cid:98)consistentforÎ²asnââ? (cid:112) (c) Findtheasymptoticdistributionof n (cid:161)Î² (cid:98) âÎ²(cid:162) asnââ. (cid:112) Exercise7.8 Findtheasymptoticdistributionof n (cid:161)Ï2âÏ2(cid:162) asnââ. (cid:98) Exercise7.9 ThemodelisY =XÎ²+ewith(cid:69)[e|X]=0andX â(cid:82).Considerthetwoestimators (cid:80)n X Y Î²= i=1 i i (cid:98) (cid:80)n X2 i=1 i Î² (cid:101) = 1 (cid:88) n Y i . n X i=1 i (a) UnderthestatedassumptionsarebothestimatorsconsistentforÎ²? (b) Arethereconditionsunderwhicheitherestimatorisefficient? Exercise7.10 In the homoskedastic regression model Y = X (cid:48)Î²+e with (cid:69)[e|x]=0 and (cid:69)(cid:163) e2|X (cid:164)=Ï2 supposeÎ² (cid:98)istheOLSestimatorofÎ²withcovariancematrixestimatorV(cid:98)Î²(cid:98) basedonasampleofsizen.Let Ï (cid:98) 2betheestimatorofÏ2.Youwishtoforecastanout-of-samplevalueofY n+1 giventhatX n+1 =x.Thus theavailableinformationisthesample,theestimates(Î² (cid:98),V(cid:98)Î²(cid:98) ,Ï (cid:98) 2),theresidualse (cid:98)i ,andtheout-of-sample valueoftheregressorsX n+1 . (a) FindapointforecastofY n+1 . (b) Findanestimatorofthevarianceofthisforecast.",
    "page": 210,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 191 Exercise7.11 Takearegressionmodelwithi.i.d.observations(Y ,X )withX â(cid:82) i i Y =XÎ²+e (cid:69)[e|X]=0 â¦=(cid:69)(cid:163) X2e2(cid:164) . LetÎ² (cid:98)betheOLSestimatorofÎ²withresidualse (cid:98)i =Y i âX i Î² (cid:98).Considertheestimatorsofâ¦ â¦ (cid:101) = 1 (cid:88) n X2e2 n i i i=1 â¦ (cid:98) = n 1 (cid:88) n X i 2e (cid:98)i 2. i=1 (cid:112) (a) Findtheasymptoticdistributionof n (cid:161)â¦ (cid:101) ââ¦(cid:162) asnââ. (cid:112) (b) Findtheasymptoticdistributionof n (cid:161)â¦ (cid:98) ââ¦(cid:162) asnââ. (c) Howdoyouusetheregressionassumption(cid:69)[e |X ]=0inyouranswerto(b)? i i Exercise7.12 Considerthemodel Y =Î±+Î²X+e (cid:69)[e]=0 (cid:69)[Xe]=0 withbothY andX scalar. AssumingÎ±>0andÎ²<0supposetheparameterofinterestistheareaunder theregressioncurve(e.g.consumersurplus),whichis A=âÎ±2/2Î². (cid:112) LetÎ¸ (cid:98) =(Î± (cid:98) ,Î² (cid:98)) (cid:48) betheleastsquaresestimatorsofÎ¸=(Î±,Î²) (cid:48) sothat n (cid:161)Î¸ (cid:98) âÎ¸(cid:162)â d N(0,VÎ¸)andletV(cid:98)Î¸ beastandardestimatorforVÎ¸. (a) Giventheabove,describeanestimatorof A. (b) Constructanasymptotic1âÎ·confidenceintervalfor A. Exercise7.13 Considerani.i.d.sample{Y ,X }i =1,...,nwhereY andX arescalar.Considerthereverse i i projectionmodelX =YÎ³+uwith (cid:69)[Yu]=0anddefinetheparameterofinterestasÎ¸=1/Î³. (a) ProposeanestimatorÎ³ofÎ³. (cid:98) (b) ProposeanestimatorÎ¸ (cid:98)ofÎ¸. (c) FindtheasymptoticdistributionofÎ¸ (cid:98). (d) FindanasymptoticstandarderrorforÎ¸ (cid:98). Exercise7.14 Takethemodel Y =X Î² +X Î² +e 1 1 2 2 (cid:69)[Xe]=0 withbothÎ² â(cid:82)andÎ² â(cid:82),anddefinetheparameterÎ¸=Î² Î² . 1 2 1 2",
    "page": 211,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 192 (a) WhatistheappropriateestimatorÎ¸ (cid:98)forÎ¸? (b) FindtheasymptoticdistributionofÎ¸ (cid:98)understandardregularityconditions. (c) Showhowtocalculateanasymptotic95%confidenceintervalforÎ¸. Exercise7.15 TakethelinearmodelY =XÎ²+ewith(cid:69)[e|X]=0andX â(cid:82).Considertheestimator (cid:80)n X3Y Î² (cid:98) = i=1 i i . (cid:80)n X4 i=1 i (cid:112) Findtheasymptoticdistributionof n (cid:161)Î² (cid:98) âÎ²(cid:162) asnââ. Exercise7.16 Fromani.i.d. sample(Y ,X )ofsizenyourandomlytakehalftheobservations. Youesti- i i matealeastsquaresregressionofY onX usingonlythissub-sample. Istheestimatedslopecoefficient Î² (cid:98)consistentforthepopulationprojectioncoefficient?Explainyourreasoning. Exercise7.17 An economist reports a set of parameter estimates, including the coefficient estimates Î² (cid:98)1 =1.0,Î² (cid:98)2 =0.8,andstandarderrors s(Î² (cid:98)1 )=0.07and s(Î² (cid:98)2 )=0.07.TheauthorwritesâTheestimates showthatÎ² islargerthanÎ² .â 1 2 (a) Writedowntheformulaforanasymptotic95%confidenceintervalforÎ¸=Î² âÎ² ,expressedasa 1 2 functionofÎ² (cid:98)1 ,Î² (cid:98)2 ,s(Î² (cid:98)1 ),s(Î² (cid:98)2 )andÏ (cid:98) ,whereÏ (cid:98) istheestimatedcorrelationbetweenÎ² (cid:98)1 andÎ² (cid:98)2 . (b) CanÏbecalculatedfromthereportedinformation? (cid:98) (c) Istheauthorcorrect?Doesthereportedinformationsupporttheauthorâsclaim? Exercise7.18 Supposeaneconomicmodelsuggests m(x)=(cid:69)[Y |X =x]=Î² +Î² x+Î² x2 0 1 2 whereX â(cid:82).Youhavearandomsample(Y ,X ),i =1,...,n. i i (a) Describehowtoestimatem(x)atagivenvaluex. (b) Describe(bespecific)anappropriateconfidenceintervalform(x). Exercise7.19 TakethemodelY =X (cid:48)Î²+ewith(cid:69)[Xe]=0andsupposeyouhaveobservationsi =1,...,2n. (Thenumberofobservationsis2n.)Yourandomlysplitthesampleinhalf, (eachhasn observations), calculateÎ² (cid:98)1 byleastsquareson (cid:112) thefirstsample,andÎ² (cid:98)2 byleastsquaresonthesecondsample. Whatis theasymptoticdistributionof n (cid:161)Î² (cid:98)1 âÎ² (cid:98)2 (cid:162) ? Exercise7.20 Thevariables{Y ,X ,W }arearandomsample.TheparameterÎ²isestimatedbyminimiz- i i i ingthecriterionfunction n S(Î²)= (cid:88) W (cid:161) Y âX (cid:48)Î²(cid:162)2 i i i i=1 ThatisÎ² (cid:98) =argminÎ²S(Î²). (a) FindanexplicitexpressionforÎ² (cid:98).",
    "page": 212,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 193 (b) What population parameter Î² is Î² (cid:98)estimating? Be explicit about any assumptions you need to impose.Donotmakemoreassumptionsthannecessary. (c) FindtheprobabilitylimitforÎ² (cid:98)asnââ. (cid:112) (d) Findtheasymptoticdistributionof n (cid:161)Î² (cid:98) âÎ²(cid:162) asnââ. Exercise7.21 Takethemodel Y =X (cid:48)Î²+e (cid:69)[e|X]=0 (cid:69)(cid:163) e2|X (cid:164)=Z (cid:48)Î³ whereZ isa(vector)functionofX.Thesampleisi =1,...,nwithi.i.d.observations.AssumethatZ (cid:48)Î³>0 forallZ. SupposeyouwanttoforecastY n+1 givenX n+1 =x andZ n+1 =z foranout-of-sampleobserva- tionn+1.DescribehowyouwouldconstructapointforecastandaforecastintervalforY n+1 . Exercise7.22 Takethemodel Y =X (cid:48)Î²+e (cid:69)[e|X]=0 Z =X (cid:48)Î²Î³+u (cid:69)[u|X]=0 whereX isak vectorandZ isscalar. YourgoalistoestimatethescalarparameterÎ³.Youuseatwo-step estimator: â¢ EstimateÎ² (cid:98)byleastsquaresofY onX. â¢ EstimateÎ³ (cid:98) byleastsquaresofZ onX (cid:48)Î² (cid:98). (a) ShowthatÎ³isconsistentforÎ³. (cid:98) (b) FindtheasymptoticdistributionofÎ³whenÎ³=0. (cid:98) Exercise7.23 ThemodelisY =X+ewith(cid:69)[e|X]=0andX â(cid:82).Considertheestimator Î² (cid:101) = 1 (cid:88) n Y i . n X i=1 i FindconditionsunderwhichÎ² (cid:101)isconsistentforÎ²asnââ. Exercise7.24 TheparameterÎ²isdefinedinthemodelY =X âÎ²+e wheree isindependentof X ââ¥0, (cid:69)[e]=0,(cid:69)(cid:163) e2(cid:164)=Ï2.Theobservablesare(Y,X)whereX =X â v andv>0israndomscalemeasurement error,independentofX â ande.ConsidertheleastsquaresestimatorÎ² (cid:98)forÎ². (a) FindtheplimofÎ² (cid:98)expressedintermsofÎ²andmomentsof(X,v,e). (b) Canyou findanon-trivial conditionunderwhichÎ² (cid:98)isconsistentforÎ²?(Bynon-trivialwemean somethingotherthanv=1.)",
    "page": 213,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 194 Exercise7.25 Take the projection model Y = X (cid:48)Î²+e with (cid:69)[Xe]=0. For a positive function w(x) let W =w(X ).Considertheestimator i i (cid:195) (cid:33)â1(cid:195) (cid:33) n n Î² (cid:101) = (cid:88) W i X i X i (cid:48) (cid:88) W i X i Y i . i=1 i=1 Findtheprobabilitylimit(asnââ)ofÎ² (cid:101).Doyouneedtoaddanassumption? IsÎ² (cid:101)consistentforÎ² (cid:101)?If not,underwhatassumptionisÎ² (cid:101)consistentforÎ²? Exercise7.26 Taketheregressionmodel Y =X (cid:48)Î²+e (cid:69)[e|X]=0 (cid:69)(cid:163) e2|X =x (cid:164)=Ï2(x) withX â(cid:82)k.Assumethat(cid:80)[e=0]=0.Considertheinfeasibleestimator (cid:195) (cid:33)â1(cid:195) (cid:33) n n Î² (cid:101) = (cid:88) e i â2X i X i (cid:48) (cid:88) e i â2X i Y i . i=1 i=1 ThisisaWLSestimatorusingtheweightse â2. i (a) FindtheasymptoticdistributionofÎ² (cid:101). (b) ContrastyourresultwiththeasymptoticdistributionofinfeasibleGLS. Exercise7.27 ThemodelisY =X (cid:48)Î²+ewith(cid:69)[e|X]=0.Aneconometricianisworriedabouttheimpact ofsomeunusuallylargevaluesoftheregressors.Themodelisthusestimatedonthesubsampleforwhich |X i |â¤c forsomefixedc.LetÎ² (cid:101)denotetheOLSestimatoronthissubsample.Itequals (cid:195) (cid:33)â1(cid:195) (cid:33) n n Î² (cid:101) = (cid:88) X i X i (cid:48)1 {|X i |â¤c} (cid:88) X i Y i 1 {|X i |â¤c} . i=1 i=1 (a) ShowthatÎ² (cid:101) ââÎ². p (cid:112) (b) Findtheasymptoticdistributionof n (cid:161)Î² (cid:101) âÎ²(cid:162) . Exercise7.28 AsinExercise3.26,usethecps09mardatasetandthesubsampleofwhitemaleHispanics. Estimatetheregression lo(cid:225)g(wage)=Î² 1 education+Î² 2 experience+Î² 3 experience2/100+Î² 4 . (a) Reportthecoefficientestimatesandrobuststandarderrors. (b) LetÎ¸betheratioofthereturntooneyearofeducationtothereturntooneyearofexperiencefor experience=10.WriteÎ¸asafunctionoftheregressioncoefficientsandvariables.ComputeÎ¸ (cid:98)from theestimatedmodel. (c) WriteouttheformulafortheasymptoticstandarderrorforÎ¸ (cid:98)asafunctionofthecovariancematrix forÎ² (cid:98).Computes(Î¸ (cid:98))fromtheestimatedmodel.",
    "page": 214,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER7. ASYMPTOTICTHEORYFORLEASTSQUARES 195 (d) Constructa90%asymptoticconfidenceintervalforÎ¸fromtheestimatedmodel. (e) Computetheregressionfunctionateducation=12andexperience=20.Computea95%confidence intervalfortheregressionfunctionatthispoint. (f) Consideranout-of-sampleindividualwith16yearsofeducationand5yearsexperience.Construct an80%forecastintervalfortheirlogwageandwage. [Toobtaintheforecastintervalforthewage, applytheexponentialfunctiontobothendpoints.]",
    "page": 215,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Chapter 8 Restricted Estimation 8.1 Introduction Inthelinearprojectionmodel Y =X (cid:48)Î²+e (cid:69)[Xe]=0 a common task is to impose a constraint on the coefficient vector Î². For example, partitioning X (cid:48) = (cid:161) X (cid:48) ,X (cid:48)(cid:162) andÎ²(cid:48)=(cid:161)Î²(cid:48) ,Î²(cid:48)(cid:162) atypicalconstraintisanexclusionrestrictionoftheformÎ² =0.Inthiscase 1 2 1 2 2 theconstrainedmodelis Y =X (cid:48)Î² +e 1 1 (cid:69)[Xe]=0. Atfirstglancethisappearsthesameasthelinearprojectionmodelbutthereisoneimportantdifference: theerrore isuncorrelatedwiththeentireregressorvector X (cid:48)=(cid:161) X (cid:48) ,X (cid:48)(cid:162) notjusttheincludedregressor 1 2 X . 1 Ingeneral,asetofq linearconstraintsonÎ²takestheform R (cid:48)Î²=c (8.1) where R is kÃq, rank(R) = q < k, and c is qÃ1. The assumption that R is full rank means that the constraints are linearly independent (there are no redundant or contradictory constraints). We define therestrictedparameterspaceB asthesetofvaluesofÎ²whichsatisfy(8.1),thatis B=(cid:169)Î²:R (cid:48)Î²=c (cid:170) . Sometimes we will call (8.1) a constraint and sometimes a restriction. They are the same thing. Similarlysometimeswewillcallestimatorswhichsatisfy(8.1)constrainedestimatorsandsometimes restrictedestimators.Theymeanthesamething. TheconstraintÎ² =0discussedaboveisaspecialcaseoftheconstraint(8.1)with 2 (cid:181) (cid:182) 0 R= , (8.2) I k2 aselectormatrix,andc=0. 196",
    "page": 216,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER8. RESTRICTEDESTIMATION 197 Anothercommonrestrictionisthatasetofcoefficientssumtoaknownconstant, i.e. Î² +Î² =1. 1 2 Forexample,thisconstraintarisesinaconstant-return-to-scaleproductionfunction. Othercommon restrictionsincludetheequalityofcoefficientsÎ² =Î² ,andequalandoffsettingcoefficientsÎ² =âÎ² . 1 2 1 2 Atypicalreasontoimposeaconstraintisthatwebelieve(orhaveinformation)thattheconstraint is true. By imposing the constraint we hope to improve estimation efficiency. The goal is to obtain consistentestimateswithreducedvariancerelativetotheunconstrainedestimator. Thequestionsthenarise: Howshouldweestimatethecoefficientvector Î²imposingthelinearre- striction(8.1)?Ifweimposesuchconstraintswhatisthesamplingdistributionoftheresultingestimator? Howshouldwecalculatestandarderrors?Thesearethequestionsexploredinthischapter. 8.2 ConstrainedLeastSquares Anintuitivelyappealingmethodtoestimateaconstrainedlinearprojectionistominimizetheleast squarescriterionsubjecttotheconstraintR (cid:48)Î²=c. Theconstrainedleastsquaresestimatoris Î² (cid:101)cls =argminSSE(Î²) (8.3) R(cid:48)Î²=c where n SSE(Î²)= (cid:88)(cid:161) Y âX (cid:48)Î²(cid:162)2=Y (cid:48) Y â2Y (cid:48) XÎ²+Î²(cid:48) X (cid:48) XÎ². (8.4) i i i=1 TheestimatorÎ² (cid:101)cls minimizesthesumofsquarederrorsoverallÎ²suchthatÎ²âB,orequivalentlysuch that the restriction (8.1) holds. We call Î² (cid:101)cls the constrained least squares (CLS) estimator. We follow theconventionofusingatildeâ~âratherthanahatâ^âtoindicatethatÎ² (cid:101)cls isarestrictedestimatorin contrasttotheunrestrictedleastsquaresestimatorÎ² (cid:98)andwriteitasÎ² (cid:101)cls tobeclearthattheestimation methodisCLS. One methodto findthe solutionto (8.3)usesthetechniqueofLagrange multipliers. Theproblem (8.3)isequivalenttofindingthecriticalpointsoftheLagrangian L(Î²,Î»)= 1 SSE(Î²)+Î»(cid:48)(cid:161) R (cid:48)Î²âc (cid:162) (8.5) 2 over (Î²,Î») where Î» is an sÃ1 vector of Lagrange multipliers. The solution is a saddlepoint. The La- grangianisminimizedoverÎ²whilemaximizedoverÎ».Thefirst-orderconditionsforthesolutionof(8.5) are â âÎ² L(Î² (cid:101)cls ,Î» (cid:101)cls )=âX (cid:48) Y +X (cid:48) XÎ² (cid:101)cls +RÎ» (cid:101)cls =0 (8.6) and â âÎ» L(Î² (cid:101)cls ,Î» (cid:101)cls )=R (cid:48)Î² (cid:101) âc=0. (8.7) (cid:48)(cid:161) (cid:48) (cid:162)â1 Premultiplying(8.6)byR X X weobtain âR (cid:48)Î² (cid:98) +R (cid:48)Î² (cid:101)cls +R (cid:48)(cid:161) X (cid:48) X (cid:162)â1 RÎ» (cid:101)cls =0 whereÎ² (cid:98) =(cid:161) X (cid:48) X (cid:162)â1 X (cid:48) Y istheunrestrictedleastsquaresestimator.ImposingR (cid:48)Î² (cid:101)cls âc=0from(8.7)and solvingforÎ» (cid:101)cls wefind Î» (cid:101)cls = (cid:104) R (cid:48)(cid:161) X (cid:48) X (cid:162)â1 R (cid:105)â1(cid:161) R (cid:48)Î² (cid:98) âc (cid:162) .",
    "page": 217,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER8. RESTRICTEDESTIMATION 198 Noticethat (cid:161) X (cid:48) X (cid:162)â1>0andR fullrankimplythatR (cid:48)(cid:161) X (cid:48) X (cid:162)â1 R>0andishenceinvertible.(SeeSection A.10.) Substituting this expression into (8.6) and solving for Î² (cid:101)cls we find the solution to the constrained minimizationproblem(8.3) Î² (cid:101)cls =Î² (cid:98)ols â(cid:161) X (cid:48) X (cid:162)â1 R (cid:104) R (cid:48)(cid:161) X (cid:48) X (cid:162)â1 R (cid:105)â1(cid:161) R (cid:48)Î² (cid:98)ols âc (cid:162) . (8.8) (SeeExercise8.5toverifythat(8.8)satisfies(8.1).) ThisisageneralformulafortheCLSestimator.Italsocanbewrittenas Î² (cid:101)cls =Î² (cid:98)ols âQ(cid:98) â X 1 X R (cid:179) R (cid:48) Q(cid:98) â X 1 X R (cid:180)â1(cid:161) R (cid:48)Î² (cid:98)ols âc (cid:162) . (8.9) TheCLSresidualsaree (cid:101)i =Y i âX i (cid:48)Î² (cid:101)cls andarewritteninvectornotationas (cid:101) e. Toillustratewegeneratedarandomsampleof100observationsforthevariables(Y,X ,X )andcal- 1 2 culated the sum of squared errors function for the regression of Y on X and X . Figure 8.1 displays 1 2 contourplotsofthesumofsquarederrorsfunction. Thecenterofthecontourplotsistheleastsquares minimizerÎ² (cid:98)ols =(0.33,0.26) (cid:48) . Supposeitisdesiredtoestimatethecoefficientssubjecttotheconstraint Î² +Î² =1. Thisconstraintisdisplayedinthefigurebythestraightline. Theconstrainedleastsquares 1 2 estimatoristhepointonthisstraightlinewhichyieldsthesmallestsumofsquarederrors. Thisisthe point which intersects with the lowest contour plot. The solution is the point where a contour plot is tangenttotheconstraintlineandismarkedasÎ² (cid:101)cls =(0.52,0.48) (cid:48) . b 1 b 2 0.0 0.2 0.4 0.6 0.8 1.0 0.1 8.0 6.0 4.0 2.0 0.0 ~ b l cls l b^ ols Figure8.1:ImposingaConstraintontheLeastSquaresCriterion InStataconstrainedleastsquaresisimplementedusingthecnsregcommand. 8.3 ExclusionRestriction While(8.8)isageneralformulaforCLS,inmostcasestheestimatorcanbefoundbyapplyingleast squarestoareparameterizedequation. Toillustrateletusreturntothefirstexamplepresentedatthe",
    "page": 218,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER8. RESTRICTEDESTIMATION 199 beginningofthechapterâasimpleexclusionrestriction.Recallthattheunconstrainedmodelis Y =X (cid:48)Î² +X (cid:48)Î² +e, (8.10) 1 1 2 2 theexclusionrestrictionisÎ² =0,andtheconstrainedequationis 2 Y =X (cid:48)Î² +e. (8.11) 1 1 InthissettingtheCLSestimatorisOLSofY onX .(SeeExercise8.1.)Wecanwritethisas 1 (cid:195) (cid:33)â1(cid:195) (cid:33) n n Î² (cid:101)1 = (cid:88) X 1i X 1 (cid:48) i (cid:88) X 1i Y i . (8.12) i=1 i=1 TheCLSestimatoroftheentirevectorÎ²(cid:48)=(cid:161)Î²(cid:48) ,Î²(cid:48)(cid:162) is 1 2 (cid:181) Î² (cid:182) Î² (cid:101) = (cid:101)1 . (8.13) 0 Itisnotimmediatelyobviousbut(8.8)and(8.13)arealgebraicallyidentical. Toseethisthefirstcompo- nentof(8.8)with(8.2)is (cid:34) (cid:181) (cid:182)(cid:183) (cid:181) (cid:182)(cid:184)â1 (cid:35) Î² (cid:101)1 =(cid:161) I k2 0 (cid:162) Î² (cid:98) âQ(cid:98) â X 1 X I 0 (cid:161) 0 I k2 (cid:162) Q(cid:98) â X 1 X I 0 (cid:161) 0 I k2 (cid:162)Î² (cid:98) . k2 k2 Using(3.39)thisequals Î² (cid:101)1 =Î² (cid:98)1 âQ(cid:98) 12 (cid:179) Q(cid:98) 22 (cid:180)â1 Î² (cid:98)2 =Î² (cid:98)1 +Q(cid:98) â 11 1 Â·2 Q(cid:98)12 Q(cid:98) â 22 1 Q(cid:98)22Â·1 Î² (cid:98)2 =Q(cid:98) â 11 1 Â·2 (cid:179) Q(cid:98)1Y âQ(cid:98)12 Q(cid:98) â 22 1 Q(cid:98)2Y (cid:180) +Q(cid:98) â 11 1 Â·2 Q(cid:98)12 Q(cid:98) â 22 1 Q(cid:98)22Â·1 Q(cid:98) â 22 1 Â·1 (cid:179) Q(cid:98)2y âQ(cid:98)21 Q(cid:98) â 11 1 Q(cid:98)1Y (cid:180) =Q(cid:98) â 11 1 Â·2 (cid:179) Q(cid:98)1Y âQ(cid:98)12 Q(cid:98) â 22 1 Q(cid:98)21 Q(cid:98) â 11 1 Q(cid:98)1Y (cid:180) =Q(cid:98) â 11 1 Â·2 (cid:179) Q(cid:98)11 âQ(cid:98)12 Q(cid:98) â 22 1 Q(cid:98)21 (cid:180) Q(cid:98) â 11 1 Q(cid:98)1Y =Q(cid:98) â 11 1 Q(cid:98)1Y whichis(8.13)asoriginallyclaimed. 8.4 FiniteSampleProperties InthissectionweexploresomeofthepropertiesoftheCLSestimatorinthelinearregressionmodel Y =X (cid:48)Î²+e (8.14) (cid:69)[e|X]=0. (8.15) First,itisusefultowritetheestimatorandtheresidualsaslinearfunctionsoftheerrorvector.These arealgebraicrelationshipsanddonotrelyonthelinearregressionassumptions.",
    "page": 219,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER8. RESTRICTEDESTIMATION 200 Theorem8.1 TheCLSestimatorsatisfies 1. R (cid:48)Î² (cid:98) âc=R (cid:48)(cid:161) X (cid:48) X (cid:162)â1 X (cid:48) e 2. Î² (cid:101)cls âÎ²= (cid:179) (cid:161) X (cid:48) X (cid:162)â1 X (cid:48)âAX (cid:48) (cid:180) e 3. e=(cid:161) IâP+XAX (cid:48)(cid:162) e (cid:101) 4. I âP+XAX (cid:48) issymmetricandidempotent n 5. tr (cid:161) I âP+XAX (cid:48)(cid:162)=nâk+q n whereP=X (cid:161) X (cid:48) X (cid:162)â1 X (cid:48) and A=(cid:161) X (cid:48) X (cid:162)â1 R (cid:179) R (cid:48)(cid:161) X (cid:48) X (cid:162)â1 R (cid:180)â1 R (cid:48)(cid:161) X (cid:48) X (cid:162)â1 . ForaproofseeExercise8.6. GiventhelinearityofTheorem8.1.2itisnothardtoshowthattheCLSestimatorisunbiasedforÎ². Theorem8.2 In the linear regression model (8.14)-(8.15) under (8.1), (cid:69)(cid:163)Î² (cid:101)cls |X (cid:164)=Î². ForaproofseeExercise8.7. WecanalsocalculatethecovariancematrixofÎ² (cid:101)cls . First,forsimplicitytakethecaseofconditional homoskedasticity. Theorem8.3 Inthehomoskedasticlinearregressionmodel(8.14)-(8.15)with (cid:69)(cid:163) e2|X (cid:164)=Ï2,under(8.1), V0 Î²(cid:101) =var (cid:163)Î² (cid:101)cls |X (cid:164) = (cid:181) (cid:161) X (cid:48) X (cid:162)â1â(cid:161) X (cid:48) X (cid:162)â1 R (cid:179) R (cid:48)(cid:161) X (cid:48) X (cid:162)â1 R (cid:180)â1 R (cid:48)(cid:161) X (cid:48) X (cid:162)â1 (cid:182) Ï2. ForaproofseeExercise8.8. We use the V0 notation to emphasize that this is the covariance matrix under the assumption of Î²(cid:101) conditionalhomoskedasticity. ForinferenceweneedanestimateofV0.Anaturalestimatoris Î²(cid:101) V(cid:98) 0 Î²(cid:101) = (cid:181) (cid:161) X (cid:48) X (cid:162)â1â(cid:161) X (cid:48) X (cid:162)â1 R (cid:179) R (cid:48)(cid:161) X (cid:48) X (cid:162)â1 R (cid:180)â1 R (cid:48)(cid:161) X (cid:48) X (cid:162)â1 (cid:182) s c 2 ls where s2 = 1 (cid:88) n e2 (8.16) cls nâk+q (cid:101)i i=1",
    "page": 220,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER8. RESTRICTEDESTIMATION 201 isabiased-correctedestimatorofÏ2. StandarderrorsforthecomponentsofÎ²arethenfoundbytaking thesquaresrootsofthediagonalelementsofV(cid:98)Î²(cid:101) ,forexample (cid:114)(cid:104) (cid:105) s(Î² (cid:98)j )= V(cid:98) 0 Î²(cid:101) jj . Theestimator(8.16)hasthepropertythatitisunbiasedforÏ2underconditionalhomoskedasticity. Toseethis,usingthepropertiesofTheorem8.1, (cid:161) nâk+q (cid:162) s2 =e (cid:48) e cls (cid:101)(cid:101) =e (cid:48)(cid:161) I âP+XAX (cid:48)(cid:162)(cid:161) I âP+XAX (cid:48)(cid:162) e n n =e (cid:48)(cid:161) I âP+XAX (cid:48)(cid:162) e. (8.17) n WedefertheremainderoftheprooftoExercise8.9. Theorem8.4 Inthehomoskedasticlinearregressionmodel(8.14)-(8.15)with (cid:104) (cid:105) (cid:69)(cid:163) e2|X (cid:164)=Ï2,under(8.1),(cid:69)(cid:163) s c 2 ls |X (cid:164)=Ï2and(cid:69) V(cid:98) 0 Î²(cid:101) |X =V0 Î²(cid:101) . Now consider the distributional properties in the normal regression model Y = X (cid:48)Î²+e with e â¼ N(0,Ï2).BythelinearityofTheorem8.1.2,conditionalonX,Î² (cid:101)cls âÎ²isnormal.GivenTheorems8.2and (cid:179) (cid:180) 8.3wededucethatÎ² (cid:101)cls â¼N Î²,V0 Î²(cid:101) . Similarly,fromExericise8.1weknowe=(cid:161) I âP+XAX (cid:48)(cid:162) e islinearine soisalsoconditionallynor- (cid:101) n mal. Furthermore, since (cid:161) I n âP+XAX (cid:48)(cid:162) (cid:179) X (cid:161) X (cid:48) X (cid:162)â1âXA (cid:180) =0, (cid:101) e and Î² (cid:101)cls are uncorrelated and thus independent.Thuss c 2 ls andÎ² (cid:101)cls areindependent. From(8.17)andthefactthatI âP+XAX (cid:48) isidempotentwithranknâk+q itfollowsthat n s2 â¼Ï2Ï2 / (cid:161) nâk+q (cid:162) . cls nâk+q Itfollowsthatthet-statistichastheexactdistribution Î² âÎ² T = (cid:98) s j (Î² (cid:98)j ) j â¼ (cid:114) Ï2 N( (cid:46) 0, ( 1 n ) âk+q) â¼t nâk+q nâk+q astudentt distributionwithnâk+q degreesoffreedom. TherelevanceofthiscalculationisthattheâdegreesoffreedomâforCLSregressionequalnâk+q ratherthannâk asinOLS.Essentiallythemodelhaskâq freeparametersinsteadofk. Anotherway ofthinkingaboutthisisthatestimationofamodelwithk coefficientsandq restrictionsisequivalentto estimationwithkâq coefficients. Wesummarizethepropertiesofthenormalregressionmodel.",
    "page": 221,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER8. RESTRICTEDESTIMATION 202 Theorem8.5 In the normal linear regression model (8.14)-(8.15) with con- straint(8.1), (cid:179) (cid:180) Î² (cid:101)cls â¼N Î²,V0 Î²(cid:101) (cid:161) nâk+q (cid:162) s2 cls â¼Ï2 Ï2 nâk+q T â¼t nâk+q . Aninterestingrelationshipisthatinthehomoskedasticregressionmodel cov (cid:161)Î² (cid:98)ols âÎ² (cid:101)cls ,Î² (cid:101)cls |X (cid:162)=(cid:69) (cid:104) (cid:161)Î² (cid:98)ols âÎ² (cid:101)cls (cid:162)(cid:161)Î² (cid:101)cls âÎ²(cid:162)(cid:48) |X (cid:105) =(cid:69) (cid:104) AX (cid:48) ee (cid:48) (cid:179) X (cid:161) X (cid:48) X (cid:162)â1âXA (cid:180) |X (cid:105) =AX (cid:48) (cid:179) X (cid:161) X (cid:48) X (cid:162)â1âXA (cid:180) Ï2=0. ThismeansthatÎ² (cid:98)ols âÎ² (cid:101)cls andÎ² (cid:101)cls areconditionallyuncorrelatedandhenceindependent.Acorollaryis cov (cid:161)Î² (cid:98)ols ,Î² (cid:101)cls |X (cid:162)=var (cid:163)Î² (cid:101)cls |X (cid:164) . Asecondcorollaryis var (cid:163)Î² (cid:98)ols âÎ² (cid:101)cls |X (cid:164)=var (cid:163)Î² (cid:98)ols |X (cid:164)âvar (cid:163)Î² (cid:101)cls |X (cid:164) (8.18) =(cid:161) X (cid:48) X (cid:162)â1 R (cid:179) R (cid:48)(cid:161) X (cid:48) X (cid:162)â1 R (cid:180)â1 R (cid:48)(cid:161) X (cid:48) X (cid:162)â1Ï2. ThisalsoshowsthatthedifferencebetweentheCLSandOLSvariancesmatricesequals var (cid:163)Î² (cid:98)ols |X (cid:164)âvar (cid:163)Î² (cid:101)cls |X (cid:164)=(cid:161) X (cid:48) X (cid:162)â1 R (cid:179) R (cid:48)(cid:161) X (cid:48) X (cid:162)â1 R (cid:180)â1 R (cid:48)(cid:161) X (cid:48) X (cid:162)â1Ï2â¥0 thefinalequalitymeaningpositivesemi-definite. Itfollowsthatvar (cid:163)Î² (cid:98)ols |X (cid:164)â¥var (cid:163)Î² (cid:101)cls |X (cid:164) intheposi- tivedefinitesense,andthusCLSismoreefficientthanOLS.Bothestimatorsareunbiased(inthelinear regressionmodel)andCLShasalowercovariancematrix(inthelinearhomoskedasticregressionmodel). Therelationship(8.18)isratherinterestingandwillappearagain. Theexpressionsaysthatthevari- anceofthedifferencebetweentheestimatorsisequaltothedifferencebetweenthevariances. Thisis ratherspecial.Itoccursgenericallywhenwearecomparinganefficientandaninefficientestimator.We call(8.18)theHausmanEqualityasitwasfirstpointedoutineconometricsbyHausman(1978). 8.5 MinimumDistance The previous section explored the finite sample distribution theory under the assumptions of the linearregressionmodel,homoskedasticregressionmodel,andnormalregressionmodel.Wenowreturn tothegeneralprojectionmodelwherewedonotimposelinearity,homoskedasticity,nornormality. We areinterestedinthequestion:CanwedobetterthanCLSinthissetting? Aminimumdistanceestimatortriestofindaparametervaluesatisfyingtheconstraintwhichisas closeaspossibletotheunconstrainedestimator.LetÎ² (cid:98)betheunconstrainedleastsquaresestimator,and forsomekÃk positivedefiniteweightmatrixW(cid:99) >0definethequadraticcriterionfunction J (cid:161)Î²(cid:162)=n (cid:161)Î² (cid:98) âÎ²(cid:162)(cid:48) W(cid:99) (cid:161)Î² (cid:98) âÎ²(cid:162) . (8.19)",
    "page": 222,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER8. RESTRICTEDESTIMATION 203 Thisisa(squared)weightedEuclideandistancebetweenÎ² (cid:98)andÎ². J (cid:161)Î²(cid:162) issmallifÎ²isclosetoÎ² (cid:98),andis minimizedatzeroonlyifÎ²=Î² (cid:98).AminimumdistanceestimatorÎ² (cid:101)md forÎ²minimizesJ (cid:161)Î²(cid:162) subjecttothe constraint(8.1),thatis, Î² (cid:101)md =argmin J (cid:161)Î²(cid:162) . R(cid:48)Î²=c TheCLSestimatoristhespecialcasewhenW(cid:99) =Q(cid:98)XX andwewritethiscriterionfunctionas J0(cid:161)Î²(cid:162)=n (cid:161)Î² (cid:98) âÎ²(cid:162)(cid:48) Q(cid:98)XX (cid:161)Î² (cid:98) âÎ²(cid:162) . (8.20) ToseetheequalityofCLSandminimumdistancerewritetheleastsquarescriterionasfollows.Substitute theunconstrainedleastsquaresfittedequationY i =X i (cid:48)Î² (cid:98) +e (cid:98)i intoSSE(Î²)toobtain n SSE(Î²)= (cid:88)(cid:161) Y âX (cid:48)Î²(cid:162)2 i i i=1 n = (cid:88)(cid:161) X i (cid:48)Î² (cid:98) +e (cid:98)i âX i (cid:48)Î²(cid:162)2 i=1 (cid:195) (cid:33) = (cid:88) n e (cid:98)i 2+(cid:161)Î² (cid:98) âÎ²(cid:162)(cid:48) (cid:88) n X i X i (cid:48) (cid:161)Î² (cid:98) âÎ²(cid:162) i=1 i=1 =nÏ2+J0(cid:161)Î²(cid:162) (8.21) (cid:98) wherethethirdequalityusesthefactthat (cid:80)n i=1 X i e (cid:98)i =0,andthelastlineuses (cid:80)n i=1 X i X i (cid:48)=nQ(cid:98)XX .Theex- pression(8.21)onlydependsonÎ²throughJ0(cid:161)Î²(cid:162) .ThusminimizationofSSE(Î²)andJ0(cid:161)Î²(cid:162) areequivalent, andhenceÎ² (cid:101)md =Î² (cid:101)cls whenW(cid:99) =Q(cid:98)XX . WecansolveforÎ² (cid:101)md explicitlybythemethodofLagrangemultipliers.TheLagrangianis L(Î²,Î»)= 1 J (cid:161)Î²,W(cid:99) (cid:162)+Î»(cid:48)(cid:161) R (cid:48)Î²âc (cid:162) . 2 Thesolutiontothepairoffirstorderconditionsis Î» (cid:101)md =n (cid:179) R (cid:48) W(cid:99) â1 R (cid:180)â1(cid:161) R (cid:48)Î² (cid:98) âc (cid:162) (8.22) Î² (cid:101)md =Î² (cid:98) âW(cid:99) â1 R (cid:179) R (cid:48) W(cid:99) â1 R (cid:180)â1(cid:161) R (cid:48)Î² (cid:98) âc (cid:162) . (8.23) (See Exercise 8.10.) Comparing (8.23) with (8.9) we can see that Î² (cid:101)md specializes to Î² (cid:101)cls when we set W(cid:99) =Q(cid:98)XX . AnobviousquestioniswhichweightmatrixW(cid:99)isbest. Wewilladdressthisquestionafterwederive theasymptoticdistributionforageneralweightmatrix. 8.6 AsymptoticDistribution We first show that the class of minimum distance estimators are consistent for the population pa- rameterswhentheconstraintsarevalid. Assumption8.1 R (cid:48)Î²=c whereR iskÃq withrank(R)=q.",
    "page": 223,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER8. RESTRICTEDESTIMATION 204 Assumption8.2 W(cid:99) ââW >0. p Theorem8.6 Consistency UnderAssumptions7.1,8.1,and8.2,Î² (cid:101)md ââÎ²asnââ. p ForaproofseeExercise8.11. Theorem 8.6 shows that consistency holds for any weight matrix with a positive definite limit so includestheCLSestimator. Similarly,theconstrainedestimatorsareasymptoticallynormallydistributed. Theorem8.7 AsymptoticNormality UnderAssumptions7.2,8.1,and8.2, (cid:112) n (cid:161)Î² (cid:101)md âÎ²(cid:162)ââN (cid:161) 0,VÎ²(W) (cid:162) d asnââ,where VÎ²(W)=VÎ² âW â1R (cid:161) R (cid:48) W â1R (cid:162)â1 R (cid:48) VÎ² âVÎ²R (cid:161) R (cid:48) W â1R (cid:162)â1 R (cid:48) W â1 +W â1R (cid:161) R (cid:48) W â1R (cid:162)â1 R (cid:48) VÎ²R (cid:161) R (cid:48) W â1R (cid:162)â1 R (cid:48) W â1 (8.24) andVÎ² =Q â1 â¦Q â1 . XX XX ForaproofseeExercise8.12. Theorem 8.7 shows that the minimum distance estimator is asymptotically normal for all positive definiteweightmatrices. TheasymptoticvariancedependsonW. ThetheoremincludestheCLSesti- matorasaspecialcasebysettingW =Q . XX Theorem8.8 AsymptoticDistributionofCLSEstimator UnderAssumptions7.2and8.1,asnââ (cid:112) n (cid:161)Î² (cid:101)cls âÎ²(cid:162)ââN(0,V cls ) d where V cls =VÎ² âQ â X 1 X R (cid:161) R (cid:48) Q â X 1 X R (cid:162)â1 R (cid:48) VÎ² âVÎ²R (cid:161) R (cid:48) Q â1 R (cid:162)â1 R (cid:48) Q â1 XX XX +Q â1 R (cid:161) R (cid:48) Q â1 R (cid:162)â1 R (cid:48) VÎ²R (cid:161) R (cid:48) Q â1 R (cid:162)â1 R (cid:48) Q â1 . XX XX XX XX",
    "page": 224,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER8. RESTRICTEDESTIMATION 205 ForaproofseeExercise8.13. 8.7 VarianceEstimationandStandardErrors Earlierweintroducedthecovariancematrixestimatorundertheassumptionofconditionalhomoskedas- ticity.Wenowintroduceanestimatorwhichdoesnotimposehomoskedasticity. TheasymptoticcovariancematrixV cls maybeestimatedbyreplacingVÎ² withaconsistentestima- torsuchasV(cid:98)Î². Amoreefficientestimatorcanbeobtainedbyusingtherestrictedcoefficientestimator whichwenowshow. Giventheconstrainedleastsquaressquaresresidualse (cid:101)i =Y i âX i (cid:48)Î² (cid:101)cls wecanesti- matethematrixâ¦=(cid:69)(cid:163) XX (cid:48) e2(cid:164) by â¦ (cid:101) = nâk 1 +q (cid:88) n X i X i (cid:48) e (cid:101)i 2. i=1 Notice that we have used an adjusted degrees of freedom. This is an ad hoc adjustment designed to mimicthatusedforestimationoftheerrorvarianceÏ2.ThemomentestimatorofVÎ²is V(cid:101)Î² =Q(cid:98) â X 1 X â¦ (cid:101)Q(cid:98) â X 1 X andthatforV is cls V(cid:101)cls =V(cid:101)Î² âQ(cid:98) â X 1 X R (cid:179) R (cid:48) Q(cid:98) â X 1 X R (cid:180)â1 R (cid:48) V(cid:101)Î² âV(cid:101)Î²R (cid:179) R (cid:48) Q(cid:98) â X 1 X R (cid:180)â1 R (cid:48) Q(cid:98) â xx 1 +Q(cid:98) â X 1 X R (cid:179) R (cid:48) Q(cid:98) â X 1 X R (cid:180)â1 R (cid:48) V(cid:101)Î²R (cid:179) R (cid:48) Q(cid:98) â X 1 X R (cid:180)â1 R (cid:48) Q(cid:98) â X 1 X . We can calculate standard errors for any linear combination h (cid:48)Î² (cid:101)cls such that h does not lie in the rangespaceofR.Astandarderrorforh (cid:48)Î² (cid:101)is s (cid:161) h (cid:48)Î² (cid:101) cls (cid:162)=(cid:161) n â1h (cid:48) V(cid:101)cls h (cid:162)1/2 . 8.8 EfficientMinimumDistanceEstimator Theorem 8.7 shows that minimum distance estimators, which include CLS as a special case, are asymptotically normal with an asymptotic covariance matrix which depends on the weight matrixW. TheasymptoticallyoptimalweightmatrixistheonewhichminimizestheasymptoticvarianceVÎ²(W). ThisturnsouttobeW =V â1asisshowninTheorem8.9below.SinceV â1isunknownthisweightmatrix Î² Î² cannotbeusedforafeasibleestimatorbutwecanreplaceV â Î² 1withaconsistentestimatorV(cid:98) â Î² 1 andthe asymptoticdistribution(andefficiency)areunchanged. Wecalltheminimumdistanceestimatorwith W(cid:99) =V(cid:98) â Î² 1 theefficientminimumdistanceestimatorandtakestheform Î² (cid:101)emd =Î² (cid:98) âV(cid:98)Î²R (cid:161) R (cid:48) V(cid:98)Î²R (cid:162)â1(cid:161) R (cid:48)Î² (cid:98) âc (cid:162) . (8.25) Theasymptoticdistributionof(8.25)canbededucedfromTheorem8.7. (SeeExercises8.14and8.15, andtheproofinSection8.16.)",
    "page": 225,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER8. RESTRICTEDESTIMATION 206 Theorem8.9 EfficientMinimumDistanceEstimator UnderAssumptions7.2and8.1, (cid:112) n (cid:161)Î² (cid:101)emd âÎ²(cid:162)ââN (cid:161) 0,VÎ²,emd (cid:162) d asnââ,where VÎ²,emd =VÎ² âVÎ²R (cid:161) R (cid:48) VÎ²R (cid:162)â1 R (cid:48) VÎ². (8.26) Since VÎ²,emd â¤VÎ² (8.27) theestimator(8.25)haslowerasymptoticvariancethantheunrestrictedesti- mator.Furthermore,foranyW, VÎ²,emd â¤VÎ²(W) (8.28) so (8.25) is asymptotically efficient in the class of minimum distance estima- tors. Theorem8.9showsthattheminimumdistanceestimatorwiththesmallestasymptoticvarianceis (8.25).Oneimplicationisthattheconstrainedleastsquaresestimatorisgenerallyinefficient.Theinter- estingexceptionisthecaseofconditionalhomoskedasticityinwhichcasetheoptimalweightmatrixis (cid:179) (cid:180)â1 W = V0 sointhiscaseCLSisanefficientminimumdistanceestimator. Otherwisewhentheerror Î² isconditionallyheteroskedasticthereareasymptoticefficiencygainsbyusingminimumdistancerather thanleastsquares. ThefactthatCLSisgenerallyinefficientiscounter-intuitiveandrequiressomereflectiontounder- stand. Standardintuitionsuggeststoapplythesameestimationmethod(leastsquares)totheuncon- strainedandconstrainedmodelsandthisisthecommonempiricalpractice.ButTheorem8.9showsthat thisisinefficient.Why?Thereasonisthattheleastsquaresestimatordoesnotmakeuseoftheregressor X .Itignorestheinformation(cid:69)[X e]=0. Thisinformationisrelevantwhentheerrorisheteroskedastic 2 2 andtheexcludedregressorsarecorrelatedwiththeincludedregressors. Inequality(8.27)showsthattheefficientminimumdistanceestimatorÎ² (cid:101)emd hasasmallerasymptotic variancethantheunrestrictedleastsquaresestimatorÎ² (cid:98).Thismeansthatefficientestimationisattained byimposingcorrectrestrictionswhenweusetheminimumdistancemethod. 8.9 ExclusionRestrictionRevisited Wereturntotheexampleofestimationwithasimpleexclusionrestriction.Themodelis Y =X (cid:48)Î² +X (cid:48)Î² +e 1 1 2 2 with the exclusion restriction Î² = 0. We have introduced three estimators of Î² . The first is uncon- 2 1 strained least squares applied to (8.10) which can be written as Î² (cid:98)1 =Q(cid:98) â 11 1 Â·2 Q(cid:98)1YÂ·2 . From Theorem 7.25 andequation(7.14)itsasymptoticvarianceis avar (cid:163)Î² (cid:98)1 (cid:164)=Q â 11 1 Â·2 (cid:161)â¦ 11 âQ 12 Q â 22 1â¦ 21 ââ¦ 12 Q â 22 1Q 21 +Q 12 Q â 22 1â¦ 22 Q â 22 1Q 21 (cid:162) Q â 11 1 Â·2 .",
    "page": 226,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER8. RESTRICTEDESTIMATION 207 ThesecondestimatorofÎ² 1 isCLS,whichcanbewrittenasÎ² (cid:101)1 =Q(cid:98) â 11 1 Q(cid:98)1Y .Itsasymptoticvariancecan bededucedfromTheorem8.8,butitissimplertoapplytheCLTdirectlytoshowthat avar (cid:163)Î² (cid:101)1 (cid:164)=Q â 11 1â¦ 11 Q â 11 1. (8.29) ThethirdestimatorofÎ² isefficientminimumdistance.Applying(8.25),itequals 1 Î² 1 =Î² (cid:98)1 âV(cid:98)12 V(cid:98) â 22 1Î² (cid:98)2 (8.30) wherewehavepartitioned (cid:183) (cid:184) V(cid:98)Î² = V(cid:98)11 V(cid:98)12 . V(cid:98)21 V(cid:98)22 FromTheorem8.9itsasymptoticvarianceis (cid:104) (cid:105) avar Î² =V âV V â1V . (8.31) 1 11 12 22 21 SeeExercise8.16toverifyequations(8.29),(8.30),and(8.31). In general the three estimators are different and they have different asymptotic variances. It is in- structivetocomparethevariancestoassesswhetherornottheconstrainedestimatorismoreefficient thantheunconstrainedestimator. First, assume conditional homoskedasticity. In this case the two covariance matrices simplify to avar (cid:163)Î² (cid:98)1 (cid:164)=Ï2Q â 11 1 Â·2 and avar (cid:163)Î² (cid:101)1 (cid:164)=Ï2Q â 11 1. IfQ 12 =0 (so X 1 and X 2 are uncorrelated) then these two variancematricesareequalandthetwoestimatorshaveequalasymptoticefficiency. Otherwise,since Q Q â1Q â¥0,thenQ â¥Q âQ Q â1Q andconsequently 12 22 21 11 11 12 22 21 Q â1Ï2â¤(cid:161) Q âQ Q â1Q (cid:162)â1Ï2. 11 11 12 22 21 ThismeansthatunderconditionalhomoskedasticityÎ² (cid:101)1 hasalowerasymptoticcovariancematrixthan Î² (cid:98)1 .Thereforeinthiscontextconstrainedleastsquaresismoreefficientthanunconstrainedleastsquares. Thisisconsistentwithourintuitionthatimposingacorrectrestriction(excludinganirrelevantregressor) improvesestimationefficiency. However,inthegeneralcaseofconditionalheteroskedasticitythisrankingisnotguaranteed.Infact whatisreallyamazingisthatthevariancerankingcanbereversed. TheCLSestimatorcanhavealarger asymptoticvariancethantheunconstrainedleastsquaresestimator. To see this letâs use the simple heteroskedastic example from Section 7.4. In that example, Q = 11 1 7 3 Q 22 =1,Q 12 = ,â¦ 11 =â¦ 22 =1,andâ¦ 12 = .Wecancalculate(seeExercise8.17)thatQ 11Â·2 = and 2 8 4 avar (cid:163)Î² (cid:98)1 (cid:164)= 2 (8.32) 3 avar (cid:163)Î² (cid:101)1 (cid:164)=1 (8.33) (cid:104) (cid:105) 5 avar Î² = . (8.34) 1 8 Thus the CLS estimator Î² (cid:101)1 has a larger variance than the unrestricted least squares estimator Î² (cid:98)1 ! The minimumdistanceestimatorhasthesmallestvarianceofthethree,asexpected. What we have found is that when the estimation method is least squares, deleting the irrelevant variableX canactuallyincreaseestimationvariance,orequivalently,addinganirrelevantvariablecan 2 decreasetheestimationvariance.",
    "page": 227,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER8. RESTRICTEDESTIMATION 208 Torepeatthisunexpectedfinding,wehaveshownthatitispossibleforleastsquaresappliedtothe short regression (8.11) to be less efficient for estimation of Î² than least squares applied to the long 1 regression(8.10)eventhoughtheconstraintÎ² =0isvalid! Thisresultisstronglycounter-intuitive. It 2 seemstocontradictourinitialmotivationforpursuingconstrainedestimationâtoimproveestimation efficiency. Itturnsoutthatamorerefinedanswerisappropriate. Constrainedestimationisdesirablebutnot necessarilyCLS.Whileleastsquaresisasymptoticallyefficientforestimationoftheunconstrainedpro- jectionmodelitisnotanefficientestimatoroftheconstrainedprojectionmodel. 8.10 VarianceandStandardErrorEstimation WehavediscussedcovariancematrixestimationforCLSbutnotyetfortheEMDestimator. Theasymptoticcovariancematrix(8.26)maybeestimatedbyreplacingVÎ²withaconsistentestima- tor. It is best to construct the variance estimate using Î² (cid:101)emd . The EMD residuals are e (cid:101)i =Y i âX i (cid:48)Î² (cid:101)emd . Usingthesewecanestimatethematrixâ¦=(cid:69)(cid:163) XX (cid:48) e2(cid:164) by â¦ (cid:101) = nâk 1 +q (cid:88) n X i X i (cid:48) e (cid:101)i 2. i=1 Following the formula for CLS we recommend an adjusted degrees of freedom. Given â¦ (cid:101) the moment estimatorofVÎ²isV(cid:101)Î² =Q(cid:98) â X 1 X â¦ (cid:101)Q(cid:98) â X 1 X .Giventhis,weconstructthevarianceestimator V(cid:101)Î²,emd =V(cid:101)Î² âV(cid:101)Î²R (cid:161) R (cid:48) V(cid:101)Î²R (cid:162)â1 R (cid:48) V(cid:101)Î². (8.35) Astandarderrorforh (cid:48)Î² (cid:101)isthen s (cid:161) h (cid:48)Î² (cid:101) (cid:162)=(cid:161) n â1h (cid:48) V(cid:101)Î²,emd h (cid:162)1/2 . (8.36) 8.11 HausmanEquality Form(8.25)wehave (cid:112) (cid:112) n (cid:161)Î² (cid:98)ols âÎ² (cid:101)emd (cid:162)=V(cid:98)Î²R (cid:161) R (cid:48) V(cid:98)Î²R (cid:162)â1 n (cid:161) R (cid:48)Î² (cid:98)ols âc (cid:162) ââN (cid:179) 0,VÎ²R (cid:161) R (cid:48) VÎ²R (cid:162)â1 R (cid:48) VÎ² (cid:180) . d Itfollowsthattheasymptoticvariancesoftheestimatorssatisfytherelationship avar (cid:163)Î² (cid:98)ols âÎ² (cid:101)emd (cid:164)=avar (cid:163)Î² (cid:98)ols (cid:164)âavar (cid:163)Î² (cid:101)emd (cid:164) . (8.37) Wecall(8.37)theHausmanEquality:theasymptoticvarianceofthedifferencebetweenanefficientand anotherestimatoristhedifferenceintheasymptoticvariances. 8.12 Example: Mankiw,RomerandWeil(1992) We illustrate the methods by replicating some of the estimates reported in a well-known paper by Mankiw, Romer, and Weil (1992). The paper investigates the implications of the Solow growth model usingcross-countryregressions. Akeyequationintheirpaperregressesthechangebetween1960and 1985 in log GDP per capita on (1) log GDP in 1960, (2) the log of the ratio of aggregate investment to",
    "page": 228,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER8. RESTRICTEDESTIMATION 209 Table8.1:EstimatesofSolowGrowthModel Î² Î² Î² (cid:98)ols (cid:98)cls (cid:98)emd logGDP â0.29 â0.30 â0.30 1960 (0.05) (0.05) (0.05) log I 0.52 0.50 0.46 GDP (0.11) (0.09) (0.08) log (cid:161) n+g+Î´(cid:162) â0.51 â0.74 â0.71 (0.24) (0.08) (0.07) log(School) 0.23 0.24 0.25 (0.07) (0.07) (0.06) Intercept 3.02 2.46 2.48 (0.74) (0.44) (0.44) Standarderrorsareheteroskedasticity-consistent GDP,(3)thelogofthesumofthepopulationgrowthraten,thetechnologicalgrowthrateg,andtherate ofdepreciationÎ´,and(4)thelogofthepercentageoftheworking-agepopulationthatisinsecondary school(School),thelatteraproxyforhuman-capitalaccumulation. ThedataisavailableonthetextbookwebpageinthefileMRW1992. The sample is 98 non-oil-producing countries and the data was reported in the published paper. As g and Î´ were unknown the authors set g +Î´=0.05. We report least squares estimates in the first columnofTable8.1. TheestimatesareconsistentwiththeSolowtheoryduetothepositivecoefficients on investment and human capital and negative coefficient for population growth. The estimates are alsoconsistentwiththeconvergencehypothesis(thatincomelevelstendtowardsacommonmeanover time)asthecoefficientonintialGDPisnegative. TheauthorsshowthatintheSolowmodelthe2nd,3rd and4th coefficientssumtozero. Theyrees- timated the equation imposing this constraint. We present constrained least squares estimates in the secondcolumnofTable8.1andefficientminimumdistanceestimatesinthethirdcolumn. Mostofthe coefficientsandstandarderrorsonlyexhibitsmallchangesbyimposingtheconstraint. Theoneexcep- tion is the coefficient on log population growth which increases in magnitude and its standard error decreasessubstantially.ThedifferencesbetweentheCLSandEMDestimatesaremodest. WenowpresentStata,RandMATLABcodewhichimplementstheseestimates. YoumaynoticethattheStatacodehasasectionwhichusestheMatamatrixprogramminglanguage. ThisisusedbecauseStatadoesnotimplementtheefficientminimumdistanceestimator,soneedstobe separatelyprogrammed.Asillustratedhere,theMatalanguageallowsaStatausertoimplementmethods usingcommandswhicharequitesimilartoMATLAB.",
    "page": 229,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER8. RESTRICTEDESTIMATION 210 StatadoFile use\"MRW1992.dta\",clear genlndY=log(Y85)-log(Y60) genlnY60=log(Y60) genlnI=log(invest/100) genlnG=log(pop_growth/100+0.05) genlnS=log(school/100) *Unrestrictedregression reglndYlnY60lnIlnGlnSifN==1,r *Storeresultforefficientminimumdistance matb=e(b)â scalark=e(rank) matV=e(V) *Constrainedregression constraintdefine1lnI+lnG+lnS=0 cnsreglndYlnY60lnIlnGlnSifN==1,constraints(1)r *Efficientminimumdistance mata{ data=st_data(.,(\"lnY60\",\"lnI\",\"lnG\",\"lnS\",\"lndY\",\"N\")) data_select=select(data,data[.,6]:==1) y=data_select[.,5] n=rows(y) x=(data_select[.,1..4],J(n,1,1)) k=cols(x) invx=invsym(xâ*x) b_ols=st_matrix(\"b\") V_ols=st_matrix(\"V\") R=(0\\1\\1\\1\\0) b_emd=b_ols-V_ols*R*invsym(Râ*V_ols*R)*Râ*b_ols e_emd=J(1,k,y-x*b_emd) xe_emd=x:*e_emd xe_emdâ*xe_emd V2=(n/(n-k+1))*invx*(xe_emdâ*xe_emd)*invx V_emd=V2-V2*R*invsym(Râ*V2*R)*Râ*V2 se_emd=diagonal(sqrt(V_emd)) st_matrix(\"b_emd\",b_emd) st_matrix(\"se_emd\",se_emd)} matlistb_emd matlistse_emd",
    "page": 230,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER8. RESTRICTEDESTIMATION 211 RProgramFile data<-read.table(\"MRW1992.txt\",header=TRUE) N<-matrix(data$N,ncol=1) lndY<-matrix(log(data$Y85)-log(data$Y60),ncol=1) lnY60<-matrix(log(data$Y60),ncol=1) lnI<-matrix(log(data$invest/100),ncol=1) lnG<-matrix(log(data$pop_growth/100+0.05),ncol=1) lnS<-matrix(log(data$school/100),ncol=1) xx<-as.matrix(cbind(lnY60,lnI,lnG,lnS,matrix(1,nrow(lndY),1))) x<-xx[N==1,] y<-lndY[N==1] n<-nrow(x) k<-ncol(x) #Unrestrictedregression invx<-solve(t(x)%*%x) b_ols<-solve((t(x)%*%x),(t(x)%*%y)) e_ols<-rep((y-x%*%beta_ols),times=k) xe_ols<-x*e_ols V_ols<-(n/(n-k))*invx%*%(t(xe_ols)%*%xe_ols)%*%invx se_ols<-sqrt(diag(V_ols)) print(beta_ols) print(se_ols) #Constrainedregression R<-c(0,1,1,1,0) iR<-invx%*%R%*%solve(t(R)%*%invx%*%R)%*%t(R) b_cls<-b_ols-iR%*%b_ols e_cls<-rep((y-x%*%b_cls),times=k) xe_cls<-x*e_cls V_tilde<-(n/(n-k+1))*invx%*%(t(xe_cls)%*%xe_cls)%*%invx V_cls<-V_tilde-iR%*%V_tilde-V_tilde%*%t(iR)+iR%*%V_tilde%*%t(iR) print(b_cls)print(se_cls) #Efficientminimumdistance Vr<-V_ols%*%R%*%solve(t(R)%*%V_ols%*%R)%*%t(R) b_emd<-b_ols-Vr%*%b_ols e_emd<-rep((y-x%*%b_emd),times=k) xe_emd<-x*e_emd V2<-(n/(n-k+1))*invx%*%(t(xe_emd)%*%xe_emd)%*%invx V_emd<-V2-V2%*%R%*%solve(t(R)%*%V2%*%R)%*%t(R)%*%V2 se_emd<-sqrt(diag(V_emd))",
    "page": 231,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER8. RESTRICTEDESTIMATION 212 MATLABProgramFile data=xlsread(âMRW1992.xlsxâ); N=data(:,1); Y60=data(:,4); Y85=data(:,5); pop_growth=data(:,7); invest=data(:,8); school=data(:,9); lndY=log(Y85)-log(Y60); lnY60=log(Y60); lnI=log(invest/100); lnG=log(pop_growth/100+0.05); lnS=log(school/100); xx=[lnY60,lnI,lnG,lnS,ones(size(lndY,1),1)]; x=xx(N==1,:); y=lndY(N==1); [n,k]=size(x); %Unrestrictedregression invx=inv(xâ*x); beta_ols=(xâ*x)\\(xâ*y); e_ols=repmat((y-x*beta_ols),1,k); xe_ols=x.*e_ols; V_ols=(n/(n-k))*invx*(xe_olsâ*xe_ols)*invx; se_ols=sqrt(diag(V_ols)); display(beta_ols); display(se_ols); %Constrainedregression R=[0;1;1;1;0]; iR=invx*R*inv(Râ*invx*R)*Râ; beta_cls=beta_ols-iR*beta_ols; e_cls=repmat((y-x*beta_cls),1,k); xe_cls=x.*e_cls; V_tilde=(n/(n-k+1))*invx*(xe_clsâ*xe_cls)*invx; V_cls=V_tilde-iR*V_tilde-V_tilde*(iRâ)+iR*V_tilde*(iRâ); se_cls=sqrt(diag(V_cls)); display(beta_cls);display(se_cls); %Efficientminimumdistance beta_emd=beta_ols-V_ols*R*inv(Râ*V_ols*R)*Râ*beta_ols; e_emd=repmat((y-x*beta_emd),1,k); xe_emd=x.*e_emd; V2=(n/(n-k+1))*invx*(xe_emdâ*xe_emd)*invx; V_emd=V2-V2*R*inv(Râ*V2*R)*Râ*V2; se_emd=sqrt(diag(V_emd)); display(beta_emd); display(se_emd);",
    "page": 232,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER8. RESTRICTEDESTIMATION 213 8.13 Misspecification WhataretheconsequencesforaconstrainedestimatorÎ² (cid:101)iftheconstraint(8.1)isincorrect? Tobe specificsupposethatthetruthis R (cid:48)Î²=c â â wherec isnotnecessarilyequaltoc. ThissituationisageneralizationoftheanalysisofâomittedvariablebiasâfromSection2.24where wefoundthattheshortregression(e.g. (8.12))isestimatingadifferentprojectioncoefficientthanthe longregression(e.g.(8.10)). Oneansweristoapplyformula(8.23)tofindthat Î² (cid:101)md â p âÎ²â md =Î²âW â1R (cid:161) R (cid:48) W â1R (cid:162)â1(cid:161) c ââc (cid:162) . (8.38) Thesecondterm,W â1R (cid:161) R (cid:48) W â1R (cid:162)â1 (c ââc), showsthatimposinganincorrectconstraintleadstoin- consistencyâanasymptoticbias. WecancallthelimitingvalueÎ²â theminimum-distanceprojection md coefficientorthepseudo-truevalueimpliedbytherestriction. However,wecansaymore. Forexample,wecandescribesomecharacteristicsoftheapproximatingprojections. TheCLSesti- matorprojectioncoefficienthastherepresentation (cid:104) (cid:105) Î²â =argmin(cid:69) (cid:161) Y âX (cid:48)Î²(cid:162)2 , cls R(cid:48)Î²=c thebestlinearpredictorsubjecttotheconstraint(8.1). Theminimumdistanceestimatorconvergesin probabilityto Î²â =argmin (cid:161)Î²âÎ² (cid:162)(cid:48) W (cid:161)Î²âÎ² (cid:162) md 0 0 R(cid:48)Î²=c whereÎ² isthetruecoefficient. Thatis, Î²â isthecoefficientvectorsatisfying(8.1)closesttothetrue 0 md valueintheweightedEuclideannorm. Thesecalculationsshowthattheconstrainedestimatorsarestill reasonable in the sense that they produce good approximations to the true coefficient conditional on beingrequiredtosatisfytheconstraint. WecanalsoshowthatÎ² (cid:101)md hasanasymptoticnormaldistribution.Thetrickistodefinethepseudo- truevalue Î²â n =Î²âW(cid:99) â1 R (cid:179) R (cid:48) W(cid:99) â1 R (cid:180)â1(cid:161) c ââc (cid:162) . (8.39) (Notethat(8.38)and(8.39)aredifferent!)Then (cid:112) n (cid:161)Î² (cid:101)md âÎ²â n (cid:162)= (cid:112) n (cid:161)Î² (cid:98) âÎ²(cid:162)âW(cid:99) â1 R (cid:179) R (cid:48) W(cid:99) â1 R (cid:180)â1 (cid:112) n (cid:161) R (cid:48)Î² (cid:98) âc â(cid:162) = (cid:181) I k âW(cid:99) â1 R (cid:179) R (cid:48) W(cid:99) â1 R (cid:180)â1 R (cid:48) (cid:182)(cid:112) n (cid:161)Î² (cid:98) âÎ²(cid:162) ââ (cid:179) I k âW â1R (cid:161) R (cid:48) W â1R (cid:162)â1 R (cid:48) (cid:180) N (cid:161) 0,VÎ² (cid:162) d =N (cid:161) 0,VÎ²(W) (cid:162) . (8.40) Inparticular (cid:112) (cid:179) (cid:180) n (cid:161)Î² (cid:101)emd âÎ²â n (cid:162)ââN 0,V â Î² . d Thismeansthatevenwhentheconstraint(8.1)ismisspecifiedtheconventionalcovariancematrixes- timator(8.35)andstandarderrors(8.36)areappropriatemeasuresofthesamplingvariancethoughthe",
    "page": 233,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER8. RESTRICTEDESTIMATION 214 distributions are centered at the pseudo-true values (projections) Î²â rather than Î². The fact that the n estimatorsarebiasedisanunavoidableconsequenceofmisspecification. Analternativeapproachtotheasymptoticdistributiontheoryundermisspecificationusesthecon- cept of local alternatives. It is a technical device which might seem a bit artificial but it is a powerful methodtoderiveusefuldistributionalapproximationsinawidevarietyofcontexts.Theideaistoindex thetruecoefficientÎ² bynviatherelationship n R (cid:48)Î² =c+Î´n â1/2. (8.41) n forsomeÎ´â(cid:82)q. Equation(8.41)specifiesthatÎ² violates(8.1)andthustheconstraintismisspecified. n However, theconstraintisâcloseâtocorrectasthedifferenceR (cid:48)Î² âc =Î´n â1/2 isâsmallâinthesense n thatitdecreaseswiththesamplesizen.Wecall(8.41)localmisspecification. Theasymptotictheoryisderivedasnââunderthesequenceofprobabilitydistributionswiththe coefficientsÎ² . ThewaytothinkaboutthisisthatthetruevalueoftheparameterisÎ² anditisâcloseâ n n tosatisfying(8.1).Thereasonwhythedeviationisproportionalton â1/2isbecausethisistheonlychoice underwhichthelocalizingparameterÎ´appearsintheasymptoticdistributionbutdoesnotdominateit. Thebestwaytoseethisistoworkthroughtheasymptoticapproximation. SinceÎ² isthetruecoefficientvalue,thenY =X (cid:48)Î² +e andwehavethestandardrepresentationfor n n theunconstrainedestimator,namely (cid:112) n (cid:161)Î² (cid:98) âÎ² n (cid:162)= (cid:195) n 1 i (cid:88) = n 1 X i X i (cid:48) (cid:33)â1(cid:195) (cid:112) 1 n i (cid:88) = n 1 X i e i (cid:33) â d âN (cid:161) 0,VÎ² (cid:162) . (8.42) Thereisnodifferenceunderfixed(classical)orlocalasymptoticssincetheright-hand-sideisindepen- dentofthecoefficientÎ² . n Adifferencearisesfortheconstrainedestimator.Using(8.41),c=R (cid:48)Î² âÎ´n â1/2so n R (cid:48)Î² (cid:98) âc=R (cid:48)(cid:161)Î² (cid:98) âÎ² n (cid:162)+Î´n â1/2 and Î² (cid:101)md =Î² (cid:98) âW(cid:99) â1 R (cid:179) R (cid:48) W(cid:99) â1 R (cid:180)â1(cid:161) R (cid:48)Î² (cid:98) âc (cid:162) =Î² (cid:98) âW(cid:99) â1 R (cid:179) R (cid:48) W(cid:99) â1 R (cid:180)â1 R (cid:48)(cid:161)Î² (cid:98) âÎ² n (cid:162)+W(cid:99) â1 R (cid:179) R (cid:48) W(cid:99) â1 R (cid:180)â1 Î´n â1/2. Itfollowsthat (cid:112) n (cid:161)Î² (cid:101)md âÎ² n (cid:162)= (cid:181) I k âW(cid:99) â1 R (cid:179) R (cid:48) W(cid:99) â1 R (cid:180)â1 R (cid:48) (cid:182)(cid:112) n (cid:161)Î² (cid:98) âÎ² n (cid:162)+W(cid:99) â1 R (cid:179) R (cid:48) W(cid:99) â1 R (cid:180)â1 Î´. Thefirsttermisasymptoticallynormal(from8.42)).Thesecondtermconvergesinprobabilitytoacon- (cid:112) stant.Thisisbecausethen â1/2localscalingin(8.41)isexactlybalancedbythe nscalingoftheestima- tor.Noalternativeratewouldhaveproducedthisresult. Consequentlywefindthattheasymptoticdistributionequals (cid:112) n (cid:161)Î² (cid:101)md âÎ² n (cid:162)ââN (cid:161) 0,VÎ² (cid:162)+W â1R (cid:161) R (cid:48) W â1R (cid:162)â1Î´=N (cid:161)Î´â ,VÎ²(W) (cid:162) (8.43) d whereÎ´â=W â1R (cid:161) R (cid:48) W â1R (cid:162)â1Î´",
    "page": 234,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Consequentlywefindthattheasymptoticdistributionequals (cid:112) n (cid:161)Î² (cid:101)md âÎ² n (cid:162)ââN (cid:161) 0,VÎ² (cid:162)+W â1R (cid:161) R (cid:48) W â1R (cid:162)â1Î´=N (cid:161)Î´â ,VÎ²(W) (cid:162) (8.43) d whereÎ´â=W â1R (cid:161) R (cid:48) W â1R (cid:162)â1Î´. Theasymptoticdistribution(8.43)isanapproximationofthesamplingdistributionoftherestricted estimator under misspecification. The distribution (8.43) contains an asymptotic bias component Î´â . The approximation is not fundamentally different from (8.40) â they both have the same asymptotic variancesandbothreflectthebiasduetomisspecification. Thedifferenceisthat(8.40)putsthebiason theleft-sideoftheconvergencearrowwhile(8.43)hasthebiasontheright-side.Thereisnosubstantive differencebetweenthetwo. However,(8.43)ismoreconvenientforsomepurposessuchastheanalysis ofthepoweroftestsaswewillexploreinthenextchapter.",
    "page": 234,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER8. RESTRICTEDESTIMATION 215 8.14 NonlinearConstraints InsomecasesitisdesirabletoimposenonlinearconstraintsontheparametervectorÎ².Theycanbe writtenas r(Î²)=0 (8.44) wherer :(cid:82)kâ(cid:82)q.Thisincludesthelinearconstraints(8.1)asaspecialcase.Anexampleof(8.44)which cannotbewrittenas(8.1)isÎ² Î² =1,whichis(8.44)withr(Î²)=Î² Î² â1. 1 2 1 2 TheconstrainedleastsquaresandminimumdistanceestimatorsofÎ²subjectto(8.44)solvethemin- imizationproblems Î² (cid:101)cls =argminSSE(Î²) (8.45) r(Î²)=0 Î² (cid:101)md =argmin J (cid:161)Î²(cid:162) (8.46) r(Î²)=0 whereSSE(Î²)andJ (cid:161)Î²(cid:162) aredefinedin(8.4)and(8.19),respectively.ThesolutionssolvetheLagrangians L(Î²,Î»)= 1 SSE(Î²)+Î»(cid:48) r(Î²) 2 or L(Î²,Î»)= 1 J (cid:161)Î²(cid:162)+Î»(cid:48) r(Î²) (8.47) 2 over(Î²,Î»). Computationallythereisnogeneralclosed-formsolutionsotheymustbefoundnumerically. Algo- rithmstonumericallysolve(8.45)and(8.46)areknownasconstrainedoptimizationmethodsandare availableinprogramminglanguagesincludingMATLAB,GAUSSandR.SeeChapter12ofIntroduction toEconometrics. Assumption8.3 1. r(Î²)=0. 2. r(Î²)iscontinuouslydifferentiableatthetrueÎ². â 3. rank(R)=q,whereR= r(Î²) (cid:48) . âÎ² Theasymptoticdistributionisasimplegeneralizationofthecaseofalinearconstraintbuttheproof ismoredelicate.",
    "page": 235,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER8. RESTRICTEDESTIMATION 216 Theorem8.10 UnderAssumptions7.2, 8.2, and8.3, forÎ² (cid:101) =Î² (cid:101)md andÎ² (cid:101) =Î² (cid:101)cls definedin(8.45)and(8.46), (cid:112) n (cid:161)Î² (cid:101) âÎ²(cid:162)ââN (cid:161) 0,VÎ²(W) (cid:162) d asnââwhereVÎ²(W)isdefinedin(8.24). ForÎ² (cid:101)cls ,W =Q XX andVÎ²(W)= V cls asdefinedinTheorem8.8. VÎ²(W)isminimizedwithW =V â Î² 1 inwhich casetheasymptoticvarianceis V â Î² =VÎ² âVÎ²R (cid:161) R (cid:48) VÎ²R (cid:162)â1 R (cid:48) VÎ². Theasymptoticcovariancematrixfortheefficientminimumdistanceestimatorcanbeestimatedby â (cid:179) (cid:48) (cid:180)â1 (cid:48) V(cid:98)Î² =V(cid:98)Î² âV(cid:98)Î²R(cid:98) R(cid:98) V(cid:98)Î²R(cid:98) R(cid:98) V(cid:98)Î² where â R(cid:98) = âÎ² r(Î² (cid:101)md ) (cid:48) . (8.48) StandarderrorsfortheelementsofÎ² (cid:101)md arethesquarerootsofthediagonalelementsofV(cid:98) â Î²(cid:101) =n â1V(cid:98) â Î². 8.15 InequalityRestrictions InequalityconstraintsontheparametervectorÎ²taketheform r(Î²)â¥0 (8.49) forsomefunctionr :(cid:82)kâ(cid:82)q.Themostcommonexampleisanon-negativeconstraintÎ² â¥0. 1 Theconstrainedleastsquaresandminimumdistanceestimatorscanbewrittenas Î² (cid:101)cls =argminSSE(Î²) (8.50) r(Î²)â¥0 and Î² (cid:101)md =argmin J (cid:161)Î²(cid:162) . (8.51) r(Î²)â¥0 Exceptinspecialcasestheconstrainedestimatorsdonothavesimplealgebraicsolutions.Animpor- tantexceptioniswhenthereisasinglenon-negativityconstraint,e.g. Î² â¥0withq=1.Inthiscasethe 1 constrainedestimatorcanbefoundbythefollowingapproach.ComputetheuncontrainedestimatorÎ² (cid:98). IfÎ² (cid:98)1 â¥0thenÎ² (cid:101) =Î² (cid:98).OtherwiseifÎ² (cid:98)1 <0thenimposeÎ² 1 =0(eliminatetheregressorX 1 )andre-estimate. This method yields the constrained least squares estimator. While this method works when there is a singlenon-negativityconstraint,itdoesnotimmediatelygeneralizetoothercontexts. Thecomputationproblems(8.50)and(8.51)areexamplesofquadraticprogramming. Quickcom- puteralgorithmsareavailableinprogramminglanguagesincludingMATLAB,GAUSSandR. Inferenceoninequality-constrainedestimatorsisunfortunatelyquitechallenging.Theconventional asymptotic theory gives rise to the following dichotomy. If the true parameter satisfies the strict in- equalityr(Î²)>0thenasymptoticallytheestimatorisnotsubjecttotheconstraintandtheinequality- constrainedestimatorhasanasymptoticdistributionequaltotheunconstrainedcase. Howeverifthe",
    "page": 236,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER8. RESTRICTEDESTIMATION 217 trueparameterisontheboundary, e.g., r(Î²)=0, thentheestimatorhasatruncatedstructure. Thisis (cid:112) easiesttoseeintheone-dimensionalcase. IfwehaveanestimatorÎ² (cid:98)whichsatisfies n (cid:161)Î² (cid:98) âÎ²(cid:162)ââZ = d N (cid:112) (cid:161) 0,VÎ² (cid:162) andÎ²=0,thentheconstrainedestimatorÎ² (cid:101) =max[Î² (cid:98),0]willhavetheasymptoticdistribution nÎ² (cid:101) ââmax[Z,0],aâhalf-normalâdistribution. d 8.16 TechnicalProofs* ProofofTheorem8.9,equation(8.28)LetRâ¥beafullrankkÃ(cid:161) kâq (cid:162) matrixsatisfyingR (cid:48) â¥ VÎ²R=0and thensetC =[R,Râ¥]whichisfullrankandinvertible.Thenwecancalculatethat C (cid:48) V â Î² C = (cid:34) R (cid:48) (cid:48) V â Î² â R R (cid:48) (cid:48) V â Î² â Râ¥ (cid:35) = (cid:183) 0 (cid:48) 0 (cid:184) R â¥ V Î² R R â¥ V Î² Râ¥ 0 R â¥ VÎ²Râ¥ and (cid:48) C VÎ²(W)C (cid:34) (cid:48) â (cid:48) â (cid:35) = R V Î² (W)R R V Î² (W)Râ¥ (cid:48) â (cid:48) â R â¥ V Î² (W)R R â¥ V Î² (W)Râ¥ (cid:34) (cid:35) 0 0 = 0 R (cid:48) â¥ VÎ²Râ¥ +R (cid:48) â¥ WR (cid:161) R (cid:48) WR (cid:162)â1 R (cid:48) VÎ²R (cid:161) R (cid:48) WR (cid:162)â1 R (cid:48) WR â¥ . Thus (cid:179) (cid:180) C (cid:48) VÎ²(W)âV â Î² C =C (cid:48) VÎ²(W)CâC (cid:48) V â Î² C (cid:34) (cid:35) 0 0 = (cid:48) (cid:161) (cid:48) (cid:162)â1 (cid:48) (cid:161) (cid:48) (cid:162)â1 (cid:48) 0 R WR R WR R V R R WR R WR â¥ Î² â¥ â¥0 SinceC isinvertibleitfollowsthatVÎ²(W)âV â Î² â¥0whichis(8.28). â  ProofofTheorem8.10 We show the result for the minimum distance estimator Î² (cid:101) =Î² (cid:101)md as the proof for the constrained least squares estimator is similar. For simplicity we assume that the constrained estimator is consistent Î² (cid:101) ââÎ². This can be shown with more effort, but requires a deeper treatment p thanappropriateforthistextbook. Foreachelementr (Î²)oftheq-vectorr(Î²),bythemeanvaluetheoremthereexistsaÎ²â ontheline j j segmentjoiningÎ² (cid:101)andÎ²suchthat â r j (Î² (cid:101))=r j (Î²)+ âÎ² r j (Î²â j ) (cid:48)(cid:161)Î² (cid:101) âÎ²(cid:162) . (8.52) LetR â bethekÃq matrix n (cid:183) â â â (cid:184) R â= r (Î²â ) r (Î²â ) Â·Â·Â· r (Î²â ) . âÎ² 1 1 âÎ² 2 2 âÎ² q q",
    "page": 237,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER8. RESTRICTEDESTIMATION 218 SinceÎ² (cid:101) ââÎ²itfollowsthatÎ²âââÎ²,andbytheCMT,R âââR.Stackingthe(8.52),weobtain p j p p r(Î² (cid:101))=r(Î²)+R â(cid:48)(cid:161)Î² (cid:101) âÎ²(cid:162) . Sincer (cid:161)Î² (cid:101) (cid:162)=0byconstructionandr(Î²)=0byAssumption8.1thisimplies 0=R â(cid:48)(cid:161)Î² (cid:101) âÎ²(cid:162) . (8.53) Thefirst-orderconditionfor(8.47)isW(cid:99) (cid:161)Î² (cid:98) âÎ² (cid:101) (cid:162)=R(cid:98) Î» (cid:101)whereR(cid:98) isdefinedin(8.48). Premultiplyingby â(cid:48) â1 R W(cid:99) ,inverting,andusing(8.53),wefind Î» (cid:101) = (cid:179) R â(cid:48) W(cid:99) â1 R(cid:98) (cid:180)â1 R â(cid:48)(cid:161)Î² (cid:98) âÎ² (cid:101) (cid:162)= (cid:179) R â(cid:48) W(cid:99) â1 R(cid:98) (cid:180)â1 R â(cid:48)(cid:161)Î² (cid:98) âÎ²(cid:162) . Thus Î² (cid:101) âÎ²= (cid:181) I k âW(cid:99) â1 R(cid:98) (cid:179) R â n (cid:48) W(cid:99) â1 R(cid:98) (cid:180)â1 R â n (cid:48) (cid:182) (cid:161)Î² (cid:98) âÎ²(cid:162) . (8.54) FromTheorem7.3andTheorem7.6wefind (cid:112) n (cid:161)Î² (cid:101) âÎ²(cid:162)= (cid:181) I k âW(cid:99) â1 R(cid:98) (cid:179) R â n (cid:48) W(cid:99) â1 R(cid:101) (cid:180)â1 R â n (cid:48) (cid:182)(cid:112) n (cid:161)Î² (cid:98) âÎ²(cid:162) ââ (cid:179) I k âW â1R (cid:161) R (cid:48) W â1R (cid:162)â1 R (cid:48) (cid:180) N (cid:161) 0,VÎ² (cid:162) d =N (cid:161) 0,VÎ²(W) (cid:162) . â  _____________________________________________________________________________________________ 8.17 Exercises Exercise8.1 InthemodelY =X (cid:48)Î² +X (cid:48)Î² +e,showdirectlyfromdefinition(8.3)thattheCLSestimator 1 1 2 2 ofÎ²=(Î² ,Î² )subjecttotheconstraintthatÎ² =0istheOLSregressionofY onX . 1 2 2 1 Exercise8.2 InthemodelY =X (cid:48)Î² +X (cid:48)Î² +e,showdirectlyfromdefinition(8.3)thattheCLSestimator 1 1 2 2 ofÎ²=(Î² ,Î² )subjecttotheconstraintÎ² =c (wherec issomegivenvector)isOLSofY âX (cid:48) c onX . 1 2 1 1 2 Exercise8.3 In the model Y = X (cid:48)Î² +X (cid:48)Î² +e, with Î² and Î² each kÃ1, find the CLS estimator of 1 1 2 2 1 2 Î²=(Î² ,Î² )subjecttotheconstraintthatÎ² =âÎ² . 1 2 1 2 Exercise8.4 InthelinearprojectionmodelY =Î±+X (cid:48)Î²+econsidertherestrictionÎ²=0. (a) FindtheCLSestimatorofÎ±undertherestrictionÎ²=0. (b) FindanexpressionfortheefficientminimumdistanceestimatorofÎ±undertherestrictionÎ²=0. Exercise8.5 VerifythatforÎ² (cid:101)cls definedin(8.8)thatR (cid:48)Î² (cid:101)cls =c. Exercise8.6 ProveTheorem8.1. Exercise8.7 ProveTheorem8.2,thatis,(cid:69)(cid:163)Î² (cid:101)cls |X (cid:164)=Î²,undertheassumptionsofthelinearregression regressionmodeland(8.1).(Hint:UseTheorem8.1.)",
    "page": 238,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER8. RESTRICTEDESTIMATION 219 Exercise8.8 ProveTheorem8.3. Exercise8.9 ProveTheorem8.4.Thatis,show(cid:69)(cid:163) s2 |X (cid:164)=Ï2undertheassumptionsofthehomoskedas- cls ticregressionmodeland(8.1). Exercise8.10 Verify(8.22),(8.23),andthattheminimumdistanceestimatorÎ² (cid:101)md withW(cid:99) =Q(cid:98)XX equals theCLSestimator. Exercise8.11 ProveTheorem8.6. Exercise8.12 ProveTheorem8.7. Exercise8.13 ProveTheorem8.8.(Hint:UsethatCLSisaspecialcaseofTheorem8.7.) Exercise8.14 Verifythat(8.26)isVÎ²(W)withW =V â Î² 1. Exercise8.15 Prove(8.27).Hint:Use(8.26). Exercise8.16 Verify(8.29),(8.30)and(8.31). Exercise8.17 Verify(8.32),(8.33),and(8.34). Exercise8.18 Supposeyouhavetwoindependentsampleseachwithn observationswhichsatisfythe modelsY =X (cid:48)Î² +e with(cid:69)[X e ]=0andY =X (cid:48)Î² +e with(cid:69)[X e ]=0whereÎ² andÎ² arebothkÃ1. 1 1 1 1 1 1 2 2 2 2 2 2 1 2 YouestimateÎ² andÎ² byOLSoneachsample,withconsistentasymptoticcovariancematrixestimators 1 2 V(cid:98)Î² andV(cid:98)Î² .ConsiderefficientminimumdistanceestimationundertherestrictionÎ² 1 =Î² 2 . 1 2 (a) FindtheestimatorÎ² (cid:101)ofÎ²=Î² 1 =Î² 2 . (b) FindtheasymptoticdistributionofÎ² (cid:101). (c) Howwouldyouapproachtheproblemifthesamplesizesaredifferent,sayn andn ? 1 2 Exercise8.19 Usethecps09mardatasetandthesubsampleofwhitemaleHispanics. (a) Estimatetheregression lo(cid:225)g(wage)=Î² 1 education+Î² 2 experience+Î² 3 experience2/100+Î² 4 married 1 +Î² married +Î² married +Î² widowed+Î² divorced+Î² separated+Î² 5 2 6 3 7 8 9 10 wheremarried ,married ,andmarried arethefirstthreemaritalcodeslistedinSection3.22. 1 2 3 (b) EstimatetheequationbyCLSimposingtheconstraintsÎ² =Î² andÎ² =Î² . Reporttheestimates 4 7 8 9 andstandarderrors. (c) Estimatetheequationusingefficientminimumdistanceimposingthesameconstraints. Report theestimatesandstandarderrors. (d) Underwhatconstraintonthecoefficientsisthewageequationnon-decreasinginexperiencefor experienceupto50? (e) EstimatetheequationimposingÎ² =Î² ,Î² =Î² ,andtheinequalityfrompart(d). 4 7 8 9",
    "page": 239,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER8. RESTRICTEDESTIMATION 220 Exercise8.20 Takethemodel Y =m(X)+e m(x)=Î² +Î² x+Î² x2+Â·Â·Â·+Î² xp 0 1 2 p (cid:104) (cid:105) (cid:69) Xje =0, j =0,...,p d g(x)= m(x) dx withi.i.d.observations(Y ,X ),i =1,...,n.Theorderofthepolynomialp isknown. i i (a) Howshouldweinterpretthefunctionm(x)giventheprojectionassumption? Howshouldwein- terpretg(x)?(Briefly) (b) Describeanestimatorg(x)ofg(x). (cid:98) (cid:112) (c) Findtheasymptoticdistributionof n (cid:161) g(x)âg(x) (cid:162) asnââ. (cid:98) (d) Showhowtoconstructanasymptotic95%confidenceintervalforg(x)(forasinglex). (e) Assumep=2.Describehowtoestimateg(x)imposingtheconstraintthatm(x)isconcave. (f) Assumep =2.Describehowtoestimate g(x)imposingtheconstraintthatm(u)isincreasingon theregionuâ[x ,x ]. L U Exercise8.21 TakethelinearmodelwithrestrictionsY =X (cid:48)Î²+e with(cid:69)[Xe]=0andR (cid:48)Î²=c. Consider threeestimatorsforÎ²: â¢ Î² (cid:98)theunconstrainedleastsquaresestimator â¢ Î² (cid:101)theconstrainedleastsquaresestimator â¢ Î²theconstrainedefficientminimumdistanceestimator Forthethreeestimatordefinetheresidualse (cid:98)i =Y i âX i (cid:48)Î² (cid:98),e (cid:101)i =Y i âX i (cid:48)Î² (cid:101),e i =Y i âX i (cid:48)Î²,andvariance estimatorsÏ2=n â1(cid:80)n e2,Ï2=n â1(cid:80)n e2,andÏ2=n â1(cid:80)n e2. (cid:98) i=1(cid:98)i (cid:101) i=1(cid:101)i i=1 i (a) AsÎ²isthemostefficientestimatorandÎ² (cid:98)theleast,doyouexpectÏ2<Ï (cid:101) 2<Ï (cid:98) 2inlargesamples? (b) Considerthestatistic n T =Ïâ2(cid:88) (e âe )2. n (cid:98) (cid:98)i (cid:101)i i=1 FindtheasymptoticdistributionforT whenR (cid:48)Î²=c istrue. n (c) Doestheresultofthepreviousquestionsimplifywhentheerrore ishomoskedastic? i Î² Exercise8.22 TakethelinearmodelY =X Î² +X Î² +ewith(cid:69)[Xe]=0.Considertherestriction 1 =2. 1 1 2 2 Î² 2 (a) FindanexplicitexpressionfortheCLSestimatorÎ² (cid:101) =(Î² (cid:101)1 ,Î² (cid:101)2 )ofÎ²=(Î² 1 ,Î² 2 )undertherestriction. Youranswershouldbespecifictotherestriction.Itshouldnotbeagenericformulaforanabstract generalrestriction. (b) DerivetheasymptoticdistributionofÎ² (cid:101)1 undertheassumptionthattherestrictionistrue.",
    "page": 240,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Chapter 9 Hypothesis Testing InChapter5webrieflyintroducedhypothesistestinginthecontextofthenormalregressionmodel. Inthischapterweexplorehypothesistestingingreaterdetailwithaparticularemphasisonasymptotic inference.FormoredetailonthefoundationsseeChapter13ofIntroductiontoEconometrics. 9.1 Hypotheses InChapter8wediscussedestimationsubjecttorestrictions,includinglinearrestrictions(8.1),non- linear restrictions(8.44), andinequality restrictions(8.49). Inthischapterwe discuss testsofsuchre- strictions. Hypothesistestsattempttoassesswhetherthereisevidencecontrarytoaproposedrestriction. Let Î¸=r(Î²)beaqÃ1parameterofinterestwherer :(cid:82)k âÎâ(cid:82)q issometransformation. Forexample,Î¸ maybeasinglecoefficient,e.g. Î¸=Î² j ,thedifferencebetweentwocoefficients,e.g. Î¸=Î² j âÎ² (cid:96),orthe ratiooftwocoefficients,e.g.Î¸=Î² j /Î² (cid:96). ApointhypothesisconcerningÎ¸isaproposedrestrictionsuchas Î¸=Î¸ (9.1) 0 whereÎ¸ isahypothesized(known)value. 0 Moregenerally,lettingÎ²âB â(cid:82)k betheparameterspace,ahypothesisisarestrictionÎ²âB where 0 B isapropersubsetofB.Thisspecializesto(9.1)bysettingB =(cid:169)Î²âB:r(Î²)=Î¸ (cid:170) . 0 0 0 Inthischapterwewillfocusexclusivelyonpointhypothesesoftheform(9.1)astheyarethemost commonandrelativelysimpletohandle. Thehypothesistobetestediscalledthenullhypothesis. Definition9.1 Thenullhypothesis(cid:72) istherestrictionÎ¸=Î¸ orÎ²âB . 0 0 0 Weoftenwritethenullhypothesisas(cid:72) :Î¸=Î¸ or(cid:72) :r(Î²)=Î¸ . 0 0 0 0 Thecomplementofthenullhypothesis(thecollectionofparametervalueswhichdonotsatisfythe nullhypothesis)iscalledthealternativehypothesis. Definition9.2 The alternative hypothesis (cid:72) is the set {Î¸âÎ:Î¸(cid:54)=Î¸ } or 1 0 (cid:169)Î²âB:Î²âB (cid:170) . 0 221",
    "page": 241,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 222 Weoftenwritethealternativehypothesisas(cid:72) :Î¸(cid:54)=Î¸ or(cid:72) :r(Î²)(cid:54)=Î¸ .Forsimplicity,weoftenrefer 1 0 1 0 tothehypothesesasâthenullâandâthealternativeâ.Figure9.1(a)illustratesthedivisionoftheparameter spaceintonullandalternativehypotheses. H T<c 1 H S 0 0 S 1 T>c (a)NullandAlternativeHypotheses (b)AcceptanceandRejectionRegions Figure9.1:HypothesisTesting Inhypothesistesting,weassumethatthereisatrue(butunknown)valueofÎ¸ andthisvalueeither satisfies(cid:72) ordoesnotsatisfy(cid:72) .Thegoalofhypothesistestingistoassesswhetherornot(cid:72) istrueby 0 0 0 askingif(cid:72) isconsistentwiththeobserveddata. 0 Tobespecific,takeourexampleofwagedeterminationandconsiderthequestion:Doesunionmem- bershipaffectwages?Wecanturnthisintoahypothesistestbyspecifyingthenullastherestrictionthat a coefficient on union membership is zero in a wage regression. Consider, for example, the estimates reportedinTable4.1. ThecoefficientforâMaleUnionMemberâis0.095(awagepremiumof9.5%)and the coefficient for âFemale Union Memberâ is 0.022 (a wage premium of 2.2%). These are estimates, notthetruevalues. Thequestionis: Arethetruecoefficientszero? Toanswerthisquestionthetesting methodasksthequestion:Aretheobservedestimatescompatiblewiththehypothesis,inthesensethat thedeviationfromthehypothesiscanbereasonablyexplainedbystochasticvariation? Oraretheob- servedestimatesincompatiblewiththehypothesis,inthesensethatthattheobservedestimateswould behighlyunlikelyifthehypothesisweretrue? 9.2 AcceptanceandRejection Ahypothesistesteitheracceptsthenullhypothesisorrejectsthenullhypothesisinfavorofthealter- nativehypothesis. WecandescribethesetwodecisionsasâAccept(cid:72) âandâReject(cid:72) â. Intheexample 0 0 givenintheprevioussectionthedecisioniseithertoacceptthehypothesisthatunionmembershipdoes notaffectwages,ortorejectthehypothesisinfavorofthealternativethatunionmembershipdoesaffect wages. The decision is based on the data and so is a mapping from the sample space to the decision set. ThissplitsthesamplespaceintotworegionsS andS suchthatiftheobservedsamplefallsintoS we 0 1 0 accept(cid:72) ,whileifthesamplefallsintoS wereject(cid:72) . ThesetS iscalledtheacceptanceregionand 0 1 0 0",
    "page": 242,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 223 thesetS therejectionorcriticalregion. 1 Itisconvenienttoexpressthismappingasareal-valuedfunctioncalledateststatistic T =T((Y ,X ),...,(Y ,X )) 1 1 n n relativetoacriticalvaluec.Thehypothesistestthenconsistsofthedecisionrule: 1. Accept(cid:72) ifT â¤c. 0 2. Reject(cid:72) ifT >c. 0 Figure9.1(b)illustratesthedivisionofthesamplespaceintoacceptanceandrejectionregions. AteststatisticT shouldbedesignedsothatsmallvaluesarelikelywhen(cid:72) istrueandlargevalues 0 arelikelywhen(cid:72) istrue. Thereisawelldevelopedstatisticaltheoryconcerningthedesignofoptimal 1 tests. Wewillnotreviewthattheoryhere,butinsteadreferthereadertoLehmannandRomano(2005). Inthischapterwewillsummarizethemainapproachestothedesignofteststatistics. Themostcommonlyusedteststatisticistheabsolutevalueofthet-statistic T =|T(Î¸ )| (9.2) 0 where Î¸âÎ¸ (cid:98) T(Î¸)= (9.3) s(Î¸ (cid:98)) isthet-statisticfrom(7.33),Î¸ (cid:98)isapointestimator,ands(Î¸ (cid:98))itsstandarderror.T isanappropriatestatistic whentestinghypothesesonindividualcoefficientsorreal-valuedparametersÎ¸=h(Î²)andÎ¸ isthehy- 0 pothesizedvalue. QuitetypicallyÎ¸ =0,asinterestfocusesonwhetherornotacoefficientequalszero, 0 butthisisnottheonlypossibility.Forexample,interestmayfocusonwhetheranelasticityÎ¸equals1,in whichcasewemaywishtotest(cid:72) :Î¸=1. 0 9.3 TypeIError Afalserejectionofthenullhypothesis(cid:72) (rejecting(cid:72) when(cid:72) istrue)iscalledaTypeIerror. The 0 0 0 probabilityofaTypeIerroriscalledthesizeofthetest. (cid:80)(cid:163) Reject(cid:72) |(cid:72) true (cid:164)=(cid:80)[T >c|(cid:72) true]. (9.4) 0 0 0 The uniform size of the test is the supremum of (9.4) across all data distributions which satisfy (cid:72) . A 0 primarygoaloftestconstructionistolimittheincidenceofTypeIerrorbyboundingthesizeofthetest. ForthereasonsdiscussedinChapter7,intypicaleconometricmodelstheexactsamplingdistribu- tions of estimators and test statistics are unknown and hence we cannot explicitly calculate (9.4). In- stead,wetypicallyrelyonasymptoticapproximations. Supposethattheteststatistichasanasymptotic distributionunder(cid:72) .Thatis,when(cid:72) istrue 0 0 T ââÎ¾ (9.5) d asnââforsomecontinuously-distributedrandomvariableÎ¾. Thisisnotasubstantiverestrictionas mostconventionaleconometrictestssatisfy(9.5). LetG(u)=(cid:80)[Î¾â¤u]denotethedistributionofÎ¾. We callÎ¾(orG)theasymptoticnulldistribution. ItisdesirabletodesignteststatisticsT whoseasymptoticnulldistributionG isknownanddoesnot dependonunknownparameters.InthiscasewesaythatT isasymptoticallypivotal.",
    "page": 243,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 224 Forexample,iftheteststatisticequalstheabsolutet-statisticfrom(9.2),thenweknowfromTheorem 7.11thatifÎ¸=Î¸ (thatis, thenullhypothesisholds), thenT ââ|Z|asn ââwhere Z â¼N(0,1). This 0 d meansthatG(u)=(cid:80)[|Z|â¤u]=2Î¦(u)â1,thedistributionoftheabsolutevalueofthestandardnormal asshownin(7.34).Thisdistributiondoesnotdependonunknownsandispivotal. WedefinetheasymptoticsizeofthetestastheasymptoticprobabilityofaTypeIerror: lim (cid:80)[T >c|(cid:72) true]=(cid:80)[Î¾>c]=1âG(c). nââ 0 WeseethattheasymptoticsizeofthetestisasimplefunctionoftheasymptoticnulldistributionG and thecriticalvaluec.Forexample,theasymptoticsizeofatestbasedontheabsolutet-statisticwithcritical valuec is2(1âÎ¦(c)). In the dominant approach to hypothesis testing the researcher pre-selects a significancelevel Î±â (0,1) and then selects c so the asymptotic size is no larger than Î±. When the asymptotic null distribu- tion G is pivotal we accomplish this by setting c equal to the 1âÎ± quantile of the distribution G. (If the distributionG is not pivotal more complicated methods must be used.) We call c the asymptotic critical value because it has been selected from the asymptotic null distribution. For example, since 2(1âÎ¦(1.96))=0.05itfollowsthatthe5%asymptoticcriticalvaluefortheabsolutet-statisticisc=1.96. Calculationofnormalcriticalvaluesisdonenumericallyinstatisticalsoftware.Forexample,inMATLAB thecommandisnorminv(1-Î±/2). 9.4 ttests Aswementionedearlier, themostcommontestoftheone-dimensionalhypothesis(cid:72) :Î¸=Î¸ â(cid:82) 0 0 againstthealternative(cid:72) :Î¸(cid:54)=Î¸ istheabsolutevalueofthet-statistic(9.3). Wenowformallystateits 1 0 asymptoticnulldistribution,whichisasimpleapplicationofTheorem7.11. Theorem9.1 Under Assumptions 7.2, 7.3, and (cid:72) :Î¸=Î¸ â(cid:82), T(Î¸ )ââ Z â¼ 0 0 0 d N(0,1). For c satisfying Î± = 2(1âÎ¦(c)), (cid:80)[|T(Î¸ )|>c|(cid:72) ] â Î±, and the test 0 0 âReject(cid:72) if|T(Î¸ )|>câhasasymptoticsizeÎ±. 0 0 Theorem9.1showsthatasymptoticcriticalvaluescanbetakenfromthenormaldistribution. Asin ourdiscussionofasymptoticconfidenceintervals(Section7.13)thecriticalvaluecouldalternativelybe takenfromthestudenttdistribution,whichwouldbetheexacttestinthenormalregressionmodel(Sec- tion5.12).Indeed,t criticalvaluesarethedefaultinpackagessuchasStata.Sincethecriticalvaluesfrom thestudentt distributionare(slightly)largerthanthosefromthenormaldistribution,studentt critical values slightly decrease the rejection probability of the test. In practical applications the difference is typicallyunimportantunlessthesamplesizeisquitesmall(inwhichcasetheasymptoticapproximation shouldbequestionedaswell). ThealternativehypothesisÎ¸(cid:54)=Î¸ issometimescalledaâtwo-sidedâalternative. Incontrast,some- 0 timeswe areinterested intestingfor one-sided alternatives suchas (cid:72) :Î¸>Î¸ or (cid:72) :Î¸<Î¸ . Tests of 1 0 1 0 Î¸ =Î¸ against Î¸ >Î¸ or Î¸ <Î¸ are based on the signed t-statistic T =T(Î¸ )",
    "page": 244,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". In practical applications the difference is typicallyunimportantunlessthesamplesizeisquitesmall(inwhichcasetheasymptoticapproximation shouldbequestionedaswell). ThealternativehypothesisÎ¸(cid:54)=Î¸ issometimescalledaâtwo-sidedâalternative. Incontrast,some- 0 timeswe areinterested intestingfor one-sided alternatives suchas (cid:72) :Î¸>Î¸ or (cid:72) :Î¸<Î¸ . Tests of 1 0 1 0 Î¸ =Î¸ against Î¸ >Î¸ or Î¸ <Î¸ are based on the signed t-statistic T =T(Î¸ ). The hypothesis Î¸ =Î¸ is 0 0 0 0 0 rejectedinfavor of Î¸>Î¸ if T >c where c satisfies Î±=1âÎ¦(c). Negative values ofT are not taken as 0 evidenceagainst(cid:72) 0 ,aspointestimatesÎ¸ (cid:98)lessthanÎ¸ 0 donotpointtoÎ¸>Î¸ 0 .Sincethecriticalvaluesare takenfromthesingletailofthenormaldistributiontheyaresmallerthanfortwo-sidedtests.Specifically, theasymptotic5%criticalvalueisc=1.645.Thus,werejectÎ¸=Î¸ infavorofÎ¸>Î¸ ifT >1.645. 0 0",
    "page": 244,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 225 Conversely,testsofÎ¸=Î¸ againstÎ¸<Î¸ reject(cid:72) fornegativet-statistics,e.g.ifT <âc.Largepositive 0 0 0 valuesofT arenotevidencefor(cid:72) :Î¸<Î¸ .Anasymptotic5%testrejectsifT <â1.645. 1 0 There seems to be an ambiguity. Should we use the two-sided critical value 1.96 or the one-sided critical value 1.645? The answer is that in most cases the two-sided critical value is appropriate. We shouldusetheone-sidedcriticalvaluesonlywhentheparameterspaceisknowntosatisfyaone-sided restrictionsuchasÎ¸â¥Î¸ .ThisiswhenthetestofÎ¸=Î¸ againstÎ¸>Î¸ makessense. Iftherestriction 0 0 0 Î¸â¥Î¸ isnotknownapriorithenimposingthisrestrictiontotestÎ¸=Î¸ againstÎ¸>Î¸ doesnotmakes 0 0 0 sense. Since linear regression coefficients typically do not have a priori sign restrictions, the standard conventionistousetwo-sidedcriticalvalues. Thismayseemcontrarytothewaytestingispresentedinstatisticaltextbookswhichoftenfocuson one-sidedalternativehypotheses.Thelatterfocusisprimarilyforpedagogyastheone-sidedtheoretical problemiscleanerandeasiertounderstand. 9.5 TypeIIErrorandPower Afalseacceptanceofthenullhypothesis(cid:72) (accepting(cid:72) when(cid:72) istrue)iscalledaTypeIIerror. 0 0 1 Therejectionprobabilityunderthealternativehypothesisiscalledthepowerofthetest, andequals1 minustheprobabilityofaTypeIIerror: Ï(Î¸)=(cid:80)(cid:163) Reject(cid:72) |(cid:72) true (cid:164)=(cid:80)[T >c|(cid:72) true]. 0 1 1 WecallÏ(Î¸)thepowerfunctionandiswrittenasafunctionofÎ¸toindicateitsdependenceonthetrue valueoftheparameterÎ¸. Inthedominantapproachtohypothesistestingthegoaloftestconstructionistohavehighpower subjecttotheconstraintthatthesizeofthetestislowerthanthepre-specifiedsignificancelevel. Gen- erally,thepowerofatestdependsonthetruevalueoftheparameterÎ¸,andforawell-behavedtestthe powerisincreasingbothasÎ¸movesawayfromthenullhypothesisÎ¸ andasthesamplesizenincreases. 0 Giventhetwopossiblestatesoftheworld((cid:72) or(cid:72) )andthetwopossibledecisions(Accept(cid:72) or 0 1 0 Reject(cid:72) )therearefourpossiblepairingsofstatesanddecisionsasisdepictedinTable9.1. 0 Table9.1:HypothesisTestingDecisions Accept(cid:72) Reject(cid:72) 0 0 (cid:72) true CorrectDecision TypeIError 0 (cid:72) true TypeIIError CorrectDecision 1 GivenateststatisticT, increasingthecriticalvaluec increasestheacceptanceregionS whilede- 0 creasingtherejectionregionS . ThisdecreasesthelikelihoodofaTypeIerror(decreasesthesize)but 1 increasesthelikelihoodofaTypeIIerror(decreasesthepower).Thusthechoiceofcinvolvesatrade-off betweensizeandthepower.ThisiswhythesignificancelevelÎ±ofthetestcannotbesetarbitrarilysmall. Otherwisethetestwillnothavemeaningfulpower. Itisimportanttoconsiderthepowerofatestwheninterpretinghypothesistestsasanoverlynarrow focusonsizecanleadtopoordecisions. Forexample, itiseasytodesignatestwhichhasperfectsize yethastrivialpower. Specifically,foranyhypothesiswecanusethefollowingtest: Generatearandom variableU â¼U[0,1] and reject (cid:72) ifU <Î±. This test has exact size of Î±",
    "page": 245,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Otherwisethetestwillnothavemeaningfulpower. Itisimportanttoconsiderthepowerofatestwheninterpretinghypothesistestsasanoverlynarrow focusonsizecanleadtopoordecisions. Forexample, itiseasytodesignatestwhichhasperfectsize yethastrivialpower. Specifically,foranyhypothesiswecanusethefollowingtest: Generatearandom variableU â¼U[0,1] and reject (cid:72) ifU <Î±. This test has exact size of Î±. Yet the test also has power 0 precisely equal to Î±. When the power of a test equals the size we say that the test has trivial power. Nothingislearnedfromsuchatest.",
    "page": 245,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 226 9.6 StatisticalSignificance Testingrequiresapre-selectedchoiceofsignificancelevelÎ±yetthereisnoobjectivescientificbasis forchoiceofÎ±.Nevertheless,thecommonpracticeistosetÎ±=0.05(5%). Alternativecommonvalues areÎ±=0.10(10%)andÎ±=0.01(1%).Thesechoicesaresomewhattheby-productoftraditionaltablesof criticalvaluesandstatisticalsoftware. The informal reasoning behind the 5% critical value is to ensure that Type I errors should be rela- tivelyunlikelyâthatthedecisionâReject(cid:72) âhasscientificstrengthâyetthetestretainspoweragainst 0 reasonablealternatives. ThedecisionâReject(cid:72) âmeansthattheevidenceisinconsistentwiththenull 0 hypothesisinthesensethatitisrelativelyunlikely(1in20)thatdatageneratedbythenullhypothesis wouldyieldtheobservedtestresult. Incontrast,thedecisionâAccept(cid:72) âisnotastrongstatement. Itdoesnotmeanthattheevidence 0 supports(cid:72) onlythatthereisinsufficientevidencetoreject(cid:72) .Becauseofthisitismoreaccuratetouse 0 0 thelabelâDonotReject(cid:72) âinsteadofâAccept(cid:72) â. 0 0 Whenatestrejects(cid:72) atthe5%significancelevelitiscommontosaythatthestatisticisstatistically 0 significantandifthetestaccepts(cid:72) itiscommontosaythatthestatisticisnotstatisticallysignificant 0 or that it is statistically insignificant. It is helpful to remember that this is simply a compact way of saying âUsing the statistic T the hypothesis (cid:72) can [cannot] be rejected at the asymptotic 5% level.â 0 Furthermore,whenthenullhypothesis(cid:72) :Î¸=0isrejecteditiscommontosaythatthecoefficientÎ¸is 0 statisticallysignificant,becausethetesthasrejectedthehypothesisthatthecoefficientisequaltozero. LetusreturntotheexampleabouttheunionwagepremiumasmeasuredinTable4.1. Theabsolute t-statisticforthecoefficientonâMaleUnionMemberâis0.095/0.020=4.7,whichisgreaterthanthe5% asymptotic critical value of 1.96. Therefore we reject the hypothesis that union membership does not affectwagesformen. Inthiscasewecansaythatunionmembershipisstatisticallysignificantformen. However, the absolute t-statistic for the coefficient on âFemale Union Memberâ is 0.023/0.020 = 1.2, whichislessthan1.96andthereforewedonotrejectthehypothesisthatunionmembershipdoesnot affectwagesforwomen.Inthiscasewefindthatmembershipforwomenisnotstatisticallysignificant. Whenatestacceptsanullhypothesis(whenatestisnotstatisticallysignificant)acommonmisin- terpretationisthatthisisevidencethatthenullhypothesisistrue. Thisisincorrect. Failuretorejectis byitselfnotevidence. WithoutananalysisofpowerwedonotknowthelikelihoodofmakingaTypeII error and thus are uncertain. In our wage example it would be a mistake to write that âthe regression findsthatfemaleunionmembershiphasnoeffectonwagesâ. Thisisanincorrectandmostunfortunate interpretation. Thetesthasfailedtorejectthehypothesisthatthecoefficientiszerobutthatdoesnot meanthatthecoefficientisactuallyzero",
    "page": 246,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Thisisincorrect. Failuretorejectis byitselfnotevidence. WithoutananalysisofpowerwedonotknowthelikelihoodofmakingaTypeII error and thus are uncertain. In our wage example it would be a mistake to write that âthe regression findsthatfemaleunionmembershiphasnoeffectonwagesâ. Thisisanincorrectandmostunfortunate interpretation. Thetesthasfailedtorejectthehypothesisthatthecoefficientiszerobutthatdoesnot meanthatthecoefficientisactuallyzero. When a test rejects a null hypothesis (when a test is statistically significant) it is strong evidence againstthehypothesis(sinceifthehypothesisweretruethenrejectionisanunlikelyevent). Rejection shouldbetakenasevidenceagainstthenullhypothesis. However,wecanneverconcludethatthenull hypothesisisindeedfalseaswecannotexcludethepossibilitythatwearemakingaTypeIerror. Perhapsmoreimportantly, thereisanimportantdistinctionbetweenstatisticalandeconomicsig- nificance. Ifwecorrectlyrejectthehypothesis(cid:72) :Î¸=0itmeansthatthetruevalueofÎ¸ isnon-zero. 0 This includes the possibility that Î¸ may be non-zero but close to zero in magnitude. This only makes senseifweinterprettheparametersinthecontextoftheirrelevantmodels. Inourwageregressionex- ample we might consider wage effects of 1% magnitude or less as being âclose to zeroâ. In a log wage regressionthiscorrespondstoadummyvariablewithacoefficientlessthan0.01. Ifthestandarderror issufficientlysmall(lessthan0.005)thenacoefficientestimateof0.01willbestatisticallysignificantbut noteconomicallysignificant. Thisoccursfrequentlyinapplicationswithverylargesamplesizeswhere standarderrorscanbequitesmall. Thesolutionistofocuswheneverpossibleonconfidenceintervalsandtheeconomicmeaningofthe",
    "page": 246,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 227 coefficients. Forexample, ifthecoefficientestimateis0.005withastandarderrorof0.002thena95% confidenceintervalwouldbe[0.001,0.009]indicatingthatthetrueeffectislikelybetween0%and1%, andhenceisslightlypositivebutsmall. Thisismuchmoreinformativethanthemisleadingstatement âtheeffectisstatisticallypositiveâ. 9.7 P-Values Continuing with the wage regression estimates reported in Table 4.1, consider another question: Doesmarriagestatusaffectwages? Totestthehypothesisthatmarriagestatushasnoeffectonwages, we examine the t-statistics for the coefficients on âMarried Maleâ and âMarried Femaleâ in Table 4.1, whichare0.211/0.010=22and0.016/0.010=1.7,respectively.Thefirstexceedstheasymptotic5%criti- calvalueof1.96sowerejectthehypothesisformen. Thesecondissmallerthan1.96sowefailtoreject the hypothesis for women. Taking a second look at the statistics we see that the statistic for men (22) isexceptionallyhighandthatforwomen(1.7)isonlyslightlybelowthecriticalvalue. Supposethatthe t-statisticforwomenwereslightlyincreasedto2.0. Thisislargerthanthecriticalvaluesowouldleadto thedecisionâReject(cid:72) âratherthanâAccept(cid:72) â. Shouldwereallybemakingadifferentdecisionifthe 0 0 t-statisticis2.0ratherthan1.7?Thedifferenceinvaluesissmall,shouldnâtthedifferenceinthedecision bealsosmall? ThinkingthroughtheseexamplesitseemsunsatisfactorytosimplyreportâAccept(cid:72) âor 0 âReject(cid:72) â.Thesetwodecisionsdonotsummarizetheevidence.Instead,themagnitudeofthestatistic 0 T suggestsaâdegreeofevidenceâagainst(cid:72) .Howcanwetakethisintoaccount? 0 Theansweristoreportwhatisknownastheasymptoticp-value p=1âG(T). SincethedistributionfunctionGismonotonicallyincreasing,thep-valueisamonotonicallydecreasing functionofT andisanequivalentteststatistic.Insteadofrejecting(cid:72) atthesignificancelevelÎ±ifT >c, 0 wecanreject(cid:72) ifp<Î±.Thusitissufficienttoreportp,andletthereaderdecide.Inpractice,thep-value 0 iscalculatednumerically.Forexample,inMATLABthecommandis2*(1-normalcdf(abs(t))). Itisinstructivetointerpretpasthemarginalsignificancelevel:thesmallestvalueofÎ±forwhichthe testT ârejectsâthenullhypothesis. Thatis, p =0.11meansthatT rejects(cid:72) forallsignificancelevels 0 greaterthan0.11,butfailstoreject(cid:72) forsignificancelevelslessthan0.11. 0 Furthermore,theasymptoticp-valuehasaveryconvenientasymptoticnulldistribution.SinceT ââ d Î¾under(cid:72) ,thenp=1âG(T)ââ1âG(Î¾),whichhasthedistribution 0 d (cid:80)[1âG(Î¾)â¤u]=(cid:80)[1âuâ¤G(Î¾)] =1â(cid:80)(cid:163)Î¾â¤G â1(1âu) (cid:164) =1âG (cid:161) G â1(1âu) (cid:162) =1â(1âu) =u, which is the uniform distribution on [0,1]. (This calculation assumes that G(u) is strictly increasing which is true for conventional asymptotic distributions such as the normal.) Thus p ââU[0,1]. This d meansthattheâunusualnessâofp iseasiertointerpretthantheâunusualnessâofT. An important caveat is that the p-value p should not be interpreted as the probability that either hypothesis is true",
    "page": 247,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". (This calculation assumes that G(u) is strictly increasing which is true for conventional asymptotic distributions such as the normal.) Thus p ââU[0,1]. This d meansthattheâunusualnessâofp iseasiertointerpretthantheâunusualnessâofT. An important caveat is that the p-value p should not be interpreted as the probability that either hypothesis is true. A common mis-interpretation is that p is the probability âthat the null hypothesis is true.â This is incorrect. Rather, p is the marginal significance level â a measure of the strength of informationagainstthenullhypothesis.",
    "page": 247,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 228 Forat-statisticthep-valuecanbecalculatedeitherusingthenormaldistributionorthestudent t distribution,thelatterpresentedinSection5.12. p-valuescalculatedusingthestudentt willbeslightly larger,thoughthedifferenceissmallwhenthesamplesizeislarge. Returningtoourempiricalexample,forthetestthatthecoefficientonâMarriedMaleâiszerothep- valueis0.000. Thismeansthatitwouldbenearlyimpossibletoobserveat-statisticaslargeas22when thetruevalueofthecoefficientiszero.Whenpresentedwithsuchevidencewecansaythatweâstrongly rejectâthenullhypothesis,thatthetestisâhighlysignificantâ,orthatâthetestrejectsatanyconventional criticalvalueâ. Incontrast,thep-valueforthecoefficientonâMarriedFemaleâis0.094. Inthiscontextit istypicaltosaythatthetestisâclosetosignificantâ,meaningthatthep-valueislargerthan0.05,butnot toomuchlarger. Arelatedbutinferiorempiricalpracticeistoappendasterisks(*)tocoefficientestimatesorteststatis- ticstoindicatethelevelofsignificance. Acommonpracticetotoappendasingleasterisk(*)foranesti- mateorteststatisticwhichexceedsthe10%criticalvalue(i.e.,issignificantatthe10%level),appenda doubleasterisk(**)foratestwhichexceedsthe5%criticalvalue,andappendatripleasterisk(***)fora testwhichexceedsthe1%criticalvalue. Suchapracticecanbebetterthanatableofrawteststatistics astheasteriskspermitaquickinterpretationofsignificance. Ontheotherhand,asterisksareinferiorto p-values,whicharealsoeasyandquicktointerpret.Thegoalisessentiallythesame;itiswisertoreport p-valueswheneverpossibleandavoidtheuseofasterisks. Ourrecommendationisthatthebestempiricalpracticeistocomputeandreporttheasymptoticp- valuepratherthansimplytheteststatisticT,thebinarydecisionAccept/Reject,orappendingasterisks. Thep-valueisasimplestatistic,easytointerpret,andcontainsmoreinformationthantheotherchoices. Wenowsummarizethemainfeaturesofhypothesistesting. 1. SelectasignificancelevelÎ±. 2. SelectateststatisticT withasymptoticdistributionT ââÎ¾under(cid:72) . 0 d 3. Settheasymptoticcriticalvaluec sothat1âG(c)=Î±,whereG isthedistributionfunctionofÎ¾. 4. Calculatetheasymptoticp-valuep=1âG(T). 5. Reject(cid:72) ifT >c,orequivalentlyp<Î±. 0 6. Accept(cid:72) ifT â¤c,orequivalentlypâ¥Î±. 0 7. Reportp tosummarizetheevidenceconcerning(cid:72) versus(cid:72) . 0 1 9.8 t-ratiosandtheAbuseofTesting InSection4.21wearguedthatagoodappliedpracticeistoreportcoefficientestimatesÎ¸ (cid:98)andstan- darderrorss(Î¸ (cid:98))forallcoefficientsofinterestinestimatedmodels. WithÎ¸ (cid:98)ands(Î¸ (cid:98))thereadercaneasily constructconfidenceintervals[Î¸ (cid:98) Â±2s(Î¸ (cid:98))]andt-statistics (cid:161)Î¸ (cid:98) âÎ¸ 0 (cid:162) /s(Î¸ (cid:98))forhypothesesofinterest. Some applied papers (especially older ones) report t-ratios T = Î¸ (cid:98)/s(Î¸ (cid:98)) instead of standard errors. This is poor econometric practice. While the same information is being reported (you can back out standarderrorsbydivision,e.g. s(Î¸ (cid:98))=Î¸ (cid:98)/T),standarderrorsaregenerallymorehelpfultoreadersthan t-ratios",
    "page": 248,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Some applied papers (especially older ones) report t-ratios T = Î¸ (cid:98)/s(Î¸ (cid:98)) instead of standard errors. This is poor econometric practice. While the same information is being reported (you can back out standarderrorsbydivision,e.g. s(Î¸ (cid:98))=Î¸ (cid:98)/T),standarderrorsaregenerallymorehelpfultoreadersthan t-ratios. Standard errors help the reader focus on the estimation precision and confidence intervals, while t-ratios focus attention on statistical significance. While statistical significance is important, it is less important that the parameter estimates themselves and their confidence intervals. The focus shouldbeonthemeaningoftheparameterestimates,theirmagnitudes,andtheirinterpretation,noton",
    "page": 248,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 229 listingwhichvariableshavesignificant(e.g.non-zero)coefficients.Inmanymodernapplicationssample sizesareverylargesostandarderrorscanbeverysmall. Consequentlyt-ratioscanbelargeevenifthe coefficient estimates are economically small. In such contexts it may not be interesting to announce âThecoefficientisnon-zero!âInstead,whatisinterestingtoannounceisthatâThecoefficientestimate iseconomicallyinteresting!â Inparticular,someappliedpapersreportcoefficientestimatesandt-ratiosandlimittheirdiscussion oftheresultstodescribingwhichvariablesareâsignificantâ(meaningthattheirt-ratiosexceed2)and thesignsofthecoefficientestimates.Thisisverypoorempiricalworkandshouldbestudiouslyavoided. Itisalsoarecipeforbanishmentofyourworktolowertiereconomicsjournals. Fundamentally, thecommont-ratioisatestforthehypothesisthatacoefficientequalszero. This shouldbereportedanddiscussedwhenthisisaninterestingeconomichypothesisofinterest.Butifthis isnotthecaseitisdistracting. Oneproblemisthatstandardpackages,suchasStata,bydefaultreportt-statisticsandp-valuesfor every estimated coefficient. While this can be useful (as a user doesnât need to explicitly ask to test a desiredcoefficient)itcanbemisleadingasitmayunintentionallysuggestthattheentirelistoft-statistics andp-valuesareimportant.Instead,ausershouldfocusontestsofscientificallymotivatedhypotheses. In general, when a coefficient Î¸ is of interest it is constructive to focus on the point estimate, its standarderror,anditsconfidenceinterval. Thepointestimategivesourâbestguessâforthevalue. The standarderrorisameasureofprecision. Theconfidenceintervalgivesustherangeofvaluesconsistent withthedata. IfthestandarderrorislargethenthepointestimateisnotagoodsummaryaboutÎ¸.The endpoints of the confidence interval describe the bounds on the likely possibilities. If the confidence intervalembracestoobroadasetofvaluesforÎ¸thenthedatasetisnotsufficientlyinformativetorender usefulinferencesaboutÎ¸.Ontheotherhandiftheconfidenceintervalistightthenthedatahavepro- ducedanaccurateestimateandthefocusshouldbeonthevalueandinterpretationofthisestimate. In contrast,thestatementâthet-ratioishighlysignificantâhaslittleinterpretivevalue. TheabovediscussionrequiresthattheresearcherknowswhatthecoefficientÎ¸ means(intermsof theeconomicproblem)andcaninterpretvaluesandmagnitudes,notjustsigns.Thisiscriticalforgood appliedeconometricpractice. Forexample,considerthequestionabouttheeffectofmarriagestatusonmeanlogwages. Wehad foundthattheeffectisâhighlysignificantâformenandâclosetosignificantâforwomen. Now,letâscon- structasymptotic95%confidenceintervalsforthecoefficients. Theoneformenis[0.19,0.23]andthat forwomenis[â0.00,0.03].Thisshowsthataveragewagesformarriedmenareabout19-23%higherthan forunmarriedmen,whichissubstantial,whilethedifferenceforwomenisabout0-3%,whichissmall. Thesemagnitudesaremoreinformativethantheresultsofthehypothesistests. 9.9 WaldTests Thet-testisappropriatewhenthenullhypothesisisareal-valuedrestriction",
    "page": 249,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Now,letâscon- structasymptotic95%confidenceintervalsforthecoefficients. Theoneformenis[0.19,0.23]andthat forwomenis[â0.00,0.03].Thisshowsthataveragewagesformarriedmenareabout19-23%higherthan forunmarriedmen,whichissubstantial,whilethedifferenceforwomenisabout0-3%,whichissmall. Thesemagnitudesaremoreinformativethantheresultsofthehypothesistests. 9.9 WaldTests Thet-testisappropriatewhenthenullhypothesisisareal-valuedrestriction. Moregenerallythere maybemultiplerestrictionsonthecoefficientvectorÎ².Supposethatwehaveq >1restrictionswhich canbewrittenintheform(9.1). ItisnaturaltoestimateÎ¸=r(Î²)bytheplug-inestimatorÎ¸ (cid:98) =r (cid:161)Î² (cid:98) (cid:162) .To test(cid:72) 0 :Î¸=Î¸ 0 against(cid:72) 1 :Î¸(cid:54)=Î¸ 0 oneapproachistomeasurethemagnitudeofthediscrepancyÎ¸ (cid:98) âÎ¸ 0 . As this is a vector there is more than one measure of its length. One simple measure is the weighted quadraticformknownastheWaldstatistic.Thisis(7.37)evaluatedatthenullhypothesis W =W(Î¸ 0 )=(cid:161)Î¸ (cid:98) âÎ¸ 0 (cid:162)(cid:48) V(cid:98) â Î¸(cid:98) 1(cid:161)Î¸ (cid:98) âÎ¸ 0 (cid:162) (9.6)",
    "page": 249,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 230 â whereV(cid:98)Î¸(cid:98) =R(cid:98) (cid:48) V(cid:98)Î²(cid:98) R(cid:98) isanestimatorofV Î¸(cid:98) andR(cid:98) = âÎ² r(Î² (cid:98)) (cid:48) .NoticethatwecanwriteW alternativelyas W =n (cid:161)Î¸ (cid:98) âÎ¸ 0 (cid:162)(cid:48) V(cid:98) â Î¸ 1(cid:161)Î¸ (cid:98) âÎ¸ 0 (cid:162) usingtheasymptoticvarianceestimatorV(cid:98)Î¸,orwecanwriteitdirectlyasafunctionofÎ² (cid:98)as W =(cid:161) r(Î² (cid:98))âÎ¸ 0 (cid:162)(cid:48)(cid:179) R(cid:98) (cid:48) V(cid:98)Î²(cid:98) R(cid:98) (cid:180)â1(cid:161) r(Î² (cid:98))âÎ¸ 0 (cid:162) . Also,whenr(Î²)=R (cid:48)Î²isalinearfunctionofÎ²,thentheWaldstatisticsimplifiesto W =(cid:161) R (cid:48)Î² (cid:98) âÎ¸ 0 (cid:162)(cid:48)(cid:179) R (cid:48) V(cid:98)Î²(cid:98) R (cid:180)â1(cid:161) R (cid:48)Î² (cid:98) âÎ¸ 0 (cid:162) . TheWaldstatisticW isaweightedEuclideanmeasureofthelengthofthevectorÎ¸ (cid:98) âÎ¸ 0 .Whenq=1 thenW =T2,thesquareofthet-statistic, sohypothesistestsbasedonW and|T|areequivalent. The Waldstatistic(9.6)isageneralizationofthet-statistictothecaseofmultiplerestrictions. AstheWald statistic is symmetric in the argument Î¸ (cid:98) âÎ¸ 0 it treats positive and negative alternatives symmetrically. Thustheinherentalternativeisalwaystwo-sided. AsshowninTheorem7.13,whenÎ²satisfiesr(Î²)=Î¸ thenW ââÏ2,achi-squarerandomvariable 0 q d withqdegreesoffreedom.LetG (u)denotetheÏ2 distributionfunction.ForagivensignificancelevelÎ± q q theasymptoticcriticalvaluec satisfiesÎ±=1âG (c). Forexample,the5%criticalvaluesforq=1,q=2, q andq=3are3.84,5.99,and7.82,respectively,andingeneralthelevelÎ±criticalvaluecanbecalculated inMATLABaschi2inv(1-Î±,q). Anasymptotictestrejects(cid:72) infavorof(cid:72) ifW >c.Aswitht-tests,it 0 1 isconventionaltodescribeaWaldtestasâsignificantâifW exceedsthe5%asymptoticcriticalvalue. Theorem9.2 UnderAssumptions7.2,7.3,7.4,and(cid:72) :Î¸=Î¸ â(cid:82)q,thenW ââ 0 0 d Ï2. Forc satisfyingÎ±=1âG (c),(cid:80)(W >c|(cid:72) )ââÎ±sothetestâReject(cid:72) if q q 0 0 W >câhasasymptoticsizeÎ±. NoticethattheasymptoticdistributioninTheorem9.2dependssolelyonq,thenumberofrestric- tionsbeingtested.Itdoesnotdependonk,thenumberofparametersestimated. Theasymptoticp-valueforW isp=1âG (W),andthisisparticularlyusefulwhentestingmultiple q restrictions. For example, if you write that a Wald test on eight restrictions (q =8) has the valueW = 11.2itisdifficultforareadertoassessthemagnitudeofthisstatisticunlesstheyhavequickaccesstoa statisticaltableorsoftware. Instead,ifyouwritethatthep-valueisp=0.19(asisthecaseforW =11.2 and q =8) then it is simple for a reader to interpret its magnitude as âinsignificantâ. To calculate the asymptoticp-valueforaWaldstatisticinMATLABusethecommand1-chi2cdf(w,q). Somepackages(includingStata)andpapersreportF versionsofWaldstatistics.ForanyWaldstatis- ticW whichtestsaq-dimensionalrestriction,theF versionofthetestis F =W/q. WhenF isreported, itisconventionaltouseF q,nâk criticalvaluesandp-valuesratherthanÏ2 q values",
    "page": 250,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Instead,ifyouwritethatthep-valueisp=0.19(asisthecaseforW =11.2 and q =8) then it is simple for a reader to interpret its magnitude as âinsignificantâ. To calculate the asymptoticp-valueforaWaldstatisticinMATLABusethecommand1-chi2cdf(w,q). Somepackages(includingStata)andpapersreportF versionsofWaldstatistics.ForanyWaldstatis- ticW whichtestsaq-dimensionalrestriction,theF versionofthetestis F =W/q. WhenF isreported, itisconventionaltouseF q,nâk criticalvaluesandp-valuesratherthanÏ2 q values. TheconnectionbetweenWaldandFstatisticsisdemonstratedinSection9.14whereweshowthatwhen Wald statistics are calculated using a homoskedastic covariance matrix then F =W/q is identicial to",
    "page": 250,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 231 the F statistic of (5.19). While there is no formal justification to using the F q,nâk distribution for non- homoskedasticcovariancematrices,theF q,nâk distributionprovidescontinuitywiththeexactdistribu- tiontheoryundernormalityandisabitmoreconservativethantheÏ2 distribution. (Furthermore,the q differenceissmallwhennâk ismoderatelylarge.) ToimplementatestofzerorestrictionsinStataaneasymethodistousethecommandtest X1 X2 whereX1andX2arethenamesofthevariableswhosecoefficientsarehypothesizedtoequalzero.TheF versionoftheWaldstatisticisreportedusingthecovariancematrixcalculatedbythemethodspecified intheregressioncommand.Ap-valueisreported,calculatedusingtheF q,nâk distribution. To illustrate, consider the empirical results presented in Table 4.1. The hypothesis âUnion mem- bership does not affect wagesâ is the joint restriction that both coefficients on âMale Union Memberâ andâFemaleUnionMemberâarezero. WecalculatetheWaldstatisticforthisjointhypothesisandfind W =23(orF =12.5)withap-valueof p =0.000.Thuswerejectthenullhypothesisinfavoroftheal- ternativethatatleastoneofthecoefficientsisnon-zero. Thisdoesnotmeanthatbothcoefficientsare non-zero,justthatoneofthetwoisnon-zero. ThereforeexaminingboththejointWaldstatisticandthe individualt-statisticsisusefulforinterpretation. Asasecondexamplefromthesameregression,takethehypothesisthatmarriedstatushasnoeffect on mean wages for women. This is the joint restriction that the coefficients on âMarried Femaleâ and âFormerly Married Femaleâ are zero. The Wald statistic for this hypothesis isW =6.4 (F =3.2) with a p-valueof0.04. Suchap-valueistypicallycalledâmarginallysignificantâinthesensethatitisslightly smallerthan0.05. TheWaldstatisticwasproposedbyWald(1943). AbrahamWald The Hungarian mathematician/statistician/econometrician Abraham Wald (1902-1950) developed an optimality property for the Wald test in terms of weighted average power. He also developed the field of sequential testing, the designofexperiments,andoneofthefirstinstrumentalvariableestimators. 9.10 HomoskedasticWaldTests IftheerrorisknowntobehomoskedasticthenitisappropriatetousethehomoskedasticWaldstatis- 0 tic(7.38)whichreplacesV(cid:98)Î¸(cid:98) withthehomoskedasticestimatorV(cid:98)Î¸(cid:98) .Thisstatisticequals W0=(cid:161)Î¸ (cid:98) âÎ¸ 0 (cid:162)(cid:48)(cid:179) V(cid:98) 0 Î¸(cid:98) (cid:180)â1(cid:161)Î¸ (cid:98) âÎ¸ 0 (cid:162) =(cid:161) r(Î² (cid:98))âÎ¸ 0 (cid:162)(cid:48)(cid:179) R (cid:48)(cid:161) X (cid:48) X (cid:162)â1 R(cid:98) (cid:180)â1(cid:161) r(Î² (cid:98))âÎ¸ 0 (cid:162) /s2. Inthecaseoflinearhypotheses(cid:72) :R (cid:48)Î²=Î¸ wecanwritethisas 0 0 W0=(cid:161) R (cid:48)Î² (cid:98) âÎ¸ 0 (cid:162)(cid:48)(cid:179) R (cid:48)(cid:161) X (cid:48) X (cid:162)â1 R (cid:180)â1(cid:161) R (cid:48)Î² (cid:98) âÎ¸ 0 (cid:162) /s2. (9.7) We call W0 a homoskedastic Wald statistic as it is appropriate when the errors are conditionally ho- moskedastic",
    "page": 251,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Inthecaseoflinearhypotheses(cid:72) :R (cid:48)Î²=Î¸ wecanwritethisas 0 0 W0=(cid:161) R (cid:48)Î² (cid:98) âÎ¸ 0 (cid:162)(cid:48)(cid:179) R (cid:48)(cid:161) X (cid:48) X (cid:162)â1 R (cid:180)â1(cid:161) R (cid:48)Î² (cid:98) âÎ¸ 0 (cid:162) /s2. (9.7) We call W0 a homoskedastic Wald statistic as it is appropriate when the errors are conditionally ho- moskedastic. When q = 1 then W0 = T2, the square of the t-statistic where the latter is computed with a ho- moskedasticstandarderror.",
    "page": 251,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 232 Theorem9.3 UnderAssumptions7.2and7.3,(cid:69)(cid:163) e2|X (cid:164)=Ï2>0,and(cid:72) :Î¸= 0 Î¸ â(cid:82)q,thenW0ââÏ2. Forc satisfyingÎ±=1âG (c),(cid:80)(cid:163) W0>c|(cid:72) (cid:164)ââÎ±so 0 q q 0 d thetestâReject(cid:72) ifW0>câhasasymptoticsizeÎ±. 0 9.11 Criterion-BasedTests TheWaldstatisticisbasedonthelengthofthevectorÎ¸ (cid:98) âÎ¸ 0 : thediscrepancybetweentheestimator Î¸ (cid:98) =r(Î² (cid:98))andthehypothesizedvalueÎ¸ 0 .Analternativeclassoftestsisbasedonthediscrepancybetween thecriterionfunctionminimizedwithandwithouttherestriction. Criterion-basedtestingapplieswhenwehaveacriterionfunction,sayJ(Î²)withÎ²âB,whichismin- imizedforestimation,andthegoalistotest(cid:72) :Î²âB versus(cid:72) :Î²âB whereB âÎ². Minimizingthe 0 0 1 0 0 criterionfunctionoverB andB weobtaintheunrestrictedandrestrictedestimators 0 Î² (cid:98) =argmin J (cid:161)Î²(cid:162) Î²âB Î² (cid:101) =argmin J (cid:161)Î²(cid:162) . Î²âB0 Thecriterion-basedstatisticfor(cid:72) versus(cid:72) isproportionalto 0 1 J=min J (cid:161)Î²(cid:162)âmin J (cid:161)Î²(cid:162)=J(Î² (cid:101))âJ(Î² (cid:98)). Î²âB0 Î²âB Thecriterion-basedstatisticJissometimescalledadistancestatistic,aminimum-distancestatistic, oralikelihood-ratio-likestatistic. SinceB 0 isasubsetofB,J(Î² (cid:101))â¥J(Î² (cid:98))andthusJâ¥0.ThestatisticJ measuresthecostonthecriterion ofimposingthenullrestrictionÎ²âB . 0 9.12 MinimumDistanceTests Theminimumdistancetestisbasedontheminimumdistancecriterion(8.19) J (cid:161)Î²(cid:162)=n (cid:161)Î² (cid:98) âÎ²(cid:162)(cid:48) W(cid:99) (cid:161)Î² (cid:98) âÎ²(cid:162) (9.8) withÎ² (cid:98)theunrestrictedleastsquaresestimator. TherestrictedestimatorÎ² (cid:101)md minimizes(9.8)subjectto Î²âB 0 .ObservingthatJ(Î² (cid:98))=0,theminimumdistancestatisticsimplifiesto J=J(Î² (cid:101)md )=n (cid:161)Î² (cid:98) âÎ² (cid:101)md (cid:162)(cid:48) W(cid:99) (cid:161)Î² (cid:98) âÎ² (cid:101)md (cid:162) . (9.9) The efficient minimum distance estimator Î² (cid:101)emd is obtained by settingW(cid:99) =V(cid:98) â Î² 1 in (9.8) and (9.9). Theefficientminimumdistancestatisticfor(cid:72) :Î²âB istherefore 0 0 J â=n (cid:161)Î² (cid:98) âÎ² (cid:101)emd (cid:162)(cid:48) V(cid:98) â Î² 1(cid:161)Î² (cid:98) âÎ² (cid:101)emd (cid:162) . (9.10) Considertheclassoflinearhypotheses(cid:72) :R (cid:48)Î²=Î¸ .Inthiscaseweknowfrom(8.25)thattheeffi- 0 0 cientminimumdistanceestimatorÎ² (cid:101)emd subjecttotheconstraintR (cid:48)Î²=Î¸ 0 is Î² (cid:101)emd =Î² (cid:98) âV(cid:98)Î²R (cid:161) R (cid:48) V(cid:98)Î²R (cid:162)â1(cid:161) R (cid:48)Î² (cid:98) âÎ¸ 0 (cid:162)",
    "page": 252,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 233 andthus Î² (cid:98) âÎ² (cid:101)emd =V(cid:98)Î²R (cid:161) R (cid:48) V(cid:98)Î²R (cid:162)â1(cid:161) R (cid:48)Î² (cid:98) âÎ¸ 0 (cid:162) . Substitutinginto(9.10)wefind J â=n (cid:161) R (cid:48)Î² (cid:98) âÎ¸ 0 (cid:162)(cid:48)(cid:161) R (cid:48) V(cid:98)Î²R (cid:162)â1 R (cid:48) V(cid:98)Î²V(cid:98) â Î² 1 V(cid:98)Î²R (cid:161) R (cid:48) V(cid:98)Î²R (cid:162)â1(cid:161) R (cid:48)Î² (cid:98) âÎ¸ 0 (cid:162) =n (cid:161) R (cid:48)Î² (cid:98) âÎ¸ 0 (cid:162)(cid:48)(cid:161) R (cid:48) V(cid:98)Î²R (cid:162)â1(cid:161) R (cid:48)Î² (cid:98) âÎ¸ 0 (cid:162) =W, whichistheWaldstatistic(9.6). Thusforlinearhypotheses(cid:72) :R (cid:48)Î²=Î¸ , theefficientminimumdistancestatistic J â isidenticalto 0 0 theWaldstatistic(9.6). Fornonlinearhypotheses, however,theWaldandminimumdistancestatistics aredifferent. â NeweyandWest(1987a)establishedtheasymptoticnulldistributionofJ . Theorem9.4 UnderAssumptions7.2,7.3,7.4,and(cid:72) :Î¸=Î¸ â(cid:82)q,J âââÏ2. 0 0 q d â TestingusingtheminimumdistancestatisticJ issimilartotestingusingtheWaldstatisticW.Criti- calvaluesandp-valuesarecomputedusingtheÏ2 distribution.(cid:72) isrejectedinfavorof(cid:72) ifJ â exceeds q 0 1 thelevelÎ±criticalvalue, whichcanbecalculatedinMATLABaschi2inv(1-Î±,q). Theasymptoticp- valueisp=1âG (J â ).InMATLAB,usethecommand1-chi2cdf(J,q). q WenowdemonstrateTheorem9.4. TheconditionsofTheorem8.10hold,since(cid:72) impliesAssump- 0 tion8.1.From(8.54)withW(cid:99) =V(cid:98)Î²,weseethat (cid:112) (cid:112) n (cid:161)Î² (cid:98) âÎ² (cid:101)emd (cid:162)=V(cid:98)Î²R(cid:98) (cid:161) R â n (cid:48) V(cid:98)Î²R(cid:98) (cid:162)â1 R â n (cid:48) n (cid:161)Î² (cid:98) âÎ²(cid:162) ââVÎ²R (cid:161) R (cid:48) VÎ²R (cid:162)â1 R (cid:48) N(0,VÎ²)=VÎ²R Z d whereZ â¼N(0, (cid:161) R (cid:48) VÎ²R (cid:162)â1 ).Thus J â=n (cid:161)Î² (cid:98) âÎ² (cid:101)emd (cid:162)(cid:48) V(cid:98) â Î² 1(cid:161)Î² (cid:98) âÎ² (cid:101)emd (cid:162)ââZ (cid:48) R (cid:48) VÎ²V â Î² 1VÎ²R Z =Z (cid:48)(cid:161) R (cid:48) VÎ²R (cid:162) Z =Ï2 q d asclaimed. 9.13 MinimumDistanceTestsUnderHomoskedasticity IfwesetW(cid:99) =Q(cid:98)XX /s2in(9.8)weobtainthecriterion(8.20) J0(cid:161)Î²(cid:162)=n (cid:161)Î² (cid:98) âÎ²(cid:162)(cid:48) Q(cid:98)XX (cid:161)Î² (cid:98) âÎ²(cid:162) /s2. Aminimumdistancestatisticfor(cid:72) :Î²âB is 0 0 J0=min J0(cid:161)Î²(cid:162) . Î²âB0",
    "page": 253,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 234 Equation (8.21) showed that SSE(Î²) = nÏ2+s2J0(cid:161)Î²(cid:162) . So the minimizers of SSE(Î²) and J0(cid:161)Î²(cid:162) are (cid:98) identical.ThustheconstrainedminimizerofJ0(cid:161)Î²(cid:162) isconstrainedleastsquares Î² (cid:101)cls =argminJ0(cid:161)Î²(cid:162)=argminSSE(Î²) (9.11) Î²âB0 Î²âB0 andtherefore J n 0= J n 0(Î² (cid:101)cls )=n (cid:161)Î² (cid:98) âÎ² (cid:101)cls (cid:162)(cid:48) Q(cid:98)XX (cid:161)Î² (cid:98) âÎ² (cid:101)cls (cid:162) /s2. Inthespecialcaseoflinearhypotheses(cid:72) :R (cid:48)Î²=Î¸ ,theconstrainedleastsquaresestimatorsubject 0 0 toR (cid:48)Î²=Î¸ hasthesolution(8.9) 0 Î² (cid:101)cls =Î² (cid:98) âQ(cid:98) â X 1 X R (cid:179) R (cid:48) Q(cid:98) â X 1 X R (cid:180)â1(cid:161) R (cid:48)Î² (cid:98) âÎ¸ 0 (cid:162) andsolvingwefind J0=n (cid:161) R (cid:48)Î² (cid:98) âÎ¸ 0 (cid:162)(cid:48)(cid:179) R (cid:48) Q(cid:98) â X 1 X R (cid:180)â1(cid:161) R (cid:48)Î² (cid:98) âÎ¸ 0 (cid:162) /s2=W0. ThisisthehomoskedasticWaldstatistic(9.7). Thusfortestinglinearhypotheses,homoskedasticmini- mumdistanceandWaldstatisticsagree. Fornonlinearhypothesestheydisagree,buthavethesamenullasymptoticdistribution. Theorem9.5 UnderAssumptions7.2and7.3,(cid:69)(cid:163) e2|X (cid:164)=Ï2>0,and(cid:72) :Î¸= 0 Î¸ â(cid:82)q,thenJ0ââÏ2. 0 q d 9.14 FTests InSection5.13weintroducedtheF testforexclusionrestrictionsinthenormalregressionmodel.In thissectionwegeneralizethistesttoabroadersetofrestrictions.LetB â(cid:82)kbeaconstrainedparameter 0 spacewhichimposesq restrictionsonÎ². LetÎ² (cid:98)ols betheunrestrictedleastsquaresestimatorandletÏ (cid:98) 2=n â1(cid:80)n i=1 (cid:161) Y i âX i (cid:48)Î² (cid:98)ols (cid:162)2 betheassoci- atedestimatorofÏ2.LetÎ² (cid:101)cls betheCLSestimator(9.11)satisfyingÎ² (cid:101)cls âB 0 andletÏ2=n â1(cid:80)n i=1 (cid:161) Y i âX i (cid:48)Î² (cid:101)cls (cid:162)2 betheassociatedestimatorofÏ2. TheF statisticfortesting(cid:72) :Î²âB is 0 0 (cid:161)Ï2âÏ2(cid:162) /q F = (cid:101) (cid:98) . (9.12) Ï2/(nâk) (cid:98) Wecanalternativelywrite F = SSE(Î² (cid:101)cls )âSSE(Î² (cid:98)ols ) (9.13) qs2 whereSSE(Î²)=(cid:80)n (cid:161) Y âX (cid:48)Î²(cid:162)2 isthesum-of-squarederrors. i=1 i i ThisshowsthatF isacriterion-basedstatistic.Using(8.21)wecanalsowriteF =J0/q,sotheFstatis- ticisidenticaltothehomoskedasticminimumdistancestatisticdividedbythenumberofrestrictions q. Aswediscussedintheprevioussection,inthespecialcaseoflinearhypotheses(cid:72) :R (cid:48)Î²=Î¸ , J0= 0 0 W0. It follows that in this case F = W0/q. Thus for linear restrictions the F statistic equals the ho- moskedasticWaldstatisticdividedbyq.Itfollowsthattheyareequivalenttestsfor(cid:72) against(cid:72) . 0 1",
    "page": 254,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 235 Theorem9.6 For tests of linear hypotheses (cid:72) :R (cid:48)Î²=Î¸ â(cid:82)q, the F statistic 0 0 equals F =W0/q whereW0 is the homoskedastic Wald statistic. Thus under 7.2,(cid:69)(cid:163) e2|X (cid:164)=Ï2>0,and(cid:72) :Î¸=Î¸ ,thenF ââÏ2/q. 0 0 q d WhenusinganF statisticitisconventionaltousetheF q,nâk distributionforcriticalvaluesandp- values. CriticalvaluesaregiveninMATLABbyfinv(1-Î±,q,n-k)andp-valuesby1-fcdf(F,q,n-k). Alternatively,theÏ2/q distributioncanbeused,usingchi2inv(1-Î±,q)/qand1-chi2cdf(F*q,q),re- q spectively. Using the F q,nâk distribution is a prudent small sample adjustment which yields exact an- swersiftheerrorsarenormalandotherwiseslightlyincreasingthecriticalvaluesandp-valuesrelative totheasymptoticapproximation.Onceagain,ifthesamplesizeissmallenoughthatthechoicemakesa differencethenprobablyweshouldnâtbetrustingtheasymptoticapproximationanyway! Anelegantfeatureabout(9.12)or(9.13)isthattheyaredirectlycomputablefromthestandardoutput fromtwosimpleOLSregressions,asthesumofsquarederrors(orregressionvariance)isatypicalprinted outputfromstatisticalpackagesandisoftenreportedinappliedtables.ThusF canbecalculatedbyhand fromstandardreportedstatisticsevenifyoudonâthavetheoriginaldata(orifyouaresittinginaseminar andlisteningtoapresentation!). IfyouarepresentedwithanF statistic(oraWaldstatistic,asyoucanjustdividebyq)butdonâthave accesstocriticalvalues,ausefulruleofthumbistoknowthatforlargenthe5%asymptoticcriticalvalue isdecreasingasq increasesandislessthan2forqâ¥7. Awordofwarning:InmanystatisticalpackageswhenanOLSregressionisestimatedanâF-statisticâ isautomaticallyreportedeventhoughnohypothesistestwasrequested. Whatthepackageisreporting isanF statisticofthehypothesisthatallslopecoefficients1 arezero. Thiswasapopularstatisticinthe earlydaysofeconometricreportingwhensamplesizeswereverysmallandresearcherswantedtoknow iftherewasâanyexplanatorypowerâtotheirregression. Thisisrarelyanissuetodayassamplesizesare typicallysufficientlylargethatthisF statisticisnearlyalwayshighlysignificant. Whiletherearespecial caseswherethisF statisticisusefulthesecasesarenottypical. Asageneralrulethereisnoreasonto reportthisF statistic. 9.15 HausmanTests Hausman(1978)introducedageneralideaabouthowtotestahypothesis(cid:72) .Ifyouhavetwoestima- 0 tors,onewhichisefficientunder(cid:72) butinconsistentunder(cid:72) ,andanotherwhichisconsistentunder 0 1 (cid:72) ,thenconstructatestasaquadraticforminthedifferencesoftheestimators. Inthecaseoftestinga 1 hypothesis(cid:72) 0 :r(Î²)=Î¸ 0 letÎ² (cid:98)ols denotetheunconstrainedleastsquaresestimatorandletÎ² (cid:101)emd denote theefficientminimumdistanceestimatorwhichimposesr(Î²)=Î¸ . Bothestimatorsareconsistentun- 0 der(cid:72) 0 butÎ² (cid:101)emd isasymptoticallyefficient. Under(cid:72) 1 , Î² (cid:98)ols isconsistentforÎ²butÎ² (cid:101)emd isinconsistent",
    "page": 255,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Inthecaseoftestinga 1 hypothesis(cid:72) 0 :r(Î²)=Î¸ 0 letÎ² (cid:98)ols denotetheunconstrainedleastsquaresestimatorandletÎ² (cid:101)emd denote theefficientminimumdistanceestimatorwhichimposesr(Î²)=Î¸ . Bothestimatorsareconsistentun- 0 der(cid:72) 0 butÎ² (cid:101)emd isasymptoticallyefficient. Under(cid:72) 1 , Î² (cid:98)ols isconsistentforÎ²butÎ² (cid:101)emd isinconsistent. Thedifferencehastheasymptoticdistribution (cid:112) n (cid:161)Î² (cid:98)ols âÎ² (cid:101)emd (cid:162)ââN (cid:179) 0,VÎ²R (cid:161) R (cid:48) VÎ²R (cid:162)â1 R (cid:48) VÎ² (cid:180) . d Let A â denotetheMoore-Penrosegeneralizedinverse.TheHausmanstatisticfor(cid:72) is 0 H=(cid:161)Î² (cid:98)ols âÎ² (cid:101)emd (cid:162)(cid:48) a (cid:129) var (cid:161)Î² (cid:98)ols âÎ² (cid:101)emd (cid:162)â(cid:161)Î² (cid:98)ols âÎ² (cid:101)emd (cid:162) =n (cid:161)Î² (cid:98)ols âÎ² (cid:101)emd (cid:162)(cid:48) (cid:181) V(cid:98)Î²R(cid:98) (cid:179) R(cid:98) (cid:48) V(cid:98)Î²R(cid:98) (cid:180)â1 R(cid:98) (cid:48) V(cid:98)Î² (cid:182)â (cid:161)Î² (cid:98)ols âÎ² (cid:101)emd (cid:162) . 1Allcoefficientsexcepttheintercept.",
    "page": 255,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 236 1/2 (cid:179) (cid:48) (cid:180)â1 (cid:48) 1/2 ThematrixV(cid:98)Î² R(cid:98) R(cid:98) V(cid:98)Î²R(cid:98) R(cid:98) V(cid:98)Î² idempotentsoitsgeneralizedinverseisitself. (SeeSectionA.11.) It followsthat (cid:181) V(cid:98)Î²R(cid:98) (cid:179) R(cid:98) (cid:48) V(cid:98)Î²R(cid:98) (cid:180)â1 R(cid:98) (cid:48) V(cid:98)Î² (cid:182)â =V(cid:98) â Î² 1/2 (cid:181) V(cid:98) 1 Î² /2 R(cid:98) (cid:179) R(cid:98) (cid:48) V(cid:98)Î²R(cid:98) (cid:180)â1 R(cid:98) (cid:48) V(cid:98) 1 Î² /2 (cid:182)â V(cid:98) â Î² 1/2 =V(cid:98) â Î² 1/2 V(cid:98) 1 Î² /2 R(cid:98) (cid:179) R(cid:98) (cid:48) V(cid:98)Î²R(cid:98) (cid:180)â1 R(cid:98) (cid:48) V(cid:98) 1 Î² /2 V(cid:98) â Î² 1/2 (cid:179) (cid:48) (cid:180)â1 (cid:48) =R(cid:98) R(cid:98) V(cid:98)Î²R(cid:98) R(cid:98) . ThustheHausmanstatisticis H=n (cid:161)Î² (cid:98)ols âÎ² (cid:101)emd (cid:162)(cid:48) R(cid:98) (cid:179) R(cid:98) (cid:48) V(cid:98)Î²R(cid:98) (cid:180)â1 R(cid:98) (cid:48)(cid:161)Î² (cid:98)ols âÎ² (cid:101)emd (cid:162) . Inthecontextoflinearrestrictions,R(cid:98) =R andR (cid:48)Î² (cid:101) =Î¸ 0 sothestatistictakestheform H=n (cid:161) R (cid:48)Î² (cid:98)ols âÎ¸ 0 (cid:162)(cid:48) R(cid:98) (cid:161) R (cid:48) V(cid:98)Î²R (cid:162)â1(cid:161) R (cid:48)Î² (cid:98)ols âÎ¸ 0 (cid:162) , whichispreciselytheWaldstatistic.WithnonlinearrestrictionsW andH candiffer. IneithercaseweseethatthattheasymptoticnulldistributionoftheHausmanstatistic H isÏ2,so q the appropriate test is to reject (cid:72) in favor of (cid:72) if H >c where c is a critical value taken from the Ï2 0 1 q distribution. Theorem9.7 ForgeneralhypothesestheHausmanteststatisticis H=n (cid:161)Î² (cid:98)ols âÎ² (cid:101)emd (cid:162)(cid:48) R(cid:98) (cid:179) R(cid:98) (cid:48) V(cid:98)Î²R(cid:98) (cid:180)â1 R(cid:98) (cid:48)(cid:161)Î² (cid:98)ols âÎ² (cid:101)emd (cid:162) . UnderAssumptions7.2,7.3,7.4,and(cid:72) :r(Î²)=Î¸ â(cid:82)q,HââÏ2. 0 0 q d 9.16 ScoreTests Scoretestsaretraditionallyderivedinlikelihoodanalysisbutcanmoregenerallybeconstructedfrom first-orderconditionsevaluatedatrestrictedestimates.Wefocusonthelikelihoodderivation. Giventheloglikelihoodfunction(cid:96) n (Î²,Ï2), arestriction(cid:72) 0 :r (cid:161)Î²(cid:162)=Î¸ 0 , andrestrictedestimatorsÎ² (cid:101) andÏ2,thescorestatisticfor(cid:72) isdefinedas (cid:101) 0 (cid:181) â (cid:182)(cid:48)(cid:181) â2 (cid:182)â1(cid:181) â (cid:182) S= âÎ² (cid:96) n (Î² (cid:101),Ï (cid:101) 2) â âÎ²âÎ²(cid:48) (cid:96) n (Î² (cid:101),Ï (cid:101) 2) âÎ² (cid:96) n (Î² (cid:101),Ï (cid:101) 2) . Theideaisthatiftherestrictionistruethentherestrictedestimatorsshouldbeclosetothemaximum ofthelog-likelihoodwherethederivativeiszero. However iftherestrictionisfalsethentherestricted estimatorsshouldbedistantfromthemaximumandthederivativeshouldbelarge. Hencesmallvalues ofSareexpectedunder(cid:72) andlargevaluesunder(cid:72) .Testsof(cid:72) rejectforlargevaluesofS",
    "page": 256,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Theideaisthatiftherestrictionistruethentherestrictedestimatorsshouldbeclosetothemaximum ofthelog-likelihoodwherethederivativeiszero. However iftherestrictionisfalsethentherestricted estimatorsshouldbedistantfromthemaximumandthederivativeshouldbelarge. Hencesmallvalues ofSareexpectedunder(cid:72) andlargevaluesunder(cid:72) .Testsof(cid:72) rejectforlargevaluesofS. 0 1 0 Weexplorethescorestatisticinthecontextofthenormalregressionmodelandlinearhypotheses r (cid:161)Î²(cid:162)=R (cid:48)Î².Recallthatinthenormalregressionlog-likelihoodfunctionis (cid:96) (Î²,Ï2)=â n log(2ÏÏ2)â 1 (cid:88) n (cid:161) Y âX (cid:48)Î²(cid:162)2 . n 2 2Ï2 i i i=1",
    "page": 256,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 237 TheconstrainedMLEunderlinearhypothesesisconstrainedleastsquares Î² (cid:101) =Î² (cid:98) â(cid:161) X (cid:48) X (cid:162)â1 R (cid:104) R (cid:48)(cid:161) X (cid:48) X (cid:162)â1 R (cid:105)â1(cid:161) R (cid:48)Î² (cid:98) âc (cid:162) e (cid:101)i =Y i âX i (cid:48)Î² (cid:101) Ï2= 1 (cid:88) n e2. (cid:101) n (cid:101)i i=1 WecancalculatethatthederivativeandHessianare â â Î² (cid:96) n (Î² (cid:101),Ï (cid:101) 2)= Ï 1 2 (cid:88) n X i (cid:161) Y i âX i (cid:48)Î² (cid:101) (cid:162)= Ï 1 2 X (cid:48) (cid:101) e (cid:101) i=1 (cid:101) â âÎ² â â 2 Î²(cid:48) (cid:96) n (Î² (cid:101),Ï (cid:101) 2)= Ï 1 2 (cid:88) n X i X i (cid:48)= Ï 1 2 X (cid:48) X. (cid:101) i=1 (cid:101) Since (cid:101) e=Y âXÎ² (cid:101)wecanfurthercalculatethat â â Î² (cid:96) n (Î² (cid:101),Ï (cid:101) 2)= Ï 1 2 (cid:161) X (cid:48) X (cid:162) (cid:179) (cid:161) X (cid:48) X (cid:162)â1 X (cid:48) Y â(cid:161) X (cid:48) X (cid:162)â1 X (cid:48) XÎ² (cid:101) (cid:180) (cid:101) = 1 (cid:161) X (cid:48) X (cid:162)(cid:161)Î² (cid:98) âÎ² (cid:101) (cid:162) Ï2 (cid:101) = 1 R (cid:104) R (cid:48)(cid:161) X (cid:48) X (cid:162)â1 R (cid:105)â1(cid:161) R (cid:48)Î² (cid:98) âc (cid:162) . Ï2 (cid:101) Togetherwefindthat S=(cid:161) R (cid:48)Î² (cid:98) âc (cid:162)(cid:48)(cid:179) R (cid:48)(cid:161) X (cid:48) X (cid:162)â1 R (cid:180)â1(cid:161) R (cid:48)Î² (cid:98) âc (cid:162) /Ï (cid:101) 2. ThisisidenticaltothehomoskedasticWaldstatisticwiths2replacedbyÏ2. WecanalsowriteS asa (cid:101) monotonictransformationoftheF statistic,since S=n (cid:161)Ï (cid:101) 2âÏ (cid:98) 2(cid:162) =n (cid:181) 1â Ï (cid:98) 2(cid:182) =n (cid:195) 1â 1 (cid:33) . Ï (cid:101) 2 Ï (cid:101) 2 1+ nâ q k F ThetestâReject(cid:72) forlargevaluesofSâisidenticaltothetestâReject(cid:72) forlargevaluesofFâsothey 0 0 areidenticaltests.SinceforthenormalregressionmodeltheexactdistributionofF isknown,itisbetter tousetheF statisticwithF p-values. Inmorecomplicatedsettingsapotentialadvantageofscoretestsisthattheyarecalculatedusingthe restrictedparameterestimatesÎ² (cid:101)ratherthantheunrestrictedestimatesÎ² (cid:98).ThuswhenÎ² (cid:101)isrelativelyeasy tocalculatetherecanbeapreferenceforscorestatistics.Thisisnotaconcernforlinearrestrictions. Moregenerally,scoreandscore-likestatisticscanbeconstructedfromfirst-orderconditionsevalu- atedatrestrictedparameterestimates. Also, whenteststatisticsareconstructedusingcovariancema- trixestimatorswhicharecalculatedusingrestrictedparameterestimates(e.g. restrictedresiduals)then theseareoftendescribedasscoretests. AnexampleofthelatteristheWald-typestatistic W =(cid:161) r(Î² (cid:98))âÎ¸ 0 (cid:162)(cid:48)(cid:179) R(cid:98) (cid:48) V(cid:101)Î²(cid:98) R(cid:98) (cid:180)â1(cid:161) r(Î² (cid:98))âÎ¸ 0 (cid:162) wherethecovariancematrixestimateV(cid:101)Î²(cid:98) iscalculatedusingtherestrictedresidualse (cid:101)i =Y i âX i (cid:48)Î² (cid:101)",
    "page": 257,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". restrictedresiduals)then theseareoftendescribedasscoretests. AnexampleofthelatteristheWald-typestatistic W =(cid:161) r(Î² (cid:98))âÎ¸ 0 (cid:162)(cid:48)(cid:179) R(cid:98) (cid:48) V(cid:101)Î²(cid:98) R(cid:98) (cid:180)â1(cid:161) r(Î² (cid:98))âÎ¸ 0 (cid:162) wherethecovariancematrixestimateV(cid:101)Î²(cid:98) iscalculatedusingtherestrictedresidualse (cid:101)i =Y i âX i (cid:48)Î² (cid:101). This maybeagoodchoicewhenÎ²andÎ¸arehigh-dimensionalasinthiscontexttheremaybeworrythatthe estimatorV(cid:98)Î²(cid:98) isimprecise.",
    "page": 257,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 238 9.17 ProblemswithTestsofNonlinearHypotheses Whilethet andWaldtestsworkwellwhenthehypothesisisalinearrestrictiononÎ²,theycanwork quitepoorlywhentherestrictionsarenonlinear. Thiscanbeseenbyasimpleexampleintroducedby LafontaineandWhite(1986). TakethemodelY â¼N(Î²,Ï2)andconsiderthehypothesis(cid:72) 0 :Î²=1. LetÎ² (cid:98) andÏ2bethesamplemeanandvarianceofY.ThestandardWaldstatistictotest(cid:72) is (cid:98) 0 (cid:161)Î² (cid:98) â1 (cid:162)2 W =n . Ï2 (cid:98) Now notice that (cid:72) is equivalent to the hypothesis (cid:72) (s):Î²s =1 for any positive integer s. Letting 0 0 r(Î²)=Î²s,andnotingR=sÎ²sâ1,wefindthattheWaldstatistictotest(cid:72) (s)is 0 (cid:161)Î² (cid:98) sâ1 (cid:162)2 W =n . s Ï (cid:98) 2s2Î² (cid:98)2sâ2 While the hypothesis Î²s =1 is unaffected by the choice of s, the statistic W varies with s. This is an s unfortunatefeatureoftheWaldstatistic. Todemonstratethiseffect,wehaveplottedinFigure9.2theWaldstatisticW asafunctionofs,set- s ting n/Ï (cid:98) 2 =10. The increasing solid line is for the case Î² (cid:98) =0.8. The decreasing dashed line is for the caseÎ² (cid:98) =1.6.Itiseasytoseethatineachcasetherearevaluesof s forwhichtheteststatisticissignifi- cantrelativetoasymptoticcriticalvalues,whilethereareothervaluesofs forwhichtheteststatisticis insignificant.Thisisdistressingsincethechoiceofsisarbitraryandirrelevanttotheactualhypothesis. 2 4 6 8 10 5 4 3 2 1 0 s W s b=0.8 b=1.6 Figure9.2:WaldStatisticasaFunctionofs Ourfirst-orderasymptotictheoryisnotusefultohelppick s,asW ââÏ2 under(cid:72) forany s.This s 1 0 d isacontextwhereMonteCarlosimulationcanbequiteusefulasatooltostudyandcomparetheexact distributionsofstatisticalproceduresinfinitesamples. Themethodusesrandomsimulationtocreate artificialdatasetstowhichweapplythestatisticaltoolsofinterest. Thisproducesrandomdrawsfrom thestatisticâssamplingdistribution.Throughrepetitionfeaturesofthisdistributioncanbecalculated.",
    "page": 258,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 239 InthepresentcontextoftheWaldstatistic,onefeatureofimportanceistheTypeIerrorofthetest using the asymptotic 5% critical value 3.84 â the probability of a false rejection, (cid:80)(cid:163) W >3.84|Î²=1 (cid:164) . s Giventhesimplicityofthemodelthisprobabilitydependsonlyon s,n,andÏ2.InTable9.2wereport theresultsofaMonteCarlosimulationwherewevarythesethreeparameters. Thevalueof s isvaried from1to10, n isvariedamong20, 100and500, andÏisvariedamong1and3. Thetablereportsthe simulationestimateoftheTypeIerrorprobabilityfrom50,000randomsamples. Eachrowofthetable correspondstoadifferentvalueofs âandthuscorrespondstoaparticularchoiceofteststatistic. The secondthroughseventhcolumnscontaintheTypeIerrorprobabilitiesfordifferentcombinationsofn andÏ. Theseprobabilitiesarecalculatedasthepercentageofthe50,000simulatedWaldstatisticsW s whicharelargerthan3.84.ThenullhypothesisÎ²s=1istruesotheseprobabilitiesareTypeIerror. TointerpretthetablerememberthattheidealTypeIerrorprobabilityis5%(.05)withdeviationsindi- catingdistortion.TypeIerrorratesbetween3%and8%areconsideredreasonable.Errorratesabove10% areconsideredexcessive.Ratesabove20%areunacceptable.Whencomparingstatisticalprocedureswe comparetheratesrowbyrow, lookingfortestsforwhichrejectionratesarecloseto5%andrarelyfall outsideofthe3%-8%range. Forthisparticularexampletheonlytestwhichmeetsthiscriterionisthe conventionalW =W test.AnyothersleadstoatestwithunacceptableTypeIerrorprobabilities. 1 Table9.2:TypeIErrorProbabilityofAsymptotic5%W(s)Test s Ï=1 Ï=3 n=20 n=100 n=500 n=20 n=100 n=500 1 0.05 0.05 0.05 0.05 0.05 0.05 2 0.07 0.06 0.05 0.14 0.08 0.06 3 0.09 0.06 0.05 0.21 0.12 0.07 4 0.12 0.07 0.05 0.25 0.15 0.08 5 0.14 0.08 0.06 0.27 0.18 0.10 6 0.16 0.09 0.06 0.30 0.20 0.12 7 0.18 0.10 0.06 0.32 0.22 0.13 8 0.20 0.12 0.07 0.33 0.24 0.14 9 0.21 0.13 0.07 0.34 0.25 0.16 10 0.23 0.14 0.08 0.35 0.26 0.17 Rejectionfrequenciesfrom50,000simulatedrandomsamples. In Table 9.2 you can also see the impact of variation in sample size. In each case the Type I error probabilityimprovestowards5%asthesamplesizen increases. Thereis,however,nomagicchoiceof n forwhichalltestsperformuniformlywell. Testperformancedeterioratesas s increaseswhichisnot surprisinggiventhedependenceofW onsasshowninFigure9.2. s Inthisexampleitisnotsurprisingthatthechoice s =1yieldsthebestteststatistic. Otherchoices arearbitraryandwouldnotbeusedinpractice. Whilethisisclearinthisparticularexample, inother examplesnaturalchoicesarenotobviousandthebestchoicesmaybecounter-intuitive. ThispointcanbeillustratedthroughanexamplebasedonGregoryandVeall(1985).Takethemodel Y =Î² +X Î² +X Î² +e (9.14) 0 1 1 2 2 (cid:69)[Xe]=0 Î² and the hypothesis (cid:72) : 1 =Î¸ where Î¸ is a known constant. Equivalently, define Î¸ =Î² /Î² so the 0 Î² 0 0 1 2 hypothesiscanbestateda 2 s(cid:72) :Î¸=Î¸ . 0 0",
    "page": 259,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 240 LetÎ² (cid:98) =(Î² (cid:98)0 ,Î² (cid:98)1 ,Î² (cid:98)2 )betheleastsquaresestimatorof(9.14),letV(cid:98)Î²(cid:98) beanestimatorofthecovariance matrixforÎ² (cid:98)andsetÎ¸ (cid:98) =Î² (cid:98)1 /Î² (cid:98)2 .Define ï£« ï£¶ 0 ï£¬ ï£· ï£¬ ï£· ï£¬ 1 ï£· ï£¬ ï£· R(cid:98)1 =ï£¬ ï£¬ Î² (cid:98)2 ï£· ï£· ï£¬ ï£· ï£¬ ï£· ï£¬ ï£· ï£¬ ï£­ â Î² (cid:98)1 ï£· ï£¸ Î²2 (cid:98) 2 (cid:179) (cid:48) (cid:180)1/2 sothatthestandarderrorforÎ¸ (cid:98)iss(Î¸ (cid:98))= R(cid:98)1 V(cid:98)Î²(cid:98) R(cid:98)1 .Inthiscaseat-statisticfor(cid:72) 0 is (cid:179)Î²(cid:98)1 âÎ¸ (cid:180) T = Î²(cid:98)2 0 . 1 s(Î¸ (cid:98)) Analternativestatisticcanbeconstructedthroughreformulatingthenullhypothesisas (cid:72) :Î² âÎ¸ Î² =0. 0 1 0 2 At-statisticbasedonthisformulationofthehypothesisis Î² âÎ¸ Î² T = (cid:98)1 0(cid:98)2 2 (cid:179) (cid:180)1/2 (cid:48) R 2 V(cid:98)Î²(cid:98) R 2 where ï£« ï£¶ 0 R 2 = ï£­ 1 ï£¸. âÎ¸ 0 To compare T and T we perform another simple Monte Carlo simulation. We let X and X be 1 2 1 2 mutually independent N(0,1) variables, e be an independent N(0,Ï2) draw with Ï=3, and normalize Î² =0andÎ² =1.ThisleavesÎ² asafreeparameteralongwithsamplesizen.WevaryÎ² among0.1, 0 1 2 2 0.25,0.50,0.75,and1.0andnamong100and500. Table9.3:TypeIErrorProbabilityofAsymptotic5%t-tests Î² n=100 n=500 2 (cid:80)(T <â1.645) (cid:80)(T >1.645) (cid:80)(T <â1.645) (cid:80)(T >1.645) T T T T T T T T 1 2 1 2 1 2 1 2 0.10 0.47 0.05 0.00 0.05 0.28 0.05 0.00 0.05 0.25 0.27 0.05 0.00 0.05 0.16 0.05 0.00 0.05 0.50 0.14 0.05 0.00 0.05 0.12 0.05 0.00 0.05 0.75 0.03 0.05 0.00 0.05 0.08 0.05 0.01 0.05 1.00 0.00 0.05 0.00 0.05 0.03 0.05 0.03 0.05 Rejectionfrequenciesfrom50,000simulatedrandomsamples. Theone-sidedTypeIerrorprobabilities(cid:80)[T <â1.645]and(cid:80)[T >1.645]arecalculatedfrom50,000 simulated samples. The results are presented in Table 9.3. Ideally, the entries in the table should be",
    "page": 260,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 241 0.05. However,therejectionratesfortheT statisticdivergegreatlyfromthisvalue,especiallyforsmall 1 valuesofÎ² .Thelefttailprobabilities(cid:80)[T <â1.645]greatlyexceed5%,whiletherighttailprobabilities 2 1 (cid:80)[T >1.645]areclosetozeroinmostcases. Incontrast,therejectionratesforthelinearT statisticare 1 2 invarianttothevalueofÎ² andequal5%forbothsamplesizes. TheimplicationofTable9.3isthatthe 2 twot-ratioshavedramaticallydifferentsamplingbehavior. ThecommonmessagefrombothexamplesisthatWaldstatisticsaresensitivetothealgebraicfor- mulationofthenullhypothesis. Asimplesolutionistousetheminimumdistancestatistic J whichequalsW withr =1inthefirst example, and|T |inthesecondexample. Theminimumdistancestatisticisinvarianttothealgebraic 2 formulationofthenullhypothesissoisimmunetothisproblem. Wheneverpossible,theWaldstatistic shouldnotbeusedtotestnonlinearhypotheses. TheoreticalinvestigationsoftheseissuesincludeParkandPhillips(1988)andDufour(1997). 9.18 MonteCarloSimulation InSection9.17weintroducedthemethodofMonteCarlosimulationtoillustratethesmallsample problemswithtestsofnonlinearhypotheses.Inthissectionwedescribethemethodinmoredetail. Recall,ourdataconsistofobservations(Y ,X )whicharerandomdrawsfromapopulationdistribu- i i tionF.LetÎ¸beaparameterandletT =T((Y ,X ),...,(Y ,X ),Î¸)beastatisticofinterest,forexamplean 1 1 n n estimatorÎ¸ (cid:98)orat-statistic(Î¸ (cid:98) âÎ¸)/s(Î¸ (cid:98)).TheexactdistributionofT is G(u,F)=(cid:80)[T â¤u|F]. WhiletheasymptoticdistributionofT mightbeknown,theexact(finitesample)distributionG isgen- erallyunknown. MonteCarlosimulationusesnumericalsimulationtocomputeG(u,F)forselectedchoicesofF.This is useful to investigate the performance of the statistic T in reasonable situations and sample sizes. The basic idea is that for any given F the distribution functionG(u,F) can be calculated numerically through simulation. The name Monte Carlo derives from the famous Mediterranean gambling resort wheregamesofchanceareplayed. ThemethodofMonteCarloissimpletodescribe. TheresearcherchoosesF (thedistributionofthe pseudodata)andthesamplesizen.AâtrueâvalueofÎ¸isimpliedbythischoice,orequivalentlythevalue Î¸isselecteddirectlybytheresearcherwhichimpliesrestrictionsonF. Thenthefollowingexperimentisconductedbycomputersimulation: 1. n independent random pairs (cid:161) Y â ,X â(cid:162) , i = 1,...,n, are drawn from the distribution F using the i i computerâsrandomnumbergenerator. 2. ThestatisticT =T (cid:161)(cid:161) Y â ,X â(cid:162) ,..., (cid:161) Y â ,X â(cid:162) ,Î¸(cid:162) iscalculatedonthispseudodata. 1 1 n n Forstep1,computerpackageshavebuilt-inrandomnumberproceduresincludingU[0,1]andN(0,1). Fromthesemostrandomvariablescanbeconstructed. (Forexample,achi-squarecanbegeneratedby sumsofsquaresofnormals.) Forstep2,itisimportantthatthestatisticbeevaluatedattheâtrueâvalueofÎ¸correspondingtothe choiceofF. The above experiment creates one random draw T from the distribution G(u,F)",
    "page": 261,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". 1 1 n n Forstep1,computerpackageshavebuilt-inrandomnumberproceduresincludingU[0,1]andN(0,1). Fromthesemostrandomvariablescanbeconstructed. (Forexample,achi-squarecanbegeneratedby sumsofsquaresofnormals.) Forstep2,itisimportantthatthestatisticbeevaluatedattheâtrueâvalueofÎ¸correspondingtothe choiceofF. The above experiment creates one random draw T from the distribution G(u,F). This is one ob- servation from an unknown distribution. Clearly, from one observation very little can be said. So the researcherrepeatstheexperimentB timeswhereB isalargenumber.Typically,wesetBâ¥1000.Wewill discussthischoicelater.",
    "page": 261,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 242 Notationally,letthebth experimentresultinthedrawT ,b=1,...,B.Theseresultsarestored. After b all B experiments have been calculated these results constitute a random sample of size B from the distributionofG(u,F)=(cid:80)[T â¤u]=(cid:80)[T â¤u|F]. b Fromarandomsamplewecanestimateanyfeatureofinterestusing(typically)amethodofmoments estimator.Wenowdescribesomespecificexamples. Supposeweareinterestedinthebias,mean-squarederror(MSE),and/orvarianceofthedistribution ofÎ¸ (cid:98) âÎ¸.WethensetT =Î¸ (cid:98) âÎ¸,runtheaboveexperiment,andcalculate b(cid:100)ias (cid:163)Î¸ (cid:98) (cid:164)= 1 (cid:88) B T b = 1 (cid:88) B Î¸ (cid:98)b âÎ¸ B B b=1 b=1 m (cid:100) se (cid:163)Î¸ (cid:98) (cid:164)= 1 (cid:88) B (T b )2= 1 (cid:88) B (cid:161)Î¸ (cid:98)b âÎ¸(cid:162)2 B B b=1 b=1 v (cid:99) ar (cid:163)Î¸ (cid:98) (cid:164)=m (cid:100) se (cid:163)Î¸ (cid:98) (cid:164)â (cid:179) b(cid:100)ias (cid:163)Î¸Ë(cid:164) (cid:180)2 SupposeweareinterestedintheTypeIerrorassociatedwithanasymptotic5%two-sidedt-test. We wouldthensetT =(cid:175) (cid:175) Î¸ (cid:98) âÎ¸(cid:175) (cid:175)/s(Î¸ (cid:98))andcalculate P(cid:98) = 1 (cid:88) B 1 {T b â¥1.96}, (9.15) B b=1 thepercentageofthesimulatedt-ratioswhichexceedtheasymptotic5%criticalvalue. Supposeweareinterestedinthe5%and95%quantileofT =Î¸ (cid:98)orT =(cid:161)Î¸ (cid:98) âÎ¸(cid:162) /s(Î¸ (cid:98)).Wethencompute the5%and95%samplequantilesofthesample{T }.FordetailsonquantileestimationseeSection11.13 b ofIntroductiontoEconometrics. The typical purpose of a Monte Carlo simulation is to investigate the performance of a statistical procedure in realistic settings. Generally, the performance will depend on n and F. In many cases an estimatorortestmayperformwonderfullyforsomevaluesandpoorlyforothers.Itisthereforeusefulto conductavarietyofexperimentsforaselectionofchoicesofnandF. AsdiscussedabovetheresearchermustselectthenumberofexperimentsB.Oftenthisiscalledthe number of replications. Quite simply, a larger B results in more precise estimates of the features of interestofGbutrequiresmorecomputationaltime.Inpractice,therefore,thechoiceofBisoftenguided bythecomputationaldemandsofthestatisticalprocedure.SincetheresultsofaMonteCarloexperiment areestimatescomputedfromarandomsampleofsizeB itisstraightforwardtocalculatestandarderrors foranyquantityofinterest.IfthestandarderroristoolargetomakeareliableinferencethenB willhave tobeincreased. Inparticular,itissimpletomakeinferencesaboutrejectionprobabilitiesfromstatisticaltests,such asthepercentageestimatereportedin(9.15).Therandomvariable 1 {T â¥1.96}isi.i.d.Bernoulli,equalling b 1withprobability p =(cid:69)[ 1 {T â¥1.96}].Theaverage(9.15)isthereforeanunbiasedestimatorof p with b (cid:113) standard error s (cid:161) p (cid:162)= p (cid:161) 1âp (cid:162) /B. As p is unknown, this may be approximated by replacing p with (cid:98) p (cid:98) orwitha (cid:112) nhypothesizedvalu(cid:112)e",
    "page": 262,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". As p is unknown, this may be approximated by replacing p with (cid:98) p (cid:98) orwitha (cid:112) nhypothesizedvalu(cid:112)e. Forexample, ifweareassessinganasymptotic5%test, thenwecan sets (cid:161) p (cid:162)= (.05)(.95)/B(cid:39).22/ B.Hence,standarderrorsforB=100,1000,and5000,are,respectively, (cid:98) s (cid:161) p (cid:162)=.022,.007,and.003. (cid:98) MostpapersineconometricmethodsandsomeempiricalpapersincludetheresultsofMonteCarlo simulations to illustrate the performance of their methods. When extending existing results it is good practice to start by replicating existing (published) results. This is not exactly possible in the case of simulationresultsastheyareinherentlyrandom. Forexamplesupposeapaperinvestigatesastatistical testandreportsasimulatedrejectionprobabilityof0.07basedonasimulationwithB=100replications.",
    "page": 262,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 243 Supposeyouattempttoreplicatethisresultandfindarejectionprobabilityof0.03(againusingB =100 simulation replications). Should you conclude that you have failed in your attempt? Absolutely not! Underthehypothesisthatbothsimulationsareidenticalyouhavetwoindependentestimates,p =0.07 (cid:98)1 a(cid:112)ndp (cid:98)2 =0.03,ofacommonprobabilityp.Theasymptotic(asB ââ)distributionoftheirdifferenceis B (cid:161) p âp (cid:162)ââN(0,2p(1âp)),soastandarderrorforp âp =0.04iss=(cid:112) 2p(1âp)/B (cid:39)0.03,using (cid:98)1 (cid:98)2 (cid:98)1 (cid:98)2 (cid:98) d theestimatep=(p +p )/2.Sincethet-ratio0.04/0.03=1.3isnotstatisticallysignificantitisincorrect (cid:98)1 (cid:98)2 torejectthenullhypothesisthatthetwosimulationsareidentical. Thedifferencebetweentheresults p =0.07andp =0.03isconsistentwithrandomvariation. (cid:98)1 (cid:98)2 Whatshouldbedone? ThefirstmistakewastocopythepreviouspaperâschoiceofB =100.Instead, supposeyousetB =10,000andnowobtain p =0.04.Then p âp =0.03andastandarderroris s = (cid:98)2 (cid:98)1 (cid:98)2 (cid:98) (cid:112) p(1âp)(1/100+1/10000) (cid:39) 0.02. Still we cannot reject the hypothesis that the two simulations are different.Eventhoughtheestimates(0.07and0.04)appeartobequitedifferent,thedifficultyisthatthe originalsimulationusedaverysmallnumberofreplications(B =100)sothereportedestimateisquite imprecise. In this case it is appropriate to conclude that your results âreplicateâ the previous study as thereisnostatisticalevidencetorejectthehypothesisthattheyareequivalent. Most journals have policies requiring authors to make available their data sets and computer pro- grams required for empirical results. Most do not have similar policies regarding simulations. Never- theless,itisgoodprofessionalpracticetomakeyoursimulationsavailable. Thebestpracticeistopost yoursimulationcodeonyourwebpage. Thisinvitesotherstobuildonanduseyourresults,leadingto possiblecollaboration,citation,and/oradvancement. 9.19 ConfidenceIntervalsbyTestInversion Thereisacloserelationshipbetweenhypothesistestsandconfidenceintervals.WeobservedinSec- tion7.13thatthestandard95%asymptoticconfidenceintervalforaparameterÎ¸is C(cid:98) =(cid:163)Î¸ (cid:98) â1.96Ãs(Î¸ (cid:98)), Î¸ (cid:98) +1.96Ãs(Î¸ (cid:98)) (cid:164)={Î¸:|T(Î¸)|â¤1.96}. (9.16) Thatis,wecandescribeC(cid:98)asâThepointestimateplusorminus2standarderrorsâorâThesetofparam- etervaluesnotrejectedbyatwo-sidedt-test.âTheseconddefinition,knownasteststatisticinversion, is a general method for finding confidence intervals, and typically produces confidence intervals with excellentproperties. GivenateststatisticT(Î¸)andcriticalvaluec,theacceptanceregionâAcceptifT(Î¸)â¤câisidentical totheconfidenceintervalC(cid:98) ={Î¸:T(Î¸)â¤c}. Sincetheregionsareidenticaltheprobabilityofcoverage (cid:80)(cid:163)Î¸âC(cid:98) (cid:164) equalstheprobabilityofcorrectacceptance(cid:80)(cid:163) Accept|Î¸(cid:164) whichisexactly1minustheTypeI errorprobability",
    "page": 263,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". GivenateststatisticT(Î¸)andcriticalvaluec,theacceptanceregionâAcceptifT(Î¸)â¤câisidentical totheconfidenceintervalC(cid:98) ={Î¸:T(Î¸)â¤c}. Sincetheregionsareidenticaltheprobabilityofcoverage (cid:80)(cid:163)Î¸âC(cid:98) (cid:164) equalstheprobabilityofcorrectacceptance(cid:80)(cid:163) Accept|Î¸(cid:164) whichisexactly1minustheTypeI errorprobability. ThusinvertingatestwithgoodTypeIerrorprobabilitiesyieldsaconfidenceinterval withgoodcoverageprobabilities. NowsupposethattheparameterofinterestÎ¸=r(Î²)isanonlinearfunctionofthecoefficientvector Î². InthiscasethestandardconfidenceintervalforÎ¸isthesetC(cid:98)asin(9.16)whereÎ¸ (cid:98) =r(Î² (cid:98))isthepoint (cid:113) (cid:48) estimatorands(Î¸ (cid:98))= R(cid:98) V(cid:98)Î²(cid:98) R(cid:98) isthedeltamethodstandarderror. Thisconfidenceintervalisinverting thet-testbasedonthenonlinearhypothesisr(Î²)=Î¸.ThetroubleisthatinSection9.17welearnedthat thereisnouniquet-statisticfortestsofnonlinearhypothesesandthatthechoiceofparameterization mattersgreatly. Forexample,ifÎ¸=Î² /Î² thenthecoverageprobabilityofthestandardinterval(9.16)is1minusthe 1 2 probabilityoftheTypeIerror,whichasshowninTable8.2canbefarfromthenominal5%. InthisexampleagoodsolutionisthesameasdiscussedinSection9.17âtorewritethehypothesisas",
    "page": 263,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 244 alinearrestriction.ThehypothesisÎ¸=Î² /Î² isthesameasÎ¸Î² =Î² .Thet-statisticforthisrestrictionis 1 2 2 1 Î² âÎ² Î¸ T(Î¸)= (cid:98)1 (cid:98)2 (cid:179) (cid:180)1/2 (cid:48) R V(cid:98)Î²(cid:98) R where (cid:181) (cid:182) 1 R= âÎ¸ andV(cid:98)Î²(cid:98) isthecovariancematrixfor(Î² (cid:98)1 Î² (cid:98)2 ).A95%confidenceintervalforÎ¸=Î² 1 /Î² 2 isthesetofvaluesof Î¸suchthat|T(Î¸)|â¤1.96.SinceT(Î¸)isanonlinearfunctionofÎ¸onemethodtofindtheconfidencesetis gridsearchoverÎ¸. Forexample,inthewageequation log(wage)=Î² experience+Î² experience2/100+Â·Â·Â· 1 2 thehighestexpectedwageoccursatexperience=â50Î² /Î² .FromTable4.1wehavethepointestimate 1 2 Î¸ (cid:98) =29.8andwecancalculatethestandarderrors(Î¸ (cid:98))=0.022fora95%confidenceinterval[29.8,29.9]. However,ifweinsteadinvertthelinearformofthetestweumericallyfindtheinterval[29.1,30.6]which is much larger. From the evidence presented in Section 9.17 we know the first interval can be quite inaccurateandthesecondintervalisgreatlypreferred. 9.20 MultipleTestsandBonferroniCorrections Inmostapplicationseconomistsexaminealargenumberofestimates,teststatistics,andp-values. Whatdoesitmean(ordoesitmeananything)ifonestatisticappearstobeâsignificantâafterexamining alargenumberofstatistics?Thisisknownastheproblemofmultipletestingormultiplecomparisons. Tobespecific,supposeweexamineasetofk coefficients,standarderrorsandt-ratios,andconsider theâsignificanceâofeachstatistic.Basedonconventionalreasoning,foreachcoefficientwewouldreject the hypothesis that the coefficient is zero with asymptotic size Î± if the absolute t-statistic exceeds the 1âÎ±criticalvalueofthenormaldistribution,orequivalentlyifthep-valueforthet-statisticissmaller thanÎ±. Ifweobservethatoneofthek statisticsisâsignificantâbasedonthiscriterion,thatmeansthat one of the p-values is smaller than Î±, or equivalently, that the smallest p-value is smaller than Î±. We canthenrephrasethequestion: Underthejointhypothesisthatasetofk hypothesesarealltrue,what is the probability that the smallest p-value is smaller than Î±? In general, we cannot provide a precise answer to this quesion, but the Bonferroni correction bounds this probability by Î±k. The Bonferroni methodfurthermoresuggeststhatifwewantthefamilywiseerrorprobability(theprobabilitythatone ofthetestsfalselyrejects)tobeboundedbelowÎ±,thenanappropriateruleistorejectonlyifthesmallest p-valueissmallerthanÎ±/k.Equivalently,theBonferronifamilywisep-valueiskmin jâ¤k p j . Formally, supposewehavek hypotheses(cid:72) , j =1,...,k. Foreachwehaveatestandassociatedp- j valuep j withthepropertythatwhen(cid:72) j istruelim nââ (cid:80)(cid:163) p j <Î±(cid:164)=Î±. Wethenobservethatamongthe k tests,oneofthek isâsignificantâifmin jâ¤k p j <Î±.Thiseventcanbewrittenas (cid:189) (cid:190) k minp <Î± = (cid:91)(cid:169) p <Î±(cid:170)",
    "page": 264,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Formally, supposewehavek hypotheses(cid:72) , j =1,...,k. Foreachwehaveatestandassociatedp- j valuep j withthepropertythatwhen(cid:72) j istruelim nââ (cid:80)(cid:163) p j <Î±(cid:164)=Î±. Wethenobservethatamongthe k tests,oneofthek isâsignificantâifmin jâ¤k p j <Î±.Thiseventcanbewrittenas (cid:189) (cid:190) k minp <Î± = (cid:91)(cid:169) p <Î±(cid:170) . j j jâ¤k j=1 (cid:34) (cid:35) k k Booleâsinequalitystatesthatforanyk events A ,(cid:80) (cid:91) A â¤ (cid:88) (cid:80)[A ].Thus j j k j=1 j=1 (cid:183) (cid:184) k (cid:80) minp <Î± â¤ (cid:88) (cid:80)(cid:163) p <Î±(cid:164)âkÎ± j j jâ¤k j=1",
    "page": 264,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 245 asstated. Thisdemonstatesthatthefamilywiserejectionprobability isatmostk timestheindividual rejectionprobability. Furthermore, (cid:183) Î±(cid:184) (cid:88) k (cid:104) Î±(cid:105) (cid:80) minp < â¤ (cid:80) p < âÎ±. j j jâ¤k k j=1 k Thisdemonstratesthatthefamilywiserejectionprobabilitycanbecontrolled(boundedbelowÎ±)ifeach individualtestissubjectedtothestricterstandardthatap-valuemustbesmallerthanÎ±/ktobelabeled asâsignificantâ. Toillustrate,supposewehavetwocoefficientestimateswithindividualp-values0.04and0.15.Based onaconventional5%levelthestandardindividualtestswouldsuggestthatthefirstcoefficientestimate is âsignificantâ but not the second. A Bonferroni 5% test, however, does not reject as it would require thatthesmallestp-valuebesmallerthan0.025,whichisnotthecaseinthisexample.Alternatively,the Bonferronifamilywisep-valueis0.04Ã2=0.08,whichisnotsignificantatthe5%level. Incontrast,ifthetwop-valueswere0.01and0.15,thentheBonferronifamilywisep-valuewouldbe 0.01Ã2=0.02,whichissignificantatthe5%level. 9.21 PowerandTestConsistency Thepowerofatestistheprobabilityofrejecting(cid:72) when(cid:72) istrue. 0 1 (cid:112) (cid:179) (cid:180) ForsimplicitysupposethatY isi.i.d.N(Î¸,Ï2)withÏ2known,considerthet-statisticT(Î¸)= n Y âÎ¸ /Ï, i andtestsof(cid:72) :Î¸=0against(cid:72) :Î¸>0.Wereject(cid:72) ifT =T(0)>c.Notethat 0 1 0 (cid:112) T =T(Î¸)+ nÎ¸/Ï andT(Î¸)hasanexactN(0,1)distribution. ThisisbecauseT(Î¸)iscenteredatthetruemeanÎ¸,whilethe teststatisticT(0)iscenteredatthe(false)hypothesizedmeanof0. Thepowerofthetestis (cid:112) (cid:112) (cid:80)[T >c|Î¸]=(cid:80)(cid:163) Z+ nÎ¸/Ï>c (cid:164)=1âÎ¦(cid:161) câ nÎ¸/Ï(cid:162) . ThisfunctionismonotonicallyincreasinginÂµandn,anddecreasinginÏandc. Notice that for any c and Î¸(cid:54)=0 the power increases to 1 as n ââ. This means that for Î¸â(cid:72) the 1 testwillreject(cid:72) withprobabilityapproaching1asthesamplesizegetslarge. Wecallthispropertytest 0 consistency. Definition9.3 A test of (cid:72) :Î¸ âÎ is consistentagainstfixedalternatives if 0 0 forallÎ¸âÎ ,(cid:80)(cid:163) Reject(cid:72) |Î¸(cid:164)â1asnââ. 1 0 For tests of the form âReject (cid:72) if T > câ, a sufficient condition for test consistency is that the T 0 divergestopositiveinfinitywithprobabilityoneforallÎ¸âÎ . 1 Definition9.4 WesaythatT âââasnââifforall M <â,(cid:80)[T â¤M]â0 p as n â â. Similarly, we say that T ââ ââ as n â â if for all M < â, p (cid:80)[T â¥âM]â0asnââ.",
    "page": 265,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 246 Ingeneral,t-testsandWaldtestsareconsistentagainstfixedalternatives. Takeat-statisticforatest (cid:113) of(cid:72) 0 :Î¸=Î¸ 0 ,T =(cid:161)Î¸ (cid:98) âÎ¸ 0 (cid:162) /s(Î¸ (cid:98))whereÎ¸ 0 isaknownvalueands(Î¸ (cid:98))= n â1V(cid:98)Î¸ .Notethat (cid:112) T = Î¸ (cid:98) âÎ¸ + n(Î¸âÎ¸ 0 ) . s(Î¸ (cid:98)) (cid:113) V(cid:98)Î¸ Thefirsttermontheright-hand-sideconvergesindistributiontoN(0,1).Thesecondtermontheright- hand-sideequalszeroifÎ¸=Î¸ ,convergesinprobabilityto+âifÎ¸>Î¸ ,andconvergesinprobability 0 0 to ââ if Î¸ < Î¸ . Thus the two-sided t-test is consistent against (cid:72) : Î¸ (cid:54)= Î¸ , and one-sided t-tests are 0 1 0 consistentagainstthealternativesforwhichtheyaredesigned. Theorem9.8 UnderAssumptions7.2,7.3,and7.4,forÎ¸=r(Î²)(cid:54)=Î¸ andq=1, 0 then|T|âââ.Foranyc<âthetestâReject(cid:72) if|T|>câisconsistentagainst 0 p fixedalternatives. TheWaldstatisticfor(cid:72) 0 :Î¸=r(Î²)=Î¸ 0 against(cid:72) 1 :Î¸(cid:54)=Î¸ 0 isW =n (cid:161)Î¸ (cid:98) âÎ¸ 0 (cid:162)(cid:48) V(cid:98) â Î¸ 1(cid:161)Î¸ (cid:98) âÎ¸ 0 (cid:162) . Under(cid:72) 1 , Î¸ (cid:98) ââÎ¸(cid:54)=Î¸ 0 .Thus (cid:161)Î¸ (cid:98) âÎ¸ 0 (cid:162)(cid:48) V(cid:98) â Î¸ 1(cid:161)Î¸ (cid:98) âÎ¸ 0 (cid:162)ââ(Î¸âÎ¸ 0 ) (cid:48) V â Î¸ 1(Î¸âÎ¸ 0 )>0.Henceunder(cid:72) 1 ,W âââ. Again, p p p thisimpliesthatWaldtestsareconsistent. Theorem9.9 Under Assumptions 7.2, 7.3, and 7.4, for Î¸ = r(Î²) (cid:54)= Î¸ , then 0 W ââ â. For any c < â the test âReject (cid:72) if W > câ is consistent against 0 p fixedalternatives. 9.22 AsymptoticLocalPower Consistencyisagoodpropertyforatestbutisdoesnotprovidedatooltocalculatetestpower. To approximatethepowerfunctionweneedadistributionalapproximation. Thestandardasymptoticmethodforpoweranalysisuseswhatarecalledlocalalternatives. Thisis similartoouranalysisofrestrictionestimationundermisspecification(Section8.13). Thetechniqueis toindextheparameterbysamplesizesothattheasymptoticdistributionofthestatisticiscontinuous inalocalizingparameter. Inthissectionweconsidert-testsonreal-valuedparametersandinthenext sectionWaldtests. Specifically, weconsiderparametervectorsÎ² whichareindexedbysamplesizen n andsatisfythereal-valuedrelationship Î¸ =r(Î² )=Î¸ +n â1/2h (9.17) n n 0 wherethescalarhiscalledalocalizingparameter. WeindexÎ² andÎ¸ bysamplesizetoindicatetheir n n dependenceonn. Thewaytothinkof(9.17)isthatthetruevalueoftheparametersareÎ² andÎ¸ . The n n parameterÎ¸ isclosetothehypothesizedvalueÎ¸ ,withdeviationn â1/2h. n 0 Thespecification(9.17)statesthatforanyfixedh,Î¸ approachesÎ¸ asngetslarge.ThusÎ¸ isâcloseâ n 0 n orâlocalâtoÎ¸ .Theconceptofalocalizingsequence(9.17)mightseemoddsinceintheactualworldthe 0",
    "page": 266,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 247 samplesizecannotmechanicallyaffectthevalueoftheparameter.Thus(9.17)shouldnotbeinterpreted literally. Instead,itshouldbeinterpretedasatechnicaldevicewhichallowstheasymptoticdistribution tobecontinuousinthealternativehypothesis. Toevaluatetheasymptoticdistributionoftheteststatisticwestartbyexaminingthescaledestima- tor centered at the hypothesized value Î¸ . Breaking it into a term centered at the true value Î¸ and a 0 n remainderwefind (cid:112) (cid:112) (cid:112) (cid:112) n (cid:161)Î¸ (cid:98) âÎ¸ 0 (cid:162)= n (cid:161)Î¸ (cid:98) âÎ¸ n (cid:162)+ n(Î¸ n âÎ¸ 0 )= n (cid:161)Î¸ (cid:98) âÎ¸ n (cid:162)+h wherethesecondequalityis(9.17).Thefirsttermisasymptoticallynormal: (cid:112) n (cid:161)Î¸ (cid:98) âÎ¸ n (cid:162)ââ (cid:112) VÎ¸Z d whereZ â¼N(0,1).Therefore (cid:112) n (cid:161)Î¸ (cid:98) âÎ¸ 0 (cid:162)ââ (cid:112) VÎ¸Z+hâ¼N(h,VÎ¸). d Thisasymptoticdistributiondependscontinuouslyonthelocalizingparameterh. Appliedtothetstatisticwefind T = Î¸ (cid:98) âÎ¸ 0 ââ (cid:112) VÎ¸Z+h â¼Z+Î´ (9.18) s(Î¸ (cid:98)) d (cid:112) VÎ¸ whereÎ´=h/ (cid:112) VÎ¸.ThisgeneralizesTheorem9.1(whichassumes(cid:72) 0 istrue)toallowforlocalalternatives oftheform(9.17). Considerat-testof(cid:72) againsttheone-sidedalternative(cid:72) :Î¸>Î¸ whichrejects(cid:72) forT >c where 0 1 0 0 Î¦(c)=1âÎ±. The asymptotic local power of this test is the limit (as the sample size diverges) of the rejectionprobabilityunderthelocalalternative(9.17) lim (cid:80)(cid:163) Reject(cid:72) (cid:164)= lim (cid:80)[T >c] nââ 0 nââ =(cid:80)[Z+Î´>c] =1âÎ¦(câÎ´) =Î¦(Î´âc) d=efÏ(Î´). WecallÏ(Î´)theasymptoticlocalpowerfunction. InFigure9.3(a)weplotthelocalpowerfunctionÏ(Î´)asafunctionofÎ´â[â1,4]fortestsofasymptotic sizeÎ±=0.10, Î±=0.05, andÎ±=0.01. Î´=0correspondstothenullhypothesissoÏ(Î´)=Î±. Thepower functions are monotonically increasing in Î´. Note that the power is lower than Î± for Î´<0 due to the one-sidednatureofthetest. WecanseethatthethreepowerfunctionsarerankedbyÎ±sothatthetestwithÎ±=0.10hashigher powerthanthetestwithÎ±=0.01.Thisistheinherenttrade-offbetweensizeandpower.Decreasingsize inducesadecreaseinpower,andconversely. ThecoefficientÎ´canbeinterpretedastheparameterdeviationmeasuredasamultipleofthestan- (cid:113) darderrors(Î¸ (cid:98)).Toseethis,recallthats(Î¸ (cid:98))=n â1/2 V(cid:98)Î¸ (cid:39)n â1/2(cid:112) VÎ¸ andthennotethat h n â1/2h Î¸ âÎ¸ Î´= (cid:39) = n 0 . (cid:112) VÎ¸ s(Î¸ (cid:98)) s(Î¸ (cid:98)) ThusÎ´approximatelyequalsthedeviationÎ¸ n âÎ¸ 0 expressedasmultiplesofthestandarderrors(Î¸ (cid:98)).Thus asweexamineFigure9.3(a)wecaninterpretthepowerfunctionatÎ´=1(e.g. 26%fora5%sizetest)as",
    "page": 267,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 248 â1 0 1 2 3 4 0.1 8.0 6.0 4.0 2.0 0.0 d rewoP a= 0.1 a= 0.05 a= 0.01 0 5 10 15 (a)One-SidedtTest 0.1 8.0 6.0 4.0 2.0 0.0 l rewoP q=1 q=2 q=3 (b)Varyingq Figure9.3:AsymptoticLocalPowerFunction thepowerwhentheparameterÎ¸ isonestandarderrorabovethehypothesizedvalue.Forexample,from n Table4.1thestandarderrorforthecoefficientonâMarriedFemaleâis0.010. Thus,inthisexampleÎ´=1 correspondstoÎ¸ =0.010oran1.0%wagepremiumformarriedfemales.Ourcalculationsshowthatthe n asymptoticpowerofaone-sided5%testagainstthisalternativeisabout26%. Thedifferencebetweenpowerfunctionscanbemeasuredeitherverticallyorhorizontally.Forexam- ple,inFigure9.3(a)thereisaverticaldash-dottedlineatÎ´=1,showingthattheasymptoticlocalpower functionÏ(Î´)equals39%forÎ±=0.10,equals26%forÎ±=0.05,andequals9%forÎ±=0.01. Thisisthe differenceinpoweracrosstestsofdifferingsize,holdingfixedtheparameterinthealternative. Ahorizontalcomparisoncanalsobeilluminating. Toillustrate,inFigure9.3(a)thereisahorizontal dash-dottedlineat50%power.50%powerisausefulbenchmarkasitisthepointwherethetesthasequal oddsofrejectionandacceptance. ThedottedlinecrossesthethreepowercurvesatÎ´=1.29(Î±=0.10), Î´ = 1.65 (Î± = 0.05), and Î´ = 2.33 (Î± = 0.01). This means that the parameter Î¸ must be at least 1.65 standarderrorsabovethehypothesizedvalueforaone-sided5%testtohave50%(approximate)power. Theratioofthesevalues(e.g. 1.65/1.29=1.28fortheasymptotic5%versus10%tests)measuresthe relativeparametermagnitudeneededtoachievethesamepower.(Thus,fora5%sizetesttoachieve50% power,theparametermustbe28%largerthanfora10%sizetest.) Evenmoreinteresting,thesquareof thisratio(e.g.(1.65/1.29)2=1.64)istheincreaseinsamplesizeneededtoachievethesamepowerunder fixed parameters. That is, to achieve 50% power, a 5% size test needs 64% more observations than a 10%sizetest. Thisinterpretationfollowsbythefollowinginformalargument. Bydefinitionand(9.17) (cid:112) Î´=h/ (cid:112) VÎ¸ = n(Î¸ n âÎ¸ 0 )/ (cid:112) VÎ¸.ThusholdingÎ¸andVÎ¸ fixed,Î´2isproportionalton. Theanalysisofatwo-sidedttestissimilar.(9.18)impliesthat (cid:175) (cid:175) T = (cid:175) (cid:175) Î¸ (cid:98) âÎ¸ 0 (cid:175) (cid:175)ââ|Z+Î´| (cid:175) (cid:175) s(Î¸ (cid:98)) (cid:175) (cid:175) d andthusthelocalpowerofatwo-sidedttestis lim (cid:80)(cid:163) Reject(cid:72) (cid:164)= lim (cid:80)[T >c]=(cid:80)[|Z+Î´|>c]=Î¦(Î´âc)+Î¦(âÎ´âc) nââ 0 nââ",
    "page": 268,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 249 whichismonotonicallyincreasingin|Î´|. Theorem9.10 UnderAssumptions7.2,7.3,7.4,andÎ¸ =r(Î² )=r +n â1/2h, n n 0 then Î¸âÎ¸ T(Î¸ )= (cid:98) 0 ââZ+Î´ 0 s(Î¸ (cid:98)) d whereZ â¼N(0,1)andÎ´=h/ (cid:112) VÎ¸.Forc suchthatÎ¦(c)=1âÎ±, (cid:80)[T(Î¸ )>c]ââÎ¦(Î´âc). 0 Furthermore,forc suchthatÎ¦(c)=1âÎ±/2, (cid:80)[|T(Î¸ )|>c]ââÎ¦(Î´âc)+Î¦(âÎ´âc). 0 9.23 AsymptoticLocalPower,VectorCase Inthissectionweextendthelocalpoweranalysisoftheprevioussectiontothecaseofvector-valued alternatives.Wegeneralize(9.17)tovector-valuedÎ¸ .Thelocalparameterizationis n Î¸ =r(Î² )=Î¸ +n â1/2h (9.19) n n 0 wherehisqÃ1. Under(9.19), (cid:112) (cid:112) n (cid:161)Î¸ (cid:98) âÎ¸ 0 (cid:162)= n (cid:161)Î¸ (cid:98) âÎ¸ n (cid:162)+hââZ h â¼N(h,VÎ¸), d anormalrandomvectorwithmeanhandcovariancematrixVÎ¸. AppliedtotheWaldstatisticwefind W =n (cid:161)Î¸ (cid:98) âÎ¸ 0 (cid:162)(cid:48) V(cid:98) â Î¸ 1(cid:161)Î¸ (cid:98) âÎ¸ 0 (cid:162)ââZ h (cid:48) V â Î¸ 1Z h â¼Ï2 q (Î») (9.20) d whereÎ»=h (cid:48) V â1h. Ï2(Î»)isanon-centralchi-squarerandomvariablewithnon-centralityparameterÎ». q (Theorem5.3.6.) Theconvergence(9.20)showsthatunderthelocalalternatives(9.19),W ââÏ2(Î»).Thisgeneralizes q d thenullasymptoticdistributionwhichobtainsasthespecialcaseÎ»=0.Wecanusethisresulttoobtain acontinuousasymptoticapproximationtothepowerfunction. ForanysignificancelevelÎ±>0setthe (cid:104) (cid:105) asymptoticcriticalvaluec sothat(cid:80) Ï2 >c =Î±.Thenasnââ, q (cid:104) (cid:105) (cid:80)[W >c]ââ(cid:80) Ï2(Î»)>c d=efÏ(Î»). q TheasymptoticlocalpowerfunctionÏ(Î»)dependsonlyonÎ±,q,andÎ». Theorem9.11 UnderAssumptions7.2,7.3,7.4,andÎ¸ =r(Î² )=Î¸ +n â1/2h, n n 0 (cid:104) (cid:105) thenW ââÏ2(Î»)whereÎ»=h (cid:48) V â1h.Furthermore,forc suchthat(cid:80) Ï2 >c = q Î¸ q d (cid:104) (cid:105) Î±,(cid:80)[W >c]ââ(cid:80) Ï2(Î»)>c . q",
    "page": 269,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 250 Figure9.3(b)plotsÏ(Î»)asafunctionofÎ»forq =1, q =2,andq =3,andÎ±=0.05. Theasymptotic powerfunctionsaremonotonicallyincreasinginÎ»andasymptotetoone. Figure9.3(b)alsoshowsthepowerlossforfixednon-centralityparameterÎ»asthedimensionalityof the test increases. The power curves shift to the right as q increases, resulting in a decrease in power. Thisisillustratedbythedash-dottedlineat50%power. Thedash-dottedlinecrossesthethreepower curvesatÎ»=3.85(q =1),Î»=4.96(q =2),andÎ»=5.77(q =3). TheratiooftheseÎ»valuescorrespond totherelativesamplesizesneededtoobtainthesamepower. Thusincreasingthedimensionofthetest fromq =1toq =2requiresa28%increaseinsamplesize,oranincreasefromq =1toq =3requiresa 50%increaseinsamplesize,toobtainatestwith50%power. _____________________________________________________________________________________________ 9.24 Exercises 2 Exercise9.1 ProvethatifanadditionalregressorX k+1 isaddedtoX,TheilâsadjustedR increasesifand onlyif|T k+1 |>1,whereT k+1 =Î² (cid:98)k+1 /s(Î² (cid:98)k+1 )isthet-ratioforÎ² (cid:98)k+1 and s(Î² (cid:98)k+1 )=(cid:161) s2[(X (cid:48) X) â1] k+1,k+1 (cid:162)1/2 isthehomoskedasticity-formulastandarderror. Exercise9.2 Youhavetwoindependentsamples(Y ,X )and(Y ,X )bothwithsamplesizesnwhich 1i 1i 2i 2i satisfyY 1 =X 1 Î² 1 +e 1 andY 2 =X 2 Î² 2 +e 2 ,where(cid:69)[X 1 e 1 ]=0and(cid:69)[X 2 e 2 ]=0. LetÎ² (cid:98)1 andÎ² (cid:98)2 betheOLS estimatorsofÎ² â(cid:82)k andÎ² â(cid:82)k. 1 2 (cid:112) (a) Findtheasymptoticdistributionof n (cid:161)(cid:161)Î² (cid:98)2 âÎ² (cid:98)1 (cid:162)â(cid:161)Î² 2 âÎ² 1 (cid:162)(cid:162) asnââ. (b) Findanappropriateteststatisticfor(cid:72) :Î² =Î² . 0 2 1 (c) Findtheasymptoticdistributionofthisstatisticunder(cid:72) . 0 Exercise9.3 LetT beat-statisticfor(cid:72) :Î¸=0versus(cid:72) :Î¸(cid:54)=0. Since|T|â |Z|under(cid:72) , someone 0 1 d 0 suggeststhetestâReject(cid:72) if|T|<c or|T|>c ,wherec istheÎ±/2quantileof|Z|andc isthe1âÎ±/2 0 1 2 1 2 quantileof|Z|. (a) ShowthattheasymptoticsizeofthetestisÎ±. (b) Isthisagoodtestof(cid:72) versus(cid:72) ?Whyorwhynot? 0 1 Exercise9.4 LetW beaWaldstatisticfor(cid:72) :Î¸=0versus(cid:72) :Î¸(cid:54)=0,whereÎ¸ is qÃ1. SinceW ââÏ2 0 1 q d under H ,someonesuggeststhetestâReject(cid:72) ifW <c orW >c ,wherec istheÎ±/2quantileofÏ2 0 0 1 2 1 q andc isthe1âÎ±/2quantileofÏ2. 2 q (a) ShowthattheasymptoticsizeofthetestisÎ±. (b) Isthisagoodtestof(cid:72) versus(cid:72) ?Whyorwhynot? 0 1 Exercise9.5 TakethelinearmodelY =X (cid:48)Î² +X (cid:48)Î² +e with(cid:69)[Xe]=0wherebothX andX areqÃ1. 1 1 2 2 1 2 Showhowtotestthehypotheses(cid:72) :Î² =Î² against(cid:72) :Î² (cid:54)=Î² . 0 1 2 1 1 2",
    "page": 270,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 251 Exercise9.6 Supposearesearcherwantstoknowwhichofasetof20regressorshasaneffectonavari- abletestscore.Heregressestestscoreonthe20regressorsandreportstheresults.Oneofthe20regressors (studytime)hasalarget-ratio(about2.5),whiletheothert-ratiosareinsignificant(smallerthan2inab- solutevalue).Hearguesthatthedatashowthatstudytimeisthekeypredictorfortestscore.Doyouagree withthisconclusion?Isthereadeficiencyinhisreasoning? Exercise9.7 TakethemodelY =XÎ² +X2Î² +ewith(cid:69)[e|X]=0whereY iswages(dollarsperhour)and 1 2 X isage.Describehowyouwouldtestthehypothesisthattheexpectedwagefora40-year-oldworkeris $20anhour. Exercise9.8 You want to test (cid:72) : Î² = 0 against (cid:72) : Î² (cid:54)= 0 in the model Y = X (cid:48)Î² +X (cid:48)Î² +e with 0 2 1 2 1 1 2 2 (cid:69)[Xe]=0.Youreadapaperwhichestimatesthemodel Y =X (cid:48)Î³ +(X âX ) (cid:48)Î³ +u 1(cid:98)1 2 1 (cid:98)2 andreportsatestof(cid:72) :Î³ =0against(cid:72) :Î³ (cid:54)=0.Isthisrelatedtothetestyouwantedtoconduct? 0 2 1 2 Exercise9.9 Supposearesearcherusesonedatasettotestaspecifichypothesis(cid:72) against(cid:72) andfinds 0 1 thathecanreject(cid:72) .Asecondresearchergathersasimilarbutindependentdataset,usessimilarmeth- 0 ods and finds that she cannot reject (cid:72) . How should we (as interested professionals) interpret these 0 mixedresults? (cid:112) Exercise9.10 InExercise7.8youshowedthat n (cid:161)Ï (cid:98) 2âÏ2(cid:162)ââN(0,V)asnââforsomeV. LetV(cid:98) be d anestimatorofV. (a) Usingthisresultconstructat-statisticfor(cid:72) :Ï2=1against(cid:72) :Ï2(cid:54)=1. 0 1 (cid:112) (b) UsingtheDeltaMethodfindtheasymptoticdistributionof n(ÏâÏ). (cid:98) (c) Usethepreviousresulttoconstructat-statisticfor(cid:72) :Ï=1against(cid:72) :Ï(cid:54)=1. 0 1 (d) Arethenullhypothesesin(a)and(c)thesameoraretheydifferent?Arethetestsin(a)and(c)the sameoraretheydifferent? Iftheyaredifferent, describeacontextinwhichthetwotestswould givecontradictoryresults. Exercise9.11 ConsideraregressionsuchasTable4.1wherebothexperienceanditssquareareincluded. Aresearcherwantstotestthehypothesisthatexperience doesnotaffectmeanwagesanddoesthisby computing the t-statistic for experience. Is this the correct approach? If not, what is the appropriate testingmethod? Exercise9.12 Aresearcherestimatesaregressionandcomputesatestof(cid:72) against(cid:72) andfindsap- 0 1 valueof p =0.08,orânotsignificantâ. ShesaysâIneedmoredata. IfIhadalargersamplethetestwill havemorepowerandthenthetestwillreject.âIsthisinterpretationcorrect? Exercise9.13 AcommonviewisthatâIfthesamplesizeislargeenough,anyhypothesiswillberejected.â Whatdoesthismean?Interpretandcomment. Exercise9.14 TakethemodelY =X (cid:48)Î²+ewith(cid:69)[Xe]=0andparameterofinterestÎ¸=R (cid:48)Î²withR kÃ1. LetÎ² (cid:98)betheleastsquaresestimatorandV(cid:98)Î²(cid:98) itsvarianceestimator.",
    "page": 271,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 252 (a) WritedownC(cid:98),the95%asymptoticconfidenceintervalforÎ¸,intermsofÎ² (cid:98),V(cid:98)Î²(cid:98) ,R,andz=1.96(the 97.5%quantileofN(0,1)). (b) ShowthatthedecisionâReject(cid:72) 0 ifÎ¸ 0 âC(cid:98)âisanasymptotic5%testof(cid:72) 0 :Î¸=Î¸ 0 . Exercise9.15 Youareataseminarwhereacolleaguepresentsasimulationstudyofatestofahypothesis (cid:72) withnominalsize5%. BasedonB =100simulationreplicationsunder(cid:72) theestimatedsizeis7%. 0 0 Yourcolleaguesays:âUnfortunatelythetestover-rejects.â (a) Doyouagreeordisagreewithyourcolleague?Explain.Hint:Useanasymptotic(largeB)approxi- mation. (b) Suppose the number of simulation replications were B =1000 yet the estimated size is still 7%. Doesyouranswerchange? Exercise9.16 Considertwoalternativeregressionmodels Y =X (cid:48)Î² +e (9.21) 1 1 1 (cid:69)[X e ]=0 1 1 Y =X (cid:48)Î² +e (9.22) 2 2 2 (cid:69)[X e ]=0 2 2 where X and X have at least some different regressors. (For example, (9.21) is a wage regression on 1 2 geographicvariablesand(2)isawageregressiononpersonalappearancemeasurements.) Youwantto knowifmodel(9.21)ormodel(9.22)fitsthedatabetter. DefineÏ2=(cid:69)(cid:163) e2(cid:164) andÏ2=(cid:69)(cid:163) e2(cid:164) . Youdecide 1 1 2 2 thatthemodelwiththesmallervariancefit(e.g.,model(9.21)fitsbetterifÏ2<Ï2.)Youdecidetotestfor 1 2 thisbytestingthehypothesisofequalfit(cid:72) :Ï2=Ï2 againstthealternativeofunequalfit(cid:72) :Ï2(cid:54)=Ï2. 0 1 2 1 1 2 Forsimplicity,supposethate ande areobserved. 1i 2i (a) ConstructanestimatorÎ¸ (cid:98)ofÎ¸=Ï2âÏ2. 1 2 (cid:112) (b) Findtheasymptoticdistributionof n (cid:161)Î¸ (cid:98) âÎ¸(cid:162) asnââ. (c) FindanestimatoroftheasymptoticvarianceofÎ¸ (cid:98). (d) ProposeatestofasymptoticsizeÎ±of(cid:72) against(cid:72) . 0 1 (e) Supposethetestaccepts(cid:72) .Briefly,whatisyourinterpretation? 0 Exercise9.17 You have two regressors X and X and estimate a regression with all quadratic terms 1 2 included Y =Î±+Î² X +Î² X +Î² X2+Î² X2+Î² X X +e. 1 1 2 2 3 1 4 2 5 1 2 Oneofyouradvisorsasks:CanweexcludethevariableX fromthisregression? 2 Howdoyoutranslatethisquestionintoastatisticaltest? Whenansweringthesequestions,bespe- cific,notgeneral. (a) Whatistherelevantnullandalternativehypotheses? (b) Whatisanappropriateteststatistic?",
    "page": 272,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 253 (c) Whatistheappropriateasymptoticdistributionforthestatistic? (d) Whatistheruleforacceptance/rejectionofthenullhypothesis? Exercise9.18 Theobserveddatais{Y ,X ,Z }â(cid:82)Ã(cid:82)kÃ(cid:82)(cid:96) ,k>1and(cid:96)>1,i =1,...,n.Aneconometrician i i i firstestimatesY i =X i (cid:48)Î² (cid:98) +e (cid:98)i byleastsquares. Theeconometriciannextregressestheresiduale (cid:98)i on Z i , whichcanbewrittenase =Z (cid:48)Î³+u . (cid:98)i i(cid:101) (cid:101)i (a) DefinethepopulationparameterÎ³beingestimatedinthissecondregression. (b) FindtheprobabilitylimitforÎ³. (cid:101) (c) SupposetheeconometricianconstructsaWaldstatisticW for(cid:72) :Î³=0fromthesecondregres- 0 sion,ignoringthetwo-stageestimationprocess.WritedowntheformulaforW. (d) Assume(cid:69)(cid:163) ZX (cid:48)(cid:164)=0.FindtheasymptoticdistributionforW under(cid:72) :Î³=0. 0 (e) If(cid:69)(cid:163) ZX (cid:48)(cid:164)(cid:54)=0willyouranswerto(d)change? Exercise9.19 An economist estimates Y = X (cid:48)Î² +X Î² +e by least squares and tests the hypothesis 1 1 2 2 (cid:72) :Î² =0against(cid:72) :Î² (cid:54)=0. AssumeÎ² â(cid:82)k andÎ² â(cid:82). SheobtainsaWaldstatisticW =0.34. The 0 2 1 2 1 2 samplesizeisn=500. (a) Whatisthecorrectdegrees offreedomfor theÏ2 distributiontoevaluate thesignificanceofthe Waldstatistic? (b) The Wald statisticW is very small. Indeed, is it less than the 1% quantile of the appropriate Ï2 distribution?Ifso,shouldyoureject(cid:72) ?Explainyourreasoning. 0 Exercise9.20 Youarereadingapaper,anditreportstheresultsfromtwonestedOLSregressions: Y i =X 1 (cid:48) i Î² (cid:101)1 +e (cid:101)i Y i =X 1 (cid:48) i Î² (cid:98)1 +X 2 (cid:48) i Î² (cid:98)2 +e (cid:98)i . Somesummarystatisticsarereported: ShortRegression LongRegression R2=.20 R2=.26 (cid:80)n e2=106 (cid:80)n e2=100 i=1(cid:101)i i=1(cid:98)i #ofcoefficients=5 #ofcoefficients=8 n=50 n=50 YouarecuriousiftheestimateÎ² (cid:98)2 isstatisticallydifferentfromthezerovector.Isthereawaytodetermine ananswerfromthisinformation?Doyouhavetomakeanyassumptions(beyondthestandardregularity conditions)tojustifyyouranswer? Exercise9.21 TakethemodelY =X Î² +X Î² +X Î² +X Î² +ewith(cid:69)[Xe]=0.Describehowtotest 1 1 2 2 3 3 4 4 Î² Î² (cid:72) : 1 = 3 0 Î² Î² 2 4 against Î² Î² (cid:72) : 1 (cid:54)= 3 . 1 Î² Î² 2 4",
    "page": 273,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 254 Exercise9.22 YouhavearandomsamplefromthemodelY =XÎ² +X2Î² +e with(cid:69)[e|X]=0whereY 1 2 iswages(dollarsperhour)and X isage. Describehowyouwouldtestthehypothesisthattheexpected wagefora40-year-oldworkeris$20anhour. Exercise9.23 Let T be a test statistic such that under (cid:72) , T ââ Ï2. Since (cid:80)(cid:163)Ï2>7.815 (cid:164) = 0.05, an 0 3 3 d asymptotic 5% test of (cid:72) rejects when T >7.815. An econometrician is interested in the Type I error 0 ofthistestwhenn=100andthedatastructureiswellspecified.SheperformsthefollowingMonteCarlo experiment. â¢ B=200samplesofsizen=100aregeneratedfromadistributionsatisfying(cid:72) . 0 â¢ Oneachsample,theteststatisticT iscalculated. b â¢ Shecalculatesp=B â1(cid:80)B 1 {T >7.815}=0.070. (cid:98) b=1 b â¢ TheeconometricianconcludesthatthetestT isoversizedinthiscontextâitrejectstoofrequently under(cid:72) . 0 Isherconclusioncorrect,incorrect,orincomplete?Bespecificinyouranswer. Exercise9.24 DoaMonteCarlosimulation. TakethemodelY =Î±+XÎ²+e with(cid:69)[Xe]=0wherethe parameterofinterestisÎ¸=exp(Î²).Yourdatageneratingprocess(DGP)forthesimulationis: X isU[0,1], e â¼N(0,1)isindependentof X,andn=50. SetÎ±=0andÎ²=1. GenerateB =1000independentsam- pleswithÎ±. Oneach, estimatetheregressionbyleastsquares, calculatethecovariancematrixusinga standard(heteroskedasticity-robust)formula,andsimilarlyestimateÎ¸ anditsstandarderror. Foreach replication,storeÎ² (cid:98),Î¸ (cid:98),TÎ² =(cid:161)Î² (cid:98) âÎ²(cid:162) /s (cid:161)Î² (cid:98) (cid:162) ,andTÎ¸ =(cid:161)Î¸ (cid:98) âÎ¸(cid:162) /s (cid:161)Î¸ (cid:98) (cid:162) . (a) DoesthevalueofÎ±matter?ExplainwhythedescribedstatisticsareinvarianttoÎ±andthussetting Î±=0isirrelevant. (b) Fromthe1000replicationsestimate(cid:69)(cid:163)Î² (cid:98) (cid:164) and(cid:69)(cid:163)Î¸ (cid:98) (cid:164) .Discussifyouseeevidenceifeitherestimator isbiasedorunbiased. (c) Fromthe1000replicationsestimate(cid:80)(cid:163) TÎ² >1.645 (cid:164) and(cid:80)[TÎ¸ >1.645]. Whatdoesasymptoticthe- orypredicttheseprobabilitiesshouldbeinlargesamples? Whatdoyoursimulationresultsindi- cate? Exercise9.25 The data set Invest1993 on the textbook website contains data on 1962 U.S. firms ex- tractedfromCompustat,assembledbyBronwynHall,andusedinHallandHall(1993). Thevariablesweuseinthisexerciseareinthetablebelow. Theflowvariablesareannualsums. The stockvariablesarebeginningofyear. year yearoftheobservation I inva InvestmenttoCapitalRatio Q vala TotalMarketValuetoAssetRatio(TobinâsQ) C cfa CashFlowtoAssetRatio D debta LongTermDebttoAssetRatio (a) Extractthesub-sampleofobservationsfor1987. Thereshouldbe1028observations. Estimatea linear regression of I (investment to capital ratio) on the other variables. Calculate appropriate standarderrors.",
    "page": 274,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 255 (b) Calculateasymptoticconfidenceintervalsforthecoefficients. (c) ThisregressionisrelatedtoTobinâsqtheoryofinvestment,whichsuggeststhatinvestmentshould bepredictedsolelybyQ (TobinâsQ).ThistheorypredictsthatthecoefficientonQ shouldbepos- itive and the others should be zero. Test the joint hypothesis that the coefficients on cash flow (C) and debt (D) are zero. Test the hypothesis that the coefficient onQ is zero. Are the results consistentwiththepredictionsofthetheory? (d) Nowtryanonlinear(quadratic)specification.RegressI onQ,C,D,Q2,C2,D2,QÃC,QÃD,CÃD. Testthejointhypothesisthatthesixinteractionandquadraticcoefficientsarezero. Exercise9.26 Inapaperin1963,MarcNerloveanalyzedacostfunctionfor145Americanelectriccom- panies. Nerlovwasinterestedinestimatingacostfunction:C = f(Q,PL,PF,PK)wherethevariablesare listedinthetablebelow.HisdatasetNerlove1963isonthetextbookwebsite. C TotalCost Q Output PL Unitpriceoflabor PK Unitpriceofcapital PF Unitpriceoffuel (a) First,estimateanunrestrictedCobb-Douglassspecification logC =Î² +Î² logQ+Î² logPL+Î² logPK+Î² logPF+e. (9.23) 1 2 3 4 5 Reportparameterestimatesandstandarderrors. (b) Whatistheeconomicmeaningoftherestriction(cid:72) :Î² +Î² +Î² =1? 0 3 4 5 (c) Estimate (9.23) by constrained least squares imposing Î² +Î² +Î² =1. Report your parameter 3 4 5 estimatesandstandarderrors. (d) Estimate(9.23)byefficientminimumdistanceimposingÎ² +Î² +Î² =1. Reportyourparameter 3 4 5 estimatesandstandarderrors. (e) Test(cid:72) :Î² +Î² +Î² =1usingaWaldstatistic. 0 3 4 5 (f) Test(cid:72) :Î² +Î² +Î² =1usingaminimumdistancestatistic. 0 3 4 5 Exercise9.27 InSection8.12wereportedestimatesfromMankiw,RomerandWeil(1992). Wereported estimation both by unrestricted least squares and by constrained estimation, imposing the constraint thatthreecoefficients(2nd,3rd and4th coefficients)sumtozeroasimpliedbytheSolowgrowththeory. UsingthesamedatasetMRW1992estimatetheunrestrictedmodelandtestthehypothesisthatthethree coefficientssumtozero. Exercise9.28 Usingthecps09mardatasetandthesubsampleofnon-HispanicBlackindividuals(race code=2)testthehypothesisthatmarriagestatusdoesnotaffectmeanwages. (a) Take the regression reported in Table 4.1. Which variables will need to be omitted to estimate a regressionforthissubsample?",
    "page": 275,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER9. HYPOTHESISTESTING 256 (b) Expressthehypothesisâmarriagestatusdoesnotaffectmeanwagesâasarestrictiononthecoeffi- cients.Howmanyrestrictionsisthis? (c) FindtheWald(orF)statisticforthishypothesis. Whatistheappropriatedistributionforthetest statistic?Calculatethep-valueofthetest. (d) Whatdoyouconclude? Exercise9.29 Usingthecps09mardatasetandthesubsampleofnon-HispanicBlackindividuals(race code=2)andwhiteindividuals(racecode=1)testthehypothesisthatthereturnstoeducationiscom- monacrossgroups. (a) Allowthereturntoeducationtovaryacrossthefourgroups(whitemale,whitefemale,Blackmale, Blackfemale)byinteractingdummyvariableswitheducation. Estimateanappropriateversionof theregressionreportedinTable4.1. (b) FindtheWald(orF)statisticforthishypothessis. Whatistheappropriatedistributionforthetest statistic?Calculatethep-valueofthetest. (c) Whatdoyouconclude?",
    "page": 276,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Chapter 10 Resampling Methods 10.1 Introduction Sofarinthistextbookwehavediscussedtwoapproachestoinference: exactandasymptotic. Both have their strengths and weaknesses. Exact theory provides a useful benchmark but is based on the unrealisticandstringentassumptionofthehomoskedasticnormalregressionmodel.Asymptotictheory providesamoreflexibledistributiontheorybutisexplicitlyanapproximationwithuncertainaccuracy inpractice. Inthischapterweintroduceasetofalternativeinferencemethodswhicharebasedaroundthecon- ceptofresamplingâwhichmeansusingsamplinginformationextractedfromtheempiricaldistribution ofthedata. Thesearepowerfulmethods,widelyapplicable,andoftenmoreaccuratethanexactmeth- odsandasymptoticapproximations. Twodisadvantages,however,are(1)resamplingmethodstypically requiremorecomputationpower;and(2)thetheoryisconsiderablymorechallenging. Aconsequence ofthecomputationrequirementisthatmostempiricalresearchersuseasymptoticapproximationsfor routinecalculationswhileresamplingapproximationsareusedforfinalreporting. Wewilldiscusstwocategoriesofresamplingmethodsusedinstatisticalandeconometricpractice: jackknifeandbootstrap. Mostofourattentionwillbegiventothebootstrapasitisthemostcommonly usedresamplingmethodineconometricpractice. The jackknife is the distribution obtained from the n leave-one-out estimators (see Section 3.20). Thejackknifeismostcommonlyusedforvarianceestimation. Thebootstrapisthedistributionobtainedbyestimationonsamplescreatedbyi.i.d. samplingwith replacement from the dataset. (There are other variants of bootstrap sampling, including parametric samplingandresidualsampling.) Thebootstrapiscommonlyusedforvarianceestimation,confidence intervalconstruction,andhypothesistesting. Thereisathirdcategoryofresamplingmethodsknownassub-samplingwhichwewillnotcoverin thistextbook. Sub-samplingisthedistributionobtainedbyestimationonsub-samples(samplingwith- outreplacement)ofthedataset. Sub-samplingcanbeusedformostofsamepurposesasthebootstrap. SeetheexcellentmonographbyPolitis,RomanoandWolf(1999). 10.2 Example TomotivateourdiscussionwefocusontheapplicationpresentedinSection3.7,whichisabivariate regressionappliedtotheCPSsubsampleofmarriedBlackfemalewageearnerswith12yearspotential workexperienceanddisplayedinTable3.1.Theregressionequationis log(wage)=Î² education+Î² +e. 1 2 257",
    "page": 277,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 258 Theestimatesasreportedin(4.39)are log(wage)= 0.155 education+ 0.698 +e (cid:98) (0.031) (0.493) Ï2= 0.144 (cid:98) (0.043) n=20. Wefocusonfourestimatesconstructedfromthisregression. Thefirsttwoarethecoefficientesti- matesÎ² (cid:98)1 andÎ² (cid:98)2 .ThethirdisthevarianceestimateÏ (cid:98) 2.Thefourthisanestimateoftheexpectedlevelof wagesforanindividualwith16yearsofeducation(acollegegraduate),whichturnsouttobeanonlinear functionoftheparameters.Underthesimplifyingassumptionthattheerroreisindependentofthelevel ofeducationandnormallydistributedwefindthattheexpectedlevelofwagesis Âµ=(cid:69)(cid:163) wage|education=16 (cid:164) =(cid:69)(cid:163) exp (cid:161) 16Î² +Î² +e (cid:162)(cid:164) 1 2 =exp (cid:161) 16Î² +Î² (cid:162)(cid:69)(cid:163) exp(e) (cid:164) 1 2 =exp (cid:161) 16Î² +Î² +Ï2/2 (cid:162) . 1 2 Thefinalequalityis(cid:69)(cid:163) exp(e) (cid:164)=exp (cid:161)Ï2/2 (cid:162) whichcanbeobtainedfromthenormalmomentgenerating function.TheparameterÂµisanonlinearfunctionofthecoefficients.ThenaturalestimatorofÂµreplaces theunknownsbythepointestimators.Thus Âµ (cid:98) =exp (cid:161) 16Î² (cid:98)1 +Î² (cid:98)2 +Ï (cid:98) 2/2 (cid:162)= 25.80 (2.29) ThestandarderrorforÂµcanbefoundbyextendingExercise7.8tofindthejointasymptoticdistribution (cid:98) ofÏ2andtheslopeestimates,andthenapplyingthedeltamethod. (cid:98) Weareinterestedincalculatingstandarderrorsandconfidenceintervalsforthefourestimatesde- scribedabove. 10.3 JackknifeEstimationofVariance Thejackknifeestimatesmomentsofestimatorsusingthedistributionoftheleave-one-outestima- tors. The jackknife estimators of bias and variance were introduced by Quenouille (1949) and Tukey (1958),respectively.TheideawasexpandedfurtherinthemonographsofEfron(1982)andShaoandTu (1995). LetÎ¸ (cid:98)beanyestimatorofavector-valuedparameterÎ¸whichisafunctionofarandomsampleofsize n. LetV Î¸(cid:98) =var (cid:163)Î¸ (cid:98) (cid:164) bethevarianceofÎ¸ (cid:98). Definetheleave-one-outestimatorsÎ¸ (cid:98)(âi) whicharecomputed usingtheformulaforÎ¸ (cid:98)exceptthatobservationi isdeleted.TukeyâsjackknifeestimatorforV Î¸(cid:98) isdefined asascaleofthesamplevarianceoftheleave-one-outestimators: V(cid:98) j Î¸(cid:98) ack= n n â1 i (cid:88) = n 1 (cid:179) Î¸ (cid:98)(âi) âÎ¸ (cid:180)(cid:179) Î¸ (cid:98)(âi) âÎ¸ (cid:180)(cid:48) (10.1)",
    "page": 278,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 259 whereÎ¸ isthesamplemeanoftheleave-one-outestimatorsÎ¸=n â1(cid:80)n i=1 Î¸ (cid:98)(âi) . ForscalarestimatorsÎ¸ (cid:98) (cid:114) thejackknifestandarderroristhesquarerootof(10.1): s jack= V(cid:98) jack . Î¸(cid:98) Î¸(cid:98) jack AconvenientfeatureofthejackknifeestimatorV(cid:98) isthattheformula(10.1)isquitegeneraland Î¸(cid:98) doesnotrequireanytechnical(exactorasymptotic)calculations.Adownsideisthatcanrequirensepa- rateestimations,whichinsomecasescanbecomputationallycostly. jack InmostcasesV(cid:98) willbesimilartoarobustasymptoticcovariancematrixestimator. Themainat- Î¸(cid:98) tractionsofthejackknifeestimatorarethatitcanbeusedwhenanexplicitasymptoticvarianceformula isnotavailableandthatitcanbeusedasacheckonthereliabilityofanasymptoticformula. The formula(10.1)isnotimmediately intuitive somaybenefitfromsome motivation. Westartby examiningthesamplemeanY = 1(cid:80)n Y forY â(cid:82)m.Theleave-one-outestimatoris n i=1 i 1 (cid:88) n 1 Y (âi) = nâ1 Y j = nâ1 Y â nâ1 Y i . (10.2) j(cid:54)=i Thesamplemeanoftheleave-one-outestimatorsis 1 (cid:88) n n 1 n Y (âi) = nâ1 Y â nâ1 Y =Y. i=1 Thedifferenceis 1 (cid:179) (cid:180) Y (âi) âY = nâ1 Y âY i . Thejackknifeestimateofvariance(10.1)isthen V(cid:98) j Y ack= n n â1 (cid:88) n (cid:181) nâ 1 1 (cid:182)2(cid:179) Y âY i (cid:180)(cid:179) Y âY i (cid:180)(cid:48) i=1 1 (cid:181) 1 (cid:182) (cid:88) n (cid:179) (cid:180)(cid:179) (cid:180)(cid:48) = Y âY Y âY . (10.3) n nâ1 i i i=1 ThisisidenticaltotheconventionalestimatorforthevarianceofY.Indeed,Tukeyproposedthe(nâ1)/n jack scalingin(10.1)sothatV(cid:98) preciselyequalstheconventionalestimator. Y Wenextexaminethecaseofleastsquaresregressioncoefficientestimator.Recallfrom(3.43)thatthe leave-one-outOLSestimatorequals Î² (cid:98)(âi) =Î² (cid:98) â(cid:161) X (cid:48) X (cid:162)â1 X i e (cid:101)i (10.4) where e =(1âh ) â1e and h = X (cid:48)(cid:161) X (cid:48) X (cid:162)â1 X . The sample mean of the leave-one-out estimators is (cid:101)i ii (cid:98)i ii i i Î²=Î² (cid:98) â(cid:161) X (cid:48) X (cid:162)â1Âµ (cid:101) whereÂµ (cid:101) =n â1(cid:80)n i=1 X i e (cid:101)i .ThusÎ² (cid:98)(âi) âÎ²=â(cid:161) X (cid:48) X (cid:162)â1(cid:161) X i e (cid:101)i âÂµ (cid:101) (cid:162) .Thejackknifeestimate ofvarianceforÎ² (cid:98)is V(cid:98) j Î²(cid:98) ack= n n â1 i (cid:88) = n 1 (cid:179) Î² (cid:98)(âi) âÎ² (cid:180)(cid:179) Î² (cid:98)(âi) âÎ² (cid:180)(cid:48) (cid:195) (cid:33) = nâ1(cid:161) X (cid:48) X (cid:162)â1 (cid:88) n X X (cid:48) e2ânÂµÂµ(cid:48) (cid:161) X (cid:48) X (cid:162)â1 n i i(cid:101)i (cid:101)(cid:101) i=1 = n n â1 V(cid:98) H Î²(cid:98) C3â(nâ1) (cid:161) X (cid:48) X (cid:162)â1Âµ (cid:101) Âµ (cid:101) (cid:48)(cid:161) X (cid:48) X (cid:162)â1 (10.5)",
    "page": 279,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 260 HC3 whereV(cid:98)Î²(cid:98) istheHC3covarianceestimator(4.34)basedonpredictionerrors.Thesecondtermin(10.5) is typically quite small since Âµ (cid:101) is typically small in magnitude. Thus V(cid:98) j Î²(cid:98) ack (cid:39)V(cid:98) H Î²(cid:98) C3 . Indeed the HC3 estimator was originally motivated as a simplification of the jackknife estimator. This shows that for regressioncoefficientsthejackknifeestimatorofvarianceissimilartoaconventionalrobustestimator. Thisisaccomplishedwithouttheuserâknowingâtheformoftheasymptoticcovariancematrix. Thisis furtherconfirmationthatthejackknifeismakingareasonablecalculation. Third, weexaminethejackknifeestimatorforafunctionÎ¸ (cid:98) =r(Î² (cid:98))ofaleastsquaresestimator. The leave-one-outestimatorofÎ¸is Î¸ (cid:98)(âi) =r(Î² (cid:98)(âi) ) =r (cid:179) Î² (cid:98) â(cid:161) X (cid:48) X (cid:162)â1 X i e (cid:101)i (cid:180) (cid:39)Î¸ (cid:98) âR(cid:98) (cid:48)(cid:161) X (cid:48) X (cid:162)â1 X i e (cid:101)i . The second equality is (10.4). The final approximation is obtained by a mean-value expansion, using r(Î² (cid:98))=Î¸ (cid:98)andsettingR(cid:98) =(cid:161)â/âÎ²(cid:162) r(Î² (cid:98)) (cid:48) .ThisapproximationholdsinlargesamplessinceÎ² (cid:98)(âi) areuniformly consistentforÎ².ThejackknifevarianceestimatorforÎ¸ (cid:98)thusequals V(cid:98) j Î¸(cid:98) ack= n n â1 i (cid:88) = n 1 (cid:179) Î¸ (cid:98)(âi) âÎ¸ (cid:180)(cid:179) Î¸ (cid:98)(âi) âÎ¸ (cid:180)(cid:48) (cid:195) (cid:33) (cid:39) n n â1 R(cid:98) (cid:48)(cid:161) X (cid:48) X (cid:162)â1 (cid:88) n X i X i (cid:48) e (cid:101)i 2ânÂµ (cid:101) Âµ (cid:101) (cid:48) (cid:161) X (cid:48) X (cid:162)â1 R(cid:98) i=1 =R(cid:98) (cid:48) V(cid:98) jack R(cid:98) Î²(cid:98) (cid:48) (cid:39)R(cid:98) V(cid:101)Î²(cid:98) R(cid:98). The final line equals a delta-method estimator for the variance of Î¸ (cid:98) constructed with the covariance estimator(4.34).ThisshowsthatthejackknifeestimatorofvarianceforÎ¸ (cid:98)isapproximatelyanasymptotic delta-method estimator. While this is an asymptotic approximation, it again shows that the jackknife produces an estimator which is asymptotically similar to one produced by asymptotic methods. This isdespitethefactthatthejackknifeestimatoriscalculatedwithoutreferencetoasymptotictheoryand doesnotrequirecalculationofthederivativesofr(Î²). Thisargumentextendsdirectlytoanyâsmoothfunctionâestimator.Mostoftheestimatorsdiscussed (cid:179) (cid:180) so far in this textbook take the form Î¸ (cid:98) =g W whereW =n â1(cid:80)n i=1 W i andW i is some vector-valued (cid:179) (cid:180) functionofthedata. ForanysuchestimatorÎ¸ (cid:98)theleave-one-outestimatorequalsÎ¸ (cid:98)(âi) =g W (âi) and itsjackknifeestimatorofvarianceis(10.1)",
    "page": 280,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Thisargumentextendsdirectlytoanyâsmoothfunctionâestimator.Mostoftheestimatorsdiscussed (cid:179) (cid:180) so far in this textbook take the form Î¸ (cid:98) =g W whereW =n â1(cid:80)n i=1 W i andW i is some vector-valued (cid:179) (cid:180) functionofthedata. ForanysuchestimatorÎ¸ (cid:98)theleave-one-outestimatorequalsÎ¸ (cid:98)(âi) =g W (âi) and itsjackknifeestimatorofvarianceis(10.1). Using(10.2)andamean-valueexpansionwehavethelarge- sampleapproximation (cid:179) (cid:180) Î¸ (cid:98)(âi) =g W (âi) (cid:181) (cid:182) n 1 =g W â W nâ1 nâ1 i (cid:179) (cid:180) 1 (cid:179) (cid:180)(cid:48) (cid:39)g W â G W W nâ1 i whereG(x)=(â/âx)g(x) (cid:48) .Thus 1 (cid:179) (cid:180)(cid:48)(cid:179) (cid:180) Î¸ (cid:98)(âi) âÎ¸(cid:39)â nâ1 G W W i âW",
    "page": 280,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 261 andthejackknifeestimatorofthevarianceofÎ¸ (cid:98)approximatelyequals V(cid:98) j Î¸(cid:98) ack= n n â1 i (cid:88) = n 1 (cid:161)Î¸ (cid:98)(âi) âÎ¸ (cid:98)(Â·) (cid:162)(cid:161)Î¸ (cid:98)(âi) âÎ¸ (cid:98)(Â·) (cid:162)(cid:48) (cid:195) (cid:33) nâ1 (cid:179) (cid:180)(cid:48) 1 (cid:88) n (cid:179) (cid:180)(cid:179) (cid:180)(cid:48) (cid:179) (cid:180) (cid:39) G W W âW W âW G W n (nâ1)2 i=1 i i (cid:179) (cid:180)(cid:48) (cid:179) (cid:180) =G W V(cid:98) jack G W W jack whereV(cid:98) asdefinedin(10.3)istheconventional(andjackknife)estimatorforthevarianceofW.Thus W jack V(cid:98) isapproximatelythedelta-methodestimator.Onceagain,weseethatthejackknifeestimatorau- Î¸(cid:98) tomaticallycalculateswhatiseffectivelythedelta-methodvarianceestimator,butwithoutrequiringthe usertoexplicitlycalculatethederivativeofg(x). 10.4 Example Weillustratebyreportingtheasymptoticandjackknifestandarderrorsforthefourparameteresti- matesgivenearlier. InTable10.1wereporttheactualvaluesoftheleave-one-outestimatesforeachof thetwentyobservationsinthesample.Thejackknifestandarderrorsarecalculatedasthescaledsquare roots of the sample variances of these leave-one-out estimates and are reported in the second-to-last row.Forcomparisontheasymptoticstandarderrorsarereportedinthefinalrow. Forallestimatesthejackknifeandasymptoticstandarderrorsarequitesimilar. Thisreinforcesthe credibilityofbothstandarderrorestimates. ThelargestdifferencesariseforÎ² (cid:98)2 andÂµ (cid:98) ,whosejackknife standarderrorsareabout5%largerthantheasymptoticstandarderrors. Thetake-awayfromourpresentationisthatthejackknifeisasimpleandflexiblemethodforvari- ance and standard error calculation. Circumventing technical asymptotic and exact calculations, the jackknifeproducesestimateswhichinmanycasesaresimilartoasymptoticdelta-methodcounterparts. Thejackknifeisespeciallyappealingincaseswhereasymptoticstandarderrorsarenotavailableorare difficulttocalculate. Theycanalsobeusedasadouble-checkonthereasonabilityofasymptoticdelta- methodcalculations. In Stata, jackknife standard errors for coefficient estimates in many models are simply obtained by the vce(jackknife) option. For nonlinear functions of the coefficients or other estimators the jackknifecommandcanbecombinedwithanyothercommandtoobtainjackknifestandarderrors. To illustrate, below we list the Stata commands which will calculate the jackknife standard errors listedabove. Thefirstlineisleastsquaresestimationwithstandarderrorscalculatedbythejackknife. ThesecondlinecalculatestheerrorvarianceestimateÏ2 withajackknifestandarderror. Thethirdline (cid:98) doesthesamefortheestimateÂµ. (cid:98) StataCommands regwageeducationifmbf12==1,vce(jackknife) jackknife(e(rss)/e(N)):regwageeducationifmbf12==1 jackknifeexp(16*_b[education]+_b[_cons]+e(rss)/e(N)/2):/// regwageeducationifmbf12==1",
    "page": 281,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 262 Table10.1:Leave-one-outEstimatorsandJackknifeStandardErrors Observation Î² (cid:98)1(âi) Î² (cid:98)2(âi) Ï (cid:98) 2 (âi) Âµ (cid:98)(âi) 1 0.150 0.764 0.150 25.63 2 0.148 0.798 0.149 25.48 3 0.153 0.739 0.151 25.97 4 0.156 0.695 0.144 26.31 5 0.154 0.701 0.146 25.38 6 0.158 0.655 0.151 26.05 7 0.152 0.705 0.114 24.32 8 0.146 0.822 0.147 25.37 9 0.162 0.588 0.151 25.75 10 0.157 0.693 0.139 26.40 11 0.168 0.510 0.141 26.40 12 0.158 0.691 0.118 26.48 13 0.139 0.974 0.141 26.56 14 0.169 0.451 0.131 26.26 15 0.146 0.852 0.150 24.93 16 0.156 0.696 0.148 26.06 17 0.165 0.513 0.140 25.22 18 0.155 0.698 0.151 25.90 19 0.152 0.742 0.151 25.73 20 0.155 0.697 0.151 25.95 sjack 0.032 0.514 0.046 2.39 sasy 0.031 0.493 0.043 2.29 10.5 JackknifeforClusteredObservations In Section 4.23 we introduced the clustered regression model, cluster-robust variance estimators, andcluster-robuststandarderrors.Jackknifevarianceestimationcanalsobeusedforclusteredsamples but with some natural modifications. Recall that the least squares estimator in the clustered sample contextcanbewrittenas (cid:195) (cid:33)â1(cid:195) (cid:33) G G Î² (cid:98) = (cid:88) X (cid:48) g X g (cid:88) X (cid:48) g Y g g=1 g=1 where g =1,...,G indexes the cluster. Instead of leave-one-out estimators, it is natural to use delete- clusterestimators,whichdeleteoneclusteratatime.Theytaketheform(4.53): Î² (cid:98)(âg) =Î² (cid:98) â(cid:161) X (cid:48) X (cid:162)â1 X (cid:48) g(cid:101) e g where e = (cid:179) I âX (cid:161) X (cid:48) X (cid:162)â1 X (cid:48) (cid:180)â1 e (cid:101)g ng g g (cid:98)g (cid:98) e g =Y g âX g Î² (cid:98).",
    "page": 282,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 263 Thedelete-clusterjackknifeestimatorofthevarianceofÎ² (cid:98)is V(cid:98) j Î²(cid:98) ack= G G â1 g (cid:88) G =1 (cid:179) Î² (cid:98)(âg) âÎ² (cid:180)(cid:179) Î² (cid:98)(âg) âÎ² (cid:180)(cid:48) 1 (cid:88) G Î²= Î² (cid:98)(âg) . G g=1 jack WecanalsocallV(cid:98) acluster-robustjackknifeestimatorofvariance. Î²(cid:98) Usingthesameapproximationsastheprevioussectionwecanshowthatthedelete-clusterjackknife estimatorisasymptoticallyequivalenttothecluster-robustcovariancematrixestimator(4.54)calculated withthedelete-clusterpredictionerrors.Thisverifiesthatthedelete-clusterjackknifeistheappropriate jackknifeapproachforclustereddependence. ForparameterswhicharefunctionsÎ¸ (cid:98) =r(Î² (cid:98))oftheleastsquaresestimator, thedelete-clusterjack- knifeestimatorofthevarianceofÎ¸ (cid:98)is V(cid:98) j Î¸(cid:98) ack= G G â1 g (cid:88) G =1 (cid:179) Î¸ (cid:98)(âg) âÎ¸ (cid:180)(cid:179) Î¸ (cid:98)(âg) âÎ¸ (cid:180)(cid:48) Î¸ (cid:98)(âi) =r(Î² (cid:98)(âg) ) 1 (cid:88) G Î¸= Î¸ (cid:98)(âg) . G g=1 Usingamean-valueexpansionwecanshowthatthisestimatorisasymptoticallyequivalenttothedelta- methodcluster-robustcovariancematrixestimatorforÎ¸ (cid:98). Thisshowsthatthejackknifeestimatorisap- propriateforcovariancematrixestimation. Asinthecontextofi.i.d.samples,oneadvantageofthejackknifecovariancematrixestimatorsisthat theydonotrequiretheusertomakeatechnicalcalculationoftheasymptoticdistribution. Adownside isanincreaseincomputationcost,asG separateregressionsareeffectivelyestimated. InStata,jackknifestandarderrorsforcoefficientestimateswithclusteredobservationsareobtained byusingtheoptionscluster(id) vce(jackknife)whereiddenotestheclustervariable. 10.6 TheBootstrapAlgorithm ThebootstrapisapowerfulapproachtoinferenceandisduetothepioneeringworkofEfron(1979). Therearemanytextbookandmonographtreatmentsofthebootstrap,includingEfron(1982),Hall(1992), EfronandTibshirani(1993),ShaoandTu(1995),andDavisonandHinkley(1997).Reviewsforeconome- triciansareprovidedbyHall(1994)andHorowitz(2001) Thereareseveralwaystodescribeordefinethebootstrapandthereareseveralformsoftheboot- strap.Westartinthissectionbydescribingthebasicnonparametricbootstrapalgorithm.Insubsequent sectionswegivemoreformaldefinitionsofthebootstrapaswellastheoreticaljustifications. Briefly,thebootstrapdistributionisobtainedbyestimationonindependentsamplescreatedbyi.i.d. sampling(samplingwithreplacement)fromtheoriginaldataset. To understand this it is useful to start with the concept of sampling with replacement from the dataset. To continue the empirical example used earlier in the chapter we focus on the dataset dis- played in Table 3.1, which has n =20 observations. Sampling from this distribution means randomly selectingonerowfromthistable.Mathematicallythisisthesameasrandomlyselectinganintegerfrom theset{1,2,...,20}. Toillustrate, MATLABhasarandomintegergenerator(thefunctionrandi). Using",
    "page": 283,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 264 therandomnumberseedof13(anarbitrarychoice)weobtaintherandomdraw16.Thismeansthatwe drawobservationnumber16fromTable3.1. Examiningthetablewecanseethatthisisanindividual withwage$18.75andeducationof16years. Werepeatbydrawinganotherrandomintegerontheset {1,2,...,20}andthistimeobtain5. Thismeanswetakeobservation5fromTable3.1, whichisanindi- vidualwithwage$33.17andeducationof16years. Wecontinueuntilwehaven=20suchdraws. This randomsetofobservationsare{16,5,17,20,20,10,13,16,13,15,1,6,2,18,8,14,6,7,1,8}. Wecallthis thebootstrapsample. Notice that the observations 1, 6, 8, 13, 16, 20 each appear twice in the bootstrap sample, and the observations3,4,9,11,12,19donotappearatall.Thatisokay.Infact,itisnecessaryforthebootstrapto work.Thisisbecausewearedrawingwithreplacement.(Ifweinsteadmadedrawswithoutreplacement thentheconstructeddatasetwouldhaveexactlythesameobservationsasinTable3.1,onlyindifferent order.) WecanalsoaskthequestionâWhatistheprobabilitythatanindividualobservationwillappear atleastonceinthebootstrapsample?âTheansweris (cid:80)(cid:163) ObservationinBootstrapSample (cid:164)=1â (cid:181) 1â 1 (cid:182)n (10.6) n â1âe â1(cid:39)0.632. The limit holds as n ââ. The approximation 0.632 is excellent even for small n. For example, when n =20 the probability (10.6) is 0.641. These calculations show that an individual observation is in the bootstrapsamplewithprobabilitynear2/3. Once again, the bootstrap sample is the constructed dataset with the 20 observations drawn ran- domlyfromtheoriginalsample.Notationally,wewritetheithbootstrapobservationas (cid:161) Y â ,X â(cid:162) andthe i i (cid:161) â â(cid:162) (cid:161) â â(cid:162) bootstrapsampleas{ Y ,X ,..., Y ,X }. Inourpresentexample withY denotingthelogwage the 1 1 n n bootstrapsampleis { (cid:161) Y â ,X â(cid:162) ,..., (cid:161) Y â ,X â(cid:162) }={(2.93,16),(3.50,16)...,(3.76,18)}. 1 1 n n The bootstrap estimate Î² (cid:98) â is obtained by applying the least squares estimation formula to the boot- strap sample. Thus we regress Y â on X â . The other bootstrap estimates, in our example Ï2â and (cid:98) Âµ (cid:98) â , are obtained by applying their estimation formulae to the bootstrap sample as well. Writing Î¸ (cid:98) â = (cid:161)Î² (cid:98) â 1 ,Î² (cid:98) â 2 ,Ï (cid:98) â2,Âµ (cid:98) â(cid:162)(cid:48) wehavethebootstrapestimateoftheparametervectorÎ¸=(cid:161)Î² 1 ,Î² 2 ,Ï2,Âµ(cid:162)(cid:48) . Inourex- ample(thebootstrapsampledescribedabove)Î¸ (cid:98) â=(0.195,0.113,0.107,26.7) (cid:48) . Thisisonedrawfromthe bootstrapdistributionoftheestimates. The estimate Î¸ (cid:98) â as described is one random draw from the distribution of estimates obtained by i.i.d. samplingfromtheoriginaldata. Withonedrawwecansayrelativelylittle. Butwecanrepeatthis exercisetoobtainmultipledrawsfromthisbootstrapdistribution. Todistinguishbetweenthesedraws weindexthebootstrapsamplesbyb=1,...,B,andwritethebootstrapestimatesasÎ¸ (cid:98) â orÎ¸ (cid:98) â (b)",
    "page": 284,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Thisisonedrawfromthe bootstrapdistributionoftheestimates. The estimate Î¸ (cid:98) â as described is one random draw from the distribution of estimates obtained by i.i.d. samplingfromtheoriginaldata. Withonedrawwecansayrelativelylittle. Butwecanrepeatthis exercisetoobtainmultipledrawsfromthisbootstrapdistribution. Todistinguishbetweenthesedraws weindexthebootstrapsamplesbyb=1,...,B,andwritethebootstrapestimatesasÎ¸ (cid:98) â orÎ¸ (cid:98) â (b). b Tocontinueourillustrationwedraw20morerandomintegers{19,5,7,19,1,2,13,18,1,15,17,2, 14,11,10,20,1,5,15,7}andconstructasecondbootstrapsample.Onthissampleweagainestimatethe parametersandobtainÎ¸ (cid:98) â (2)=(0.175,0.52,0.124,29.3) (cid:48) .Thisisasecondrandomdrawfromthedistribu- tionofÎ¸ (cid:98) â . WerepeatthisB times,storingtheparameterestimatesÎ¸ (cid:98) â (b). Wehavethuscreatedanew datasetofbootstrapdraws (cid:169)Î¸ (cid:98) â (b):b=1,...,B (cid:170) . Byconstructionthedrawsareindependentacrossband identicallydistributed. The number of bootstrap draws, B, is often called the ânumber of bootstrap replicationsâ. Typical choices for B are 1000, 5000, and 10,000. We discuss selecting B later, but roughly speaking, larger B results in a more precise estimate at an increased computation cost. For our application we set B = 10,000.",
    "page": 284,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 265 Toillustrate,Figure13.1displaysthedensitiesofthedistributionsofthebootstrapestimatesÎ² (cid:98) â and 1 Âµ (cid:98) â across10,000draws.Thedashedlinesshowthepointestimate.YoucannoticethatthedensityforÎ² (cid:98) â 1 isslightlyskewedtotheleft. 0.00 0.05 0.10 0.15 0.20 0.25 0.30 41 21 01 8 6 4 2 0 Return to Education ytisneD partstooB 15 20 25 30 35 51.0 01.0 50.0 00.0 Mean Wage of College Graduates ytisneD partstooB Figure10.1:BootstrapDistributionsofÎ² (cid:98) â 1 andÂµ (cid:98) â 10.7 BootstrapVarianceandStandardErrors Given the bootstrap draws we can estimate features of the bootstrap distribution. The bootstrap estimatorofvarianceofanestimatorÎ¸ (cid:98)isthesamplevarianceacrossthebootstrapdrawsÎ¸ (cid:98) â (b).Itequals V(cid:98) b Î¸(cid:98) oot= B 1 â1 (cid:88) B (cid:179) Î¸ (cid:98) â (b)âÎ¸ â(cid:180)(cid:179) Î¸ (cid:98) â (b)âÎ¸ â(cid:180)(cid:48) (10.7) b=1 Î¸ â = 1 (cid:88) B Î¸ (cid:98) â (b). B b=1 For a scalar estimator Î¸ (cid:98)the bootstrapstandarderror is the square root of the bootstrap estimator of variance: (cid:113) s Î¸ b (cid:98) oot= V(cid:98) b Î¸(cid:98) oot . Thisisaverysimplestatistictocalculateandisthemostcommonuseofthebootstrapinappliedecono- metricpractice. Acaveat(discussedinmoredetailinSection10.15)isthatinmanycasesitisbetterto useatrimmedestimator. Standarderrorsareconventionallyreportedtoconveytheprecisionoftheestimator. Theyarealso commonlyusedtoconstructconfidenceintervals. Bootstrapstandarderrorscanbeusedforthispur- pose.Thenormal-approximationbootstrapconfidenceintervalis (cid:104) (cid:105) Cnb= Î¸ (cid:98) âz 1âÎ±/2 s Î¸ b (cid:98) oot, Î¸ (cid:98) +z 1âÎ±/2 s Î¸ b (cid:98) oot where z 1âÎ±/2 is the 1âÎ±/2 quantile ofthe N(0,1)distribution. ThisintervalCnb is identical in format to an asymptotic confidence interval, but with the bootstrap standard error replacing the asymptotic",
    "page": 285,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 266 standarderror.CnbisthedefaultconfidenceintervalreportedbyStatawhenthebootstraphasbeenused tocalculatestandarderrors.However,thenormal-approximationintervalisingeneralapoorchoicefor confidence interval construction as it relies on the normal approximation to the t-ratio which can be inaccurate in finite samples. There are other methods â such as the bias-corrected percentile method to be discussed in Section 10.17 â which are just as simple to compute but have better performance. In general, bootstrap standard errors should be used as estimates of precision rather than as tools to constructconfidenceintervals. boot SinceB isfinite,allbootstrapstatistics,suchasV(cid:98)Î¸(cid:98) ,areestimatesandhencerandom.Theirvalues willvaryacrossdifferentchoicesforB andsimulationruns(dependingonhowthesimulationseedis set).Thusyoushouldnotexpecttoobtaintheexactsamebootstrapstandarderrorsasotherresearchers whenreplicatingtheirresults.Theyshouldbesimilar(uptosimulationsamplingerror)butnotprecisely thesame. InTable10.2wereportthefourparameterestimatesintroducedinSection10.2alongwithasymp- totic,jackknifeandbootstrapstandarderrors. Wealsoreportfourbootstrapconfidenceintervalswhich willbeintroducedinsubsequentsections. Forthesefourestimatorswecanseethatthebootstrapstandarderrorsarequitesimilartotheasymp- totic and jackknife standard errors. The most noticable difference arises for Î² (cid:98)2 , where the bootstrap standarderrorisabout10%largerthantheasymptoticstandarderror. Table10.2:ComparisonofMethods Î² Î² Ï2 Âµ (cid:98)1 (cid:98)2 (cid:98) (cid:98) Estimate 0.155 0.698 0.144 25.80 Asymptotics.e. (0.031) (0.493) (0.043) (2.29) Jackknifes.e. (0.032) (0.514) (0.046) (2.39) Bootstraps.e. (0.034) (0.548) (0.041) (2.38) 95%PercentileInterval [0.08,0.21] [â0.27,1.91] [0.06,0.22] [21.4,30.7] 95%BCPercentileInterval [0.08,0.21] [â0.25,1.93] [0.09,0.28] [22.0,31.5] 95%BC PercentileInterval [0.08,0.21] [â0.25,1.93] [0.09,0.28] [22.0,31.5] a 95%Percentile-tInterval [0.09,0.21] [â0.20,1.81] [0.08,0.34] [21.6,32.2] In Stata, bootstrap standard errors for coefficient estimates in many models are obtained by the vce(bootstrap, reps(#)) option, where # is the number of bootstrap replications. For nonlinear functions of the coefficients or other estimators the bootstrap command can be combined with any othercommandtoobtainbootstrapstandarderrors.Synonymsforbootstraparebstrapandbs. Toillustrate, belowwelisttheStatacommandswhichwillcalculate1 thebootstrapstandarderrors listedabove. StataCommands regwageeducationifmbf12==1,vce(bootstrap,reps(10000)) bs(e(rss)/e(N)),reps(10000):regwageeducationifmbf12==1 bs(exp(16*_b[education]+_b[_cons]+e(rss)/e(N)/2)),reps(10000):/// regwageeducationifmbf12==1 1TheywillnotpreciselyreplicatethestandarderrorssincethoseinTable10.2wereproducedinMatlabwhichusesadifferent randomnumbersequence.",
    "page": 286,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 267 10.8 PercentileInterval Thesecondmostcommonuseofbootstrapmethodsisforconfidenceintervals. Therearemultiple bootstrapmethodstoformconfidenceintervals. Apopularandsimplemethodiscalledthepercentile interval.Itisbasedonthequantilesofthebootstrapdistribution. In Section 10.6 we described the bootstrap algorithm which creates an i.i.d. sample of bootstrap estimates (cid:169)Î¸ (cid:98) â ,Î¸ (cid:98) â ,...,Î¸ (cid:98) â(cid:170) corresponding to an estimator Î¸ (cid:98)of a parameter Î¸. We focus on the case of a 1 2 B scalarparameterÎ¸. Forany0<Î±<1wecancalculatetheempiricalquantileqÎ± â ofthesebootstrapestimates.Thisisthe numbersuchthatnÎ±bootstrapestimatesaresmallerthanqÎ± â ,andistypicallycalculatedbytakingthe nÎ±th orderstatisticoftheÎ¸ (cid:98) â . SeeSection11.13ofIntroductiontoEconometricsforaprecisediscussion b ofempiricalquantilesandcommonquantileestimators. Thepercentilebootstrap100(1âÎ±)%confidenceintervalis Cpc=(cid:163) q â ,q â (cid:164) . (10.8) Î±/2 1âÎ±/2 (cid:104) (cid:105) Forexample,ifB=1000,Î±=0.05,andtheempiricalquantileestimatorisused,thenCpc= Î¸ (cid:98) â ,Î¸ (cid:98) â . (25) (975) Toillustrate,the0.025and0.975quantilesofthebootstrapdistributionsofÎ² (cid:98) â 1 andÂµ (cid:98) â areindicated inFigure13.1bythearrows.Theintervalsbetweenthearrowsarethe95%percentileintervals. Thepercentileintervalhastheconveniencethatitdoesnotrequirecalculationofastandarderror. Thisisparticularlyconvenientincontextswhereasymptoticstandarderrorcalculationiscomplicated, burdensome,orunknown. Cpc isasimpleby-productofthebootstrapalgorithmanddoesnotrequire meaningfulcomputationalcostabovethatrequiredtocalculatethebootstrapstandarderror. Thepercentileintervalhastheusefulpropertythatitistransformation-respecting. Takeamono- toneparametertransformationm(Î¸). Thepercentileintervalform(Î¸)issimplythepercentileinterval forÎ¸mappedbym(Î¸).Thatis,if (cid:163) q â ,q â (cid:164) isthepercentileintervalforÎ¸,then (cid:163) m (cid:161) q â (cid:162) ,m (cid:161) q â (cid:162)(cid:164) Î±/2 1âÎ±/2 Î±/2 1âÎ±/2 isthepercentileintervalform(Î¸).Thispropertyfollowsdirectlyfromtheequivariancepropertyofsam- plequantiles.Manyconfidence-intervalmethods,suchasthedelta-methodasymptoticintervalandthe normal-approximationintervalCnb,donotsharethisproperty. Toillustratetheusefulnessofthetransformation-respectingpropertyconsiderthevarianceÏ2. In some cases it is useful to report the variance Ï2 and in other cases it is useful to report the standard deviation Ï. Thus we may be interested in confidence intervals for Ï2 or Ï. To illustrate, the asymp- totic95%normalconfidenceintervalforÏ2 whichwecalculatefromTable13.2is[0.060,0.228]. Taking squarerootsweobtainanintervalforÏof[0.244,0.477]. Alternatively,thedeltamethodstandarderror forÏ=0.379is0.057, leadingtoanasymptotic95%confidenceintervalforÏof[0.265,0.493]whichis (cid:98) different. Thisshowsthatthedeltamethodisnottransformation-respecting",
    "page": 287,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Thus we may be interested in confidence intervals for Ï2 or Ï. To illustrate, the asymp- totic95%normalconfidenceintervalforÏ2 whichwecalculatefromTable13.2is[0.060,0.228]. Taking squarerootsweobtainanintervalforÏof[0.244,0.477]. Alternatively,thedeltamethodstandarderror forÏ=0.379is0.057, leadingtoanasymptotic95%confidenceintervalforÏof[0.265,0.493]whichis (cid:98) different. Thisshowsthatthedeltamethodisnottransformation-respecting. Incontrast,the95%per- centileintervalforÏ2is[0.062,0.220]andthatforÏis[0.249,0.469]whichisidenticaltothesquareroots oftheintervalforÏ2. ThebootstrappercentileintervalsforthefourestimatorsarereportedinTable13.2. InStata,percentileconfidenceintervalscanbeobtainedbyusingthecommandestat bootstrap, percentileorthecommandestat bootstrap, allafteranestimationcommandwhichcalculates standarderrorsviathebootstrap. 10.9 TheBootstrapDistribution Forapplicationsitisoftensufficientifoneunderstandsthebootstrapasanalgorithm. However,for theoryitismoreusefultoviewthebootstrapasaspecificestimatorofthesamplingdistribution.Forthis itisusefultointroducesomeadditionalnotation.",
    "page": 287,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 268 Thekeyisthatthedistributionofanyestimatororstatisticisdeterminedbythedistributionofthe data. Whilethelatterisunknownitcanbeestimatedbytheempiricaldistributionofthedata. Thisis whatthebootstrapdoes. Tofixnotation,letF denotethedistributionofanindividualobservationW.(Inregression,W isthe pair(Y,X).)LetG n (u,F)denotethedistributionofanestimatorÎ¸ (cid:98).Thatis, G n (u,F)=(cid:80)(cid:163)Î¸ (cid:98) â¤u|F (cid:164) . We write the distribution G as a function of n and F since they (generally) affect the distribution of n Î¸ (cid:98). We are interested in the distributionG n . For example, we want to know its variance to calculate a standarderrororitsquantilestocalculateapercentileinterval. In principle, if we knew the distribution F we should be able to determine the distributionG . In n practice there are two barriers to implementation. The first barrier is that the calculation ofG (u,F) n isgenerallyinfeasibleexceptincertainspecialcasessuchasthenormalregressionmodel. Thesecond barrieristhatingeneralwedonotknowF. Thebootstrapsimultaneouslycircumventsthesetwobarriersbytwocleverideas.First,thebootstrap proposesestimationofF bytheempiricaldistributionfunction(EDF)F ,whichisthesimplestnonpara- n metricestimatorofthejointdistributionoftheobservations.TheEDFisF (x)=n â1(cid:80)n 1 {X â¤x}.(See n i=1 i Section11.2ofIntroductiontoEconometricsfordetailsandproperties.) ReplacingF withF weobtain n theidealizedbootstrapestimatorofthedistributionofÎ¸ (cid:98) G â (u)=G (u,F ). (10.9) n n n â ThebootstrapâssecondcleverideaistoestimateG bysimulation. Thisisthebootstrapalgorithmde- n scribedintheprevioussections. TheessentialideaisthatsimulationfromF issamplingwithreplace- n ment from the original data, which is computationally simple. Applying the estimation formula for Î¸ (cid:98) â weobtaini.i.d. drawsfromthedistributionG (u). BymakingalargenumberB ofsuchdrawswecan n â estimateanyfeatureofG ofinterest. Thebootstrapcombinesthesetwoideas:(1)estimateG (u,F)by n n G (u,F );(2)estimateG (u,F )bysimulation. Theseideasareintertwined. Onlybyconsideringthese n n n n stepstogetherdoweobtainafeasiblemethod. â ThewaytothinkabouttheconnectionbetweenG andG isasfollows.G isthedistributionofthe n n n estimatorÎ¸ (cid:98)obtainedwhentheobservationsaresampledi.i.d. fromthepopulationdistributionF. G â n isthedistributionofthesamestatistic,denotedÎ¸ (cid:98) â ,obtainedwhentheobservationsaresampledi.i.d. fromtheempiricaldistributionF .Itisusefultoconceptualizetheâuniverseâwhichseparatelygenerates n thedatasetandthebootstrapsample. TheâsamplinguniverseâisthepopulationdistributionF. Inthis universethetrueparameterisÎ¸.TheâbootstrapuniverseâistheempircaldistributionF .Whendrawing n fromthebootstrapuniversewearetreatingF asifitisthetruedistribution.Thusanythingwhichistrue n aboutF shouldbetreatedastrueinthebootstrapuniverse. Inthebootstrapuniversetheâtrueâvalue n oftheparameterÎ¸isthevaluedeterminedbytheEDFF n . InmostcasesthisistheestimateÎ¸ (cid:98)",
    "page": 288,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". TheâsamplinguniverseâisthepopulationdistributionF. Inthis universethetrueparameterisÎ¸.TheâbootstrapuniverseâistheempircaldistributionF .Whendrawing n fromthebootstrapuniversewearetreatingF asifitisthetruedistribution.Thusanythingwhichistrue n aboutF shouldbetreatedastrueinthebootstrapuniverse. Inthebootstrapuniversetheâtrueâvalue n oftheparameterÎ¸isthevaluedeterminedbytheEDFF n . InmostcasesthisistheestimateÎ¸ (cid:98). Itisthe truevalueofthecoefficientwhenthetruedistributionisF . n Wenowcarefullyexplaintheconnectionwiththebootstrapalgorithmaspreviouslydescribed. First, observe that sampling with replacement from the sample {Y ,...,Y } is identical to sampling 1 n from the EDF F . This is because the EDF is the probability distribution which puts probability mass n 1/noneachobservation. ThussamplingfromF meanssamplinganobservationwithprobability1/n, n whichissamplingwithreplacement. Second, observe that the bootstrap estimator Î¸ (cid:98) â described here is identical to the bootstrap algo- rithm described in Section 10.6. That is, Î¸ (cid:98) â is the random vector generated by applying the estimator formulaÎ¸ (cid:98)tosamplesobtainedbyrandomsamplingfromF n .",
    "page": 288,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 269 Third,observethatthedistributionofthesebootstrapestimatorsisthebootstrapdistribution(10.9). Thisisapreciseequality.Thatis,thebootstrapalgorithmgeneratesi.i.d.samplesfromF ,andwhenthe n estimatorsareappliedweobtainrandomvariablesÎ¸ (cid:98) â withthedistributionG â . n Fourth, observe that the bootstrap statistics described earlier â bootstrap variance, standard error, â andquantilesâareestimatorsofthecorrespondingfeaturesofthebootstrapdistributionG . n â ThisdiscussionismeanttocarefullydescribewhythenotationG (u)isusefultohelpunderstandthe n propertiesofthebootstrapalgorithm. SinceF isthenaturalnonparametricestimatoroftheunknown n distributionF,G â (u)=G (u,F )isthenaturalplug-inestimatoroftheunknownG (u,F).Furthermore, n n n n sinceF isuniformlyconsistentforF bytheGlivenko-CantelliLemma(Theorem18.8inIntroductionto n â Econometrics)wealsocanexpectG (u)tobeconsistentforG (u).Makingthispreciseisabitchalleng- n n ing since F andG are functions. In the next several sections we develop an asymptotic distribution n n theory for the bootstrap distribution based on extending asymptotic theory to the case of conditional distributions. 10.10 TheDistributionoftheBootstrapObservations â â LetY bearandomdrawfromthesample{Y ,...,Y }.WhatisthedistributionofY ? 1 n Sincewearefixingtheobservations,thecorrectquestionis: Whatistheconditionaldistributionof â Y ,conditionalontheobserveddata? TheempiricaldistributionfunctionF summarizestheinforma- n tioninthesample,soequivalentlywearetalkingaboutthedistributionconditionalonF .Consequently n wewillwritethebootstrapprobabilityfunctionandexpectationas (cid:80)â(cid:163) Y ââ¤x (cid:164)=(cid:80)(cid:163) Y ââ¤x|F (cid:164) n (cid:69)â(cid:163) Y â(cid:164)=(cid:69)(cid:163) Y â|F (cid:164) . n Notationally,thestarreddistributionandexpectationareconditionalgiventhedata. â The (conditional) distribution of Y is the empirical distribution function F , which is a discrete n distributionwithmasspoints1/n oneachobservationY . Thuseveniftheoriginaldatacomefroma i continuousdistribution,thebootstrapdatadistributionisdiscrete. â The(conditional)meanandvarianceofY arecalculatedfromtheEDF,andequalthesamplemean andvarianceofthedata.Themeanis (cid:69)â(cid:163) Y â(cid:164)= (cid:88) n Y (cid:80)â(cid:163) Y â=Y (cid:164)= (cid:88) n Y 1 =Y (10.10) i i i n i=1 i=1 andthevarianceis var â(cid:163) Y â(cid:164)=(cid:69)â(cid:163) Y â Y â(cid:48)(cid:164)â(cid:161)(cid:69)â(cid:163) Y â(cid:164)(cid:162)(cid:161)(cid:69)â(cid:163) Y â(cid:164)(cid:162)(cid:48) = (cid:88) n Y Y (cid:48)(cid:80)â(cid:163) Y â=Y (cid:164)âY Y (cid:48) i i i i=1 = (cid:88) n Y Y (cid:48)1 âY Y (cid:48) i i n i=1 =Î£ (cid:98). (10.11) â Tosummarize,theconditionaldistributionofY ,givenF ,isthediscretedistributionon{Y ,...,Y }with n 1 n meanY andcovariancematrixÎ£ (cid:98)",
    "page": 289,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". (10.11) â Tosummarize,theconditionaldistributionofY ,givenF ,isthediscretedistributionon{Y ,...,Y }with n 1 n meanY andcovariancematrixÎ£ (cid:98). Wecanextendthisanalysistoanyintegermomentr.AssumeY isscalar.Therth momentofY â is Âµâ(cid:48)=(cid:69)â(cid:163) Y âr(cid:164)= (cid:88) n Yr(cid:80)â(cid:163) Y â=Y (cid:164)= 1 (cid:88) n Yr =Âµ(cid:48) , r i i n i (cid:98)r i=1 i=1",
    "page": 289,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 270 therth samplemoment.Therth centralmomentofY â is Âµâ=(cid:69)â (cid:104)(cid:179) Y ââY (cid:180)r(cid:105) = 1 (cid:88) n (cid:179) Y âY (cid:180)r =Âµ , r n i (cid:98)r i=1 therth centralsamplemoment.Similarly,therth cumulantofY â isÎºâ=Îº ,therth samplecumulant. r (cid:98)r 10.11 TheDistributionoftheBootstrapSampleMean Thebootstrapsamplemeanis Y â = 1 (cid:88) n Y â . n i i=1 Wecancalculateits(conditional)meanandvariance.Themeanis (cid:34) (cid:35) (cid:69)â (cid:104) Y â(cid:105) =(cid:69)â 1 (cid:88) n Y â = 1 (cid:88) n (cid:69)â(cid:163) Y â(cid:164)= 1 (cid:88) n Y =Y. (10.12) n i n i n i=1 i=1 i=1 â using(10.10). ThusthebootstrapsamplemeanY hasadistributioncenteredatthesamplemeanY. â ThisisbecausethebootstrapobservationsY aredrawnfromthebootstrapuniverse,whichtreatsthe i EDFasthetruth,andthemeanofthelatterdistributionisY. The(conditional)varianceofthebootstrapsamplemeanis (cid:34) (cid:35) var â (cid:104) Y â(cid:105) =var â 1 (cid:88) n Y â = 1 (cid:88) n var â(cid:163) Y â(cid:164)= 1 (cid:88) n Î£ (cid:98) = 1 Î£ (cid:98) (10.13) n i n2 i n2 n i=1 i=1 i=1 using(10.11). Inthescalarcase,var â (cid:104) Y â(cid:105) =Ï2/n. ThisshowsthatthebootstrapvarianceofY â ispre- (cid:98) ciselydescribedbythesamplevarianceoftheoriginalobservations.Again,thisisbecausethebootstrap â observationsY aredrawnfromthebootstrapuniverse. i Wecanextendthistoanyintegermomentr. AssumeY isscalar. Definethenormalizedbootstrap samplemean Z â= (cid:112) n (cid:179) Y â âY (cid:180) . UsingexpressionsfromSection6.17ofIntroductiontoEconometrics, n the3rd through6th conditionalmomentsofZ â are n (cid:69)â(cid:163) Z â3(cid:164)=Îº /n1/2 n (cid:98)3 (cid:69)â(cid:163) Z â4(cid:164)=Îº /n+3Îº2 (10.14) n (cid:98)4 (cid:98)2 (cid:69)â(cid:163) Z â5(cid:164)=Îº /n3/2+10Îº Îº /n1/2 n (cid:98)5 (cid:98)3(cid:98)2 (cid:69)â(cid:163) Z â6(cid:164)=Îº /n2+(cid:161) 15Îº Îº +10Îº2(cid:162) /n+15Îº3 n (cid:98)6 (cid:98)4 2 (cid:98)3 (cid:98)2 where Îº is the rth sample cumulant. Similar expressions can be derived for higher moments. The (cid:98)r moments(10.14)areexact,notapproximations. 10.12 BootstrapAsymptotics â ThebootstrapmeanY isasampleaverageovern i.i.d. randomvariables,sowemightexpectitto convergeinprobabilitytoitsexpectation. Indeed,thisisthecase,butwehavetobeabitcarefulsince thebootstrapmeanhasaconditionaldistribution(giventhedata)soweneedtodefineconvergencein probabilityforconditionaldistributions.",
    "page": 290,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 271 â Definition10.1 WesaythatarandomvectorZ convergesinbootstrapprob- n abilitytoZ asnââ,denotedZ âââZ,ifforall(cid:178)>0 n pâ (cid:80)â(cid:163)(cid:176) (cid:176)Z ââZ (cid:176) (cid:176) >(cid:178)(cid:164)ââ0. n p To understand this definition recall that conventional convergence in probability Z ââ Z means n p that for a sufficiently large sample size n, the probability is high that Z is arbitrarily close to its limit n Z. Incontrast,Definition10.1saysZ âââZ meansthatforasufficientlylargen,theprobabilityishigh n pâ â that the conditional probability that Z is close to its limit Z is high. Note that there are two uses of n probabilityâbothunconditionalandconditional. Our label âconvergence in bootstrap probabilityâ is a bit unusual. The label used in much of the statisticalliteratureisâconvergenceinprobability,inprobabilityâbutthatseemslikeamouthful. That literaturemoreoftenfocusesontherelatedconceptofâconvergenceinprobability,almostsurelyâwhich holdsifwereplacetheââââconvergencewithalmostsureconvergence. Wedonotusethisconceptin p thischapterasitisanunnecessarycomplication. WhilewehavestatedDefinition10.1forthespecificconditionalprobabilitydistribution(cid:80)â ,theidea ismoregeneralandcanbeusedforanyconditionaldistributionandanysequenceofrandomvectors. Thefollowingmayseemobviousbutitisusefultostateforclarity.ItsproofisgiveninSection10.31. Theorem10.1 IfZ ââZ asnââthenZ ââZ. n n p pâ GivenDefinition10.1,wecanestablishalawoflargenumbersforthebootstrapsamplemean. Theorem10.2 Bootstrap WLLN. If Y are independent and uniformly inte- i â â grablethenY âY ââ0andY ââÂµ=(cid:69)[Y]asnââ. pâ pâ Theproof(presentedinSection10.31)issomewhatdifferentfromtheclassicalcaseasitisbasedon theMarcinkiewiczWLLN(Theorem10.20,presentedinSection10.31). NoticethattheconditionsforthebootstrapWLLNarethesamefortheconventionalWLLN.Notice aswellthatwestatetworelatedbutslightlydifferentresults. Thefirstisthatthedifferencebetweenthe â bootstrapsamplemeanY andthesamplemeanY diminishesasthesamplesizediverges.Thesecond resultisthatthebootstrapsamplemeanconvergestothepopulationmeanÂµ.Thelatterisnotsurprising (sincethesamplemeanY convergesinprobabilitytoÂµ)butitisconstructivetobeprecisesinceweare dealingwithanewconvergenceconcept. Theorem10.3 BootstrapContinuousMappingTheorem. If Z âââc asnâ n pâ âandg(Â·)iscontinuousatc,theng(Z â )ââg(c)asnââ. n pâ",
    "page": 291,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 272 TheproofisessentiallyidenticaltothatofTheorem6.6soisomitted. Wenextwouldliketoshowthatthebootstrapsamplemeanisasymptoticallynormallydistributed, butforthatweneedadefinitionofconvergenceforconditionaldistributions. â Definition10.2 LetZ beasequenceofrandomvectorswithconditionaldis- n tributionsG â (x)=(cid:80)â(cid:163) Z ââ¤x (cid:164) .Wesaythat Z â convergesinbootstrapdistri- n n n butionto Z asnââ,denoted Z âââZ,ifforallx atwhichG(x)=(cid:80)[Z â¤x] n dâ iscontinuous,G â (x)ââG(x)asnââ. n p ThedifferencewiththeconventionaldefinitionisthatDefinition10.2treatstheconditionaldistribu- tionasrandom.AnalternativelabelforDefinition10.2isâconvergenceindistribution,inprobabilityâ. WenowstateaCLTforthebootstrapsamplemean,withaproofgiveninSection10.31. Theorem10.4 BootstrapCLT.IfY arei.i.d.,(cid:69)(cid:107)Y(cid:107)2<â,andÎ£=var[Y]>0, (cid:112) (cid:179) â (cid:180) i thenasnââ, n Y âY ââN(0,Î£). dâ Theorem10.4showsthatthenormalizedbootstrapsamplemeanhasthesameasymptoticdistribu- tion as the sample mean. Thus the bootstrap distribution is asymptotically the same as the sampling distribution. Anotabledifference,however,isthatthebootstrapsamplemeanisnormalizedbycenter- ingatthesamplemean,notatthepopulationmean.ThisisbecauseY isthetruemeaninthebootstrap universe. Wenextstatethedistributionalformofthecontinuousmappingtheoremforbootstrapdistributions andtheBootstrapDeltaMethod. Theorem10.5 BootstrapContinuousMappingTheorem If Z âââZ asn ââand g :(cid:82)m â(cid:82)k hasthesetofdiscontinuitypointsD n dâ g suchthat(cid:80)â(cid:163) Z ââD (cid:164)=0,theng(Z â )ââg(Z)asnââ. g n dâ Theorem10.6 BootstrapDeltaMethod: (cid:112) IfÂµââÂµ, n (cid:161)ÂµââÂµ(cid:162)ââÎ¾,andg(u)iscontinuouslydifferentiableinaneigh- (cid:98) (cid:98) (cid:98) p dâ borhoodofÂµ,thenasnââ (cid:112) n (cid:161) g (cid:161)Âµâ(cid:162)âg(Âµ) (cid:162)ââG (cid:48)Î¾ (cid:98) (cid:98) dâ whereG(x)= â g(x) (cid:48) andG=G(Âµ).Inparticular,ifÎ¾â¼N(0,V)thenasnââ âx (cid:112) n (cid:161) g (cid:161)Âµâ(cid:162)âg(Âµ) (cid:162)ââN (cid:161) 0,G (cid:48) VG (cid:162) . (cid:98) (cid:98) dâ",
    "page": 292,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 273 Foraproof,seeExercise10.7. WestateananalogofTheorem6.10,whichpresentedtheasymptoticdistributionforgeneralsmooth functionsofsamplemeans,whichcoversmosteconometricestimators. Theorem10.7 Under the assumptions of Theorem 6.10, that is, if Y is i.i.d., i â Âµ=(cid:69)[h(Y)], Î¸ = g (cid:161)Âµ(cid:162) , (cid:69)(cid:107)h(Y)(cid:107)2 <â, andG(x)= g(x) (cid:48) is continuous in âx aneighborhoodofÂµ, forÎ¸ (cid:98) =g (cid:161)Âµ (cid:98) (cid:162) withÂµ (cid:98) = n 1(cid:80)n i=1 h(Y i )andÎ¸ (cid:98) â=g (cid:161)Âµ (cid:98) â(cid:162) with Âµâ= 1(cid:80)n h (cid:161) Y â(cid:162) ,asnââ (cid:98) n i=1 i (cid:112) n (cid:161)Î¸ (cid:98) ââÎ¸ (cid:98) (cid:162)ââN(0,VÎ¸) dâ whereVÎ¸ =G (cid:48) VG ,V =(cid:69) (cid:104) (cid:161) h(Y)âÂµ(cid:162)(cid:161) h(Y)âÂµ(cid:162)(cid:48)(cid:105) andG=G (cid:161)Âµ(cid:162) . Foraproof,seeExercise10.8. Theorem 10.7 shows that the asymptotic distribution of the bootstrap estimator Î¸ (cid:98) â is identical to thatofthesampleestimatorÎ¸ (cid:98). ThismeansthatwecanlearnthedistributionofÎ¸ (cid:98)fromthebootstrap distribution,andhenceperformasymptoticallycorrectinference. Forsomebootstrapapplicationsweusebootstrapestimatesofvariance.Theplug-inestimatorofVÎ¸ isV(cid:98)Î¸ =G(cid:98) (cid:48) V(cid:98)G(cid:98) whereG(cid:98) =G (cid:161)Âµ (cid:98) (cid:162) and V(cid:98) = 1 (cid:88) n (cid:161) h(Y i )âÂµ (cid:98) (cid:162)(cid:161) h(Y i )âÂµ (cid:98) (cid:162)(cid:48) . n i=1 Thebootstrapversionis â â(cid:48) â â V(cid:98)Î¸ =G(cid:98) V(cid:98) G(cid:98) G(cid:98) â =G (cid:161)Âµ (cid:98) â(cid:162) V(cid:98) â = n 1 (cid:88) n (cid:161) h (cid:161) Y i â(cid:162)âÂµ (cid:98) â(cid:162)(cid:161) h (cid:161) Y i â(cid:162)âÂµ (cid:98) â(cid:162)(cid:48) . i=1 â ApplicationofthebootstrapWLLNandbootstrapCMTshowthatV(cid:98)Î¸ isconsistentforVÎ¸. â Theorem10.8 UndertheassumptionsofTheorem10.7,V(cid:98)Î¸ ââVÎ¸ asnââ. pâ Foraproof,seeExercise10.9. 10.13 ConsistencyoftheBootstrapEstimateofVariance Recall the definition (10.7) of the bootstrap estimator of variance V(cid:98) b Î¸(cid:98) oot of an estimator Î¸ (cid:98). In this sectionweexploreconditionsunderwhichV(cid:98) b Î¸(cid:98) oot isconsistentfortheasymptoticvarianceofÎ¸ (cid:98).",
    "page": 293,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 274 Todosoitisusefultofocusonanormalizedversionoftheestimatorsothattheasymptoticvariance isnotdegenerate.Supposethatforsomesequencea wehave n Z n =a n (cid:161)Î¸ (cid:98) âÎ¸(cid:162)ââÎ¾ (10.15) d and Z n â=a n (cid:161)Î¸ (cid:98) ââÎ¸ (cid:98) (cid:162)â d â â Î¾ (10.16) forsomelimitdistributionÎ¾. Thatis,forsomenormalization,bothÎ¸ (cid:98)andÎ¸ (cid:98) â havethesameasymptotic distribution. This is quite general as it includes the smooth function model. The conventional boot- strapestimatorofthevarianceof Z isthesamplevarianceofthebootstrapdraws (cid:169) Z â (b):b=1,...,B (cid:170) . n n Thisequalstheestimator(10.7)multipliedbya2. Thusitisequivalent(uptoscale)whetherwediscuss n estimatingthevarianceofÎ¸ (cid:98)orZ n . ThebootstrapestimatorofvarianceofZ is n V(cid:98) b Î¸ oot,B= B 1 â1 (cid:88) B (cid:161) Z n â (b)âZ n â(cid:162)(cid:161) Z n â (b)âZ n â(cid:162)(cid:48) b=1 Z â = 1 (cid:88) B Z â (b). n B n b=1 NoticethatweindextheestimatorbythenumberofbootstrapreplicationsB. â SinceZ convergesinbootstrapdistributiontothesameasymptoticdistributionasZ ,itseemsrea- n n sonabletoguessthatthevarianceofZ â willconvergetothatofÎ¾.However,convergenceindistribution n is not sufficient for convergence in moments. For the variance to converge it is also necessary for the â sequenceZ tobeuniformlysquareintegrable. n (cid:176) â(cid:176)2 Theorem10.9 If(10.15)and(10.16)holdforsomesequence a n and(cid:176)Z n (cid:176) is uniformlyintegrable,thenasBââ V(cid:98) b Î¸ oot,Bâ p â â V(cid:98) b Î¸ oot=var (cid:163) Z n â(cid:164) , andasnââ V(cid:98) b Î¸ ootââVÎ¸ =var[Î¾]. pâ Thisraisesthequestion:IsthenormalizedsequenceZ uniformlyintegrable?Wespendtheremain- n derofthissectionexploringthisquestionandturninthenextsectiontotrimmedvarianceestimators whichdonotrequireuniformintegrability. This condition is reasonably straightforward to verify for the case of a scalar sample mean with a finite variance. Thatis, suppose Z â = (cid:112) n (cid:179) Y â âY (cid:180) and (cid:69)(cid:163) Y2(cid:164)<â. In(10.14) we calculatedthe exact n â fourthcentralmomentofZ : n (cid:69)â(cid:163) Z â4(cid:164)= Îº (cid:98)4 +3Ï4= Âµ (cid:98)4 â3Ï (cid:98) 4 +3Ï4 n n (cid:98) n (cid:98)",
    "page": 294,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 275 where Ï2 = n â1(cid:80)n (cid:179) Y âY (cid:180)2 and Âµ = n â1(cid:80)n (cid:179) Y âY (cid:180)4 . The assumption (cid:69)(cid:163) Y2(cid:164) < â implies that (cid:98) i=1 i (cid:98)4 i=1 i (cid:69)(cid:163)Ï2(cid:164) = O(1) so Ï2 = O (1). Furthermore, n â1Âµ = n â2(cid:80)n (cid:179) Y âY (cid:180)4 = o (1) by the Marcinkiewicz (cid:98) (cid:98) p (cid:98)4 i=1 i p WLLN(Theorem10.20).Itfollowsthat (cid:183) (cid:184) (cid:69)â(cid:163) Z â4(cid:164)=n2(cid:69)â (cid:179) Y â âY (cid:180)4 =O (1). (10.17) n p Theorem 6.14 shows that this implies that Z â2 is uniformly integrable. Thus if Y has a finite variance n the normalized bootstrap sample mean is uniformly square integrable and the bootstrap estimate of varianceisconsistentbyTheorem10.9. NowconsiderthesmoothfunctionmodelofTheorem10.7.Wecanestablishthefollowingresult. Theorem10.10 In the smooth function model of Theorem 10.7, if for some (cid:112) p â¥1the pth-orderderivativesof g(x)arebounded, then Z â= n (cid:161)Î¸ (cid:98) ââÎ¸ (cid:98) (cid:162) is n uniformlysquareintegrableandthebootstrapestimatorofvarianceisconsis- tentasinTheorem10.9. ForaproofseeSection10.31. Thisshowsthatthebootstrapestimateofvarianceisconsistentforareasonablybroadclassofesti- mators.Theclassoffunctionsg(x)coveredbythisresultincludesallpth-orderpolynomials. 10.14 TrimmedEstimatorofBootstrapVariance Theorem10.10showedthatthebootstrapestimatorofvarianceisconsistentforsmoothfunctions withaboundedpth orderderivative. Thisisafairlybroadclassbutexcludesmanyimportantapplica- tions. AnexampleisÎ¸=Âµ /Âµ whereÂµ =(cid:69)[Y ]andÂµ =(cid:69)[Y ]. Thisfunctiondoesnothaveabounded 1 2 1 1 2 2 derivative(unlessÂµ isboundedawayfromzero)soisnotcoveredbyTheorem10.10. 2 Thisismorethanatechnicalissue. When(Y ,Y )arejointlynormallydistributedthenitisknown 1 2 thatÎ¸ (cid:98) =Y 1 /Y 2 doesnotpossessafinitevariance.Consequentlywecannotexpectthebootstrapestima- torofvariancetoperformwell.(ItisattemptingtoestimatethevarianceofÎ¸ (cid:98),whichisinfinity.) In these cases it is preferred to use a trimmed estimator of bootstrap variance. Let Ï â â be a n sequenceofpositivetrimmingnumberssatisfyingÏ =O (cid:161) en/8(cid:162) .Definethetrimmedstatistic n Z n ââ=Z n â1(cid:169)(cid:176) (cid:176)Z n â(cid:176) (cid:176) â¤Ï n (cid:170) . Thetrimmedbootstrapestimatorofvarianceis V(cid:98) b Î¸ oot,B,Ï = B 1 â1 (cid:88) B (cid:161) Z n ââ (b)âZ n ââ(cid:162)(cid:161) Z n ââ (b)âZ n ââ(cid:162)(cid:48) b=1 Z ââ= 1 (cid:88) B Z ââ (b). n B n b=1 boot,B WefirstexaminethebehaviorofV(cid:98)Î¸ asthenumberofbootstrapreplicationsB growstoinfinity. Itisasamplevarianceofindependentboundedrandomvectors.ThusbythebootstrapWLLN(Theorem boot,B,Ï ââ 10.2)V(cid:98)Î¸ convergesinbootstrapprobabilitytothevarianceofZ n .",
    "page": 295,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 276 Theorem10.11 AsBââ,V(cid:98) b Î¸ oot,B,Ï â p â â V(cid:98) b Î¸ oot,Ï =var (cid:163) Z n ââ(cid:164) . boot,Ï We next examine the behavior of the bootstrap estimator V(cid:98)Î¸ as n (cid:112) grows to infinity. We focus onthesmoothfunctionmodelofTheorem10.7,whichshowedthat Z n â= n (cid:161)Î¸ (cid:98) ââÎ¸ (cid:98) (cid:162)â d â â Z â¼N(0,VÎ¸). Since the trimming is asymptotically negligible, it follows that Z ââ ââ Z. If we can show that Z ââ is n dâ n uniformly square integrable, Theorem 10.9 will show that var (cid:163) Z n ââ(cid:164)âvar[Z]=VÎ¸ as n ââ. This is showninthefollowingresult,whoseproofispresentedinSection10.31. Theorem10.12 UndertheassumptionsofTheorem10.7,V(cid:98) b Î¸ oot,Ï ââVÎ¸. pâ Theorems10.11and10.12showthatthetrimmedbootstrapestimatorofvarianceisconsistentfor the asymptotic variance in the smooth function model, which includes most econometric estimators. Thisjustifiesbootstrapstandarderrorsasconsistentestimatorsfortheasymptoticdistribution. Animportantcaveatisthattheseresultscriticallyrelyonthetrimmedvarianceestimator. Thisisa criticalcaveatasconventionalstatisticalpackages(e.g. Stata)calculatebootstrapstandarderrorsusing theuntrimmedestimator(10.7). Thusthereisnoguaranteethatthereportedstandarderrorsarecon- sistent. The untrimmed variance estimator works in the context of Theorem 10.10 and whenever the bootstrapstatisticisuniformlysquareintegrable,butnotnecessarilyingeneralapplications. Inpractice,itmaybedifficulttoknowhowtoselectthetrimmingsequenceÏ .TheruleÏ =O (cid:161) en/8(cid:162) n n doesnotprovidepracticalguidance. Instead,itmaybeusefultothinkabouttrimmingintermsofper- centagesofthebootstrapdraws.ThuswecansetÏ sothatagivensmallpercentageÎ³ istrimmed.For n n theoreticalinterpretationwewouldsetÎ³ â0asnââ.InpracticewemightsetÎ³ =1%. n n 10.15 UnreliabilityofUntrimmedBootstrapStandardErrors Intheprevioussectionwepresentedatrimmedbootstrapvarianceestimatorwhichshouldbeused toformbootstrapstandarderrorsfornonlinearestimators. Otherwise,theuntrimmedestimatorispo- tentiallyunreliable. Thisisanunfortunatesituation,becausereportingofbootstrapstandarderrorsiscommonplacein contemporaryappliedeconometricpractice,andstandardapplications(includingStata)usetheuntrimmed estimator. Toillustratetheseriousnessoftheproblemweusethesimplewageregression(7.31)whichwerepeat here. This is the subsample of married Black women with 982 observations. The point estimates and standarderrorsare lo(cid:225)g(wage)= 0.118 education+ 0.016 experienceâ 0.022 experience2/100+ 0.947 . (0.008) (0.006) (0.012) (0.157) WeareinterestedintheexperiencelevelwhichmaximizesexpectedlogwagesÎ¸ =â50Î² /Î² .Thepoint 3 2 3 estimateandstandarderrorscalculatedwithdifferentmethodsarereportedinTable10.3below.",
    "page": 296,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 277 ThepointestimateoftheexperiencelevelwithmaximumearningsisÎ¸ (cid:98)3 =35. Theasymptoticand jackknifestandarderrorsareabout7. Thebootstrapstandarderror, however, is825! Confusedbythis unusualvaluewererunthebootstrapandobtainastandarderrorof544.Eachwascomputedwith10,000 bootstrapreplications. Thefactthatthetwobootstrapstandarderrorsareconsiderablydifferentwhen recomputed(withdifferentstartingseeds)isindicativeofmomentfailure. Whenthereisanenormous discrepancylikethisbetweentheasymptoticandbootstrapstandarderror,andbetweenbootstrapruns, itisasignalthattheremaybemomentfailureandconsequentlybootstrapstandarderrorsareunreliable. AtrimmedbootstrapwithÏ=25(settoslightlyexceedthreeasymptoticstandarderrors)producesa morereasonablestandarderrorof10. Onemessagefromthisapplicationisthatwhendifferentmethodsproduceverydifferentstandard errorsweshouldbecautiousabouttrustinganysinglemethod. Thelargediscrepanciesindicatepoor asymptoticapproximations,renderingallmethodsinaccurate.Anothermessageistobecautiousabout reportingconventionalbootstrapstandarderrors. Trimmedversionsarepreferred, especiallyfornon- linearfunctionsofestimatedcoefficients. Table10.3:ExperienceLevelWhichMaximizesExpectedlogWages Estimate 35.2 Asymptotics.e. (7.0) Jackknifes.e. (7.0) Bootstraps.e.(standard) (825) Bootstraps.e.(repeat) (544) Bootstraps.e.(trimmed) (10.1) 10.16 ConsistencyofthePercentileInterval Recallthepercentileinterval(10.8). Wenowprovideconditionsunderwhichithasasymptotically correctcoverage. Theorem10.13 Assumethatforsomesequencea n a n (cid:161)Î¸ (cid:98) âÎ¸(cid:162)ââÎ¾ (10.18) d and a n (cid:161)Î¸ (cid:98) ââÎ¸ (cid:98) (cid:162)ââÎ¾ (10.19) dâ where Î¾ is continuously distributed and symmetric about zero. Then (cid:80)(cid:163)Î¸âCpc(cid:164)â1âÎ±asnââ. Theassumptions(10.18)-(10.19)holdforthesmoothfunctionmodelofTheorem10.7,sothisresult incorporates many applications. The beauty of Theorem 10.13 is that the simple confidence interval Cpc â which does not require technical calculation of asymptotic standard errors â has asymptotically validcoverageforanyestimatorwhichfallsinthesmoothfunctionclass,aswellasanyotherestimator satisfyingtheconvergenceresults(10.18)-(10.19)withÎ¾symmetricallydistributed. Theconditionsare weaker than those required for consistent bootstrap variance estimation (and normal-approximation",
    "page": 297,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 278 confidenceintervals)becauseitisnotnecessarytoverifythatÎ¸ (cid:98) â isuniformlyintegrable,nornecessary toemploytrimming. TheproofofTheorem10.7isnotdifficult. Theconvergenceassumption(10.19)impliesthattheÎ±th quantileofa n (cid:161)Î¸ (cid:98) ââÎ¸ (cid:98) (cid:162) ,whichisa n (cid:161) qÎ± ââÎ¸ (cid:98) (cid:162) byquantileequivariance,convergesinprobabilitytotheÎ±th quantileofÎ¾,whichwecandenoteasqÎ±.Thus a n (cid:161) qÎ± ââÎ¸ (cid:98) (cid:162)ââqÎ±. (10.20) p Let H(x)=(cid:80)[Î¾â¤x] be the distribution function of Î¾. The assumption of symmetry implies H(âx)= 1âH(x).Thenthepercentileintervalhascoverage (cid:80)(cid:163)Î¸âCpc(cid:164)=(cid:80)(cid:163) q â â¤Î¸â¤q â (cid:164) Î±/2 1âÎ±/2 =(cid:80)(cid:163)âa n (cid:161) q Î± â /2 âÎ¸ (cid:98) (cid:162)â¥a n (cid:161)Î¸ (cid:98) âÎ¸(cid:162)â¥âa n (cid:161) q 1 â âÎ±/2 âÎ¸ (cid:98) (cid:162)(cid:164) â(cid:80)(cid:163)âqÎ±/2 â¥Î¾â¥âq 1âÎ±/2 (cid:164) =H (cid:161)âqÎ±/2 (cid:162)âH (cid:161)âq 1âÎ±/2 (cid:162) =H (cid:161) q 1âÎ±/2 (cid:162)âH (cid:161) qÎ±/2 (cid:162) =1âÎ±. Theconvergenceholdsby(10.18)and(10.20). ThefollowingequalityusesthedefinitionofH,thenext- to-last is the symmetry of H, and the final equality is the definition of qÎ±. This establishes Theorem 10.13. Theorem 10.13 seems quite general, but it critically rests on the assumption that the asymptotic distribution Î¾ is symmetrically distributed about zero. This may seem innocuous since conventional asymptotic distributions are normal and hence symmetric, but it deserves further scrutiny. It is not merelyatechnicalassumptionâanexaminationofthestepsinthepreceedingargumentisolatequite clearlythatifthesymmetryassumptionisviolatedthentheasymptoticcoveragewillnotbe1âÎ±.While Theorem10.13doesshowthatthepercentileintervalisasymptoticallyvalidforaconventionalasymp- toticallynormalestimator,therelianceonsymmetryintheargumentsuggeststhatthepercentilemethod will work poorly when the finite sample distribution is asymmetric. This turns out to be the case and leadsustoconsideralternativemethodsinthefollowingsections. Itisalsoworthwhiletoinvestigateafinitesamplejustificationforthepercentileintervalbasedona heuristicanalogyduetoEfron. Assume that there exists an unknown but strictly increasing transformation Ï(Î¸) such that Ï(Î¸ (cid:98))â Ï(Î¸)hasapivotaldistributionH(u)(doesnotvarywithÎ¸)whichissymmetricaboutzero. Forexample, ifÎ¸ (cid:98) â¼N(Î¸,Ï2)wecansetÏ(Î¸)=Î¸/Ï. Alternatively,ifÎ¸ (cid:98) =exp (cid:161)Âµ (cid:98) (cid:162) andÂµ (cid:98) â¼N(Âµ,Ï2)thenwecansetÏ(Î¸)= log(Î¸)/Ï. Toassessthecoverageofthepercentileinterval,observethatsincethedistributionH ispivotalthe bootstrapdistributionÏ(Î¸ (cid:98) â )âÏ(Î¸ (cid:98))alsohasdistributionH(u). LetqÎ± betheÎ±th quantileofthedistri- butionH.SinceqÎ± â istheÎ±th quantileofthedistributionofÎ¸ (cid:98) â andÏ(Î¸ (cid:98) â )âÏ(Î¸ (cid:98))isamonotonictransfor- mationofÎ¸ (cid:98) â ,bythequantileequivariancepropertywededucethat qÎ± +Ï(Î¸ (cid:98))=Ï(qÎ± â ). Thepercentile",
    "page": 298,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 279 intervalhascoverage (cid:80)(cid:163)Î¸âCpc(cid:164)=(cid:80)(cid:163) q â â¤Î¸â¤q â (cid:164) Î±/2 1âÎ±/2 =(cid:80)(cid:163)Ï(q â )â¤Ï(Î¸)â¤Ï(q â ) (cid:164) Î±/2 1âÎ±/2 =(cid:80)(cid:163)Ï(Î¸ (cid:98))âÏ(q Î± â /2 )â¥Ï(Î¸ (cid:98))âÏ(Î¸)â¥Ï(cid:161)Î¸ (cid:98) (cid:162)âÏ(q 1 â âÎ±/2 ) (cid:164) =(cid:80)(cid:163)âqÎ±/2 â¥Ï(Î¸ (cid:98))âÏ(Î¸)â¥âq 1âÎ±/2 (cid:164) =H (cid:161)âqÎ±/2 (cid:162)âH (cid:161)âq 1âÎ±/2 (cid:162) =H (cid:161) q 1âÎ±/2 (cid:162)âH (cid:161) qÎ±/2 (cid:162) =1âÎ±. The second equality applies the monotonic transformation Ï(u) to all elements. The fourth uses the relationshipqÎ± +Ï(Î¸ (cid:98))=Ï(qÎ± â ). ThefifthusesthedefintionofH. Thesixthusesthesymmetryproperty ofH,andthefinalisbythedefinitionofqÎ±astheÎ±th quantileofH. Thiscalculationshowsthatundertheseassumptionsthepercentileintervalhasexactcoverage1âÎ±. ThenicethingaboutthisargumentistheintroductionoftheunknowntransformationÏ(u)forwhich the percentile interval automatically adapts. The unpleasant feature is the assumption of symmetry. Similartotheasymptoticargumentthecalculationstronglyreliesonthesymmetryofthedistribution H(x).Withoutsymmetrythecoveragewillbeincorrect. Intuitively,weexpectthatwhentheassumptionsareapproximatelytruethenthepercentileinterval willhaveapproximatelycorrectcoverage.ThussolongasthereisatransformationÏ(u)suchthatÏ(Î¸ (cid:98))â Ï(Î¸)isapproximatelypivotalandsymmetricaboutzero,thenthepercentileintervalshouldworkwell. This argument has the following application. Suppose that the parameter of interest is Î¸ =exp(Âµ) where Âµ=(cid:69)[Y] and suppose Y has a pivotal symmetric distribution about Âµ. Then even though Î¸ (cid:98) = exp(Y)doesnothaveasymmetricdistribution,thepercentileintervalappliedtoÎ¸ (cid:98)willhavethecorrect coverage,becausethemonotonictransformationlog (cid:161)Î¸ (cid:98) (cid:162) hasapivotalsymmetricdistribution. 10.17 Bias-CorrectedPercentileInterval The accuracy of the percentile interval depends critically upon the assumption that the sampling distribution is approximately symmetrically distributed. This excludes finite sample bias, for an esti- matorwhichisbiasedcannotbesymmetricallydistributed. Manycontextsinwhichwewanttoapply bootstrapmethods(ratherthanasymptotic)arewhentheparameterofinterestisanonlinearfunction ofthemodelparameters,andnonlinearitytypicallyinducesestimationbias. Consequentlyitisdifficult toexpectthepercentilemethodtogenerallyhaveaccuratecoverage. To reduce the bias problem Efron (1982) introduced the bias-corrected (BC) percentile interval. The justification is heuristic but there is considerable evidence that the bias-corrected method is an importantimprovementonthepercentileinterval. The construction is based on the assumption is that there is a an unknown but strictly increasing transformationÏ(Î¸)andunknownconstantz suchthat 0 Z =Ï(Î¸ (cid:98))âÏ(Î¸)+z 0 â¼N(0,1). (10.21) (The assumption that Z is normal is not critical",
    "page": 299,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". To reduce the bias problem Efron (1982) introduced the bias-corrected (BC) percentile interval. The justification is heuristic but there is considerable evidence that the bias-corrected method is an importantimprovementonthepercentileinterval. The construction is based on the assumption is that there is a an unknown but strictly increasing transformationÏ(Î¸)andunknownconstantz suchthat 0 Z =Ï(Î¸ (cid:98))âÏ(Î¸)+z 0 â¼N(0,1). (10.21) (The assumption that Z is normal is not critical. It could be replaced by any known symmetric and invertibledistribution.) LetÎ¦(x)denotethenormaldistributionfunction,Î¦â1(p)itsquantilefunction, andzÎ± =Î¦â1(Î±)thenormalcriticalvalues. ThentheBCintervalcanbeconstructedfromthebootstrap estimatorsÎ¸ (cid:98) â andbootstrapquantilesqÎ± â asfollows.Set b p â= 1 (cid:88) B 1(cid:169)Î¸ (cid:98) ââ¤Î¸ (cid:98) (cid:170) (10.22) B b b=1",
    "page": 299,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 280 and z â=Î¦â1(p â ). (10.23) 0 p â isameasureofmedianbias,andz 0 isp â transformedintonormalunits. IfthebiasofÎ¸ (cid:98)iszerothen p â=0.5andz â=0. IfÎ¸ (cid:98)isupwardsbiasedthenp â<0.5andz â<0. ConverselyifÎ¸ (cid:98)isdowwardbiased 0 0 thenp â>0.5andz â>0.DefineforanyÎ±anadjustedversion 0 x(Î±)=Î¦(zÎ± +2z 0 ). (10.24) Ifz =0thenx(Î±)=Î±.Ifz >0thenx(Î±)>Î±,andconverselywhenx(Î±)<0.TheBCintervalis 0 0 Cbc=(cid:163) q â ,q â (cid:164) . (10.25) x(Î±/2) x(1âÎ±/2) Essentially,ratherthangoingfromthe2.5%to97.5%quantile,theBCintervalusesadjustedquantiles, withthedegreeofadjustmentdependingontheextentofthebias. TheconstructionoftheBCintervalisnotintuitive. Wenowshowthatassumption(10.21)implies thattheBCintervalhasexactcoverage.(10.21)impliesthat (cid:80)(cid:163)Ï(Î¸ (cid:98))âÏ(Î¸)+z 0 â¤x (cid:164)=Î¦(x). Sincethedistributionispivotaltheresultcarriesovertothebootstrapdistribution (cid:80)â(cid:163)Ï(Î¸ (cid:98) â )âÏ(Î¸ (cid:98))+z 0 â¤x (cid:164)=Î¦(x). (10.26) Evaluating(10.26)atx=z 0 wefind(cid:80)â(cid:163)Ï(Î¸ (cid:98) â )âÏ(Î¸ (cid:98))â¤0 (cid:164)=Î¦(z 0 )whichimplies(cid:80)â(cid:163)Î¸ (cid:98) ââ¤Î¸ (cid:98) (cid:164)=Î¦(z 0 ). In- verting,weobtain z 0 =Î¦â1(cid:161)(cid:80)â(cid:163)Î¸ (cid:98) ââ¤Î¸ (cid:98) (cid:164)(cid:162) (10.27) whichistheprobabilitylimitof(10.23)asB ââ. Thustheunknown z isrecovedby(10.23), andwe 0 cantreatz asifitwereknown. 0 From(10.26)wededucethat x(Î±)=Î¦(zÎ± +2z 0 ) =(cid:80)â(cid:163)Ï(Î¸ (cid:98) â )âÏ(Î¸ (cid:98))â¤zÎ± +z 0 ) (cid:164) =(cid:80)â(cid:163)Î¸ (cid:98) ââ¤Ïâ1(cid:161)Ï(Î¸ (cid:98))+z 0 +zÎ± (cid:162)(cid:164) . This equation shows that Ïâ1(cid:161)Ï(Î¸ (cid:98))+z 0 +zÎ± (cid:162) equals the x(Î±)th bootstrap quantile. That is, q x â (Î±) = Ïâ1(cid:161)Ï(Î¸ (cid:98))+z 0 +zÎ± (cid:162) .Hencewecanwrite(10.25)as Cbc=(cid:163)Ïâ1(cid:161)Ï(Î¸ (cid:98))+z 0 +zÎ±/2 (cid:162) ,Ïâ1(cid:161)Ï(Î¸ (cid:98))+z 0 +z 1âÎ±/2 (cid:162)(cid:164) . Ithascoverageprobability (cid:104) (cid:105) (cid:80) Î¸âCbc =(cid:80)(cid:163)Ïâ1(cid:161)Ï(Î¸ (cid:98))+z 0 +zÎ±/2 (cid:162)â¤Î¸â¤Ïâ1(cid:161)Ï(Î¸ (cid:98))+z 0 +z 1âÎ±/2 (cid:162)(cid:164) =(cid:80)(cid:163)Ï(Î¸ (cid:98))+z 0 +zÎ±/2 â¤Ï(Î¸)â¤Ï(Î¸ (cid:98))+z 0 +z 1âÎ±/2 (cid:164) =(cid:80)(cid:163)âzÎ±/2 â¥Ï(Î¸ (cid:98))âÏ(Î¸)+z 0 â¥âz 1âÎ±/2 (cid:164) =(cid:80)[z 1âÎ±/2 â¥Z â¥zÎ±/2 ] =Î¦(z 1âÎ±/2 )âÎ¦(zÎ±/2 ) =1âÎ±.",
    "page": 300,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 281 ThesecondequalityappliesthetransformationÏ(Î¸).Thefourthequalityusesthemodel(10.21)andthe factzÎ± =âz 1âÎ±.ThisshowsthattheBCinterval(10.25)hasexactcoverageundertheassumption(10.21). Furthermore, under the assumptions of Theorem 10.13, the BC interval has asymptotic coverage probability1âÎ±,sincethebiascorrectionisasymptoticallynegligible. AnimportantpropertyoftheBCpercentileintervalisthatitistransformation-respecting(likethe â percentileinterval). Toseethis,observethatp isinvarianttotransformationssinceitisaprobability, and thus z â and x(Î±) are invariant. Since the interval is constructed from the x(Î±/2) and x(1âÎ±/2) 0 quantiles,thequantileequivariancepropertyshowsthattheintervalistransformation-respecting. The bootstrap BC percentile intervals for the four estimators are reported in Table 13.2. They are generallysimilartothepercentileintervals, thoughtheintervalsforÏ2 andÂµaresomewhatshiftedto theright. InStata,BCpercentileconfidenceintervalscanbeobtainedbyusingthecommandestat bootstrap afteranestimationcommandwhichcalculatesstandarderrorsviathebootstrap. 10.18 BC PercentileInterval a AfurtherimprovementontheBCintervalwasmadebyEfron(1987)toaccountfortheskewnessin the sampling distribution, which can be modeled by specifying that the variance of the estimator de- pendsontheparameter. Theresultingbootstrapacceleratedbias-correctedpercentileinterval(BC ) a hasimprovedperformanceontheBCinterval,butrequiresabitmorecomputationandislessintuitive tounderstand. TheconstructionisageneralizationofthatfortheBCintervals. Theassumptionisthatthereisan unknownbutstrictlyincreasingtransformationÏ(Î¸)andunknownconstantsaandz suchthat 0 Ï(Î¸ (cid:98))âÏ(Î¸) Z = +z â¼N(0,1). (10.28) 1+aÏ(Î¸) 0 (Asbefore,theassumptionthat Z isnormalcouldbereplacedbyanyknownsymmetricandinvertible distribution.) Theconstantz isestimatedby(10.23)justasfortheBCinterval. Thereareseveralpossibleestima- 0 torsofa.EfronâssuggestionisascaledjackknifeestimatoroftheskewnessofÎ¸ (cid:98): (cid:179) (cid:180)3 (cid:80)n Î¸âÎ¸ i=1 (cid:98)(âi) a= (cid:98) (cid:181) (cid:179) (cid:180)2 (cid:182)3/2 6 (cid:80)n i=1 Î¸âÎ¸ (cid:98)(âi) 1 (cid:88) n Î¸= Î¸ (cid:98)(âi) . n i=1 ThejackknifeestimatorofamakestheBC intervalmorecomputationallycostlythanotherintervals. (cid:98) a DefineforanyÎ±theadjustedversion x(Î±)=Î¦ (cid:181) z + zÎ± +z 0 (cid:182) . 0 1âa(zÎ± +z 0 ) TheBC percentileintervalis a Cbca=(cid:163) q â ,q â (cid:164) . x(Î±/2) x(1âÎ±/2) Notethatx(Î±)simplifiesto(10.24)andCbca simpliestoCbc whena=0. WhileCbc improvesonCpc by correctingthemedianbias,Cbcamakesafurthercorrectionforskewness.",
    "page": 301,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 282 The BC a interval is only well-defined for values of Î± such that a(zÎ± +z 0 )<1. (Or equivalently, if Î±<Î¦(cid:161) a â1âz (cid:162) fora>0andÎ±>Î¦(cid:161) a â1âz (cid:162) fora<0.) 0 0 (cid:104) (cid:105) â â TheBC interval,liketheBCandpercentileintervals,istransformation-respecting.Thusif q ,q a x(Î±/2) x(1âÎ±/2) (cid:104) (cid:179) (cid:180) (cid:179) (cid:180)(cid:105) istheBC a intervalforÎ¸,then m q x â (Î±/2) ,m q x â (1âÎ±/2) istheBCÎ± intervalforÏ=m(Î¸)whenm(Î¸)is monotone. WenowgiveajustificationfortheBC interval. Themostdifficultfeaturetounderstandistheesti- a matora fora. Thisinvolveshigher-orderapproximationswhicharetooadvancedforourtreatment,so (cid:98) weinsteadreferreaderstoChapter4.1.4ofShaoandTu(1995)andsimplyassumethataisknown. Wenowshowthatassumption(10.28)withaknownimpliesthatCbcahasexactcoverage.Theargu- mentisessentiallythesameasthatgivenintheprevioussection. Assumption(10.28)impliesthatthe bootstrapdistributionsatisfies (cid:80)â (cid:34) Ï(Î¸ (cid:98) â )âÏ(Î¸ (cid:98)) +z â¤x (cid:35) =Î¦(x). (10.29) 0 1+aÏ(Î¸ (cid:98)) Evaluatingat x =z andinvertingweobtain(10.27)whichisthesameasfortheBCinterval. Thusthe 0 estimator(10.23)isconsistentasBââandwecantreatz asifitwereknown. 0 From(10.29)wededucethat x(Î±)=(cid:80)â (cid:34) Ï(Î¸ (cid:98) â )âÏ(Î¸ (cid:98)) â¤ zÎ± +z 0 (cid:35) 1+aÏ(Î¸ (cid:98)) 1âa(zÎ± +z 0 ) (cid:34) (cid:195) (cid:33)(cid:35) =(cid:80)â Î¸ (cid:98) ââ¤Ïâ1 Ï(Î¸ (cid:98))+zÎ± +z 0 . 1âa(zÎ± +z 0 ) ThisshowsthatÏâ1 (cid:179)Ï(Î¸(cid:98))+zÎ± +z0 (cid:180) equalsthex(Î±)th bootstrapquantile.HencewecanwriteCbcaas 1âa(zÎ± +z0) (cid:34) (cid:195) (cid:33) (cid:195) (cid:33)(cid:35) Cbca= Ïâ1 Ï(Î¸ (cid:98))+zÎ±/2 +z 0 , Ïâ1 Ï(Î¸ (cid:98))+z 1âÎ±/2 +z 0 . 1âa(zÎ±/2 +z 0 ) 1âa(z 1âÎ±/2 +z 0 ) Ithascoverageprobability (cid:34) (cid:195) (cid:33) (cid:195) (cid:33)(cid:35) (cid:80) (cid:104) Î¸âCbca (cid:105) =(cid:80) Ïâ1 Ï(Î¸ (cid:98))+zÎ±/2 +z 0 â¤Î¸â¤Ïâ1 Ï(Î¸ (cid:98))+z 1âÎ±/2 +z 0 1âa(zÎ±/2 +z 0 ) 1âa(z 1âÎ±/2 +z 0 ) (cid:34) (cid:35) =(cid:80) Ï(Î¸ (cid:98))+zÎ±/2 +z 0 â¤Ï(Î¸)â¤ Ï(Î¸ (cid:98))+z 1âÎ±/2 +z 0 1âa(zÎ±/2 +z 0 ) 1âa(z 1âÎ±/2 +z 0 ) (cid:34) (cid:35) Ï(Î¸ (cid:98))âÏ(Î¸) =(cid:80) âzÎ±/2 â¥ 1+aÏ(Î¸) +z 0 â¥âz 1âÎ±/2 =(cid:80)[z 1âÎ±/2 â¥Z â¥zÎ±/2 ] =1âÎ±. ThesecondequalityappliesthetransformationÏ(Î¸).Thefourthequalityusesthemodel(10.28)andthe factzÎ± =âz 1âÎ±. ThisshowsthattheBC a intervalCbcahasexactcoverageundertheassumption(10.28) withaknown. ThebootstrapBC percentileintervalsforthefourestimatorsarereportedinTable13.2. Theyare a generallysimilartotheBCintervals,thoughtheintervalsforÏ2andÂµareslightlyshiftedtotheright. InStata,BC intervalscanbeobtainedbyusingthecommandestat bootstrap, bcaorthecom- a mandestat bootstrap, allafteranestimationcommandwhichcalculatesstandarderrorsviathe bootstrapusingthebcaoption.",
    "page": 302,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 283 10.19 Percentile-tInterval Inmanycaseswecanobtainimprovementinaccuracybybootstrappingastudentizedstatisticsuch asat-ratio.LetÎ¸ (cid:98)beanestimatorofascalarparameterÎ¸ands(Î¸ (cid:98))astandarderror.Thesamplet-ratiois Î¸âÎ¸ (cid:98) T = . s(Î¸ (cid:98)) Thebootstrapt-ratiois Î¸ââÎ¸ T â= (cid:98) (cid:98) s(Î¸ (cid:98) â ) wheres(Î¸ (cid:98) â )isthestandarderrorcalculatedonthebootstrapsample.Noticethatthebootstrapt-ratiois centeredattheparameterestimatorÎ¸ (cid:98).ThisisbecauseÎ¸ (cid:98)istheâtruevalueâinthebootstrapuniverse. â Thepercentile-tintervalisformedusingthedistributionofT . Thiscanbecalculatedviatheboot- strapalgorithm. OneachbootstrapsampletheestimatorÎ¸ (cid:98) â anditsstandarderrors(Î¸ (cid:98) â )arecalculated, andthet-ratioT â=(cid:161)Î¸ (cid:98) ââÎ¸ (cid:98) (cid:162) /s(Î¸ (cid:98) â )calculatedandstored. ThisisrepeatedB times. TheÎ±th quantileqÎ± â isestimatedbytheÎ±th empiricalquantile(oranyquantileestimator)fromtheB bootstrapdrawsofT â . Thebootstrappercentile-tconfidenceintervalisdefinedas Cpt=(cid:163)Î¸ (cid:98) âs(Î¸ (cid:98))q 1 â âÎ±/2 ,Î¸ (cid:98) âs(Î¸ (cid:98))q Î± â /2 (cid:164) . Theformmayappearunusualwhencomparedwiththepercentileinterval. Theleftendpointisdeter- â minedbytheupperquantileofthedistributionofT ,andtherightendpointisdeterminedbythelower quantile.Asweshowbelow,thisconstructionisimportantfortheintervaltohavecorrectcoveragewhen thedistributionisnotsymmetric. Whentheestimatorisasymptoticallynormalandthestandarderrorareliableestimatorofthestan- darddeviationofthedistributionwewouldexpectthet-ratioT toberoughlyapproximatedbythenor- maldistribution. Inthiscasewewouldexpectq â ââq â â2. Departuresfromthisbaselineoccur 0.975 0.025 as the distribution becomes skewed or fat-tailed. If the bootstrap quantiles depart substantially from thisbaselineitisevidenceofsubstantialdeparturefromnormality.(Itmayalsoindicateaprogramming error,sointhesecasesitiswisetotriple-check!) Thepercentile-thasthefollowingadvantages. First,whenthestandarderrors(Î¸ (cid:98))isreasonablyreli- able,thepercentile-tbootstrapmakesuseoftheinformationinthestandarderror,therebyreducingthe roleofthebootstrap.Thiscanimprovetheprecisionofthemethodrelativetoothermethods.Second,as weshowlater,thepercentile-tintervalsachievehigher-orderaccuracythanthepercentileandBCper- centileintervals.Third,thepercentile-tintervalscorrespondtothesetofparametervaluesânotrejectedâ byone-sidedt-testsusingbootstrapcriticalvalues(bootstraptestsarepresentedinSection10.21). Thepercentile-tintervalhasthefollowingdisadvantages. First, theymaybeinfeasiblewhenstan- darderrorformulaareunknown",
    "page": 303,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Thepercentile-tintervalhasthefollowingdisadvantages. First, theymaybeinfeasiblewhenstan- darderrorformulaareunknown. Second,theymaybepracticallyinfeasiblewhenstandarderrorcalcu- lationsarecomputationallycostly(sincethestandarderrorcalculationneedstobeperformedoneach bootstrapsample).Third,thepercentile-tmaybeunreliableifthestandarderrorss(Î¸ (cid:98))areunreliableand thusaddmorenoisethanclarity.Fourth,thepercentile-tintervalisnottranslationpreserving,unlikethe percentile,BCpercentile,andBC percentileintervals. a Itistypicaltocalculatepercentile-tintervalswitht-ratiosconstructedwithconventionalasymptotic standarderrors. Butthisisnottheonlypossibleimplementation. Thepercentile-tintervalcanbecon- structedwithanydata-dependentmeasureofscale.Forexample,ifÎ¸ (cid:98)isatwo-stepestimatorforwhichit isunclearhowtoconstructacorrectasymptoticstandarderror,butweknowhowtocalculateastandard errors(Î¸ (cid:98))appropriateforthesecondstepalone,thens(Î¸ (cid:98))canbeusedforapercentile-t-typeintervalas",
    "page": 303,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 284 describedabove. Itwillnotpossessthehigher-orderaccuracypropertiesofthefollowingsection,butit willsatisfytheconditionsforfirst-ordervalidity. Furthermore,percentile-tintervalscanbeconstructedusingbootstrapstandarderrors. Thatis,the statisticsT andT â canbecomputedusingbootstrapstandarderrorssboot.Thisiscomputationallycostly Î¸(cid:98) as it requires what we call a ânested bootstrapâ. Specifically, for each bootstrap replication, a random sample is drawn, the bootstrap estimate Î¸ (cid:98) â computed, and then B additional bootstrap sub-samples drawn from the bootstrap sample to compute the bootstrap standard error for the bootstrap estimate Î¸ (cid:98) â . EffectivelyB2 bootstrapsamplesaredrawnandestimated, whichincreasesthecomputationalre- quirementbyanorderofmagnitude. Wenowdescribethedistributiontheoryforfirst-ordervalidityofthepercentile-tbootstrap. First, consider the smooth function model, where Î¸ (cid:98) = g (cid:161)Âµ (cid:98) (cid:162) and s(Î¸ (cid:98)) = (cid:113) n 1G(cid:98) (cid:48) V(cid:98)G(cid:98) with bootstrap analogsÎ¸ (cid:98) â=g (cid:161)Âµ (cid:98) â(cid:162) ands(Î¸ (cid:98) â )= (cid:113) n 1G(cid:98) â(cid:48) V(cid:98) â G(cid:98) â .FromTheorems6.10,10.7,and10.8 (cid:112) n (cid:161)Î¸ (cid:98) âÎ¸(cid:162) T = ââZ (cid:112) (cid:48) G(cid:98) V(cid:98)G(cid:98) d and (cid:112) T â= n (cid:161)Î¸ (cid:98) ââÎ¸ (cid:98) (cid:162) ââZ (cid:112) G(cid:98) â(cid:48) V(cid:98) â G(cid:98) â dâ whereZ â¼N(0,1).Thisshowsthatthesampleandbootstrapt-ratioshavethesameasymptoticdistribu- tion. Thismotivatesconsideringthebroadersituationwherethesampleandbootstrapt-ratioshavethe sameasymptoticdistributionbutnotnecessarilynormal.Thusassumethat T ââÎ¾ (10.30) d T âââÎ¾ (10.31) dâ forsomecontinuousdistributionÎ¾. (10.31)impliesthatthequantilesofT â convergeinprobabilityto thoseofÎ¾,thatisqÎ± âââqÎ±whereqÎ±istheÎ±th quantileofÎ¾.Thisand(10.30)imply p (cid:80)(cid:163)Î¸âCpt(cid:164)=(cid:80)(cid:163)Î¸ (cid:98) âs(Î¸ (cid:98))q 1 â âÎ±/2 â¤Î¸â¤Î¸ (cid:98) âs(Î¸ (cid:98))q Î± â /2 (cid:164) =(cid:80)(cid:163) q â â¤T â¤q â (cid:164) Î±/2 1âÎ±/2 â(cid:80)(cid:163) qÎ±/2 â¤Î¾â¤q 1âÎ±/2 (cid:164) =1âÎ±. Thusthepercentile-tisasymptoticallyvalid. Theorem10.14 If(10.30)and(10.31)holdwhereÎ¾iscontinuouslydistributed, then(cid:80)(cid:163)Î¸âCpt(cid:164)â1âÎ±asnââ. Thebootstrappercentile-tintervalsforthefourestimatorsarereportedinTable13.2.Theyaresimi- larbutsomewhatdifferentfromthepercentile-typeintervals,andgenerallywider.Thelargestdifference ariseswiththeintervalforÏ2whichisnoticablywiderthantheotherintervals.",
    "page": 304,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 285 10.20 Percentile-tAsymptoticRefinement This section uses the theory of Edgeworth and Cornish-Fisher expansions introduced in Chapter 9.8-9.10 of Introduction to Econometrics. This theory will not be familiar to most students. If you are interestedthefollowingrefinementtheoryitisadvisabletostartbyreadingthesesectionsofIntroduction toEconometrics. The percentile-t interval can be viewed as the intersection of two one-sided confidence intervals. InourdiscussionofEdgeworthexpansionsforthecoverageprobabilityofone-sidedasymptoticconfi- denceintervals(followingTheorem7.15inthecontextoffunctionsofregressioncoefficients)wefound thatone-sidedasymptoticconfidenceintervalshaveaccuracytoorderO (cid:161) n â1/2(cid:162) . Wenowshowthatthe percentile-tintervalhasimprovedaccuracy. Theorem 9.13 of Introduction to Econometrics showed that the Cornish-Fisher expansion for the quantileqÎ±ofat-ratioT inthesmoothfunctionmodeltakestheform qÎ± =zÎ± +n â1/2p 11 (zÎ±)+O (cid:161) n â1(cid:162) wherep (x)isanevenpolynomialoforder2withcoefficientsdependingonthemomentsuptoorder 11 â 8.ThebootstrapquantileqÎ±hasasimilarCornish-Fisherexpansion qÎ± â=zÎ± +n â1/2p 1 â 1 (zÎ±)+O p (cid:161) n â1(cid:162) â wherep (x)isthesameasp (x)exceptthatthepopulationmomentsarereplacedbythecorrespond- 11 11 ingsamplemoments. Samplemomentsareestimatedattheraten â1/2. Thuswecanreplace p â with 11 p withoutaffectingtheorderofthisexpansion: 11 qÎ± â=zÎ± +n â1/2p 11 (zÎ±)+O p (cid:161) n â1(cid:162)=qÎ± +O p (cid:161) n â1(cid:162) . This shows that the bootstrap quantiles qÎ± â of the studentized t-ratio are within O p (cid:161) n â1(cid:162) of the exact quantilesqÎ±. BytheEdgeworthexpansionDeltamethod(Theorem9.12ofIntroductiontoEconometrics), T and T+(qÎ± âqÎ± â )=T+O p (cid:161) n â1(cid:162) havethesameEdgeworthexpansiontoorderO(n â1).Thus (cid:80)(cid:163) T â¤qÎ± â(cid:164)=(cid:80)(cid:163) T+(qÎ± âqÎ± â )â¤qÎ± (cid:164) =(cid:80)(cid:163) T â¤qÎ± (cid:164)+O(n â1) =Î±+O(n â1). Thusthecoverageofthepercentile-tintervalis (cid:80)(cid:163)Î¸âCpt(cid:164)=(cid:80)(cid:163) q â â¤T â¤q â (cid:164) Î±/2 1âÎ±/2 =(cid:80)(cid:163) qÎ±/2 â¤T â¤q 1âÎ±/2 (cid:164)+O(n â1) =1âÎ±+O(n â1). Thisisanimprovedrateofconvergencerelativetotheone-sidedasymptoticconfidenceinterval. Theorem10.15 Under the assumptions of Theorem 9.11 of Introduction to Econometrics,(cid:80)(cid:163)Î¸âCpt(cid:164)=1âÎ±+O(n â1).",
    "page": 305,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 286 Thefollowingdefinitionoftheaccuracyofaconfidenceintervalisuseful. Definition10.3 AconfidencesetC forÎ¸iskth-orderaccurateif (cid:179) (cid:180) (cid:80)[Î¸âC]=1âÎ±+O n âk/2 . Examiningourresultswefindthatone-sidedasymptoticconfidenceintervalsarefirst-orderaccu- ratebutpercentile-tintervalsaresecond-orderaccurate. Whenabootstrapconfidenceinterval(ortest) achieveshigh-orderaccuracythantheanalogousasymptoticinterval(ortest),wesaythatthebootstrap methodachievesanasymptoticrefinement.Here,wehaveshownthatthepercentile-tintervalachieves anasymptoticrefinement. In order to achieve this asymptotic refinement it is important that the t-ratio T (and its bootstrap â counter-part T ) are constructed with asymptotically valid standard errors. This is because the first termintheEdgeworthexpansionisthestandardnormaldistributionandthisrequiresthatthet-ratio isasymptoticallynormal. Thisalsohasthepracticalfinite-sampleimplicationthattheaccuracyofthe percentile-t interval in practice depends on the accuracy of the standard errors used to construct the t-ratio. We do not go through the details, but normal-approximation bootstrap intervals, percentile boot- strapintervals,andbias-correctedpercentilebootstrapintervalsareallfirst-orderaccurateanddonot achieveanasymptoticrefinement. TheBC interval,however,canbeshowntobeasymptoticallyequivalenttothepercentile-tinterval, a andthusachievesanasymptoticrefinement.Wedonotmakethisdemonstrationhereasitisadvanced. SeeSection3.10.4ofHall(1992). PeterHall Peter Gavin Hall (1951-2016) of Australia was one of the most influential and prolifictheoreticalstatisticiansinhistory.Hemadewide-rangingcontributions. Some of the most relevant for econometrics are theoretical investigations of bootstrapmethodsandnonparametrickernelmethods. 10.21 BootstrapHypothesisTests Totestthehypothesis(cid:72) :Î¸=Î¸ against(cid:72) :Î¸(cid:54)=Î¸ themostcommonapproachisat-test.Wereject 0 0 1 0 (cid:72) 0 infavorof(cid:72) 1 forlargeabsolutevaluesofthet-statisticT =(cid:161)Î¸ (cid:98) âÎ¸ 0 (cid:162) /s(Î¸ (cid:98))whereÎ¸ (cid:98)isanestimatorof Î¸ands(Î¸ (cid:98))isastandarderrorforÎ¸ (cid:98). Forabootstraptestweusethebootstrapalgorithmtocalculatethe criticalvalue. The bootstrap algorithm samples with replacement from the dataset. Given a bootstrap sample the bootstrap estimator Î¸ (cid:98) â and standard error s(Î¸ (cid:98) â ) are calculated. Given these values the bootstrap t-statisticisT â=(cid:161)Î¸ (cid:98) ââÎ¸ (cid:98) (cid:162) /s(Î¸ (cid:98) â ). Therearetwoimportantfeaturesaboutthebootstrapt-statistic. First, T â iscenteredatthesampleestimateÎ¸ (cid:98),notatthehypothesizedvalueÎ¸ 0 . ThisisdonebecauseÎ¸ (cid:98)isthe",
    "page": 306,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 287 truevalueinthebootstrapuniverse,andthedistributionofthet-statisticmustbecenteredatthetrue â valuewithinthebootstrapsamplingframework. Second,T iscalculatedusingthebootstrapstandard errors(Î¸ (cid:98) â ).Thisallowsthebootstraptoincorporatetherandomnessinstandarderrorestimation. ThefailuretoproperlycenterthebootstrapstatisticatÎ¸ (cid:98)isacommonerrorinapplications. Often thisisbecausethehypothesistobetestedis(cid:72) 0 :Î¸=0,sotheteststatisticisT =Î¸ (cid:98)/s(Î¸ (cid:98)). Thisintuitively suggeststhebootstrapstatisticT â=Î¸ (cid:98) â /s(Î¸ (cid:98) â ),butthisiswrong. ThecorrectbootstrapstatisticisT â= (cid:161)Î¸ (cid:98) ââÎ¸ (cid:98) (cid:162) /s(Î¸ (cid:98) â ). The bootstrap algorithm creates B draws T â (b) = (cid:161)Î¸ (cid:98) â (b)âÎ¸ (cid:98) (cid:162) /s(Î¸ (cid:98) â (b)), b = 1,...,B. The bootstrap 100Î±%criticalvalueisq 1 â âÎ± ,whereqÎ± â istheÎ±th quantileoftheabsolutevaluesofthebootstrapt-ratios |T â (b)|. For a 100Î±% test we reject (cid:72) :Î¸ =Î¸ in favor of (cid:72) :Î¸ (cid:54)=Î¸ if |T|> q â and fail to reject if 0 0 1 0 1âÎ± |T|â¤q â . 1âÎ± It is generally better to report p-values rather than critical values. Recall that a p-value is p =1â G (|T|) where G (u) is the null distribution of the statistic |T|. The bootstrap p-value is defined as n n p â=1âG â (|T|),whereG â (u)isthebootstrapdistributionof|T â|. Thisisestimatedfromthebootstrap n n algorithmas p â= 1 (cid:88) B 1(cid:169)(cid:175) (cid:175)T â (b) (cid:175) (cid:175) >|T|(cid:170) , B b=1 thepercentageofbootstrapt-statisticsthatarelargerthantheobservedt-statistic.Intuitively,wewantto knowhowâunusualâistheobservedstatisticT whenthenullhypothesisistrue.Thebootstrapalgorithm â generatesalargenumberofindependentdrawsfromthedistributionT (whichisanapproximationto theunknowndistributionofT). Ifthepercentageofthe|T â|thatexceed|T|isverysmall(say1%)this tellsusthat|T|isanunusuallylargevalue.However,ifthepercentageislarger,say15%,thenwecannot interpret|T|asunusuallylarge. If desired, the bootstrap test can be implemented as a one-sided test. In this case the statistic is thesignedversionofthet-ratio, andbootstrapcriticalvaluesarecalculatedfromtheuppertailofthe distributionforthealternative(cid:72) :Î¸>Î¸ ,andfromthelowertailforthealternative(cid:72) :Î¸<Î¸ . Thereis 1 0 1 0 aconnectionbetweentheone-sidedtestsandthepercentile-tconfidenceinterval. Thelatteristheset ofparametervaluesÎ¸whicharenotrejectedbyeitherone-sided100Î±/2%bootstrapt-test. Bootstraptestscanalsobeconductedwithotherstatistics.Whenstandarderrorsarenotavailableor arenotreliablewecanusethenon-studentizedstatisticT =Î¸ (cid:98) âÎ¸ 0 .ThebootstrapversionisT â=Î¸ (cid:98) ââÎ¸ (cid:98)",
    "page": 307,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Thereis 1 0 1 0 aconnectionbetweentheone-sidedtestsandthepercentile-tconfidenceinterval. Thelatteristheset ofparametervaluesÎ¸whicharenotrejectedbyeitherone-sided100Î±/2%bootstrapt-test. Bootstraptestscanalsobeconductedwithotherstatistics.Whenstandarderrorsarenotavailableor arenotreliablewecanusethenon-studentizedstatisticT =Î¸ (cid:98) âÎ¸ 0 .ThebootstrapversionisT â=Î¸ (cid:98) ââÎ¸ (cid:98). LetqÎ± â betheÎ±thquantileofthebootstrapstatistics (cid:175) (cid:175) Î¸ (cid:98) â (b)âÎ¸ (cid:98) (cid:175) (cid:175).Abootstrap100Î±%testrejects(cid:72) 0 :Î¸=Î¸ 0 if (cid:175) (cid:175) Î¸ (cid:98) âÎ¸ 0 (cid:175) (cid:175) >q 1 â âÎ± .Thebootstrapp-valueis p â= 1 (cid:88) B 1(cid:169)(cid:175) (cid:175) Î¸ (cid:98) â (b)âÎ¸ (cid:98) (cid:175) (cid:175) >(cid:175) (cid:175) Î¸ (cid:98) âÎ¸ 0 (cid:175) (cid:175) (cid:170) . B b=1 Theorem10.16 If(10.30)and(10.31)holdwhereÎ¾iscontinuouslydistributed, thenthebootstrapcriticalvaluesatisfiesq 1 â âÎ± â p âq 1âÎ±whereq 1âÎ±isthe1âÎ±th quantileof|Î¾|. ThebootstraptestâReject(cid:72) infavorof(cid:72) if|T|>q â âhas 0 1 1âÎ± asymptoticsizeÎ±:(cid:80)(cid:163)|T|>q â |(cid:72) (cid:164)ââÎ±asnââ. 1âÎ± 0 Inthesmoothfunctionmodelthet-test(withcorrectstandarderrors)hasthefollowingperformance.",
    "page": 307,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 288 Theorem10.17 Under the assumptions of Theorem 9.11 of Introduction to Econometrics, q 1 â âÎ± =z 1âÎ± +o p (cid:161) n â1(cid:162) wherezÎ± =Î¦â1((1+Î±)/2)istheÎ±thquantileof|Z|.TheasymptotictestâReject (cid:72) 0 infavorof(cid:72) 1 if|T|>z 1âÎ±âhasaccuracy (cid:80)(cid:163)|T|>z 1âÎ± |(cid:72) 0 (cid:164)=1âÎ±+O (cid:161) n â1(cid:162) andthebootstraptestâReject(cid:72) infavorof(cid:72) if|T|>q â âhasaccuracy 0 1 1âÎ± (cid:80)(cid:163)|T|>q â |(cid:72) (cid:164)=1âÎ±+o (cid:161) n â1(cid:162) . 1âÎ± 0 Thisshowsthatthebootstraptestachievesarefinementrelativetotheasymptotictest. The reasoning is as follows. We have shown that the Edgeworth expansion for the absolute t-ratio takestheform (cid:80)[|T|â¤x]=2Î¦(x)â1+n â12p (x)+o(n â1). 2 ThismeanstheasymptotictesthasaccuracyoforderO(n â1). GiventheEdgeworthexpansion,theCornish-FisherexpansionfortheÎ±th quantileqÎ± ofthedistri- butionof|T|takestheform qÎ± =zÎ± +n â1p 21 (zÎ±)+o (cid:161) n â1(cid:162) . â ThebootstrapquantileqÎ±hastheCornish-Fisherexpansion qÎ± â=zÎ± +n â1p 2 â 1 (zÎ±)+o (cid:161) n â1(cid:162) =zÎ± +n â1p 21 (zÎ±)+o p (cid:161) n â1(cid:162) =qÎ± +o p (cid:161) n â1(cid:162) â wherep (x)isthesameasp (x)exceptthatthepopulationmomentsarereplacedbythecorrespond- 21 21 ingsamplemoments.Thebootstraptesthasrejectionprobability,usingtheEdgeworthexpansionDelta method(Theorem11.12ofofIntroductiontoEconometrics) (cid:80)(cid:163)|T|>q 1 â âÎ± |(cid:72) 0 (cid:164)=(cid:80)(cid:163)|T|+(q 1âÎ± âq 1 â âÎ± )>q 1âÎ± (cid:164) =(cid:80)(cid:163)|T|>q 1âÎ± (cid:164)+o(n â1) =1âÎ±+o(n â1) asclaimed. 10.22 Wald-TypeBootstrapTests IfÎ¸isavectorthentotest(cid:72) :Î¸=Î¸ against(cid:72) :Î¸(cid:54)=Î¸ atsizeÎ±,acommontestisbasedontheWald 0 0 1 0 statisticW =(cid:161)Î¸ (cid:98) âÎ¸ 0 (cid:162)(cid:48) V(cid:98) â Î¸(cid:98) 1(cid:161)Î¸ (cid:98) âÎ¸ 0 (cid:162) whereÎ¸ (cid:98)isanestimatorofÎ¸ andV(cid:98)Î¸(cid:98) isacovariancematrixestimator. Forabootstraptestweusethebootstrapalgorithmtocalculatethecriticalvalue. Thebootstrapalgorithmsampleswithreplacementfromthedataset. Givenabootstrapsamplethe bootstrapestimatorÎ¸ (cid:98) â andcovariancematrixestimatorV(cid:98) â Î¸(cid:98) arecalculated.Giventhesevaluestheboot- strapWaldstatisticis W â=(cid:161)Î¸ (cid:98) ââÎ¸ (cid:98) (cid:162)(cid:48) V(cid:98) â Î¸(cid:98) â1(cid:161)Î¸ (cid:98) ââÎ¸ (cid:98) (cid:162) .",
    "page": 308,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 289 Asforthet-testitisessentialthatthebootstrapWaldstatisticW â iscenteredatthesampleestimatorÎ¸ (cid:98) insteadofthehypothesizedvalueÎ¸ 0 .ThisisbecauseÎ¸ (cid:98)isthetruevalueinthebootstrapuniverse. BasedonB bootstrapreplicationswecalculatetheÎ±th quantile qÎ± â ofthedistributionoftheboot- strapWaldstatisticsW â . Thebootstraptestrejects(cid:72) infavorof(cid:72) ifW >q â . Morecommonly,we 0 1 1âÎ± calculateabootstrapp-value.Thisis p â= 1 (cid:88) B 1(cid:169) W â (b)>W (cid:170) . B b=1 The asymptotic performance of the Wald test mimics that of the t-test. In general, the bootstrap Waldtestisfirst-ordercorrect(achievesthecorrectsizeasymptotically)andunderconditionsforwhich anEdgeworthexpansionexists,hasaccuracy (cid:80)(cid:163) W >q â |(cid:72) (cid:164)=1âÎ±+o(n â1) 1âÎ± 0 andthusachievesarefinementrelativetotheasymptoticWaldtest. If a reliable covariance matrix estimator V(cid:98)Î¸(cid:98) is not available a Wald-type test can be implemented withanypositive-definiteweightmatrixinsteadofV(cid:98)Î¸(cid:98) .Thisincludessimplechoicessuchastheidentity matrix. The bootstrap algorithm can be used to calculate critical values and p-values for the test. So long as the estimator Î¸ (cid:98)has an asymptotic distribution this bootstrap test will be asymptotically first- order valid. The test will not achieve an asymptotic refinement but provides a simple method to test hypotheseswhencovariancematrixestimatesarenotavailable. 10.23 Criterion-BasedBootstrapTests Acriterion-basedestimatortakestheform Î² (cid:98) =argmin J (cid:161)Î²(cid:162) Î² forsomecriterionfunction J (cid:161)Î²(cid:162) . Thisincludesleastsquares,maximumlikelihood,andminimumdis- tance.Givenahypothesis(cid:72) :Î¸=Î¸ whereÎ¸=r (cid:161)Î²(cid:162) ,therestrictedestimatorwhichsatisfies(cid:72) is 0 0 0 Î² (cid:101) =argmin J (cid:161)Î²(cid:162) . r(Î²)=Î¸ 0 Acriterion-basedstatistictotest(cid:72) is 0 J= min J (cid:161)Î²(cid:162)âmin J (cid:161)Î²(cid:162)=J(Î² (cid:101))âJ(Î² (cid:98)). r(Î²)=Î¸ 0 Î² A criterion-based test rejects (cid:72) for large values of J. Abootstraptestuses the bootstrap algorithm to 0 calculatethecriticalvalue. InthiscontextweneedtobeabitthoughtfulabouthowtoconstructbootstrapversionsofJ.Itmight seemnaturaltoconstructtheexactsamestatisticonthebootstrapsamplesasontheoriginalsample, but this is incorrect. It makes the same error as calculating a t-ratio or Wald statistic centered at the hypothesizedvalue. Inthebootstrapuniverse, thetruevalueofÎ¸ isnotÎ¸ 0 , ratheritisÎ¸ (cid:98) =r (cid:161)Î² (cid:98) (cid:162) . Thus whenusingthenonparametricbootstrap, wewanttoimposetheconstraintr (cid:161)Î²(cid:162)=r (cid:161)Î² (cid:98) (cid:162)=Î¸ (cid:98)toobtain thebootstrapversionofJ. Thus,thecorrectwaytocalculateabootstrapversionofJ isasfollows.Generateabootstrapsample by random sampling from the dataset. Let J â(cid:161)Î²(cid:162) be the the bootstrap version of the criterion. On a",
    "page": 309,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 290 bootstrapsamplecalculatetheunrestrictedestimatorÎ² (cid:98) â=argmin J â(cid:161)Î²(cid:162) andtherestrictedversionÎ² (cid:101) â= Î² argmin J â(cid:161)Î²(cid:162) whereÎ¸ (cid:98) =r (cid:161)Î² (cid:98) (cid:162) .Thebootstrapstatisticis r(Î²)=Î¸(cid:98) J â= min J â(cid:161)Î²(cid:162)âmin J â(cid:161)Î²(cid:162)=J â (Î² (cid:101) â )âJ â (Î² (cid:98) â ). r(Î²)=Î¸(cid:98) Î² CalculateJ â oneachbootstrapsample. Takethe1âÎ±th quantileq â . Thebootstraptestrejects(cid:72) 1âÎ± 0 infavorof(cid:72) ifJ>q â .Thebootstrapp-valueis 1 1âÎ± p â= 1 (cid:88) B 1(cid:169) J â (b)>J (cid:170) . B b=1 Specialcasesofcriterion-basedtestsareminimumdistancetests,Ftests,andlikelihoodratiotests. TaketheFtestforalinearhypothesisR (cid:48)Î²=Î¸ .TheF statisticis 0 (cid:161)Ï2âÏ2(cid:162) /q F= (cid:101) (cid:98) Ï2/(nâk) (cid:98) whereÏ2istheunrestrictedestimatoroftheerrorvariance,Ï2istherestrictedestimator,qisthenumber (cid:98) (cid:101) ofrestrictionsandk isthenumberofestimatedcoefficients.ThebootstrapversionoftheF statisticis (cid:161)Ïâ2âÏâ2(cid:162) /q F â= (cid:101) (cid:98) Ïâ2/(nâk) (cid:98) where Ïâ2 is the unrestricted estimator on the bootstrap sample, and Ïâ2 is the restricted estimator (cid:98) (cid:101) whichimposestherestrictionÎ¸ (cid:98) =R (cid:48)Î² (cid:98). Takethelikelihoodratio(LR)testforthehypothesisr (cid:161)Î²(cid:162)=Î¸ .TheLRteststatisticis 0 LR=2 (cid:161)(cid:96) n (cid:161)Î² (cid:98) (cid:162)â(cid:96) n (cid:161)Î² (cid:101) (cid:162)(cid:162) whereÎ² (cid:98)istheunrestrictedMLEandÎ² (cid:101)istherestrictedMLE(imposingr (cid:161)Î²(cid:162)=Î¸ 0 ).Thebootstrapversion is LR â=2 (cid:161)(cid:96)â(cid:161)Î² (cid:98) â(cid:162)â(cid:96)â(cid:161)Î² (cid:101) â(cid:162)(cid:162) n n where (cid:96)â (Î²) is the log-likelihood function calculated on the bootstrap sample, Î² (cid:98) â is the unrestricted n maximizer,andÎ² (cid:101) â istherestrictedmaximizerimposingtherestrictionr (cid:161)Î²(cid:162)=r (cid:161)Î² (cid:98) (cid:162) . 10.24 ParametricBootstrap Throughout this chapter we have described the most popular form of the bootstrap known as the nonparametricbootstrap.Howeverthereareotherformsofthebootstrapalgorithmincludingthepara- metric bootstrap. This is appropriate when there is a full parametric model for the distribution as in likelihoodestimation. First, consider the context where the model specifies the full distribution of the random vector Y, e.g.Y â¼F(y|Î²)wherethedistributionfunctionF isknownbuttheparameterÎ²isunknown.LetÎ² (cid:98)bean estimatorofÎ²suchasthemaximumlikelihoodestimator.Theparametricbootstrapalgorithmgenerates bootstrapobservationsY â bydrawingrandomvectorsfromthedistributionfunctionF(y|Î² (cid:98)).Whenthis i isdone,thetruevalueofÎ²inthebootstrapuniverseisÎ² (cid:98). Everythingwhichhasbeendiscussedinthe chaptercanbeappliedusingthisbootstrapalgorithm.",
    "page": 310,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 291 Second,considerthecontextwherethemodelspecifiestheconditionaldistributionoftherandom vector Y given the random vector X, e.g. Y | X =x â¼F(y |x,Î²). An example is the normal linear re- gressionmodel, whereY |X =x â¼N (cid:161) x (cid:48)Î²,Ï2(cid:162) . Inthiscontextwecanholdtheregressors X fixedand i thendrawthebootstrapobservationsY i â fromtheconditionaldistributionF(y|X i ,Î² (cid:98)).Intheexampleof thenormalregressionmodelthisisequivalenttodrawinganormalerrore ââ¼N (cid:161) 0,Ï2(cid:162) andthensetting i (cid:98) Y â=X (cid:48)Î² (cid:98) +e â . Again,inthisalgorithmthetruevalueofÎ²isÎ² (cid:98)andeverythingwhichisdiscussedinthis i i i chaptercanbeappliedasbefore. Third, considertestsofthehypothesisr (cid:161)Î²(cid:162)=Î¸ . Inthiscontextwecanalsoconstructarestricted 0 estimatorÎ² (cid:101)(forexampletherestrictedMLE)whichsatisfiesthehypothesisr (cid:161)Î² (cid:101) (cid:162)=Î¸ 0 .Thenwecangen- eratebootstrapsamplesbysimulatingfromthedistributionY ââ¼F(y |Î² (cid:101)),orintheconditionalcontext i from Y i â â¼F(y | X i ,Î² (cid:101)). When this is done the true value of Î² in the bootstrap is Î² (cid:101)which satisfies the hypothesis.Sointhiscontextthecorrectvaluesofthebootstrapstatisticsare Î¸ââÎ¸ T â= (cid:98) 0 s(Î¸ (cid:98) â ) W â=(cid:161)Î¸ (cid:98) ââÎ¸ 0 (cid:162)(cid:48) V(cid:98) â Î¸(cid:98) â1(cid:161)Î¸ (cid:98) ââÎ¸ 0 (cid:162) J â= min J â(cid:161)Î²(cid:162)âmin J â(cid:161)Î²(cid:162) r(Î²)=Î¸ 0 Î² (cid:181) (cid:182) LR â=2 max(cid:96)â(cid:161)Î²(cid:162)â max (cid:96)â(cid:161)Î²(cid:162) Î² n r(Î²)=Î¸ 0 n and (cid:161)Ïâ2âÏâ2(cid:162) /q F â= (cid:101) (cid:98) Ïâ2/(nâk) (cid:98) whereÏâ2istheunrestrictedestimatoronthebootstrapsampleandÏâ2istherestrictedestimatorwhich (cid:98) (cid:101) imposestherestrictionR (cid:48)Î²=Î¸ . 0 Theprimaryadvantageoftheparametricbootstrap(relativetothenonparametricbootstrap)isthat it will be more accurate when the parametric model is correct. This may be quite important in small samples. The primary disadvantage of the parametric bootstrap is that it can be inaccurate when the parametricmodelisincorrect. 10.25 HowManyBootstrapReplications? Howmanybootstrapreplicationsshouldbeused?Thereisnouniversallycorrectanswerasthereisa trade-offbetweenaccuracyandcomputationcost.ComputationcostisessentiallylinearinB.Accuracy (either standard errors or p-values) is proportional to B â1/2. Improved accuracy can be obtained but onlyatahighercomputationalcost. In most empirical research, most calculations are quick and investigatory, not requiring full accu- racy. Butfinalresults(thosegoingintothefinalversionofthepaper)shouldbeaccurate. Thusitseems reasonabletouseasymptoticand/orbootstrapmethodswithamodestnumberofreplicationsfordaily calculations,butuseamuchlargerB forthefinalversion. Inparticular,forfinalcalculations,B=10,000isdesired,withB=1000aminimalchoice.Incontrast, fordailyquickcalculationsvaluesaslowasB=100maybesufficientforroughestimates",
    "page": 311,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". In most empirical research, most calculations are quick and investigatory, not requiring full accu- racy. Butfinalresults(thosegoingintothefinalversionofthepaper)shouldbeaccurate. Thusitseems reasonabletouseasymptoticand/orbootstrapmethodswithamodestnumberofreplicationsfordaily calculations,butuseamuchlargerB forthefinalversion. Inparticular,forfinalcalculations,B=10,000isdesired,withB=1000aminimalchoice.Incontrast, fordailyquickcalculationsvaluesaslowasB=100maybesufficientforroughestimates. A useful way to think about the accuracy of bootstrap methods stems from the calculation of p- â values. The bootstrap p-value p is an average of B Bernoulli draws. The variance of the simulation estimatorofp â isp â (1âp â )/B,whichisboundedaboveby1/4B. Tocalculatethep-valuewithin,say,",
    "page": 311,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 292 0.01 of the true value with 95% probability requires a standard error below 0.005. This is ensured if Bâ¥10,000. StatabydefaultsetsB=50.Thisisusefulforverificationthataprogramrunsbutisapoorchoicefor empiricalreporting.MakesurethatyousetB tothevalueyouwant. 10.26 SettingtheBootstrapSeed Computersdonotgeneratetruerandomnumbersbutratherpseudo-randomnumbersgeneratedby adeterministicalgorithm.Thealgorithmsgeneratesequenceswhichareindistinguishablefromrandom sequencessothisisnotaworryforbootstrapapplications. Themethods, however, necessarilyrequireastartingvalueknownasaâseedâ. Somepackages(in- cludingStataandMATLAB)implementthiswithadefaultseedwhichisreseteachtime thestatistical packageisstarted. Thismeansifyoustartthepackagefresh, runabootstrapprogram(e.g. aâdoâfile in Stata), exit the package, restart the package and then rerun the bootstrap program, you should ob- tainexactlythesameresults. Ifyouinsteadrunthebootstrapprogram(e.g. âdoâfile)twicesequentially withoutrestartingthepackage,theseedisnotresetsoadifferentsetofpseudo-randomnumberswillbe generatedandtheresultsfromthetworunswillbedifferent. TheRpackagehasadifferentimplementation.WhenRisloadedtherandomnumberseedisgener- atedbasedonthecomputerâsclock(whichresultsinanessentiallyrandomstartingseed). Thereforeif yourunabootstrapprograminR,exit,restart,andrerun,youwillobtainadifferentsetofrandomdraws andthereforeadifferentbootstrapresult. Packagesallowuserstosettheirownseed. (InStata,thecommandisset seed #. InMATLABthe commandisrng(#). InRthecommandisset.seed(#).) Iftheseedissettoaspecificnumberatthe startofafilethentheexactsamepseudo-randomnumberswillbegeneratedeachtimetheprogramis run.Ifthisisthecase,theresultsofabootstrapcalculation(standarderrorortest)willbeidenticalacross computerruns. Thefactthatthebootstrapresultscanbefixedbysettingtheseedinthereplicationfilehasmotivated manyresearcherstofollowthischoice.Theysettheseedatthestartofthereplicationfilesothatrepeated executionsresultinthesamenumericalfindings. Fixing seeds, however, should be done cautiously. It may be a wise choice for a final calculation (whenapaperisfinished)butisanunwisechoicefordailycalculations. Ifyouuseasmallnumberof replicationsinyourpreliminarywork,sayB =100,thebootstrapcalculationswillbeinaccurate. Butas yourunyourresultsagainandagain(asistypicalinempiricalprojects)youwillobtainthesamenumeri- calstandarderrorsandtestresults,givingyouafalsesenseofstabilityandaccuracy.Ifinsteadadifferent seedisusedeachtimetheprogramisrunthenthebootstrapresultswillvaryacrossruns,andyouwill observethattheresultsvaryacrosstheseruns,givingyouimportantandmeaningfulinformationabout the(lackof)accuracyinyourresults. Onewaytoensurethisistosettheseedaccordingtothecurrent clock. InMATLABusethecommandrng(âshuffleâ). InRuseset.seed(seed=NULL).Statadoesnot havethisoption",
    "page": 312,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Onewaytoensurethisistosettheseedaccordingtothecurrent clock. InMATLABusethecommandrng(âshuffleâ). InRuseset.seed(seed=NULL).Statadoesnot havethisoption. Theseconsiderationsleadtoarecommendedhybridapproach.Fordailyempiricalinvestigationsdo notfixthebootstrapseedinyourprogramunlessyouhaveitsetbytheclock.Foryourfinalcalculations settheseedtoaspecificarbitrarychoice,andsetB=10,000sothattheresultsareinsensitivetotheseed. 10.27 BootstrapRegression AmajorfocusofthistextbookhasbeenontheleastsquaresestimatorÎ² (cid:98)intheprojectionmodel.The bootstrapcanbeusedtocalculatestandarderrorsandconfidenceintervalsforsmoothfunctionsofthe",
    "page": 312,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 293 coefficientestimates. Thenonparametricbootstrapalgorithm,asdescribedbefore,samplesobservationsrandomlywith (cid:161) â â(cid:162) (cid:161) â â(cid:162) replacementfromthedataset,creatingthebootstrapsample{ Y ,X ,..., Y ,X },orinmatrixnota- 1 1 n n â â tion(Y ,X )Itisimportanttorecognizethatentireobservations(pairsofY andX )aresampled. This i i isoftencalledthepairsbootstrap. Giventhisbootstrapsample,wecalculatetheregressionestimator Î² (cid:98) â=(cid:161) X â(cid:48) X â(cid:162)â1(cid:161) X â(cid:48) Y â(cid:162) . (10.32) ThisisrepeatedB times.Thebootstrapstandarderrorsarethestandarddeviationsacrossthedrawsand confidenceintervalsareconstructedfromtheempiricalquantilesacrossthedraws. What is the nature of the bootstrap distribution of Î² (cid:98) â ? It is useful to start with the distribution of (cid:161) â â(cid:162) the bootstrap observations Y ,X , which is the discrete distribution which puts mass 1/n on each i i observationpair(Y ,X ). Thebootstrapuniversecanbethoughtofastheempiricalscatterplotofthe i i observations.Thetruevalueoftheprojectioncoefficientinthisbootstrapuniverseis (cid:195) (cid:33)â1(cid:195) (cid:33) (cid:161)(cid:69)â(cid:163) X i â X i â(cid:48)(cid:164)(cid:162)â1(cid:161)(cid:69)â(cid:163) X i â Y i â(cid:164)(cid:162)= n 1 (cid:88) n X i X i (cid:48) n 1 (cid:88) n X i Y i =Î² (cid:98). i=1 i=1 WeseethatthetruevalueinthebootstrapdistributionistheleastsquaresestimatorÎ² (cid:98). Thebootstrapobservationssatisfytheprojectionequation Y â=X â(cid:48)Î² (cid:98) +e â (10.33) i i i (cid:69)â(cid:163) X â e â(cid:164)=0. i i Foreachbootstrappair (cid:161) Y â ,X â(cid:162)=(cid:161) Y ,X (cid:162) thetrueerrore â=e equalstheleastsquaresresidualfrom i i j j i (cid:98)j theoriginaldataset.Thisisbecauseeachbootstrappaircorrespondstoanactualobservation. â(cid:48) â Atechnicalproblem(whichistypicallyignored)isthatitispossibleforX X tobesingularinasim- ulatedbootstrapsample,inwhichcasetheleastsquaresestimatorÎ² (cid:98) â isnotuniquelydefined. Indeed, â(cid:48) â theprobabilityispositivethat X X issingular. Forexample,theprobabilitythatabootstrapsample consistsentirelyofoneobservationrepeatedntimesisn â(nâ1).Thisisasmallprobability,butpositive.A moresignificantexampleissparsedummyvariabledesignswhereitispossibletodrawanentiresample withonlyoneobservedvalueforthedummyvariable. Forexample,ifasamplehasn=20observations withadummyvariablewithtreatment(equals1)foronlythreeofthe20observations,theprobabilityis 4%thatabootstrapsamplecontainsentirelynon-treatedvalues(all0âs).4%isquitehigh! ThestandardapproachtocircumventthisproblemistocomputeÎ² (cid:98) â onlyif X â(cid:48) X â isnon-singular asdefinedbyaconventionalnumericaltoleranceandtreatitasmissingotherwise. Abettersolutionis â(cid:48) â todefineatolerancewhichbounds X X awayfromnon-singularity. Definetheratioofthesmallest eigenvalueofthebootstrapdesignmatrixtothatofthedatadesignmatrix Î» (cid:161) X â(cid:48) X â(cid:162) Î»â= min",
    "page": 313,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Abettersolutionis â(cid:48) â todefineatolerancewhichbounds X X awayfromnon-singularity. Definetheratioofthesmallest eigenvalueofthebootstrapdesignmatrixtothatofthedatadesignmatrix Î» (cid:161) X â(cid:48) X â(cid:162) Î»â= min . Î» (cid:161) X (cid:48) X (cid:162) min If,inagivenbootstrapreplication,Î»â<Ïissmallerthanagiventolerance(ShaoandTu(1995,p. 291) recommendÏ=1/2)thentheestimatorcanbetreatedasmissing,orwecandefinethetrimmingrule ï£± ï£´ Î² (cid:98) â ifÎ»ââ¥Ï ï£² Î² (cid:98) â= (10.34) ï£´ ï£³ Î² (cid:98) ifÎ»â<Ï. ThisensuresthatthebootstrapestimatorÎ² (cid:98) â willbewellbehaved.",
    "page": 313,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 294 10.28 BootstrapRegressionAsymptoticTheory DefinetheleastsquaresestimatorÎ² (cid:98),itsbootstrapversionÎ² (cid:98) â asin(10.32),andthetransformations Î¸ (cid:98) =g(Î² (cid:98))andÎ¸ (cid:98) â=r(Î² (cid:98) â )forsomesmoothtransformationr. LetV(cid:98)Î² andV(cid:98)Î¸ denoteheteroskedasticity- â â robustcovariancematrixestimatorsforÎ² (cid:98)andÎ¸ (cid:98),andletV(cid:98)Î² andV(cid:98)Î¸ betheirbootstrapversions. When (cid:113) (cid:113) Î¸ is scalar define the standard errors s(Î¸ (cid:98)) = n â1V(cid:98)Î¸ and s(Î¸ (cid:98) â ) = n â1V(cid:98)Î¸â . Define the t-ratios T = (cid:161)Î¸ (cid:98) âÎ¸(cid:162) /s(Î¸ (cid:98))andbootstrapversionT â=(cid:161)Î¸ (cid:98) ââÎ¸ (cid:98) (cid:162) /s(Î¸ (cid:98) â ).Weareinterestedintheasymptoticdistributions ofÎ² (cid:98) â ,Î¸ (cid:98) â andT â . Sincethebootstrapobservationssatisfythemodel(10.33),weseebystandardcalculationsthat (cid:112) n (cid:161)Î² (cid:98) ââÎ² (cid:98) (cid:162)= (cid:195) 1 (cid:88) n X â X â(cid:48) (cid:33)â1(cid:195) (cid:112) 1 (cid:88) n X â e â (cid:33) . n i=1 i i n i=1 i i BythebootstrapWLLN 1 (cid:88) n X â X â(cid:48)ââ(cid:69)(cid:163) X X (cid:48)(cid:164)=Q n i=1 i i pâ i i andbythebootstrapCLT (cid:112) 1 (cid:88) n X â e âââN(0,â¦) n i=1 i i dâ whereâ¦=(cid:69)(cid:163) XX (cid:48) e2(cid:164) .AgainapplyingthebootstrapWLLNweobtain V(cid:98)Î² ââVÎ² =Q â1â¦Q â1 pâ and V(cid:98)Î¸ ââVÎ¸ =R (cid:48) VÎ²R pâ whereR=R (cid:161)Î²(cid:162) . Combining with the bootstrap CMT and delta method we establish the asymptotic distribution of thebootstrapregressionestimator. Theorem10.18 UnderAssumption7.2,asnââ (cid:112) n (cid:161)Î¸ (cid:98) ââÎ¸ (cid:98) (cid:162)ââN (cid:161) 0,VÎ² (cid:162) . dâ IfAssumption7.3alsoholdsthen (cid:112) n (cid:161)Î¸ (cid:98) ââÎ¸ (cid:98) (cid:162)ââN(0,VÎ¸). dâ IfAssumption7.4alsoholdsthen T âââN(0,1). dâ ThismeansthatthebootstrapconfidenceintervalandtestingmethodsallapplyforinferenceonÎ² andÎ¸. Thisincludesthepercentile,BCpercentile,BC ,andpercentile-tintervals,andhypothesistests a basedont-tests,Waldtests,MDtests,LRtestsandFtests.",
    "page": 314,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 295 TojustifybootstrapstandarderrorswealsoneedtoverifytheuniformsquareintegrabilityofÎ² (cid:98) â and Î¸ (cid:98) â . Thisistechnicallychallengingbecausetheleastsquaresestimatorinvolvesmatrixinversionwhich isnotgloballycontinuous. Apartialsolutionistousethetrimmedestimator(10.34). Thisboundsthe momentsofÎ² (cid:98) â bythoseofn â1(cid:80)n i=1 X i â e i â . Sincethisisasamplemean,Theorem10.10appliesandV(cid:98) â Î² â isbootstrapconsistentforVÎ². However, thisdoesnotensurethatV(cid:98)Î¸ willbeconsistentforV(cid:98)Î¸ unless the function r(x) satisfies the conditions of Theorem 10.10. For general applications use a trimmed estimatorforthebootstrapvariance.ForsomeÏ =O (cid:161) en/8(cid:162) define n (cid:112) Z â= n (cid:161)Î¸ (cid:98) ââÎ¸ (cid:98) (cid:162) n Z ââ=z â1(cid:169)(cid:176) (cid:176)Z n â(cid:176) (cid:176) â¤Ï n (cid:170) Z ââ= 1 (cid:88) B Z ââ (b) B b=1 V(cid:98) b Î¸ oot,Ï = B 1 â1 (cid:88) B (cid:161) Z ââ (b)âZ ââ(cid:162)(cid:161) Z ââ (b)âZ ââ(cid:162)(cid:48) . b=1 (cid:112) The matrixV(cid:98) b Î¸ oot is a trimmed bootstrap estimator of the variance of Z n = n (cid:161)Î¸ (cid:98) âÎ¸(cid:162) . The associated (cid:113) bootstrapstandarderrorforÎ¸ (cid:98)(inthescalarcase)iss(Î¸ (cid:98))= n â1V(cid:98) b Î¸ oot . boot ByanapplicationofTheorems10.11and10.12,wefindthatthisestimatorV(cid:98)Î¸ isconsistentforthe asymptoticvariance. Theorem10.19 UnderAssumption7.2and7.3,asnââ,V(cid:98) b Î¸ oot,Ï ââVÎ¸. pâ boot boot,Ï ProgramssuchasStatausetheuntrimmedestimatorV(cid:98)Î¸ ratherthanthetrimmedestimatorV(cid:98)Î¸ . Thismeansthatweshouldbecautiousaboutinterpretingreportedbootstrapstandarderrorsespecially fornonlinearfunctionssuchasratios. 10.29 WildBootstrap Takethelinearregressionmodel Y =X (cid:48)Î²+e (cid:69)[e|X]=0. What is special about this model is the conditional mean restriction. The nonparametric bootstrap (cid:161) â â(cid:162) (which samples the pairs Y ,X i.i.d. from the original observations) does not make use of this re- i i â â striction.Consequentlythebootstrapdistributionfor (Y ,X )doesnotsatisfytheconditionalmeanre- strictionandthereforedoesnotsatisfythelinearregressionassumption. Toimproveprecisionitseems reasonabletoimposetheconditionalmeanrestrictiononthebootstrapdistribution. â AnaturalapproachistoholdtheregressorsX fixedandthendrawtheerrorse insomewaywhich i i imposes a conditional mean of zero. The simplest approach is to draw the errors independent from the regressors, perhaps from the empirical distribution of the residuals. This procedure is known as theresidualbootstrap. However,thisimposesindependenceoftheerrorsfromtheregressorswhichis muchstrongerthantheconditionalmeanassumption.Thisisgenerallyundesirable.",
    "page": 315,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 296 Amethodwhichimposestheconditionalmeanrestrictionwhileallowinggeneralheteroskedasticity isthewildbootstrap.ItwasproposedbyLiu(1988)andextendedbyMammon(1993).Themethoduses auxiliaryrandomvariablesÎ¾â whicharei.i.d.,meanzero,andvariance1.Thebootstrapobservationsare thengeneratedasY i â=X i (cid:48)Î² (cid:98) +e i â withe i â=e (cid:98)i Î¾â i , wheretheregressors X i areheldfixedattheirsample values,Î² (cid:98)isthesampleleastsquaresestimator,ande (cid:98)i aretheleastsquaresresiduals,whicharealsoheld fixedattheirsamplevalues. â Thisalgorithmgeneratesbootstraperrorse whichareconditionallymeanzero.Thusthebootstrap i pairs (cid:161) Y i â ,X i (cid:162) satisfyalinearregressionwiththeâtrueâcoefficientofÎ² (cid:98). Theconditionalvarianceofthe wildbootstraperrorse â are(cid:69)â(cid:163) e â2|X (cid:164)=e2. Thismeansthattheconditionalvarianceofthebootstrap i i i (cid:98)i estimatorÎ² (cid:98) â is (cid:195) (cid:33) (cid:69)â (cid:104) (cid:161)Î² (cid:98) ââÎ² (cid:98) (cid:162)(cid:161)Î² (cid:98) ââÎ² (cid:98) (cid:162)(cid:48) |X (cid:105) =(cid:161) X (cid:48) X (cid:162)â1 (cid:88) n X i X i (cid:48) e (cid:98)i 2 (cid:161) X (cid:48) X (cid:162)â1 i=1 whichistheWhiteestimatorofthevarianceofÎ² (cid:98).Thusthewildbootstrapreplicatestheappropriatefirst andsecondmomentsofthedistribution. TwodistributionshavebeenproposedfortheauxilaryvariablesÎ¾â bothofwhicharetwo-pointdis- i cretedistributions.ThefirstareRademacherrandomvariableswhichsatisfy(cid:80)[Î¾â=1]= 1and(cid:80)[Î¾â=â1]= 2 1.ThesecondistheMammen(1993)two-pointdistribution 2 (cid:112) (cid:112) (cid:34) (cid:35) (cid:80) Î¾â= 1+ 5 = 5 (cid:112) â1 2 2 5 (cid:112) (cid:112) (cid:34) (cid:35) (cid:80) Î¾â= 1â 5 = 5 (cid:112) +1 . 2 2 5 ThereasoningbehindtheMammendistributionisthatthischoiceimplies(cid:69)(cid:163)Î¾â3(cid:164)=1,whichimplies that the third central moment of Î² (cid:98) â matches the natural nonparametric estimator of the third central moment ofÎ² (cid:98). Since the wild bootstrap matches the firstthree moments, the percentile-t interval and one-sidedt-testscanbeshowntoachieveasymptoticrefinements. ThereasoningbehindtheRademacherdistributionisthatthischoiceimplies(cid:69)(cid:163)Î¾â4(cid:164)=1,whichim- pliesthatthefourthcentralmomentofÎ² (cid:98) â matchesthenaturalnonparametricestimatorofthefourth central moment of Î² (cid:98). If the regression errors e are symmetrically distributed (so the third moment is zero) then the first four moments are matched. In this case the wild bootstrap should have even better performance, and additionally two-sided t-tests can be shown to achieve an asymptotic refine- ment. Whentheregressionerrorisnotsymmetricallydistributedtheseasymptoticrefinementsarenot achieved.Limitedsimulationevidenceforone-sidedt-testspresentedinDavidsonandFlachaire(2008) suggests that the Rademacher distribution (used with the restricted wild bootstrap) has better perfor- manceandistheirrecommendation",
    "page": 316,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". In this case the wild bootstrap should have even better performance, and additionally two-sided t-tests can be shown to achieve an asymptotic refine- ment. Whentheregressionerrorisnotsymmetricallydistributedtheseasymptoticrefinementsarenot achieved.Limitedsimulationevidenceforone-sidedt-testspresentedinDavidsonandFlachaire(2008) suggests that the Rademacher distribution (used with the restricted wild bootstrap) has better perfor- manceandistheirrecommendation. Forhypothesistestingimprovedprecisioncanbeobtainedbytherestrictedwildbootstrap. Con- sidertestsofthehypothesis(cid:72) 0 :r (cid:161)Î²(cid:162)=0.LetÎ² (cid:101)beaCLSorEMDestimatorofÎ²subjecttotherestriction r (cid:161)Î² (cid:101) (cid:162)=0. Lete (cid:101)i =Y i âX i (cid:48)Î² (cid:101)be the constrained residuals. The restricted wild bootstrap algorithm gen- erates observations as Y i â = X i (cid:48)Î² (cid:101) +e i â with e i â =e (cid:101)i Î¾â i . With this modification Î² (cid:101)is the true value in the bootstrapuniversesothenullhypothesis(cid:72) holds.Thusbootstraptestsareconstructedthesameasfor 0 theparametricbootstrapusingarestrictedparameterestimator. 10.30 BootstrapforClusteredObservations Bootstrapmethodscanalsobeappliedintoclusteredsamplesthoughthemethodologicalliterature isrelativelythin.HerewereviewmethodsdiscussedinCameron,GelbachandMiller(2008).",
    "page": 316,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 297 LetY =(Y ,...,Y ) (cid:48) andX =(X ,...,X ) (cid:48) denotethen Ã1vectorofdependentvariablesand g 1g ngg g 1g ngg g n Ãk matrixofregressorsforthegth cluster. AlinearregressionmodelusingclusternotationisY = g g X Î²+e wheree =(e ,...,e ) (cid:48) isann Ã1errorvector.ThesamplehasG clusterpairs(Y ,X ). g g g 1g ngg g g g ThepairsclusterbootstrapsamplesG clusterpairs(Y ,X )tocreatethebootstrapsample. Least g g squares is applied to the bootstrap sample to obtain the coefficient estimators. By repeating B times bootstrap standard errors for coefficients estimates, or functions of the coefficient estimates, can be calculated.Percentile,BCpercentile,andBC confidenceintervalscanbecalculated. a TheBC intervalrequiresanestimatoroftheaccelerationcoefficient a whichisascaledjackknife a estimateofthethirdmomentoftheestimator.Inthecontextofclusteredobservationsthedelete-cluster jackknifeshouldbeusedforestimationofa. Furthermore, on each bootstrap sample the cluster-robust standard errors can be calculated and usedtocomputebootstrapt-ratios,fromwhichpercentile-tconfidenceintervalscanbecalculated. The wildclusterbootstrap fixes the clusters and regressors, and generates the bootstrap observa- tionsas Y â g =X g Î² (cid:98) +e â g e â=e Î¾â g (cid:98)i g where Î¾â is a scalar auxilary random variable as described in the previous section. Notice that Î¾â is g g interactedwiththeentirevectorofresidualsfromclusterg. Cameron,GelbachandMiller(2008)follow therecommendationofDavidsonandFlachaire(2008)anduseRademacherrandomvariablesforÎ¾â . g Forhypothesistesting,Cameron,GelbachandMiller(2008)recommendtherestrictedwildcluster bootstrap.Fortestsof(cid:72) 0 :r (cid:161)Î²(cid:162)=0letÎ² (cid:101)beaCLSorEMDestimatorofÎ²subjecttotherestrictionr (cid:161)Î² (cid:101) (cid:162)= 0. Let (cid:101) e g =Y g âX g Î² (cid:101)be the constrained cluster-level residuals. The restricted wild cluster bootstrap algorithmgeneratesobservationsas Y â g =X g Î² (cid:101) +e â g e â=e Î¾â . g (cid:101)i g Oneachbootstrapsampletheteststatisticfor(cid:72) (t-ratio,Wald,LR,orF)isapplied. Sincethebootstrap 0 algorithmsatisfies(cid:72) thesestatisticsarecenteredatthehypothesizedvalue.p-valuesarethencalculated 0 conventionallyandusedtoassessthesignificanceoftheteststatistic. Thereareseveralreasonswhyconventionalasymptoticapproximationsmayworkpoorlywithclus- teredobservations. First,whilethesamplesizen maybelarge,theeffectivesamplesizeisthenumber ofclustersG. Thisisbecausewhenthedependencestructurewithineachclusterisunconstrainedthe centrallimittheoremeffectivelytreatseachclusterasasingleobservation.Thus,ifG issmallweshould treatinferenceasasmallsampleproblem",
    "page": 317,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Thereareseveralreasonswhyconventionalasymptoticapproximationsmayworkpoorlywithclus- teredobservations. First,whilethesamplesizen maybelarge,theeffectivesamplesizeisthenumber ofclustersG. Thisisbecausewhenthedependencestructurewithineachclusterisunconstrainedthe centrallimittheoremeffectivelytreatseachclusterasasingleobservation.Thus,ifG issmallweshould treatinferenceasasmallsampleproblem. Second,cluster-robustcovariancematrixestimationexplic- itlytreatseachclusterasasingleobservation.Consequentlytheaccuracyofnormalapproximationstot- ratiosandWaldstatisticsismoreaccuratelyviewedasasmallsampledistributionproblem.Third,when clustersizesn areheterogeneousthismeansthattheestimationproblemsjustdescribedalsoinvolve g heterogeneousvariances.Specifically,heterogeneousclustersizesinducesahighdegreeofeffectivehet- eroskedasticity(sincethevarianceofawithin-clustersumisproportionalton ). WhenG issmallthis g means that cluster-robust inference is similar to finite-sample inference with a small heteroskedastic sample. Fourth,interestoftenconcernstreatmentwhichisappliedatthelevelofacluster(suchasthe effectoftrackingdiscussedinSection4.23).Ifthenumberoftreatedclustersissmallthisisequivalentto estimationwithahighlysparsedummyvariabledesigninwhichcasecluster-robustcovariancematrix estimationcanbeunreliable.",
    "page": 317,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 298 These concerns suggest that conventional normal approximations may be poor in the context of clusteredobservationswithasmallnumberofgroupsG,motivatingtheuseofbootstrapmethods.How- ever,theseconcernsalsocancausechallengeswiththeaccuracyofbootstrapapproximations.Whenthe numberofclustersGissmall,theclustersizesn heterogeneous,orthenumberoftreatedclusterssmall, g bootstrapmethodsmaybeinaccurate.Insuchcasesinferenceshouldproceedcautiously. To illustrate the use of the pairs cluster bootstrap, Table 10.4 reports the estimates of the example fromSection4.23oftheeffectoftrackingontestscoresfromDuflo,DupasandKremer(2011).Inaddition totheasymptoticclusterstandarderrorwereporttheclusterjackknifeandclusterbootstrapstandard errorsaswellasthreepercentile-typeconfidenceintervals.Weuse10,000bootstrapreplications.Inthis exampletheasymptotic,jackknife,andclusterbootstrapstandarderrorsareidentical,whichreflectsthe goodbalanceofthisparticularregressiondesign. Table10.4:ComparisonofMethodsforEstimateofEffectofTracking CoefficientonTracking 0.138 Asymptoticclusters.e. (0.078) Jackknifeclusters.e. (0.078) ClusterBootstraps.e. (0.078) 95%PercentileInterval [â0.013,0.291] 95%BCPercentileInterval [â0.015,0.289] 95%BC PercentileInterval [â0.018,0.286] a InStata,toobtainclusterbootstrapstandarderrorsandconfidenceintervalsusetheoptionscluster(id) vce(bootstrap, reps(#)),whereidistheclustervariableand#isthenumberofreplications. 10.31 TechnicalProofs* Someoftheasymptoticresultsarefacilitatedbythefollowingconvergenceresult. Theorem10.20 MarcinkiewiczWLLNIfu areindependentanduniformlyintegrable,thenforanyr > i 1,asnââ,n âr(cid:80)n |u |r ââ0. i=1 i p ProofofTheorem10.20 n âr (cid:88) n |u |r â¤ (cid:181) n â1 max |u | (cid:182)râ1 1 (cid:88) n |u |ââ0 i i i i=1 1â¤iâ¤n n i=1 p bytheWLLN,Theorem6.16,andr >1. ProofofTheorem10.1Fix(cid:178)>0.SinceZ ââZ thereisannsufficientlylargesuchthat n p (cid:80)[(cid:107)Z âZ(cid:107)>(cid:178)]<(cid:178). n Sincetheevent(cid:107)Z âZ(cid:107)>(cid:178)isnon-randomundertheconditionalprobability(cid:80)â ,forsuchn, n (cid:80)â [(cid:107)Z âZ(cid:107)>(cid:178)]= (cid:189) 0 withprobabilityexceeding1â(cid:178) n 1 withprobabilitylessthan(cid:178). SinceÎµisarbitraryweconclude(cid:80)â [(cid:107)Z âZ(cid:107)>(cid:178)]ââ0asrequired. â  n p",
    "page": 318,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 299 ProofofTheorem10.2Fix(cid:178)>0. ByMarkovâsinequality(B.36),thefacts(10.12)and(10.13),andfinally theMarcinkiewiczWLLN(Theorem10.20)withr =2andu =(cid:107)Y (cid:107), i i (cid:80)â (cid:104)(cid:176) (cid:176)Y â âY (cid:176) (cid:176)>(cid:178) (cid:105) â¤(cid:178)â2(cid:69)â (cid:176) (cid:176)Y â âY (cid:176) (cid:176) 2 (cid:176) (cid:176) (cid:176) (cid:176) =(cid:178)â2tr (cid:179) var â (cid:104) Y â(cid:105)(cid:180) (cid:181) (cid:182) =(cid:178)â2tr 1 Î£ (cid:98) n n â¤(cid:178)â2n â2(cid:88) Y (cid:48) Y i i i=1 ââ0. p â ThisestablishesthatY âY ââ0. pâ â â SinceY âÂµââ0bytheWLLN,Y âÂµââ0byTheorem10.1.SinceY âÂµ=Y âY +Y âÂµ,wededuce p pâ â thatY âÂµââ0. â  pâ Proof of Theorem 10.4 We verify conditions for the multivariate Lindeberg CLT (Theorem 6.4). (We cannotuse the LindebergâLÃ©vy CLTsince the conditional distributiondepends on n.) Conditional on F n ,thebootstrapdrawsY i ââY arei.i.d. withmean0andcovariancematrixÎ£ (cid:98). SetÎ½2 n =Î» min (Î£ (cid:98)). Note thatbytheWLLN,Î½2 ââÎ½2=Î» (Î£)>0. Thusfornsufficientlylarge,Î½2 >0withhighprobability. Fix n min n p (cid:178)>0.Equation(6.2)equals 1 (cid:88) n (cid:69)â (cid:183)(cid:176) (cid:176)Y ââY (cid:176) (cid:176) 2 1 (cid:189)(cid:176) (cid:176)Y ââY (cid:176) (cid:176) 2 â¥(cid:178)nÎ½2 (cid:190)(cid:184) = 1 (cid:69)â (cid:183)(cid:176) (cid:176)Y ââY (cid:176) (cid:176) 2 1 (cid:189)(cid:176) (cid:176)Y ââY (cid:176) (cid:176) 2 â¥(cid:178)nÎ½2 (cid:190)(cid:184) nÎ½2 n i=1 (cid:176) i (cid:176) (cid:176) i (cid:176) n Î½2 n (cid:176) i (cid:176) (cid:176) i (cid:176) n â¤ 1 (cid:69)â (cid:176) (cid:176)Y ââY (cid:176) (cid:176) 4 (cid:178)nÎ½4 (cid:176) i (cid:176) n â¤ 24 (cid:69)â(cid:176) (cid:176)Y â(cid:176) (cid:176) 4 (cid:178)nÎ½4 i n = 24 (cid:88) n (cid:107)Y (cid:107)4 (cid:178)n2Î½4 n i=1 i ââ0. p The second inequality uses Minkowskiâs inequality (B.34), Liapunovâs inequality (B.35), and the c in- r equality(B.6). Thefollowingequalityis(cid:69)â(cid:176) (cid:176)Y i â(cid:176) (cid:176) 4=n â1(cid:80)n i=1 (cid:107)Y i (cid:107)4,whichissimilarto(10.10). Thefinal convergenceholdsbytheMarcinkiewiczWLLN(Theorem10.20)withr =2andu =(cid:107)Y (cid:107)2. Thecondi- i i tionsforTheorem6.4holdandweconclude Î£ (cid:98) â1/2 (cid:112) n (cid:179) Y â âY (cid:180) ââN(0,I). dâ (cid:112) (cid:179) â (cid:180) SinceÎ£ (cid:98) ââÎ£wededucethat n Y âY ââN(0,Î£)asclaimed. â  pâ dâ ProofofTheorem10.10FornotationalsimplicityassumeÎ¸andÂµarescalar.Seth =h(Y ).Theassump- i i tionthatthepth derivativeofg(u)isboundedimplies (cid:175) (cid:175)g(p)(u) (cid:175) (cid:175) â¤C forsomeC <â. Takingapth order Taylorseriesexpansion",
    "page": 319,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 300 (cid:179) (cid:180) Î¸ (cid:98) ââÎ¸ (cid:98) =g(h â )âg(h)= p (cid:88) â1g(j) h (cid:179) h â âh (cid:180)j + g(p)(cid:161)Î¶â n (cid:162) (cid:179) h â âh (cid:180)p j! p! j=1 â whereÎ¶â liesbetweenh andh. Thisimplies n (cid:175) (cid:175)z n â(cid:175) (cid:175) = (cid:112) n (cid:175) (cid:175) Î¸ (cid:98) ââÎ¸ (cid:98) (cid:175) (cid:175) â¤ (cid:112) n (cid:88) p c j (cid:175) (cid:175) (cid:175) h â âh (cid:175) (cid:175) (cid:175) j j=1 (cid:175) (cid:179) (cid:180)(cid:175) wherec =(cid:175)g(j) h (cid:175)/j!forj <pandc =C/p!.Wefindthatthefourthcentralmomentofthenormalized j (cid:175) (cid:175) p (cid:112) bootstrapestimatorZ â= n (cid:161)Î¸ (cid:98) ââÎ¸ (cid:98) (cid:162) satisfiesthebound n (cid:69)â(cid:163) Z â4(cid:164)â¤ (cid:88) 4p a n2(cid:69)â (cid:175) (cid:175)h â âh (cid:175) (cid:175) r (10.35) n r (cid:175) (cid:175) r=4 wherethecoefficientsa areproductsofthecoefficientsc andhenceeachO (1).Weseethat(cid:69)â(cid:163) Z â4(cid:164)= r j p n O (1)ifn2(cid:69)â (cid:175) (cid:175)h â âh (cid:175) (cid:175) r =O (1)forr =4,...,4p. p (cid:175) (cid:175) p Weshowthisholdsforanyr â¥4usingRosenthalâsinequality(B.50),whichstatesthatforeachr there isaconstantR <âsuchthat r (cid:175) (cid:175)r n2(cid:69)â (cid:175) (cid:175)h â âh (cid:175) (cid:175) r =n2âr(cid:69)â (cid:175) (cid:175) (cid:88) n (cid:179) h ââh (cid:180)(cid:175) (cid:175) (cid:175) (cid:175) (cid:175) i (cid:175) (cid:175)i=1 (cid:175) (cid:40) (cid:41) â¤n2ârR (cid:181) n(cid:69)â (cid:179) h ââh (cid:180)2 (cid:182)r/2 +n(cid:69)â (cid:175) (cid:175)h ââh (cid:175) (cid:175) r r i (cid:175) i (cid:175) (cid:40) (cid:41) =R n2âr/2Ïr+ 1 (cid:88) n (cid:175) (cid:175)h âh (cid:175) (cid:175) r . (10.36) r (cid:98) nrâ2 (cid:175) i (cid:175) i=1 Since (cid:69)(cid:163) h2(cid:164) < â, Ï2 =O (1), so the first term in (10.36) is O (1). Also, by the Marcinkiewicz WLLN i (cid:98) p p (Theorem 10.20), n âr/2(cid:80)n (cid:175) (cid:175)h âh (cid:175) (cid:175) r = o (1) for any r â¥ 1, so the second term in (10.36) is o (1) for i=1(cid:175) i (cid:175) p p r â¥4.Thusforallr â¥4,(10.36)isO (1)andthus(10.35)isO (1).WededucethatZ â isuniformlysquare p p n integrable,andthebootstrapestimateofvarianceisconsistent. Thisargumentcanbeextendedtovector-valuedmeansandestimates. â  ProofofTheorem10.12 We show that (cid:69)â(cid:176) (cid:176)Z n ââ(cid:176) (cid:176) 4 =O p (1). Theorem 6.14 shows that Z n ââ is uniformly squareintegrable.SinceZ n âââ d â â Z,Theorem6.15impliesthatvar (cid:163) Z n ââ(cid:164)âvar[Z]=VÎ²asstated. â Seth =h(Y ). SinceG(x)= g(x) (cid:48) is continuous in a neighborhood of Âµ, there exists Î·>0 and i i âx M<âsuchthat (cid:176) (cid:176)xâÂµ(cid:176) (cid:176) â¤2Î·impliestr (cid:161) G(x) (cid:48) G(x) (cid:162)â¤M.BytheWLLNandbootstrapWLLNthereisan (cid:176) (cid:176) (cid:176) â (cid:176) n sufficientlylargesuchthat(cid:176)h âÂµ(cid:176)â¤Î·and(cid:176)h âh (cid:176)â¤Î·withprobabilityexceeding1âÎ·",
    "page": 320,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". â Seth =h(Y ). SinceG(x)= g(x) (cid:48) is continuous in a neighborhood of Âµ, there exists Î·>0 and i i âx M<âsuchthat (cid:176) (cid:176)xâÂµ(cid:176) (cid:176) â¤2Î·impliestr (cid:161) G(x) (cid:48) G(x) (cid:162)â¤M.BytheWLLNandbootstrapWLLNthereisan (cid:176) (cid:176) (cid:176) â (cid:176) n sufficientlylargesuchthat(cid:176)h âÂµ(cid:176)â¤Î·and(cid:176)h âh (cid:176)â¤Î·withprobabilityexceeding1âÎ·. Onthis (cid:176) n (cid:176) (cid:176) n n(cid:176) (cid:176) (cid:176) event,(cid:176)xâh (cid:176)â¤Î·impliestr (cid:161) G(x) (cid:48) G(x) (cid:162)â¤M. Usingthemean-valuetheorematapointÎ¶â intermedi- (cid:176) n(cid:176) n â atebetweenh andh n n (cid:176) (cid:176)Z n ââ(cid:176) (cid:176) 41 (cid:110)(cid:176) (cid:176) (cid:176) h â n âh n (cid:176) (cid:176) (cid:176) â¤Î· (cid:111) â¤n2 (cid:176) (cid:176) (cid:176) g (cid:179) h â n (cid:180) âg (cid:179) h n (cid:180)(cid:176) (cid:176) (cid:176) 4 1 (cid:110)(cid:176) (cid:176) (cid:176) h â n âh n (cid:176) (cid:176) (cid:176) â¤Î· (cid:111) â¤n2 (cid:176) (cid:176)G (cid:161)Î¶â(cid:162)(cid:48)(cid:179) h â âh (cid:180)(cid:176) (cid:176) 4 (cid:176) n n n (cid:176) (cid:176) â (cid:176)4 â¤M2n2(cid:176)h âh (cid:176) . (cid:176) n n(cid:176)",
    "page": 320,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 301 Then (cid:69)â(cid:176) (cid:176)Z n ââ(cid:176) (cid:176) 4â¤(cid:69)â (cid:104)(cid:176) (cid:176)Z n ââ(cid:176) (cid:176) 41 (cid:110)(cid:176) (cid:176) (cid:176) h â n âh n (cid:176) (cid:176) (cid:176) â¤Î· (cid:111)(cid:105) +Ï4 n (cid:69)â (cid:104) 1 (cid:110)(cid:176) (cid:176) (cid:176) h â n âh n (cid:176) (cid:176) (cid:176) >Î· (cid:111)(cid:105) â¤M2n2(cid:69)â (cid:176) (cid:176)h â âh (cid:176) (cid:176) 4 +Ï4(cid:80)â (cid:179)(cid:176) (cid:176)h â âh (cid:176) (cid:176)>Î· (cid:180) . (10.37) (cid:176) n n(cid:176) n (cid:176) n n(cid:176) In(10.17)weshowedthatthefirsttermin(10.37)isO (1)inthescalarcase. Thevectorcasefollows p byelement-by-elementexpansion. Now take the second term in (10.37). We apply Bernsteinâs inequality for vectors (B.41). Note that â h âh =n â1(cid:80)n u â withu â=h ââh and jth elementu â =h â âh . Theu â arei.i.d.,meanzero, n n i=1 i i i n ji ji jn i (cid:69)â (cid:104) u â ji 2 (cid:105) =Ï (cid:98) 2 j =O p (1),andsatisfythebound (cid:175) (cid:175) (cid:175) u â ji (cid:175) (cid:175) (cid:175) â¤2max i,j (cid:175) (cid:175)h ji (cid:175) (cid:175) =B n ,say. Bernsteinâsinequalitystates that (cid:195) (cid:33) (cid:80)â (cid:104)(cid:176) (cid:176)h â âh (cid:176) (cid:176)>Î· (cid:105) â¤2mexp ân1/2 Î·2 . (10.38) (cid:176) n n(cid:176) 2m2n â1/2max Ï2+2mn â1/2B Î·/3 j (cid:98)j n Theorem6.16showsthatn â1/2B =o (1). Thustheexpressioninthedenominatoroftheparentheses n p in(10.38)iso (1)asnââ,. Itfollowsthatfornsufficientlylarge(10.38)isO (cid:161) exp (cid:161)ân1/2(cid:162)(cid:162) . Hencethe p p secondtermin(10.37)isO (cid:161) exp (cid:161)ân1/2(cid:162)(cid:162) o (cid:161) exp (cid:161)ân1/2(cid:162)(cid:162)=o (1)bytheassumptiononÏ . p p p n Wehaveshownthatthetwotermsin(10.37)areeachO (1).Thiscompletestheproof. â  p _____________________________________________________________________________________________ 10.32 Exercises Exercise10.1 FindthejackknifeestimatorofvarianceoftheestimatorÂµ =n â1(cid:80)n Yr forÂµ =(cid:69)(cid:163) Yr(cid:164) . (cid:98)r i=1 i r i Exercise10.2 ShowthatifthejackknifeestimatorofvarianceofÎ² (cid:98)isV(cid:98) jack ,thenthejackknifeestimator Î²(cid:98) ofvarianceofÎ¸ (cid:98) =a+CÎ² (cid:98)isV(cid:98) jack=CV(cid:98) jack C (cid:48) . Î¸(cid:98) Î²(cid:98) Exercise10.3 Atwo-stepestimator suchas(12.49)is Î² (cid:98) =(cid:161)(cid:80)n i=1 W(cid:99)i W(cid:99) i (cid:48)(cid:162)â1(cid:161)(cid:80)n i=1 W(cid:99)i Y i (cid:162) whereW(cid:99)i = A(cid:98) (cid:48) Z i and A(cid:98) =(cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) X.DescribehowtoconstructthejackknifeestimatorofvarianceofÎ² (cid:98). Exercise10.4 ShowthatifthebootstrapestimatorofvarianceofÎ² (cid:98)isV(cid:98) b Î²(cid:98) oot ,thenthebootstrapestimator ofvarianceofÎ¸ (cid:98) =a+CÎ² (cid:98)isV(cid:98) b Î¸(cid:98) oot=CV(cid:98) b Î²(cid:98) oot C (cid:48) . Exercise10.5 ShowthatifthepercentileintervalforÎ²is[L,U]thenthepercentileintervalfora+cÎ²is [a+cL,a+cU]",
    "page": 321,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Exercise10.4 ShowthatifthebootstrapestimatorofvarianceofÎ² (cid:98)isV(cid:98) b Î²(cid:98) oot ,thenthebootstrapestimator ofvarianceofÎ¸ (cid:98) =a+CÎ² (cid:98)isV(cid:98) b Î¸(cid:98) oot=CV(cid:98) b Î²(cid:98) oot C (cid:48) . Exercise10.5 ShowthatifthepercentileintervalforÎ²is[L,U]thenthepercentileintervalfora+cÎ²is [a+cL,a+cU]. Exercise10.6 Considerthefollowingbootstrapprocedure. Usingthenon-parametricbootstrap,gener- atebootstrapsamples,calculatetheestimateÎ¸ (cid:98) â onthesesamplesandthencalculate T â=(Î¸ (cid:98) ââÎ¸ (cid:98))/s(Î¸ (cid:98)), wheres(Î¸ (cid:98))isthestandarderrorintheoriginaldata. Letq Î± â /2 andq 1 â âÎ±/2 denotetheÎ±/2th and1âÎ±/2th â quantilesofT ,anddefinethebootstrapconfidenceinterval C =(cid:163)Î¸ (cid:98) +s(Î¸ (cid:98))q Î± â /2 , Î¸ (cid:98) +s(Î¸ (cid:98))q 1 â âÎ±/2 (cid:164) . ShowthatC exactlyequalsthepercentileinterval.",
    "page": 321,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 302 Exercise10.7 ProveTheorem10.6. Exercise10.8 ProveTheorem10.7. Exercise10.9 ProveTheorem10.8. Exercise10.10 LetY i bei.i.d.,Âµ=(cid:69)[Y]>0,andÎ¸=Âµâ1.LetÂµ (cid:98) =Y n bethesamplemeanandÎ¸ (cid:98) =Âµ (cid:98) â1. (a) IsÎ¸ (cid:98)unbiasedforÎ¸? (b) IfÎ¸ (cid:98)isbiased,canyoudeterminethedirectionofthebias(cid:69)(cid:163)Î¸ (cid:98) âÎ¸(cid:164) (upordown)? (c) Isthepercentileintervalappropriateinthiscontextforconfidenceintervalconstruction? Exercise10.11 ConsiderthefollowingbootstrapprocedureforaregressionofY onX.LetÎ² (cid:98)denotethe OLSestimatorande (cid:98)i =Y i âX i (cid:48)Î² (cid:98)theOLSresiduals. (a) Drawarandomvector(X â ,e â )fromthepair{(X ,e ):i =1,...,n}.Thatis,drawarandomintegeri (cid:48) i (cid:98)i from[1,2,...,n],andsetX â=X i(cid:48) ande â=e (cid:98)i(cid:48).SetY â=X â(cid:48)Î² (cid:98) +e â .Draw(withreplacement)nsuch â â vectors,creatingarandombootstrapdataset(Y ,X ). (b) RegressY â onX â ,yieldingOLSestimatorÎ² (cid:98) â andanyotherstatisticofinterest. Showthatthisbootstrapprocedureis(numerically)identicaltothenon-parametricbootstrap. â Exercise10.12 Take p asdefinedin(10.22)fortheBCpercentileinterval. Showthatitisinvariantto replacingÎ¸withg(Î¸)foranystrictlymonotonicallyincreasingtransformationg(Î¸). Doesthisextendto â z asdefinedin(10.23)? 0 Exercise10.13 Showthatifthepercentile-tintervalforÎ²is[L,U]thenthepercentile-tintervalfora+cÎ² is[a+bL,a+bU]. Exercise10.14 Youwanttotest(cid:72) 0 :Î¸=0against(cid:72) 1 :Î¸>0.Thetestfor(cid:72) 0 istorejectifT n =Î¸ (cid:98)/s(Î¸ (cid:98))>c wherec ispickedsothatTypeIerrorisÎ±.Youdothisasfollows. Usingthenonparametricbootstrap, you generate bootstrap samples, calculate the estimates Î¸ (cid:98) â on these samples and then calculate T â = Î¸ (cid:98) â /s(Î¸ (cid:98) â ). Let q 1 â âÎ± denote the 1âÎ±th quantile of T â . You replace c with q 1 â âÎ± , and thus reject (cid:72) 0 if T n =Î¸ (cid:98)/s(Î¸ (cid:98))>q 1 â âÎ± .Whatiswrongwiththisprocedure? Exercise10.15 Suppose that in an application, Î¸ (cid:98) =1.2 and s(Î¸ (cid:98))=0.2. Using the nonparametric boot- strap,1000samplesaregeneratedfromthebootstrapdistribution,andÎ¸ (cid:98) â iscalculatedoneachsample. TheÎ¸ (cid:98) â aresorted,andthe0.025th and0.975th quantilesoftheÎ¸ (cid:98) â are.75and1.3,respectively. (a) Reportthe95%percentileintervalforÎ¸. (c) Withthegiveninformation,canyoucalculatethe95%BCpercentileintervalorpercentile-tinter- valforÎ¸? Exercise10.16 TakethenormalregressionmodelY =X (cid:48)Î²+e withe |X â¼N(0,Ï2)whereweknowthe MLEequalstheleastsquaresestimatorsÎ² (cid:98)andÏ (cid:98) 2. (a) Describetheparametricregressionbootstrapforthismodel. Showthattheconditionaldistribu- tionofthebootstrapobservationsisY i â|F n â¼N (cid:161) X i (cid:48)Î² (cid:98),Ï (cid:98) 2(cid:162) .",
    "page": 322,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 303 (b) ShowthatthedistributionofthebootstrapleastsquaresestimatorisÎ² (cid:98) â|F n â¼N (cid:179) Î² (cid:98), (cid:161) X (cid:48) X (cid:162)â1Ï (cid:98) 2 (cid:180) . (c) Show that the distribution of the bootstrap t-ratio with a homoskedastic standard error is T â â¼ t nâk . Exercise10.17 ConsiderthemodelY =X (cid:48)Î²+e with(cid:69)[e|X]=0,Y scalar,and X ak vector. Youhave a random sample (Y ,X : i =1,...,n). You are interested in estimating the regression function m(x)= i i (cid:69)[Y |X =x]atafixedvectorxandconstructinga95%confidenceinterval. (a) Writedownthestandardestimatorandasymptoticconfidenceintervalform(x). (b) Describethepercentilebootstrapconfidenceintervalform(x). (c) Describethepercentile-tbootstrapconfidenceintervalform(x). Exercise10.18 Theobserveddatais{Y ,X }â(cid:82)Ã(cid:82)k,k>1,i =1,...,n.TakethemodelY =X (cid:48)Î²+e with i i (cid:69)[Xe]=0. (a) WritedownanestimatorforÂµ =(cid:69)(cid:163) e3(cid:164) . 3 (b) Explain how to use the percentile method to construct a 90% confidence interval for Âµ in this 3 specificmodel. Exercise10.19 TakethemodelY =X (cid:48)Î²+ewith(cid:69)[Xe]=0.Describethebootstrappercentileconfidence intervalforÏ2=(cid:69)(cid:163) e2(cid:164) . Exercise10.20 The model is Y = X (cid:48)Î² +X (cid:48)Î² +e with (cid:69)[Xe]=0 and X scalar. Describe how to test 1 1 2 2 2 (cid:72) :Î² =0against(cid:72) :Î² (cid:54)=0usingthenonparametricbootstrap. 0 2 1 2 Exercise10.21 ThemodelisY =X (cid:48)Î² +X (cid:48)Î² +ewith(cid:69)[Xe]=0,andbothX andX kÃ1.Describehow 1 1 2 2 1 2 totest(cid:72) :Î² =Î² against(cid:72) :Î² (cid:54)=Î² usingthenonparametricbootstrap. 0 1 2 1 1 2 Exercise10.22 SupposeaPh.D.studenthasasample(Y ,X ,Z :i =1,...,n)andestimatesbyOLSthe i i i equationY =ZÎ±+X (cid:48)Î²+e whereÎ±isthecoefficientofinterest. Sheisinterestedintesting(cid:72) :Î±=0 0 against(cid:72) :Î±(cid:54)=0. SheobtainsÎ±=2.0withstandarderrors(Î±)=1.0sothevalueofthet-ratiofor(cid:72) is 1 (cid:98) (cid:98) 0 T =Î±/s(Î±)=2.0.Toassesssignificance,thestudentdecidestousethebootstrap.Sheusesthefollowing (cid:98) (cid:98) algorithm â â â 1. Samples (Y ,X ,Z ) randomly from the observations. (Random sampling with replacement). i i i Createsarandomsamplewithnobservations. 2. Onthispseudo-sample,estimatestheequationY â=Z âÎ±+X â(cid:48)Î²+e â byOLSandcomputesstan- i i i i darderrors,includings(Î±â ).Thet-ratiofor(cid:72) ,T â=Î±â /s(Î±â )iscomputedandstored. (cid:98) 0 (cid:98) (cid:98) 3. ThisisrepeatedB=10,000times. 4. The0.95th empiricalquantileq â =3.5ofthebootstrapabsolutet-ratios|T â|iscomputed. .95 5. Thestudentnotesthatwhile|T|=2>1.96(andthusanasymptotic5%sizetestrejects(cid:72) ),|T|= 0 2<q â =3.5andthusthebootstraptestdoesnotreject(cid:72) .Asthebootstrapismorereliable,the .95 0 studentconcludesthat(cid:72) cannotberejectedinfavorof(cid:72) . 0 1",
    "page": 323,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 304 Question:Doyouagreewiththestudentâsmethodandreasoning?Doyouseeanerrorinhermethod? Exercise10.23 TakethemodelY =X Î² +X Î² +e with(cid:69)[Xe]=0andscalar X and X . Theparam- 1 1 2 2 1 2 eterofinterestisÎ¸=Î² Î² .ShowhowtoconstructaconfidenceintervalforÎ¸ usingthefollowingthree 1 2 methods. (a) AsymptoticTheory. (b) PercentileBootstrap. (c) Percentile-tBootstrap. Youranswershouldbespecifictothisproblem,notgeneral. Exercise10.24 TakethemodelY =X Î² +X Î² +e withi.i.dobservations,(cid:69)[Xe]=0andscalarX and 1 1 2 2 1 X .Describehowyouwouldconstructthepercentile-tbootstrapconfidenceintervalforÎ¸=Î² /Î² . 2 1 2 Exercise10.25 The model isi.i.d. data, i =1,...,n, Y = X (cid:48)Î²+e and(cid:69)[e|X]=0. Doesthe presence of conditionalheteroskedasticityinvalidatetheapplicationofthenonparametricbootstrap?Explain. Exercise10.26 TheRESETspecificationtestfornonlinearityinarandomsample(duetoRamsey(1969)) isthefollowing.ThenullhypothesisisalinearregressionY =X (cid:48)Î²+ewith(cid:69)[e|X]=0.TheparameterÎ²is estimatedbyOLSyieldingpredictedvaluesY(cid:98)i .Thenasecond-stageleastsquaresregressionisestimated includingbothX i andY(cid:98)i Y i =X i (cid:48)Î² (cid:101) +(cid:161) Y(cid:98)i (cid:162)2Î³ (cid:101) +e (cid:101)i TheRESETteststatisticR isthesquaredt-ratioonÎ³. (cid:101) A colleague suggests obtaining the critical value for the test using the bootstrap. He proposes the followingbootstrapimplementation. â â â¢ Drawn observations(Y ,X )randomlyfromtheobservedsamplepairs(Y ,X )tocreateaboot- i i i i strapsample. â â¢ ComputethestatisticR onthisbootstrapsampleasdescribedabove. â¢ RepeatthisB times. SortthebootstrapstatisticsR â ,takethe0.95th quantileandusethisasthe criticalvalue. â¢ RejectthenullhypothesisifR exceedsthiscriticalvalue,otherwisedonotreject. Isthisprocedureacorrectimplementationofthebootstrapinthiscontext?Ifnot,proposeamodifi- cation. Exercise10.27 ThemodelisY =X (cid:48)Î²+ewith(cid:69)[Xe](cid:54)=0.Weknowthatinthiscase,theleastsquaresesti- matormaybebiasedfortheparameterÎ².WealsoknowthatthenonparametricBCpercentileintervalis (generally)agoodmethodforconfidenceintervalconstructioninthepresenceofbias.Explainwhether or not you expect the BC percentile interval applied to the least squares estimator will have accurate coverageinthiscontext. Exercise10.28 InExercise9.26youestimatedacostfunctionfor145electriccompaniesandtestedthe restrictionÎ¸=Î² +Î² +Î² =1. 3 4 5",
    "page": 324,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER10. RESAMPLINGMETHODS 305 (a) Estimate the regression by unrestricted least squares and report standard errors calculated by asymptotic,jackknifeandthebootstrap. (b) EstimateÎ¸=Î² +Î² +Î² andreportstandarderrorscalculatedbyasymptotic, jackknifeandthe 3 4 5 bootstrap. (c) ReportconfidenceintervalsforÎ¸usingthepercentileandBC methods. a Exercise10.29 InExercise9.27youestimatedtheMankiw,Romer,andWeil(1992)unrestrictedregres- sion.LetÎ¸bethesumofthesecond,third,andfourthcoefficients. (a) Estimate the regression by unrestricted least squares and report standard errors calculated by asymptotic,jackknifeandthebootstrap. (b) EstimateÎ¸andreportstandarderrorscalculatedbyasymptotic,jackknifeandthebootstrap. (c) ReportconfidenceintervalsforÎ¸usingthepercentileandBCmethods. Exercise10.30 InExercise7.28youestimatedawageregressionwiththecps09mardatasetandthesub- sampleofwhiteMaleHispanics.Furtherrestrictthesampletothosenever-marriedandliveintheMid- westregion. (Thissamplehas99observations.) Asinsubquestion(b)letÎ¸betheratioofthereturnto oneyearofeducationtothereturnofoneyearofexperience. (a) EstimateÎ¸andreportstandarderrorscalculatedbyasymptotic,jackknifeandthebootstrap. (b) Explainthediscrepancybetweenthestandarderrors. (c) ReportconfidenceintervalsforÎ¸usingtheBCpercentilemethod. Exercise10.31 In Exercise 4.26 you extended the work from Duflo, Dupas and Kremer (2011). Repeat thatregression,nowcalculatingthestandarderrorbyclusterbootstrap.ReportaBC confidenceinterval a foreachcoefficient.",
    "page": 325,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Part III Multiple Equation Models 306",
    "page": 326,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Chapter 11 Multivariate Regression 11.1 Introduction Multivariateregression is a system of regression equations. Multivariate regression is used as re- ducedformmodelsforinstrumental variable estimation(Chaper 12), vectorautoregressions(Chapter 15),demandsystems(demandformultiplegoods),andothercontexts. Multivariateregressionisalsocalledbythenamesystemsofregressionequations.Closelyrelatedis themethodofSeeminglyUnrelatedRegressions(SUR)introducedinSection11.7. Mostofthetoolsofsingleequationregressiongeneralizetomultivariateregression. Amajordiffer- enceisanewsetofnotationtohandlematrixestimators. 11.2 RegressionSystems AunivariatelinearregressionequationequalsY =X (cid:48)Î²+ewhereY isscalarandX isavector.Multi- variateregressionisasystemofmlinearregressions,andequals Y =X (cid:48)Î² +e (11.1) j j j j for j =1,...,m. Hereweusethesubscript j todenotethe jth dependentvariable,nottheith individual. Asanexample,Y couldbeexpendituresbyahouseholdongoodcategory j (e.g.,food,housing,trans- j portation, clothing, recreation). The regressor vectors X are k Ã1 and e is an error. The coefficient j j j vectorsÎ² arek Ã1. Thetotalnumberofcoefficientsarek =(cid:80)m k . Theregressorscanbecommon j j j=1 j across j orcanvaryacross j.InthehouseholdexpenditureexampletheregressorsX aretypicallycom- j monacross j, andincludevariablessuchashouseholdincome, numberandagesoffamilymembers, anddemographiccharacteristics.Theregressionsystemspecializestounivariateregressionwhenm=1. DefinethemÃ1errorvectore=(e ,...,e ) (cid:48) anditsmÃmcovariancematrixÎ£=(cid:69)(cid:163) ee (cid:48)(cid:164) .Thediagonal 1 m elementsarethevariancesoftheerrorse andtheoff-diagonalsarethecovariancesacrossvariables. j We can group the m equations (11.1) into a single equation as follows. Let Y =(Y ,...,Y ) (cid:48) be the 1 m mÃ1vectorofdependentvariables.DefinethemÃk matrixofregressors ï£« X (cid:48) 0 Â·Â·Â· 0 ï£¶ 1 X =ï£¬ ï£­ . . . X (cid:48) . . . ï£· ï£¸ 2 0 0 Â·Â·Â· X (cid:48) m 307",
    "page": 327,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER11. MULTIVARIATEREGRESSION 308 andthekÃ1stackedcoefficientvector ï£« Î² ï£¶ 1 Î²=ï£¬ ï£­ . . . ï£· ï£¸ . Î² m Themregressionequationscanbejointlywrittenas Y =XÎ²+e. (11.2) Thisisasystemofmequations. Fornobservationsthejointsystemcanbewritteninmatrixnotationbystacking.Define ï£« ï£¶ ï£« ï£¶ ï£« ï£¶ Y e X 1 1 1 Y =ï£¬ ï£­ . . . ï£· ï£¸ , e=ï£¬ ï£­ . . . ï£· ï£¸ , X =ï£¬ ï£¬ . . . ï£· ï£· ï£­ ï£¸ Y e X n n n whicharemnÃ1, mnÃ1,andmnÃk,respectively.ThesystemcanbewrittenasY =XÎ²+e. In many applications the regressor vectors X are common across the variables j, so X = X and j j k = k. By this we mean that the same variables enter each equation with no exclusion restrictions. j Severalimportantsimplificationsoccurinthiscontext.Oneisthatwecanwrite(11.2)usingthenotation Y =B (cid:48) X+e (11.3) whereB =(cid:161)Î² ,Î² ,Â·Â·Â·,Î² (cid:162) iskÃm. Anotheristhatwecanwritethejointsystemofobservationsinthe 1 2 m nÃmmatrixnotationY =XB+E where ï£« (cid:48) ï£¶ ï£« (cid:48) ï£¶ ï£« (cid:48) ï£¶ Y e X 1 1 1 Y =ï£¬ ï£­ . . . ï£· ï£¸ , E=ï£¬ ï£­ . . . ï£· ï£¸ , X =ï£¬ ï£­ . . . ï£· ï£¸ . (cid:48) (cid:48) (cid:48) Y e X n n n Anotherconvenientimplicationofcommonregressorsisthatwehavethesimplification ï£« X (cid:48) 0 Â·Â·Â· 0 ï£¶ ï£¬ 0 X (cid:48) 0 ï£· X =ï£¬ ï£¬ ï£¬ . . . . . . . . . ï£· ï£· ï£· =I m âX (cid:48) ï£­ ï£¸ 0 0 Â·Â·Â· X (cid:48) whereâistheKroneckerproduct(seeAppendixA.21). 11.3 LeastSquaresEstimator Theequations(11.1)canbeestimatedbyleastsquares.Thistakestheform (cid:195) (cid:33)â1(cid:195) (cid:33) n n Î² (cid:98)j = (cid:88) X ji X j (cid:48) i (cid:88) X ji Y ji . i=1 i=1 AnestimatorofÎ²isthestackedvector ï£« Î² ï£¶ (cid:98)1 Î² (cid:98) =ï£¬ ï£­ . . . ï£· ï£¸ . Î² (cid:98)m",
    "page": 328,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER11. MULTIVARIATEREGRESSION 309 Wecanalternativelywritethisestimatorusingthesystemsnotation (cid:195) (cid:33)â1(cid:195) (cid:33) (cid:179) (cid:48) (cid:180)â1(cid:179) (cid:48) (cid:180) (cid:88) n (cid:48) (cid:88) n (cid:48) Î² (cid:98) = X X X Y = X i X i X i Y i . (11.4) i=1 i=1 Toseethis,observethat ï£« ï£¶ X 1 X (cid:48) X = (cid:179) X (cid:48) 1 Â·Â·Â· X (cid:48) n (cid:180)ï£¬ ï£¬ ï£­ . . . ï£· ï£· ï£¸ X n (cid:88) n (cid:48) = X X i i i=1 ï£« X 0 Â·Â·Â· 0 ï£¶ï£« X (cid:48) 0 Â·Â·Â· 0 ï£¶ n 1i 1i = (cid:88) ï£¬ . . . . ï£·ï£¬ . . (cid:48) . . ï£· i=1 ï£­ 0 . X 0 2i Â·Â·Â· X . ï£¸ï£­ 0 . X 0 2i Â·Â·Â· X . (cid:48) ï£¸ mi mi ï£« (cid:80)n X X (cid:48) 0 Â·Â·Â· 0 ï£¶ i=1 1i 1i =ï£¬ ï£­ . . . (cid:80)n i=1 X 2i X 2 (cid:48) i . . . ï£· ï£¸ 0 0 Â·Â·Â· (cid:80)n X X (cid:48) i=1 mi mi and ï£« ï£¶ Y 1 X (cid:48) Y = (cid:179) X (cid:48) 1 Â·Â·Â· X (cid:48) n (cid:180) ï£¬ ï£­ . . . ï£· ï£¸ Y n (cid:88) n (cid:48) = X Y i i i=1 ï£« X 0 Â·Â·Â· 0 ï£¶ï£« Y ï£¶ 1i 1i n = (cid:88) ï£¬ . . . . ï£·ï£¬ . . ï£· ï£­ . X 2i . ï£¸ï£­ . ï£¸ i=1 0 0 Â·Â·Â· X Y mi mi ï£« (cid:80)n X Y ï£¶ i=1 1i 1i =ï£¬ ï£­ . . . ï£· ï£¸ . (cid:80)n X Y i=1 mi mi Hence (cid:195) (cid:33)â1(cid:195) (cid:33) (cid:179) (cid:48) (cid:180)â1(cid:179) (cid:48) (cid:180) (cid:88) n (cid:48) (cid:88) n X X X Y = X X X Y i i i i i=1 i=1 ï£« (cid:161)(cid:80)n X X (cid:48) (cid:162)â1(cid:161)(cid:80)n X Y (cid:162) ï£¶ i=1 1i 1i i=1 1i 1i =ï£¬ ï£¬ . . . ï£· ï£· ï£­ ï£¸ (cid:161)(cid:80)n X X (cid:48) (cid:162)â1(cid:161)(cid:80)n X Y (cid:162) i=1 mi mi i=1 mi mi =Î² (cid:98) asclaimed.",
    "page": 329,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER11. MULTIVARIATEREGRESSION 310 (cid:48) ThemÃ1residualvectorfortheith observationise (cid:98)i =Y i âX i Î² (cid:98). Theleastsquaresestimatorofthe mÃmerrorcovariancematrixis Î£ (cid:98) = n 1 (cid:88) n e (cid:98)i e (cid:98)i (cid:48) . (11.5) i=1 Inthecaseofcommonregressors,theleastsquarescoefficientscanbewrittenas (cid:195) (cid:33)â1(cid:195) (cid:33) n n Î² (cid:98)j = (cid:88) X i X i (cid:48) (cid:88) X i Y ji i=1 i=1 and B(cid:98) =(cid:161)Î² (cid:98)1 ,Î² (cid:98)2 ,Â·Â·Â·,Î² (cid:98)m (cid:162)=(cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48) Y (cid:162) . (11.6) InStata,multivariateregressioncanbeimplementedusingthemvregcommand. 11.4 MeanandVarianceofSystemsLeastSquares Wecancalculatethefinite-samplemeanandvarianceofÎ² (cid:98)undertheconditionalmeanassumption (cid:69)[e|X]=0 (11.7) whereX istheunionoftheregressorsX .Equation(11.7)isequivalentto(cid:69)(cid:165) Y |X (cid:166)=X (cid:48)Î² ,whichmeans j j j j thattheregressionmodeliscorrectlyspecified. Wecancentertheestimatoras (cid:195) (cid:33)â1(cid:195) (cid:33) (cid:179) (cid:48) (cid:180)â1(cid:179) (cid:48) (cid:180) (cid:88) n (cid:48) (cid:88) n (cid:48) Î² (cid:98) âÎ²= X X X e = X i X i X i e i . i=1 i=1 Takingconditionalexpectationswefind(cid:69)(cid:163)Î² (cid:98) |X (cid:164)=Î². Consequently,systemsleastsquaresisunbiased undercorrectspecification. Tocomputethevarianceoftheestimator,definetheconditionalcovariancematrixoftheerrorsof theith observation(cid:69)(cid:163) e e (cid:48) |X (cid:164)=Î£ whichingeneralisafunctionofX .Iftheobservationsaremutually i i i i i independentthen ï£®ï£« e 1 e 1 (cid:48) e 1 e 2 (cid:48) Â·Â·Â· e 1 e n (cid:48) ï£¶(cid:175) (cid:175) ï£¹ ï£« Î£ 1 0 Â·Â·Â· 0 ï£¶ (cid:69)(cid:163) ee (cid:48)|X (cid:164)=(cid:69)ï£¯ ï£° ï£¬ ï£­ . . . ... . . . ï£· ï£¸ (cid:175) (cid:175) (cid:175) Xï£º ï£» =ï£¬ ï£­ . . . ... . . . ï£· ï£¸ . e e (cid:48) e e (cid:48) Â·Â·Â· e e (cid:48) (cid:175) (cid:175) 0 0 Â·Â·Â· Î£ n 1 n 2 n n n Also,byindependenceacrossobservations, (cid:34) (cid:175) (cid:35) (cid:88) n (cid:48) (cid:175) (cid:88) n (cid:104) (cid:48) (cid:175) (cid:105) (cid:88) n (cid:48) var X e (cid:175)X = var X e (cid:175)X = X Î£ X . i i(cid:175) i i(cid:175) i i i i i=1 (cid:175) i=1 i=1 Itfollowsthat (cid:195) (cid:33) var (cid:163)Î² (cid:98) |X (cid:164)= (cid:179) X (cid:48) X (cid:180)â1 (cid:88) n X (cid:48) i Î£ i X i (cid:179) X (cid:48) X (cid:180)â1 . i=1 WhentheregressorsarecommonsothatX =I âX (cid:48) thenthecovariancematrixcanbewrittenas i m i (cid:195) (cid:33) var (cid:163)Î² (cid:98) |X (cid:164)= (cid:179) I m â(cid:161) X (cid:48) X (cid:162)â1 (cid:180) (cid:88) n (cid:161)Î£ i âX i X i (cid:48)(cid:162) (cid:179) I m â(cid:161) X (cid:48) X (cid:162)â1 (cid:180) . i=1",
    "page": 330,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER11. MULTIVARIATEREGRESSION 311 Iftheerrorsareconditionallyhomoskedastic (cid:69)(cid:163) ee (cid:48)|X (cid:164)=Î£ (11.8) thenthecovariancematrixsimplifiesto (cid:195) (cid:33) var (cid:163)Î² (cid:98) |X (cid:164)= (cid:179) X (cid:48) X (cid:180)â1 (cid:88) n X (cid:48) i Î£X i (cid:179) X (cid:48) X (cid:180)â1 . i=1 If both simplifications (common regressors and conditional homoskedasticity) hold then we have theconsiderablesimplication var (cid:163)Î² (cid:98) |X (cid:164)=Î£â(cid:161) X (cid:48) X (cid:162)â1 . 11.5 AsymptoticDistribution Foranasymptoticdistributionitissufficienttoconsidertheequation-by-equationprojectionmodel inwhichcase (cid:69)(cid:163) X e (cid:164)=0. (11.9) j j First,considerconsistency.SinceÎ² (cid:98)j arethestandardleastsquaresestimators,theyareconsistentfor theprojectioncoefficientsÎ² . j Second, consider the asymptotic distribution. Our single equation theory implies that the Î² (cid:98)j are asymptoticallynormal.ButthistheorydoesnotprovideajointdistributionoftheÎ² (cid:98)j across j,whichwe nowderive.Sincethevector ï£« ï£¶ X e 1i 1i X (cid:48) i e i =ï£¬ ï£­ . . . ï£· ï£¸ X e mi mi isi.i.d.acrossi andmeanzerounder(11.9),thecentrallimittheoremimplies 1 (cid:88) n (cid:48) (cid:112) X e ââN(0,â¦) i i n i=1 d where â¦=(cid:69) (cid:104) X (cid:48) e e (cid:48) X (cid:105) =(cid:69) (cid:104) X (cid:48) Î£ X (cid:105) . i i i i i i i The matrix â¦ is the covariance matrix of the variables X e across equations. Under conditional ji ji homoskedasticity(11.8)thematrixâ¦simplifiesto (cid:104) (cid:48) (cid:105) â¦=(cid:69) X Î£X (11.10) i i (seeExercise11.1).Whentheregressorsarecommonitsimpliesto â¦=(cid:69)(cid:163) ee (cid:48)âXX (cid:48)(cid:164) (11.11) (seeExercise11.2).Underbothconditions(homoskedasticityandcommonregressors)itsimplifiesto â¦=Î£â(cid:69)(cid:163) XX (cid:48)(cid:164) (11.12) (seeExercise11.3). Appliedtothecenteredandnormalizedestimatorweobtaintheasymptoticdistribution.",
    "page": 331,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER11. MULTIVARIATEREGRESSION 312 (cid:112) Theorem11.1 Under Assumption 7.2, n (cid:161)Î² (cid:98) âÎ²(cid:162) ââ N (cid:161) 0,VÎ² (cid:162) where VÎ² = d Q â1â¦Q â1and ï£« (cid:69)(cid:163) X X (cid:48)(cid:164) 0 Â·Â·Â· 0 ï£¶ 1 1 Q=(cid:69) (cid:104) X (cid:48) X (cid:105) =ï£¬ ï£­ . . . ... . . . ï£· ï£¸ . 0 0 Â·Â·Â· (cid:69)(cid:163) X X (cid:48) (cid:164) m m Foraproof,seeExercise11.4. WhentheregressorsarecommonthematrixQ simplifiesas Q=I â(cid:69)(cid:163) XX (cid:48)(cid:164) (11.13) m (SeeExercise11.5). If both the regressors are common and the errors are conditionally homoskedastic (11.8) then we havethesimplification VÎ² =Î£â(cid:161)(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:162)â1 (11.14) (seeExercise11.6). SometimesweareinterestedinparametersÎ¸=r(Î² ,...,Î² )=r(Î²)whicharefunctionsofthecoeffi- 1 m cientsfrommultipleequations. InthiscasetheleastsquaresestimatorofÎ¸isÎ¸ (cid:98) =r(Î² (cid:98)). Theasymptotic distributionofÎ¸ (cid:98)canbeobtainedfromTheorem11.1bythedeltamethod. (cid:112) Theorem11.2 UnderAssumptions7.2and7.3, n (cid:161)Î¸ (cid:98) âÎ¸(cid:162)ââN(0,VÎ¸)where d VÎ¸ =R (cid:48) VÎ²R andR= â â Î² r (cid:161)Î²(cid:162)(cid:48) . Foraproof,seeExercise11.7. Theorem11.2isanexamplewheremultivariateregressionisfundamentallydistinctfromunivariate regression. Onlybytreatingleastsquaresasajointestimatorcanweobtainadistributionaltheoryfor afunctionofmultipleequations. Wecantherebyconstructstandarderrors, confidenceintervals, and hypothesistests. 11.6 CovarianceMatrixEstimation Fromthefinitesampleandasymptotictheorywecanconstructappropriateestimatorsforthevari- anceofÎ² (cid:98).Inthegeneralcasewehave (cid:195) (cid:33) V(cid:98)Î²(cid:98) = (cid:179) X (cid:48) X (cid:180)â1 (cid:88) n X (cid:48) i e (cid:98)i e (cid:98)i (cid:48) X i (cid:179) X (cid:48) X (cid:180)â1 . i=1 Underconditionalhomoskedasticity(11.8)anappropriateestimatoris (cid:195) (cid:33) V(cid:98) 0 Î²(cid:98) = (cid:179) X (cid:48) X (cid:180)â1 (cid:88) n X (cid:48) i Î£ (cid:98)X i (cid:179) X (cid:48) X (cid:180)â1 . i=1",
    "page": 332,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER11. MULTIVARIATEREGRESSION 313 Whentheregressorsarecommonthentheseestimatorsequal (cid:195) (cid:33) V(cid:98)Î²(cid:98) = (cid:179) I m â(cid:161) X (cid:48) X (cid:162)â1 (cid:180) (cid:88) n (cid:161) e (cid:98)i e (cid:98)i (cid:48)âX i X i (cid:48)(cid:162) (cid:179) I m â(cid:161) X (cid:48) X (cid:162)â1 (cid:180) i=1 andV(cid:98) 0 Î²(cid:98) =Î£ (cid:98) â(cid:161) X (cid:48) X (cid:162)â1 ,respectively. CovariancematrixestimatorsforÎ¸ (cid:98)arefoundas (cid:48) V(cid:98)Î¸(cid:98) =R(cid:98) V(cid:98)Î²(cid:98) R(cid:98) V(cid:98) 0 Î¸(cid:98) =R(cid:98) (cid:48) V(cid:98) 0 Î²(cid:98) R(cid:98) â R(cid:98) = r (cid:161)Î² (cid:98) (cid:162)(cid:48) . âÎ² Theorem11.3 UnderAssumption7.2,nV(cid:98)Î²(cid:98) â p âVÎ²andnV(cid:98) 0 Î²(cid:98) â p âV0 Î² . Foraproof,seeExercise11.8. 11.7 SeeminglyUnrelatedRegression Considerthesystemsregressionmodelundertheconditionalmeanandconditionalhomoskedas- ticityassumptions Y =XÎ²+e (11.15) (cid:69)[e|X]=0 (cid:69)(cid:163) ee (cid:48)|X (cid:164)=Î£. Since the errors are correlated across equations we consider estimation by Generalized Least Squares (GLS).Toderivetheestimator,premultiply(11.15)byÎ£â1/2 sothatthetransformederrorvectorisi.i.d. withcovariancematrixI .Thenapplyleastsquaresandrearrangetofind m (cid:195) (cid:33)â1(cid:195) (cid:33) Î² (cid:98)gls = (cid:88) n X (cid:48) i Î£â1X i (cid:88) n X (cid:48) i Î£â1Y i . (11.16) i=1 i=1 (seeExercise11.9).Anotherapproachistotakethevectorrepresentation Y =XÎ²+e andcalculatethattheequationerrore hasvariance(cid:69)(cid:163) ee (cid:48)(cid:164)=I âÎ£. Premultiplytheequationby I â n n Î£â1/2sothatthetransformederrorhascovariancematrixI andthenapplyleastsquarestofind nm Î² (cid:98)gls = (cid:179) X (cid:48)(cid:161) I n âÎ£â1(cid:162) X (cid:180)â1(cid:179) X (cid:48)(cid:161) I n âÎ£â1(cid:162) Y (cid:180) (11.17) (seeExercise11.10).",
    "page": 333,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER11. MULTIVARIATEREGRESSION 314 Expressions(11.16)and(11.17)arealgebraicallyequivalent.Toseetheequivalence,observethat ï£« Î£â1 0 Â·Â·Â· 0 ï£¶ ï£« X ï£¶ 1 X (cid:48)(cid:161) I n âÎ£â1(cid:162) X = (cid:179) X (cid:48) 1 Â·Â·Â· X (cid:48) n (cid:180) ï£¬ ï£­ . . . Î£â1 . . . ï£· ï£¸ ï£¬ ï£¬ ï£­ . . . ï£· ï£· ï£¸ 0 0 Â·Â·Â· Î£â1 X n = (cid:88) n X (cid:48) Î£â1X i i i=1 and ï£« Î£â1 0 Â·Â·Â· 0 ï£¶ï£« Y ï£¶ 1 X (cid:48)(cid:161) I n âÎ£â1(cid:162) Y = (cid:179) X (cid:48) 1 Â·Â·Â· X (cid:48) n (cid:180) ï£¬ ï£­ . . . Î£â1 . . . ï£· ï£¸ ï£¬ ï£­ . . . ï£· ï£¸ 0 0 Â·Â·Â· 0 â1 Y n = (cid:88) n X (cid:48) Î£â1Y . i i i=1 SinceÎ£isunknownitmustbereplacedbyanestimator.UsingÎ£ (cid:98)from(11.5)weobtainafeasibleGLS estimator. (cid:195) (cid:33)â1(cid:195) (cid:33) Î² (cid:98)sur = (cid:88) n X (cid:48) i Î£ (cid:98) â1X i (cid:88) n X (cid:48) i Î£ (cid:98) â1Y i i=1 i=1 = (cid:179) X (cid:48)(cid:161) I n âÎ£ (cid:98) â1(cid:162) X (cid:180)â1(cid:179) X (cid:48)(cid:161) I n âÎ£ (cid:98) â1(cid:162) Y (cid:180) . (11.18) ThisistheSeeminglyUnrelatedRegression(SUR)estimatorasintroducedbyZellner(1962). (cid:48) TheestimatorÎ£ (cid:98)canbeupdatedbycalculatingtheSURresidualse (cid:98)i =Y i âX i Î² (cid:98)sur andthecovariance matrixestimatorÎ£ (cid:98) = n 1(cid:80)n i=1 e (cid:98)i e (cid:98)i (cid:48) . Substitutedinto(11.18)weobtainaniteratedSURestimator. Thiscan beiterateduntilconvergence. Underconditionalhomoskedasticity(11.8)wecanderiveitsasymptoticdistribution. Theorem11.4 UnderAssumption7.2and(11.8) (cid:112) (cid:179) (cid:180) n (cid:161)Î² (cid:98)sur âÎ²(cid:162)ââN 0,V â Î² d whereV â= (cid:179) (cid:69) (cid:104) X (cid:48) Î£â1X (cid:105)(cid:180)â1 . Î² Foraproof,seeExercise11.11. UndertheseassumptionsSURismoreefficientthanleastsquares. Theorem11.5 UnderAssumption7.2and(11.8) V â Î² = (cid:179) (cid:69) (cid:104) X (cid:48) Î£â1X (cid:105)(cid:180)â1 â¤ (cid:179) (cid:69) (cid:104) X (cid:48) X (cid:105)(cid:180)â1 (cid:69) (cid:104) X (cid:48) Î£X (cid:105)(cid:179) (cid:69) (cid:104) X (cid:48) X (cid:105)(cid:180)â1 =VÎ² andthusÎ² (cid:98)sur isasymptoticallymoreefficientthanÎ² (cid:98)ols .",
    "page": 334,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER11. MULTIVARIATEREGRESSION 315 Foraproof,seeExercise11.12. AnappropriateestimatorofthevarianceofÎ² (cid:98)sur is (cid:195) (cid:33)â1 V(cid:98)Î²(cid:98) = (cid:88) n X (cid:48) i Î£ (cid:98) â1X i . i=1 Theorem11.6 UnderAssumption7.2and(11.8)nV(cid:98)Î²(cid:98) â p âVÎ². Foraproof,seeExercise11.13. InStata,theseeminglyunrelatedregressionsestimatorisimplementedusingthesuregcommand. ArnoldZellner Arnold Zellner (1927-2010) of the United States was a founding father of the econometricsfield. HewasapioneerinBayesianeconometrics. Oneofhiscore contributionswasthemethodofSeeminglyUnrelatedRegressions. 11.8 EquivalenceofSURandLeastSquares Whentheregressorsarecommonacrossequations X =X itturnsoutthattheSURestimatorsim- j plifiestoleastsquares. Toseethis,recallthatwhenregressorsarecommonthisimpliesthatX =I âX (cid:48) .Then m X (cid:48) i Î£ (cid:98) â1=(I m âX i )Î£ (cid:98) â1 =Î£ (cid:98) â1âX i =(cid:161)Î£ (cid:98) â1âI k (cid:162) (I m âX i ) =(cid:161)Î£ (cid:98) â1âI k (cid:162) X (cid:48) i . Thus (cid:195) (cid:33)â1(cid:195) (cid:33) Î² (cid:98)sur = (cid:88) n X (cid:48) i Î£ (cid:98) â1X i (cid:88) n X (cid:48) i Î£ (cid:98) â1Y i i=1 i=1 (cid:195) (cid:33)â1(cid:195) (cid:33) = (cid:161)Î£ (cid:98) â1âI k (cid:162)(cid:88) n X (cid:48) i X i (cid:161)Î£ (cid:98) â1âI k (cid:162)(cid:88) n X (cid:48) i Y i i=1 i=1 (cid:195) (cid:33)â1(cid:195) (cid:33) (cid:88) n (cid:48) (cid:88) n (cid:48) = X i X i X i Y i =Î² (cid:98)ols . i=1 i=1 Amodelwhereregressorsarenotcommonacrossequationsisnestedwithinamodelwiththeunion of all regressors included in all equations. Thus the model with regressors common across equations",
    "page": 335,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER11. MULTIVARIATEREGRESSION 316 is a fully unrestricted model, and a model where the regressors differ across equations is a restricted model. ThustheaboveresultshowsthattheSURestimatorreducestoleastsquaresintheabsenceof restrictions,butSURcandifferfromleastsquaresotherwise. Another context where SUR=OLS is when the variance matrix is diagonal, Î£=diag (cid:169)Ï2,...,Ï2 (cid:170) . In 1 m this case Î£â1/2X i = X i diag (cid:169) I k1 Ïâ 1 1/2,...,I km Ïâ m 1/2(cid:170) from which you can calculate that Î² (cid:98)sur =Î² (cid:98)ols . The intuitionisthatthereisnodifferenceinsystemsestimationwhentheequationsareuncorrelated,which occurswhenÎ£isdiagonal. 11.9 MaximumLikelihoodEstimator Takethelinearmodelundertheassumptionthattheerrorisindependentoftheregressorsandmul- tivariatenormallydistributed. ThusY =XÎ²+e witheâ¼N(0,Î£). Inthiscasewecanconsiderthemaxi- mumlikelihoodestimator(MLE)ofthecoefficients. It is convenient to reparameterize the covariance matrix in terms of its inverse S =Î£â1. With this reparameterizationtheconditionaldensityofY givenX =xequals f (cid:161) y|x (cid:162)= det(S)1/2 exp (cid:181) â 1(cid:161) yâxÎ²(cid:162)(cid:48) S (cid:161) yâxÎ²(cid:162) (cid:182) . (2Ï)m/2 2 Thelog-likelihoodfunctionforthesampleis nm n 1 (cid:88) n (cid:179) (cid:180)(cid:48) (cid:179) (cid:180) (cid:96) (Î²,S)=â log(2Ï)+ log(det(S))â Y âX Î² S Y âX Î² . n i i i i 2 2 2 i=1 Themaximumlikelihoodestimator (cid:161)Î² (cid:98)mle ,S(cid:98)mle (cid:162) maximizesthelog-likelihoodfunction. Thefirstor- derconditionsare 0= â â Î² (cid:96) n (Î²,S) (cid:175) (cid:175) (cid:175) (cid:175) = (cid:88) n X i S(cid:98) (cid:179) Y i âX i Î² (cid:98) (cid:180) Î²=Î²(cid:98),S=S(cid:98) i=1 and 0= â â S (cid:96) n (Î²,Î£) (cid:175) (cid:175) (cid:175) (cid:175)Î²=Î²(cid:98),S=S(cid:98) = n 2 S(cid:98) â1â 1 2 tr (cid:195) i (cid:88) = n 1 (cid:179) Y i âX i Î² (cid:98) (cid:180)(cid:179) Y i âX i Î² (cid:98) (cid:180)(cid:48) (cid:33) . The second equation uses the matrix results â log(det(S)) = S â1 and â tr(AB) = A (cid:48) from Appendix âS âB A.20. SolvingandmakingthesubstitutionÎ£ (cid:98) =S(cid:98) â1 weobtain (cid:195) (cid:33)â1(cid:195) (cid:33) Î² (cid:98)mle = (cid:88) n X (cid:48) i Î£ (cid:98) â1X i (cid:88) n X (cid:48) i Î£ (cid:98) â1Y i i=1 i=1 1 (cid:88) n (cid:179) (cid:180)(cid:179) (cid:180)(cid:48) Î£ (cid:98)mle = Y i âX i Î² (cid:98) Y i âX i Î² (cid:98) . n i=1 Noticethateachequationreferstotheother. Hencethesearenotclosed-formexpressionsbutcanbe solvedviaiteration. ThesolutionisidenticaltotheiteratedSURestimator. ThustheiteratedSUResti- matorisidenticaltoMLEundernormality. RecallthattheSURestimatorsimplifiestoOLSwhentheregressorsarecommonacrossequations. ThesameoccursfortheMLE.ThuswhenX i =I m âX i (cid:48) wefindthatÎ² (cid:98)mle =Î² (cid:98)ols andÎ£ (cid:98)mle =Î£ (cid:98)ols .",
    "page": 336,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER11. MULTIVARIATEREGRESSION 317 11.10 RestrictedEstimation Inmanymultivariateregressionapplicationsitisdesiredtoimposerestrictionsonthecoefficients. Inparticular,cross-equationrestrictions(forexample,imposingSlutskysymmetryonademandsystem) canbequiteimportantandcanonlybeimposedbyamultivariateestimationmethod. Estimationsub- jecttorestrictionscanbedonebyminimumdistance,maximumlikelihood,orthegeneralizedmethod ofmoments. Minimum distance is a straightforward application of the methods of Chapter 8 to the estimators presentedinthischapter,sincesuchmethodsapplytoanyasymptoticallynormalestimator. Imposingrestrictionsonmaximumlikelihoodisalsostraightforward. Thelikelihoodismaximized subjecttotheimposedrestrictions.Oneimportantexampleisexploredindetailinthefollowingsection. Generalizedmethodofmomentsestimationofmultivariateregressionsubjecttorestrictionswillbe explored in Section 13.18. This is a particularly simple and straightforward way to estimate restricted multivariateregressionmodelsandisourgenerallypreferredapproach. 11.11 ReducedRankRegression Onecontextwheresystemsestimationisimportantiswhenitisdesiredtoimposeortestrestrictions acrossequations. Restrictedsystemsarecommonlyestimatedbymaximumlikelihoodundernormal- ity. In this section we explore one important special case of restricted multivariate regression known as reduced rank regression. The model was originally proposed by Anderson (1951) and extended by Johansen(1995). Theunrestrictedmodelis Y =B (cid:48) X+C (cid:48) Z+e (11.19) (cid:69)(cid:163) ee (cid:48)|X,Z (cid:164)=Î£ whereB iskÃm,C is(cid:96)Ãm,Y â(cid:82)m,X â(cid:82)k,andZ â(cid:82)(cid:96) .WeseparatetheregressorsasX andZ because thecoefficientmatrixB willberestrictedwhileC willbeunrestricted. ThematrixB isfullrankif rank(B)=min(k,m). Thereducedrankrestrictionisrank(B)=r <min(k,m)forsomeknownr. ThereducedrankrestrictionimpliesthatwecanwritethecoefficientmatrixB inthefactoredform B =GA (cid:48) where AismÃr andG iskÃr. ThisrepresentationisnotuniqueaswecanreplaceG withGQ and A with AQ â1(cid:48) foranyinvertibleQ andthesamerelationholds. Identificationthereforerequiresa normalizationofthecoefficients.AconventionalnormalizationisG (cid:48) DG=I forgivenD. r Equivalently,thereducedrankrestrictioncanbeimposedbyrequiringthatB satisfytherestriction BAâ¥ =GA (cid:48) Aâ¥ =0 for some mÃ(mâr) coefficient matrix Aâ¥. Since G is full rank this requires that A (cid:48) Aâ¥ =0,henceAâ¥istheorthogonalcomplementofA.NotethatAâ¥isnotuniqueasitcanbereplaced by Aâ¥Q forany(mâr)Ã(mâr)invertibleQ.Thusif Aâ¥istobeestimateditrequiresanormalization. WediscussmethodsforestimationofG,A,Î£,C,andAâ¥.Thestandardapproachismaximumlikeli- hoodundertheassumptionthateâ¼N(0,Î£).Thelog-likelihoodfunctionforthesampleis nm n (cid:96) (G,A,C,Î£)=â log(2Ï)â log(det(Î£)) n 2 2 â 1 (cid:88) n (cid:161) Y âAG (cid:48) X âC (cid:48) Z (cid:162)(cid:48) Î£â1(cid:161) Y âAG (cid:48) X âC (cid:48) Z (cid:162) . i i i i i i 2 i=1",
    "page": 337,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER11. MULTIVARIATEREGRESSION 318 Anderson(1951)derivedtheMLEbyimposingtheconstraintBAâ¥ =0viathemethodofLagrange multipliers.Thisturnsouttobealgebraicallycumbersome. Johansen(1995)insteadproposedthefollowingstraightforwardconcentrationmethod. TreatingG asifitisknown,maximizethelog-likelihoodwithrespecttotheotherparameters. Resubstitutingthese estimatorsweobtaintheconcentratedlog-likelihoodfunctionwithrespecttoG.Thiscanbemaximized tofindtheMLEforG.Theotherparameterestimatorsarethenobtainbysubstitution.Wenowdescribe thesestepsindetail. (cid:48) GivenGthelikelihoodisanormalmultivariateregressioninthevariablesG X andZ,sotheMLEfor A,C andÎ£areleastsquares.Inparticular,usingtheFrisch-Waugh-Lovellresidualregressionformulawe canwritetheestimatorsfor AandÎ£as A(cid:98)(G)= (cid:179) Y(cid:101) (cid:48) X(cid:101)G (cid:180)(cid:179) G (cid:48) X(cid:101) (cid:48) X(cid:101)G (cid:180)â1 and Î£ (cid:98)(G)= 1 (cid:181) Y(cid:101) (cid:48) Y(cid:101) âY(cid:101) (cid:48) X(cid:101)G (cid:179) G (cid:48) X(cid:101) (cid:48) X(cid:101)G (cid:180)â1 G (cid:48) X(cid:101) (cid:48) Y(cid:101) (cid:182) n whereY(cid:101) =Y âZ (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) Y andX(cid:101) =X âZ (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48)(cid:54)X. Substitutingtheseestimatorsintothelog-likelihoodfunctionweobtaintheconcentratedlikelihood function,whichisafunctionofG only. (cid:96) (cid:101)n (G)=(cid:96) n (cid:161) G,A(cid:98)(G),C(cid:98)(G),Î£ (cid:98)(G) (cid:162) = m(cid:161) nlog(2Ï)â1 (cid:162)â n log (cid:183) det (cid:181) Y(cid:101) (cid:48) Y(cid:101) âY(cid:101) (cid:48) X(cid:101)G (cid:179) G (cid:48) X(cid:101) (cid:48) X(cid:101)G (cid:180)â1 G (cid:48) X(cid:101) (cid:48) Y(cid:101) (cid:182)(cid:184) 2 2 ï£® det (cid:181) G (cid:48) (cid:181) X(cid:101) (cid:48) X(cid:101) âX(cid:101) (cid:48) Y(cid:101) (cid:179) Y(cid:101) (cid:48) Y(cid:101) (cid:180)â1 Y (cid:48) X(cid:101) (cid:182) G (cid:182)ï£¹ = m 2 (cid:161) nlog(2Ï)â1 (cid:162)â n 2 log (cid:179) det (cid:179) Y(cid:101) (cid:48) Y(cid:101) (cid:180)(cid:180) â n 2 log ï£¯ ï£¯ ï£° (cid:179) (cid:48) (cid:48) (cid:180) ï£º ï£º ï£»",
    "page": 338,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". det G X(cid:101) X(cid:101)G ThethirdequalityusesTheoremA.1.8.TheMLEG(cid:98)forGisthemaximizerof(cid:96) (cid:101)n (G),orequivalentlyequals det (cid:181) G (cid:48) (cid:181) X(cid:101) (cid:48) X(cid:101) âX(cid:101) (cid:48) Y(cid:101) (cid:179) Y(cid:101) (cid:48) Y(cid:101) (cid:180)â1 Y (cid:48) X(cid:101) (cid:182) G (cid:182) G(cid:98) =argmin (cid:179) (cid:48) (cid:48) (cid:180) (11.20) G det G X(cid:101) X(cid:101)G (cid:181) (cid:48) (cid:48) (cid:179) (cid:48) (cid:180)â1 (cid:48) (cid:182) det G X(cid:101) Y(cid:101) Y(cid:101) Y(cid:101) Y X(cid:101)G =argmax (cid:179) (cid:48) (cid:48) (cid:180) G det G X(cid:101) X(cid:101)G ={v ,...,v } 1 r (cid:48) (cid:179) (cid:48) (cid:180)â1 (cid:48) (cid:48) which are the generalized eigenvectors of X(cid:101) Y(cid:101) Y(cid:101) Y(cid:101) Y X(cid:101) with respect to X(cid:101) X(cid:101) corresponding to the r largest generalized eigenvalues. (Generalized eigenvalues and eigenvectors are discussed in Section A.14.) The estimator satisfies the normalization G(cid:98) (cid:48) X(cid:101) (cid:48) X(cid:101)G(cid:98) = I r . Letting v â j denote the eigenvectors of (11.20)wecanalsoexpressG(cid:98) =(cid:169) v m â ,...,v m â âr+1 (cid:170) . Thisiscomputationallystraightforward. InMATLAB,forexample,thegeneralizedeigenvaluesand eigenvectorsofamatrix AwithrespecttoB arefoundusingthecommandeig(A,B). (cid:48) GivenG(cid:98), the MLE A(cid:98),C(cid:98), Î£ (cid:98) are found by least squares regression of Y onG(cid:98) X and Z. In particular, (cid:48) (cid:48) (cid:48) (cid:48) A(cid:98) =G(cid:98) X(cid:101) Y(cid:101) sinceG(cid:98) X(cid:101) X(cid:101)G(cid:98) =I r .",
    "page": 338,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER11. MULTIVARIATEREGRESSION 319 Wenowdiscusstheestimator A(cid:98)â¥of Aâ¥.Itturnsoutthat det (cid:181) A (cid:48) (cid:181) Y(cid:101) (cid:48) Y(cid:101) âY(cid:101) (cid:48) X(cid:101) (cid:179) X(cid:101) (cid:48) X(cid:101) (cid:180)â1 X(cid:101) (cid:48) Y(cid:101) (cid:182) A (cid:182) A(cid:98)â¥ =argmax (cid:179) (cid:48) (cid:48) (cid:180) (11.21) A det A Y(cid:101) Y(cid:101)A ={w 1 ,...,w mâr } (cid:48) (cid:48) (cid:179) (cid:48) (cid:180)â1 (cid:48) (cid:48) theeigenvectorsofY(cid:101) Y(cid:101) âY(cid:101) X(cid:101) X(cid:101) X(cid:101) X(cid:101) Y(cid:101) withrespecttoY(cid:101) Y(cid:101) associatedwiththelargestmâr eigen- values. Bythedualeigenvaluerelation(TheoremA.5),equations(11.20)and(11.21)havethesamenon-zero eigenvaluesÎ» andtheassociatedeigenvectorsv â andw satisfytherelationship j j j w j =Î»â j 1/2 (cid:179) Y(cid:101) (cid:48) Y(cid:101) (cid:180)â1 Y(cid:101) (cid:48) X(cid:101)v â j . LettingÎ=diag{Î» m ,...,Î» mâr+1 }thisimplies {w m ,...,w mâr+1 }= (cid:179) Y(cid:101) (cid:48) Y(cid:101) (cid:180)â1 Y(cid:101) (cid:48) X(cid:101) (cid:169) v m â ,...,v m â âr+1 (cid:170)Î= (cid:179) Y(cid:101) (cid:48) Y(cid:101) (cid:180)â1 A(cid:98) Î. ThesecondequalityholdssinceG(cid:98) =(cid:169) v m â ,...,v m â âr+1 (cid:170) and A(cid:98) =Y(cid:101) (cid:48) X(cid:101)G(cid:98). Sincetheeigenvectors w j satisfy theorthogonalitypropertyw (cid:48) Y(cid:101) (cid:48) Y(cid:101)w(cid:96) =0for j (cid:54)=(cid:96),itfollowsthat j (cid:48) (cid:48) (cid:48) 0=A(cid:98)â¥Y(cid:101) Y(cid:101){w m ,...,w mâr+1 }=A(cid:98)â¥A(cid:98) Î. (cid:48) SinceÎ>0weconcludethat A(cid:98)â¥A(cid:98) =0asdesired. Thesolution A(cid:98)â¥ in(11.21)canberepresentedseveralways. Onewhichiscomputationallyconve- nientistoobservethat Y(cid:101) (cid:48) Y(cid:101) âY(cid:101) (cid:48) X(cid:101) (cid:179) X(cid:101) (cid:48) X(cid:101) (cid:180)â1 Y(cid:101) (cid:48) X(cid:101) =Y (cid:48) M X,Z Y =E(cid:101) (cid:48) E(cid:101) whereM X,Z =I n â(X,Z) (cid:161) (X,Z) (cid:48) (X,Z) (cid:162)â1 (X,Z) (cid:48) andE(cid:101) =M X,Z Y istheresidualmatrixfromtheunre- strictedmultivariateleastsquaresregressionofY on X and Z. ThefirstequalityfollowsbytheFrisch- (cid:48) (cid:48) Waugh-Lovelltheorem. Thisshowsthat A(cid:98)â¥arethegeneralizedeigenvectorsofE(cid:101) E(cid:101) withrespecttoY(cid:101) Y(cid:101) correspondingtothemâr largesteigenvalues. InMATLAB,forexample,thesecanbecomputedusing theeig(A,B)command. AnotherrepresentationistowriteM =I âZ (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) sothat Z n (cid:161) (cid:48) (cid:48) (cid:162) (cid:161) (cid:48) (cid:48) (cid:162) det A Y M YA det A Y M YA A(cid:98)â¥ =argmax (cid:161) (cid:48) (cid:48) X,Z (cid:162) =argmin (cid:161) (cid:48) (cid:48) Z (cid:162) . det A Y M YA det A Y M YA A Z A X,Z Wesummarizeourfindings.",
    "page": 339,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER11. MULTIVARIATEREGRESSION 320 Theorem11.7 TheMLEforthereducedrankmodel(11.19)undereâ¼N(0,Î£) isgivenasfollows. LetY(cid:101) andX(cid:101) betheresidualmatricesfrommultivariatere- gressionofY andX onZ,respectively.ThenG(cid:98)mle ={v 1 ,...,v r },thegeneralized (cid:48) (cid:179) (cid:48) (cid:180)â1 (cid:48) (cid:48) eigenvectorsof X(cid:101) Y(cid:101) Y(cid:101) Y(cid:101) Y X(cid:101) withrespectto X(cid:101) X(cid:101) correspondingtother largesteigenvaluesÎ» (cid:98)j . A(cid:98)mle ,C(cid:98)mle andÎ£ (cid:98)mle areobtainedbytheleastsquares regression (cid:48) (cid:48) Y i =A(cid:98)mle G(cid:98) mle X i +C(cid:98) mle Z i +e (cid:98)i Î£ (cid:98)mle = n 1 (cid:88) n e (cid:98)i e (cid:98)i (cid:48) . i=1 LetE(cid:101) betheresidualmatrixfromamultivariateregressionofY on X and Z. (cid:48) (cid:48) ThenA(cid:98)â¥equalsthegeneralizedeigenvectorsofE(cid:101) E(cid:101) withrespecttoY(cid:101) Y(cid:101) corre- spondingtothemâr smallesteigenvalues.Themaximizedlikelihoodequals (cid:96) n = m(cid:161) nlog(2Ï)â1 (cid:162)â n log (cid:179) det (cid:179) Y(cid:101) (cid:48) Y(cid:101) (cid:180)(cid:180) â n (cid:88) r log (cid:161) 1âÎ» (cid:98)j (cid:162) . 2 2 2 j=1 AnRpackageforreducedrankregressionisâRRRâ.IamunawareofaStatacommand. 11.12 PrincipalComponentAnalysis InSection4.23wedescribedtheDuflo,DupasandKremer(2011)datasetwhichisasampleofKenyan firstgradetestscores.Followingtheauthorswefocusedonthevariabletotalscorewhichiseachstudentâs composite test score. If you examine the data file you will find other pieces of information about the studentsâ performance, including each studentâs score on separate sections of the test, with the labels wordscore(wordrecognition),sentscore(sentencerecognition),letterscore(letterrecognition),spellscore (spelling),additions_score(addition),substractions_score(subtraction),multiplications_score(multipli- cation). Theâtotalâscoresumsthescoresfromtheindividualsections. Perhapsthereismoreinforma- tioninthesectionscores.Howcanwelearnaboutthisfromthedata? Principalcomponentanalysis(PCA)addressesthisissuebyorderinglinearcombinationsbytheir contributiontovariance.",
    "page": 340,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER11. MULTIVARIATEREGRESSION 321 Definition11.1 LetX beakÃ1randomvector. ThefirstprincipalcomponentisU =h (cid:48) X whereh satisfies 1 1 1 h =argmax var (cid:163) h (cid:48) X (cid:164) . 1 h(cid:48)h=1 ThesecondprincipalcomponentisU =h (cid:48) X where 2 2 h = argmax var (cid:163) h (cid:48) X (cid:164) . 2 h(cid:48)h=1,h(cid:48)h1 =0 Ingeneral,the jth principalcomponentisU =h (cid:48) X where j j h = argmax var (cid:163) h (cid:48) X (cid:164) . j h(cid:48)h=1,h(cid:48)h1 =0,...,h(cid:48)hjâ1 =0 (cid:48) TheprincipalcomponentsofX arelinearcombinationsh X rankedbycontributiontovariance. By thepropertiesofquadraticforms(SectionA.15)theweightvectorsh aretheeigenvectorsofÎ£=var[X]. j Theorem11.8 Theprincipalcomponentsof X areU =h (cid:48) X,whereh isthe j j j eigenvectorofÎ£associatedwiththe jth orderedeigenvalueÎ» ofÎ£. j AnotherwaytoseethePCAconstructionisasfollows. SinceÎ£issymmetricthespectraldecompo- sition(TheoremA.3)statesthatÎ£=HDH (cid:48) whereH =[h ,...,h ]andD =diag(d ,...,d )aretheeigen- 1 k 1 k vectors and eigenvalues of Î£. Since Î£ is positive semi-definite the eigenvalues are real, non-negative, andorderedd â¥d â¥Â·Â·Â·â¥d . LetU =(U ,...,U )betheprincipalcomponentsofX. ByTheorem11.8, 1 2 k 1 k U =H (cid:48) X.ThecovariancematrixofU is var[U]=var (cid:163) H (cid:48) X (cid:164)=H (cid:48)Î£H=D whichisdiagonal. Thisshowsthatvar (cid:163) U (cid:164)=d andtheprincipalcomponentsaremutuallyuncorre- j j lated.Therelativevariancecontributionofthe jth principalcomponentisd /tr(Î£). j PrincipalcomponentsaresensitivetothescalingofX.Consequently,itisrecommendedtofirstscale eachelementofX tohavemeanzeroandunitvariance.InthiscaseÎ£isacorrelationmatrix. Thesampleprincipalcomponentsareobtainedbyreplacingtheunknownsbysampleestimators.Let Î£ (cid:98) bethesamplecovarianceorcorrelationmatrixandh(cid:98)1 ,h(cid:98)2 ,...,h(cid:98)k itsorderedeigenvectors. Thesample (cid:48) principalcomponentsareh(cid:98) j X i . ToillustrateweusetheDuflo,DupasandKremer(2011)dataset. InTable11.1wedisplaytheseven eigenvaluesofthesamplecorrelationmatrixfortheseventestscoresdescribedabove.Theseveneigen- values sum to seven since we have applied PCA to the correlation matrix. The first eigenvalue is 4.0, implyingthatthefirstprincipalcomponentexplains57%ofthevarianceoftheseventestscores. The secondeigenvalueis1.0, implyingthatthesecondprincipalcomponentexplains15%ofthevariance. Togetherthefirsttwocomponentsexplain72%ofthevarianceoftheseventestscores. In Table 11.2 we display the weight vectors (eigenvectors) for the first two principal components. Theweightsforthefirstcomponentareallpositiveandsimilarinmagnitude. Thismeansthatthefirst",
    "page": 341,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER11. MULTIVARIATEREGRESSION 322 Table11.1:EigenvalueDecompositionofSampleCorrelationMatrix Eigenvalue Proportion 1 4.02 0.57 2 1.04 0.15 3 0.57 0.08 4 0.52 0.08 5 0.37 0.05 6 0.29 0.04 7 0.19 0.03 Table11.2:PrincipalComponentWeightVectors First Second words 0.41 â0.32 sentences 0.32 â0.49 letters 0.40 â0.13 spelling 0.43 â0.28 addition 0.38 0.41 subtraction 0.35 0.52 multiplication 0.33 0.36 principal component is similar to a simple average of the seven test scores. This is quite fascinating. This is consistent with our intuition that a simple average (e.g. the variable totalscore) captures most of the information contained in the seven test scores. The weights for the second component have a different pattern. The four literacy scores receive negative weight and the three math scores receive positiveweightwithsimilarmagnitudes. Thismeansthatthesecondprincipalcomponentissimilarto thedifferencebetweenastudentâsmathandverbaltestscores. Takentogether, theinformationinthe firsttwoprincipalcomponentsisequivalenttoâaverageverbalâandâaveragemathâtestscores. What this shows is that 57% of the variation in the seven section test scores can be explained by a simple average(e.g.totalscore),and72%canbeexplainedbyaveragesfortheverbalandmathhalvesofthetest. In Stata, principal components analysis can be implemented with the pca command. In R use prcomp or princomp. All three can be applied to either covariance matrices (unscaled data) or corre- lationmatrices(normalizeddata)buttheyhavedifferentdefaultsettings. TheStatapcacommandby defaultnormalizestheobservations.TheRcommandsbydefaultdonotnormalizetheobservations. 11.13 FactorModels Closely related to principal components are factor models. These are statistical models which de- compose random vectors into common factors and idiosyncratic errors. Factor models are popular throughout the social sciences. Consequently a variety of estimation methods have been developed. Inthenextfewsectionswefocusonmethodswhicharepopularamongeconomists. Let X = (X ,...,X ) (cid:48) be a kÃ1 random vector (for example the seven test scores described in the 1 k previoussection).AssumethattheelementsofX arescaledtohavemeanzeroandunitvariance. AsinglefactormodelforX is X =Î»F+u (11.22)",
    "page": 342,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER11. MULTIVARIATEREGRESSION 323 whereÎ»â(cid:82)k arefactorloadings,F â(cid:82)isacommonfactor,anduâ(cid:82)k isarandomerror. ThefactorF isindividual-specificwhilethecoefficientÎ»iscommonacrossindividuals. Themodel(11.22)specifies that correlation between the elements of X is due to the common factor F. In the student test score exampleitisintuitivetothinkofF asastudentâsscholasticâaptitudeâ;inthiscasethevectorÎ»describes howscholasticaptitudeaffectsthesevensubjectscores. Amultiplefactormodelhasr <k factors.Wewritethemodelas X =ÎF+u (11.23) whereÎisakÃr matrixoffactorloadingsandF =(F ,...,F ) (cid:48) isanrÃ1vectoroffactors.Inthestudent 1 r test score example possible factors could be âmath aptitudeâ, âlanguage skillsâ, âsocial skillsâ, âartistic abilityâ,âcreativityâ,etc.ThefactorloadingmatrixÎindicatestheeffectofeachfactoroneachtestscore. Thenumberoffactorsr istakenasknown.Wediscussselectionofr later. Theerrorvectoru isassumedtobemeanzero, uncorrelatedwithF, and(undercorrectspecifica- tion)tohavemutuallyuncorrelatedelements. WewriteitscovariancematrixasÎ¨=(cid:69)(cid:163) uu (cid:48)(cid:164) . Thefactor vectorF caneitherbetreatedasarandomvectororasaregressorvector. InthissectionwetreatF asa randomvector;inthenextwetreatF asregressors. TherandomfactorsF areassumedmeanzeroand arenormalizedsothat(cid:69)(cid:163) FF (cid:48)(cid:164)=I . r TheassumptionsimplythatthecorrelationmatrixÎ£=(cid:69)(cid:163) XX (cid:48)(cid:164) equals Î£=ÎÎ(cid:48)+Î¨. (11.24) ThefactoranalysisliteratureoftendescribesÎÎ(cid:48) asthecommunalityandtheidiosyncraticerrormatrix Î¨astheuniqueness. Theformeristheportionofthevariancewhichisexplainedbythefactormodel andthelatteristheunexplainedportionofthevariance. Themodelisoften1estimatedbymaximumlikelihood.Underjointnormalityof(F,u)thedistribu- tionofX isN (cid:161) 0,ÎÎ(cid:48)+Î¨(cid:162) .TheparametersareÎandÎ¨=diag (cid:161)Ï ,...,Ï (cid:162) .Thelog-likelihoodfunctionof 1 k arandomsample(X ,...,X )is 1 n (cid:96) n (Î,Î¨)=â nk log(2Ï)â n logdet (cid:161)ÎÎ(cid:48)+Î¨(cid:162)â n tr (cid:179) (cid:161)ÎÎ(cid:48)+Î¨(cid:162)â1Î£ (cid:98) (cid:180) . (11.25) 2 2 2 The MLE (cid:161)Î (cid:98),Î¨ (cid:98) (cid:162) maximizes (cid:96) n (Î,Î¨). There is not an algebraic solution so the estimator is found usingnumericalmethods. Fortunately,computationalalgorithmsareavailableinstandardpackages. A detaileddescriptionandanalysiscanbefoundinAnderson(2003,Chapter14). Theformofthelog-likelihoodisintriguing. Noticethatthelog-likelihoodisonlyafunctionofthe observationsthroughitscorrelationmatrixÎ£ (cid:98), andonlyafunctionoftheparametersthroughthepop- ulationcorrelationmatrixÎÎ(cid:48)+Î¨. Thefinaltermin(11.25)isameasureofthematchbetweenÎ£ (cid:98) and ÎÎ(cid:48)+Î¨.Together,weseethattheGaussianlog-likelihoodisessentiallyameasureofthefitofthemodel andsamplecorrelationmatrices.Itisthereforenotreliantonthenormalityassumption. ItisoftenofinteresttoestimatethefactorsF . GivenÎtheequationX =ÎF +u canbeviewedas i i i i aregressionwithcoefficientF i",
    "page": 343,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Thefinaltermin(11.25)isameasureofthematchbetweenÎ£ (cid:98) and ÎÎ(cid:48)+Î¨.Together,weseethattheGaussianlog-likelihoodisessentiallyameasureofthefitofthemodel andsamplecorrelationmatrices.Itisthereforenotreliantonthenormalityassumption. ItisoftenofinteresttoestimatethefactorsF . GivenÎtheequationX =ÎF +u canbeviewedas i i i i aregressionwithcoefficientF i . ItsleastsquaresestimatorisF(cid:98)i =(cid:161)Î(cid:48)Î(cid:162)â1Î(cid:48) X i . TheGLSestimator(tak- ingintoaccountthecovariancematrixofu i )isF(cid:98)i =(cid:161)Î(cid:48)Î¨â1Î(cid:162)â1Î(cid:48)Î¨â1X i . ThismotivatestheBartlett scoringestimator F(cid:101)i =(cid:161)Î (cid:98) (cid:48)Î¨ (cid:98) â1Î (cid:98) (cid:162)â1Î (cid:98) (cid:48)Î¨ (cid:98) â1X i . Theidealizedversionsatisfies F(cid:98)i =(cid:161)Î(cid:48)Î¨â1Î(cid:162)â1Î(cid:48)Î¨â1(ÎF i +u i )=F i +(cid:161)Î(cid:48)Î¨â1Î(cid:162)â1Î(cid:48)Î¨â1u i 1Thereareotherestimatorsusedinappliedfactoranalysis.Howeverthereislittlereasontoconsiderestimatorsbeyondthe MLEofthissectionandtheprincipalcomponentsestimatorofthenextsection.",
    "page": 343,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER11. MULTIVARIATEREGRESSION 324 which is unbiased for F and has variance (cid:161)Î(cid:48)Î¨â1Î(cid:162)â1 . Thus the Barlett scoring estimator is typically i describedasâunbiasedâthoughthisisactuallyapropertyofitsidealizedversionF(cid:98)i . AsecondestimatorforthefactorscanbeconstructedfromthemultivariatelinearprojectionofF on X.ThisisF =AX+Î¾wherethecoefficientmatrix AisrÃk.Thecoefficientmatrixequals A=(cid:69)(cid:163) FX (cid:48)(cid:164)(cid:69)(cid:163) XX (cid:48)(cid:164)â1=Î(cid:48)Î£â1, thesecondequationusing(cid:69)(cid:163) FX (cid:48)(cid:164)=(cid:69)(cid:163) F(ÎF+u) (cid:48)(cid:164)=(cid:69)(cid:163) FF (cid:48)(cid:164)Î(cid:48)+(cid:69)(cid:163) Fu (cid:48)(cid:164)=Î(cid:48) . ThepredictedvalueofF i isF â=AX =Î(cid:48)Î£â1X .Thismotivatestheregressionscoringestimator i i i F i =Î (cid:98) (cid:48)Î£ (cid:98) â1X i . The idealized version F â has conditional expectation Î(cid:48)Î£â1ÎF and is thus biased for F . Hence the i i i regressionscoringestimatorF isoftendescribedasâbiasedâ.Somealgebraicmanipulationsrevealthat i F â has MSE I âÎ(cid:48)(cid:161)Î(cid:48)Î+Î¨(cid:162)â1Î which is smaller (in a positive definite sense) than the MSE of the i r idealizedBartlettestimatorF(cid:98)i . Whichestimatorispreferred,Bartlettorregressionscoring?Thedifferencesdiminishwhenkislarge sothechoiceismostrelevantforsmalltomoderatek. Theregressionscoringestimatorhaslowerap- proximateMSE,meaningthatitisamorepreciseestimator. Thusbasedonestimationprecisionthisis ourrecommendedchoice. ThefactorloadingsÎandfactorsF arenotseparatelyidentified. Toseethis, noticethatifyoure- place(Î,F)withÎâ=ÎG andF â=G (cid:48) F whereG isrÃr andorthonormalthentheregressionmodelis identical.Suchreplacementsarecalledârotationsâinthefactoranalysisliterature.Anyorthogonalrota- tionofthefactorloadingsisanequallyvalidrepresentation. ThedefaultMLEoutputsareonespecific rotation;otherscanbeobtainedbyavarietyofalgorithms(whichwedonotreviewhere).Consequently itisunwisetoattributemeaningtotheindividualfactorloadingestimates. Anotherimportantandtrickyissueisselectionofthenumberoffactorsr. Thereisnoclearguide- line. Oneapproachistoexaminetheprincipalcomponentdecomposition,lookforadivisionbetween the âlargeâ and âsmall eigenvalues, and set r to equal to the number of âlargeâ eigenvalues. Another approachisbasedontesting. Asaby-productoftheMLE(andstandardpackageimplementations)we obtaintheLRtestforthenullhypothesisofr factorsagainstthealternativehypothesisofkfactors.Ifthe LRtestrejects(hasasmallp-value)thisisevidencethatthegivenr maybetoosmall. InStata,theMLE (cid:161)Î (cid:98),Î¨ (cid:98) (cid:162) canbecalculatedwiththefactor, ml factors(r)command. Thefactor estimatesF(cid:101)i andF i canbecalculatedbythepredictcommandwitheitherthebarlettorregression option, respectively. In R, the command factanal(X,factors=r,rotation=\"none\") calculates the MLE (cid:161)Î (cid:98),Î¨ (cid:98) (cid:162) andalsocalculatesthefactorestimatesF(cid:101)i and/orF i usingthescoresoption",
    "page": 344,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". InStata,theMLE (cid:161)Î (cid:98),Î¨ (cid:98) (cid:162) canbecalculatedwiththefactor, ml factors(r)command. Thefactor estimatesF(cid:101)i andF i canbecalculatedbythepredictcommandwitheitherthebarlettorregression option, respectively. In R, the command factanal(X,factors=r,rotation=\"none\") calculates the MLE (cid:161)Î (cid:98),Î¨ (cid:98) (cid:162) andalsocalculatesthefactorestimatesF(cid:101)i and/orF i usingthescoresoption. 11.14 ApproximateFactorModels TheMLEoftheprevioussectionisagoodchoiceforfactorestimationwhenthenumberofvariables k issmallandthefactormodelisbelievedtobecorrectlyspecified. Inmanyeconomicapplicationsof factor analysis, however, the number of variables is k is large. In such contexts the MLE can be com- putationallycostlyand/orunstable. Furthermoreitistypicallynotcredibletobelievethatthemodelis correctlyspecified; ratheritismorereasonabletoviewthefactormodelasausefulapproximation. In thissectionweexploreanapproachknownastheapproximatefactormodelwithestimationbyprinci- palcomponents. Theestimationmethodisjustifiedbyanasymptoticframeworkwherethenumberof variableskââ.",
    "page": 344,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER11. MULTIVARIATEREGRESSION 325 TheapproximatefactormodelwasintroducedbyChamberlainandRothschild(1983).Itisthesame as (11.23) but relaxes the assumption on the idiosyncratic error u so that the covariance matrix Î¨ = (cid:69)(cid:163) uu (cid:48)(cid:164) isleftunrestricted.InthiscontexttheGaussianMLEoftheprevioussectionismisspecified. ChamberlainandRothschild(andtheliteraturewhichfollowed)proposedestimationbyleastsquares. TheideaistotreatthefactorsasunknownregressorsandsimultaneouslyestimatethefactorsF andfac- i torloadingsÎ.Wefirstdescribetheestimationmethod. Let(X ,...,X )beasamplecenteredatsamplemeans.Theleastsquarescriterionis 1 n 1 (cid:88) n (X âÎF ) (cid:48) (X âÎF ). i i i i n i=1 Let (cid:161)Î (cid:98),F(cid:98)1 ,...,F(cid:98)n (cid:162) be the joint minimizers. As Î and F i are not separately identified a normalization is needed.Forcompatibilitywiththenotationoftheprevioussectionweusen â1(cid:80)n i=1 F(cid:98)i F(cid:98) i (cid:48)=I r . We use a concentration argument to find the solution. As described in the previous section, each observationsatisfiesthemultivariateequationX =ÎF +u .ForfixedÎthisisasetofk equationswith i i i r unknownsF i . TheleastsquaressolutionisF(cid:98)i (Î)=(cid:161)Î(cid:48)Î(cid:162)â1Î(cid:48) X i . Substitutingthisexpressionintothe leastsquarescriteriontheconcentratedleastsquarescriterionforÎis 1 (cid:88) n (cid:161) X i âÎF(cid:98)i (Î) (cid:162)(cid:48)(cid:161) X i âÎF(cid:98)i (Î) (cid:162)= 1 (cid:88) n (cid:179) X i âÎ(cid:161)Î(cid:48)Î(cid:162)â1Î(cid:48) X i (cid:180)(cid:48)(cid:179) X i âÎ(cid:161)Î(cid:48)Î(cid:162)â1Î(cid:48) X i (cid:180) n n i=1 i=1 = 1 (cid:88) n (cid:179) X (cid:48) X âX (cid:48)Î(cid:161)Î(cid:48)Î(cid:162)â1Î(cid:48) X (cid:180) n i i i i i=1 =tr (cid:163)Î£ (cid:98) (cid:164)âtr (cid:104) (cid:161)Î(cid:48)Î(cid:162)â1Î(cid:48)Î£ (cid:98) Î (cid:105) whereÎ£ (cid:98) =n â1(cid:80)n i=1 X i X i (cid:48) isthesamplecovariancematrix. TheleastsquaresestimatorÎ (cid:98) minimizesthis criterion. LetD(cid:98) and H(cid:98) befirstr eigenvaluesandeigenvectorsofÎ£ (cid:98). UsingthenormalizationÎ(cid:48)Î=I r , from the extrema results of Section A.15 the minimizer of the least squares criterion is Î (cid:98) = H(cid:98). More broadly any rotation of H(cid:98) is valid. Consider Î (cid:98) = H(cid:98)D(cid:98) 1/2 . Recall the expression for the factors F(cid:98)i (Î)= (cid:161)Î(cid:48)Î(cid:162)â1Î(cid:48) X .Wefindthattheestimatedfactorsare i F(cid:98)i = (cid:179) D(cid:98) 1/2 H(cid:98) (cid:48) H(cid:98)D(cid:98) 1/2 (cid:180)â1 D(cid:98) 1/2 H(cid:98) (cid:48) X i =D(cid:98) â1/2 H(cid:98) (cid:48) X i . Wecalculatethat n n â1(cid:88) F(cid:98)i F(cid:98) i (cid:48)=D(cid:98) â1/2 H(cid:98) (cid:48) Î£ (cid:98)H(cid:98)D(cid:98) â1/2(cid:48) =D(cid:98) â1/2 D(cid:98)D(cid:98) â1/2(cid:48) =I r i=1 whichisthedesirednormalization. ThisshowsthattherotationÎ (cid:98) =H(cid:98)D(cid:98) 1/2 producesfactorestimates satisfyingthisnormalization. Wehaveproventhefollowingresult",
    "page": 345,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Wecalculatethat n n â1(cid:88) F(cid:98)i F(cid:98) i (cid:48)=D(cid:98) â1/2 H(cid:98) (cid:48) Î£ (cid:98)H(cid:98)D(cid:98) â1/2(cid:48) =D(cid:98) â1/2 D(cid:98)D(cid:98) â1/2(cid:48) =I r i=1 whichisthedesirednormalization. ThisshowsthattherotationÎ (cid:98) =H(cid:98)D(cid:98) 1/2 producesfactorestimates satisfyingthisnormalization. Wehaveproventhefollowingresult. Theorem11.9 The least squares estimator of the factor model (11.23) under thenormalizationn â1(cid:80)n i=1 F(cid:98)i F(cid:98) i (cid:48)=I r hasthefollowingsolution: 1. LetD(cid:98) =diag (cid:163) d(cid:98)1 ,...,d(cid:98)r (cid:164) andH(cid:98) =(cid:163) h(cid:98)1 ,...,h(cid:98)r (cid:164) bethefirstr eigenvaluesand eigenvectorsofthesamplecovariancematrixÎ£ (cid:98). 2. Î (cid:98) =H(cid:98)D(cid:98) 1/2 . 3. F(cid:98)i =D(cid:98) â1/2 H(cid:98) (cid:48) X i .",
    "page": 345,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER11. MULTIVARIATEREGRESSION 326 Theorem11.9showsthattheleastsquaresestimatorisbasedonaneigenvaluedecompositionofthe covariancematrix.Thisiscomputationallystableeveninhighdimensions. ThefactorestimatesaretheprincipalcomponentsscaledbytheeigenvaluesofÎ£ (cid:98). Specifically, the jth factor estimate is F(cid:98)ji =d(cid:98) â j 1/2h(cid:98) (cid:48) j X. Consequently many authors call this estimator the âprincipal- componentmethodâ. Unfortunately,Î (cid:98) isinconsistentforÎifk isfixed,aswenowshow. BytheWLLNandCMT,Î£ (cid:98) ââÎ£ p andH(cid:98) ââH,thefirstr eigenvectorsofÎ£.WhenÎ¨isdiagonal,theeigenvectorsofÎ£=ÎÎ(cid:48)+Î¨donotlie p intherangespaceofÎexceptinthespecialcaseÎ¨=Ï2I k .ConsequentlytheestimatorÎ (cid:98)isinconsistent. This inconsistency should not be viewed as surprising. The sample has a total of nk observations andthemodelhasatotalofnr+krâr(r+1)/2parameters.Sincethenumberofestimatedpararameters isproportionaltosamplesizeweshouldnotexpectestimatorconsistency. AsfirstrecognizedbyChamberlainandRothschild,thisdeficiencydiminishesaskincreases.Specif- ically, assumethatk ââasn ââ. Oneimplicationisthatthenumberofobservationsnk increase at a rate faster than n, while the number of parameters increase at a rate proportional to n. Another implicationisthatask increasesthereisincreasinginformationaboutthefactors. Tomakethispreciseweaddthefollowingassumption.LetÎ» (A)andÎ» (A)denotethesmallest min max andlargesteigenvaluesofapositivesemi-definitematrix A. Assumption11.1 Askââ 1. Î» (Î¨)â¤B<â. max 2. Î» (cid:161)Î(cid:48)Î(cid:162)ââaskââ. min Assumption11.1.1boundsthecovariancematrixoftheidiosyncraticerrors.WhenÎ¨=diag (cid:161)Ï2,...,Ï2(cid:162) 1 k thisisthesameasboundingtheindividualvariances. EffectivelyAssumption11.1.1meansthatwhile theelementsofu canbecorrelatedtheycannothaveacorrelationstructuresimilartothatofafactor model. Assumption11.1.2requiresthefactorloadingmatrixtoincreaseinmagnitudeasthenumberof variablesincreases.Thisisafairlymildrequirement.Whenthefactorloadingsareofsimilarmagnitude acrossvariables,Î» (cid:161)Î(cid:48)Î(cid:162)â¼k ââ. Conceptually,Assumption11.1.2requiresadditionalvariablesto min addinformationabouttheunobservedfactors. Assumption11.1impliesthatinthecovariancematrixfactorizationÎ£=ÎÎ(cid:48)+Î¨thecomponentÎÎ(cid:48) dominatesaskincreases.Thismeansthatforlargekthefirstr eigenvectorsofÎ£areequivalenttothose ofÎÎ(cid:48) , whichareintherangespaceofÎ. ThisobservationledChamberlainandRothschild(1983)to deducethattheprincipalcomponentsestimatorisanasymptotic(largek)analogestimatorforthefactor loadingsandfactors. Bai(2003)demonstratedthattheestimatorisconsistentasn,k ââjointly. The conditionsandproofsaretechnicalsoarenotreviewedhere. Nowconsidertheestimatedfactors F(cid:98)i =D â1/2H (cid:48) X i =D â1Î(cid:48) X i whereforsimplicityweignoreestimationerror.SinceX =ÎF +u andÎ(cid:48)Î=D wecanwritethisas i i i F(cid:98)i =F i +D â1Î(cid:48) u i .",
    "page": 346,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER11. MULTIVARIATEREGRESSION 327 This shows that F(cid:98)i is an unbiased estimator for F i and has variance var (cid:163) F(cid:98)i (cid:164) = D â1Î(cid:48)Î¨ÎD â1. Under Assumption11.1, (cid:176) (cid:176)var (cid:163) F(cid:98)i (cid:164)(cid:176) (cid:176) â¤B/Î» min (cid:161)Î(cid:48)Î(cid:162)â0.ThusF(cid:98)i isconsistentforF i askââ.Bai(2003)shows thatthisextendstothefeasibleestimatorasn,kââ. InStata,theleastsquaresestimatorÎ (cid:98)andfactorsF(cid:98)i canbecalculatedwiththefactor, pcf factors(r) commandfollowedbypredict. InRafeasibleestimationapproachistocalculatethefactorsbyeigen- valuedecomposition. 11.15 FactorModelswithAdditionalRegressors Considerthemodel X =ÎF+BZ+e whereX andearekÃ1,ÎiskÃr,F isrÃ1,B iskÃ(cid:96),andZ is(cid:96)Ã1. The coefficients Î and B can be estimated by a combination of factor regression (either MLE or principalcomponents)andleastsquares.Thekeyisthefollowingtwoobservations: 1. GivenB,thecoefficientÎcanbeestimatedbyfactorregressionappliedtoXâBZ. 2. GiventhefactorsF thecoefficientsÎandB canbeestimatedbymultivariateleastsquaresofX on F andZ. Estimation iterates between these two steps. Start with a preliminary estimator of B obtained by multivariateleastsquaresofX onZ.Thenapplytheabovetwostepsanditerateunderconvergence. 11.16 Factor-AugmentedRegression Intheprevioussectionsweconsideredfactormodelswhichdecomposeasetofvariablesintocom- mon factors and idiosyncratic errors. In this section we consider factor-augmented regression, which usessuchcommonfactorsasregressorsfordimensionreduction. Supposewehavethevariables(Y,Z,X)whereY â(cid:82),Z â(cid:82)(cid:96) ,andX â(cid:82)k. Inpractice,k maybelarge andtheelementsofX maybehighlycorrelated.Thefactor-augmentedregressionmodelis Y =F (cid:48)Î²+Z (cid:48)Î³+e X =ÎF+u (cid:69)[Fe]=0 (cid:69)[Ze]=0 (cid:69)(cid:163) Fu (cid:48)(cid:164)=0 (cid:69)[ue]=0, Therandomvariablesareeâ(cid:82),F â(cid:82)r anduâ(cid:82)k.TheregressioncoefficientsareÎ²â(cid:82)k andÎ³â(cid:82)(cid:96) .The matrixÎarethefactorloadings. This model specifies that the influence of X on Y is through the common factors F. The idea is that the variation in the regressors is mostly captured by the variation in the factors, so the influence oftheregressorscanbecapturedthroughthesefactors. Thiscanbeviewedasadimension-reduction techniqueaswehavereducedthek-dimensional X tother-dimensionalF. Interesttypicallyfocuses ontheregressorsZ anditscoefficientsÎ³. ThefactorsF areincludedintheregressionasâcontrolsâand itscoefficientÎ²islesstypicallyofinterest. SinceitisdifficulttointerpretthefactorsF onlytheirrange spaceisidentifieditisgenerallyprudenttoavoidintrepretingthecoefficientsÎ².",
    "page": 347,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER11. MULTIVARIATEREGRESSION 328 The model is typically estimated in multiple steps. First, the factor loadings Î and factors F are i estimatedbyfactorregression. Inthecaseofprincipal-componentsestimationthefactorestimatesare the scaled2 principal components F(cid:98)i =D(cid:98) â1Î (cid:98) (cid:48) X i . Second, Y is regressed on the estimated factors and theotherregressorstoobtaintheestimatorofÎ²andÎ³.Thissecond-stepestimatorequals(forsimplicity assumethereisnoZ) (cid:195) (cid:33)â1(cid:195) (cid:33) n n Î² (cid:98) = (cid:88) F(cid:98)i F(cid:98) i (cid:48) (cid:88) F(cid:98)i Y i i=1 i=1 (cid:195) (cid:33)â1(cid:195) (cid:33) = D(cid:98) â1Î (cid:98) (cid:48) n 1 (cid:88) n X i X i (cid:48)Î (cid:98)D(cid:98) â1 D(cid:98) â1Î (cid:98) (cid:48) n 1 (cid:88) n X i Y i . i=1 i=1 Nowletâsinvestigateitsasymptoticbehavior.Asnââ,Î (cid:98) ââÎandD(cid:98) ââD so p p Î² (cid:98) ââÎ²â=(cid:161) D â1Î(cid:48)(cid:69)(cid:163) XX (cid:48)(cid:164)ÎD â1(cid:162)â1(cid:161) D â1Î(cid:48)(cid:69)[XY] (cid:162) . (11.26) p Recall(cid:69)(cid:163) XX (cid:48)(cid:164)=ÎÎ(cid:48)+Î¨andÎ(cid:48)Î=D.Wecalculatethat (cid:69)[XY]=(cid:69)(cid:163) (ÎF+u) (cid:161) F (cid:48)Î²+e (cid:162)(cid:164)=ÎÎ². Wefindthattheright-hand-sideof(11.26)equals Î²â=(cid:161) D â1Î(cid:48)(cid:161)ÎÎ(cid:48)+Î¨(cid:162)ÎD â1(cid:162)â1(cid:161) D â1Î(cid:48)ÎÎ²(cid:162)=(cid:161) I +D â1Î(cid:48)Î¨ÎD â1(cid:162)â1Î² r whichdoesnotequalÎ².ThusÎ² (cid:98)hasaprobabilitylimitbutisinconsistentforÎ²asnââ. Thisdeficiencydiminishesaskââ.Indeed, (cid:176) (cid:176)D â1Î(cid:48)Î¨ÎD â1(cid:176) (cid:176) â¤B (cid:176) (cid:176)D â1(cid:176) (cid:176) â0 askââ.ThisimpliesÎ²ââÎ².Hence,ifwetakethesequentialasymptoticlimitnââfollowedbykâ â,wefindÎ² (cid:98) ââÎ². Thisimpliesthattheestimatorisconsistent. Bai(2003)demonstratedconsistency p underthemorerigorousbuttechnicallychallengingsettingwheren,k ââjointly. Theimplicationof thisresultisthatfactoraugmentedregressionisconsistentifboththesamplesizeanddimensionof X arelarge. ForasymptoticnormalityofÎ² (cid:98)itturnsoutthatweneedtostrengthenAssumption11.1.2.Therelevant conditionisn â1/2Î» (cid:161)Î(cid:48)Î(cid:162)ââ. Thisissimilartotheconditionthatk2/nââ. Thisistechnicalbut min (cid:112) canbeinterpretedasmeaningthatk islargerelativeto n. Intuitively,thisrequiresthatthedimension ofX islargerthansamplesizen. In Stata, estimation takes the following steps. First, the factor command is used to estimate the factormodel. EitherMLEorprincipalcomponentsestimationcanbeused. Second,thepredictcom- mandisusedtoestimatethefactors,eitherbyBarlettorregressionscoring.Third,thefactorsaretreated asregressorsinanestimatedregression. 11.17 MultivariateNormal* Someinterestingsamplingresultsholdformatrix-valuednormalvariates. LetY beannÃmmatrix whoserowsareindependentanddistributedN (cid:161)Âµ,Î£(cid:162) .WesaythatY ismultivariatematrixnormal,and 2TheunscaledprincipalcomponentscanequivalentlybeusedifthecoefficientsÎ²(cid:98)arenotreported",
    "page": 348,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Second,thepredictcom- mandisusedtoestimatethefactors,eitherbyBarlettorregressionscoring.Third,thefactorsaretreated asregressorsinanestimatedregression. 11.17 MultivariateNormal* Someinterestingsamplingresultsholdformatrix-valuednormalvariates. LetY beannÃmmatrix whoserowsareindependentanddistributedN (cid:161)Âµ,Î£(cid:162) .WesaythatY ismultivariatematrixnormal,and 2TheunscaledprincipalcomponentscanequivalentlybeusedifthecoefficientsÎ²(cid:98)arenotreported. Thecoefficientesti- matesÎ³ (cid:98)areunaffectedbythechoiceoffactorscaling.",
    "page": 348,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER11. MULTIVARIATEREGRESSION 329 writeY â¼N (cid:161)Âµ,I âÎ£(cid:162) ,whereÂµisnÃm witheachrowequaltoÂµ(cid:48) . Thenotationisduetothefactthat n vec (cid:179) (cid:161) Y âÂµ(cid:162)(cid:48)(cid:180) â¼N(0,I âÎ£). n Definition11.2 IfnÃmY â¼N (cid:161)Âµ,I âÎ£(cid:162) thenW =Y (cid:48) Y isdistributedWishart n with n degress of freedom and covariance matrix Î£, and is written as W â¼ W (n,Î£). m TheWishartisamultivariategeneralizationofthechi-square.IfW â¼W (cid:161) n,Ï2(cid:162) thenW â¼Ï2Ï2. 1 n TheWishartarisesastheexactdistributionofasamplecovariancematrixinthenormalsampling model.Thebias-correctedestimatorofÎ£is 1 (cid:88) n (cid:179) (cid:180)(cid:179) (cid:180)(cid:48) Î£ (cid:98) = nâ1 Y i âY Y i âY . i=1 Theorem11.10 IfY i â¼N (cid:161)Âµ,Î£(cid:162) areindependentthenÎ£ (cid:98) â¼W m (cid:161) nâ1, nâ 1 1 Î£(cid:162) . Thefollowingmanipulationisuseful. Theorem11.11 IfW â¼W (n,Î£)thenformÃ1Î±, (cid:161)Î±(cid:48) W â1Î±(cid:162)â1â¼ Ï2 nâm+1 . m Î±(cid:48)Î£â1Î± Toprovethis,notethatwithoutlossofgeneralitywecantakeÎ£=I andÎ±(cid:48)Î±=1. Let H bemÃm m (cid:181) (cid:182) 1 orthonormal with first row equal to Î±. so that HÎ± = . Since the distribution of Y and YH are 0 (cid:181) (cid:182) 1 identicalwecanwithoutlossofgeneralitysetÎ±= . PartitionY =[Y ,Y ]whereY isnÃ1,Y is 1 2 1 2 0 nÃ(mâ1),andtheyareindependent.Then (cid:195) (cid:181) (cid:48) (cid:48) (cid:182)â1(cid:181) (cid:182) (cid:33)â1 (cid:161)Î±(cid:48) W â1Î±(cid:162)â1= (cid:161) 1 0 (cid:162) Y 1 (cid:48) Y 1 Y 1 (cid:48) Y 2 1 Y Y Y Y 0 2 1 2 2 =Y (cid:48) Y âY (cid:48) Y (cid:161) Y (cid:48) Y (cid:162)â1 Y (cid:48) Y 1 1 1 2 2 2 2 1 =Y (cid:48) M Y â¼Ï2 1 2 1 nâ(mâ1) where M 2 = I mâ1 âY 2 (cid:161) Y (cid:48) 2 Y 2 (cid:162)â1 Y (cid:48) 2 . The final distributional equality holds conditional on Y 2 by the sameargumentintheproofofTheorem5.7. SincethisdoesnotdependonY itistheunconditional 2 distributionaswell.Thisestablishesthestatedresult. TotesthypothesesaboutÂµaclassicalstatisticisknownasHotellingâsT2: (cid:179) (cid:180)(cid:48) (cid:179) (cid:180) T2=n Y âÂµ Î£ (cid:98) â1 Y âÂµ .",
    "page": 349,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER11. MULTIVARIATEREGRESSION 330 Theorem11.12 IfY â¼N (cid:161)Âµ,Î£(cid:162) then m T2â¼ F(m,nâm) (nâm)(nâ1) ascaledFdistribution. ToprovethisrecallthatY isindependentofÎ£ (cid:98). ApplyTheorem11.11withÎ±=Y âÂµ. Conditionalon Y andusingthefactthatÎ£ (cid:98) â¼W m (cid:161) nâ1, nâ 1 1 Î£(cid:162) , n = (cid:179)(cid:179) Y âÎ£ (cid:180)(cid:48) Î£ (cid:98) â1 (cid:179) Y âÎ£ (cid:180)(cid:180)â1 T2 Ï2 â¼ nâ1âm+1 (cid:179) Y âÂµ (cid:180)(cid:48) (cid:161) 1 Î£(cid:162)â1 (cid:179) Y âÂµ (cid:180) nâ1 Ï2 â¼n(nâ1) nâm . Ï2 m Sincethetwochi-squarevariablesareindependent,thisisthestatedresult. A very interesting property ofthis resultis that the T2 statisticisa multivariate quadratric form in normalrandomvariables,yetithastheexactF distribution. _____________________________________________________________________________________________ 11.18 Exercises Exercise11.1 Show(11.10)whentheerrorsareconditionallyhomoskedastic(11.8). Exercise11.2 Show(11.11)whentheregressorsarecommonacrossequationsX =X. j Exercise11.3 Show(11.12)whentheregressorsarecommonacrossequationsX =X andtheerrorsare j conditionallyhomoskedastic(11.8). Exercise11.4 ProveTheorem11.1. Exercise11.5 Show(11.13)whentheregressorsarecommonacrossequationsX =X. j Exercise11.6 Show(11.14)whentheregressorsarecommonacrossequationsX =X andtheerrorsare j conditionallyhomoskedastic(11.8). Exercise11.7 ProveTheorem11.2. Exercise11.8 ProveTheorem11.3. Exercise11.9 Showthat(11.16)followsfromthestepsdescribed. Exercise11.10 Showthat(11.17)followsfromthestepsdescribed. Exercise11.11 ProveTheorem11.4.",
    "page": 350,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER11. MULTIVARIATEREGRESSION 331 Exercise11.12 ProveTheorem11.5. Hint:First,showthatitissufficienttoshowthat (cid:69) (cid:104) X (cid:48) X (cid:105)(cid:179) (cid:69) (cid:104) X (cid:48) Î£â1X (cid:105)(cid:180)â1 (cid:69) (cid:104) X (cid:48) X (cid:105) â¤(cid:69) (cid:104) X (cid:48) Î£X (cid:105) . Second,rewritethisequationusingthetransformationsU =Î£1/2X andV =Î£1/2X,andthenapplythe matrixCauchy-Schwarzinequality(B.33). Exercise11.13 ProveTheorem11.6. Exercise11.14 Takethemodel Y =Ï(cid:48)Î²+e Ï=(cid:69)[X |Z]=Î(cid:48) Z (cid:69)[e|Z]=0 whereY isscalar, X isak vectorand Z isan(cid:96)vector. Î²andÏarekÃ1andÎis(cid:96)Ãk.Thesampleis (Y ,X ,Z :i =1,...,n)withÏ unobserved. i i i i ConsidertheestimatorÎ² (cid:98)forÎ²byOLSofY onÏ (cid:98) =Î (cid:98) (cid:48) Z whereÎ (cid:98)istheOLScoefficientfromthemulti- variateregressionofX onZ. (a) ShowthatÎ² (cid:98)isconsistentforÎ². (cid:112) (b) Findtheasymptoticdistribution n (cid:161)Î² (cid:98) âÎ²(cid:162) asnââassumingthatÎ²=0. (c) WhyistheassumptionÎ²=0animportantsimplifyingconditioninpart(b)? (d) Usingtheresultin(c)constructanappropriateasymptotictestforthehypothesis(cid:72) :Î²=0. 0 Exercise11.15 Theobservationsarei.i.d., (Y ,Y ,X :i =1,...,n).ThedependentvariablesY andY 1i 2i i 1 2 arereal-valued.TheregressorX isak-vector.Themodelisthetwo-equationsystem Y =X (cid:48)Î² +e 1 1 1 (cid:69)[Xe ]=0 1 Y =X (cid:48)Î² +e 2 2 2 (cid:69)[Xe ]=0. 2 (a) WhataretheappropriateestimatorsÎ² (cid:98)1 andÎ² (cid:98)2 forÎ² 1 andÎ² 2 ? (b) FindthejointasymptoticdistributionofÎ² (cid:98)1 andÎ² (cid:98)2 . (c) Describeatestfor(cid:72) :Î² =Î² . 0 1 2",
    "page": 351,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Chapter 12 Instrumental Variables 12.1 Introduction Theconceptsofendogeneityandinstrumentalvariablearefundamentaltoeconometrics,andmark asubstantialdeparturefromotherbranchesofstatistics.Theideasofendogeneityarisenaturallyineco- nomicsfrommodelsofsimultaneousequations,mostnotablytheclassicsupply/demandmodelofprice determination. TheidentificationprobleminsimultaneousequationsdatesbacktoPhilipWright(1915)andWork- ing(1927). ThemethodofinstrumentalvariablesfirstappearsinanAppendixofa1928bookbyPhilip Wright,thoughtheauthorshipissometimescreditedtohissonSewellWright. Thelabelâinstrumental variablesâwasintroducedbyReiersÃ¸l(1945).Anexcellentreviewofthehistoryofinstrumentalvariables isStockandTrebbi(2003). 12.2 Overview Wesaythatthereisendogeneityinthelinearmodel Y =X (cid:48)Î²+e (12.1) ifÎ²istheparameterofinterestand (cid:69)[Xe](cid:54)=0. (12.2) Thisisacoreproblemineconometricsandlargelydifferentiatesthefieldfromstatistics. Todistinguish (12.1)fromtheregressionandprojectionmodels,wewillcall(12.1)astructuralequationandÎ²astruc- turalparameter.When(12.2)holds,itistypicaltosaythatX isendogenousforÎ². Endogeneitycannothappenifthecoefficientisdefinedbylinearprojection. Indeed,wecandefine thelinearprojectioncoefficientÎ²â=(cid:69)(cid:163) XX (cid:48)(cid:164)â1(cid:69)[XY]andlinearprojectionequation Y =X (cid:48)Î²â+e â (cid:69)(cid:163) Xe â(cid:164)=0. However,underendogeneity(12.2)theprojectioncoefficientÎ²â doesnotequalthestructuralparameter Î².Indeed, Î²â=(cid:161)(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:162)â1(cid:69)[XY] =(cid:161)(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:162)â1(cid:69)(cid:163) X (cid:161) X (cid:48)Î²+e (cid:162)(cid:164) =Î²+(cid:161)(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:162)â1(cid:69)[Xe](cid:54)=Î² 332",
    "page": 352,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 333 thefinalrelationsince(cid:69)[Xe](cid:54)=0. Thus endogeneity requires that the coefficientbe defined differently than projection. We describe suchdefinitionsasstructural.Wewillpresentthreeexamplesinthefollowingsection. Endogeneity implies that the least squares estimator is inconsistent for the structural parameter. Indeed,underi.i.d.sampling,leastsquaresisconsistentfortheprojectioncoefficientandthusisincon- sistentforÎ². Î² (cid:98) ââ(cid:161)(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:162)â1(cid:69)[XY]=Î²â(cid:54)=Î². p Theinconsistencyofleastsquaresistypicallyreferredtoasendogeneitybiasorestimationbiasdueto endogeneity.(Thisisanimperfectlabelastheactualissueisinconsistency,notbias.) AsthestructuralparameterÎ²istheparameterofinterest,endogeneityrequiresthedevelopmentof alternativeestimationmethods.Wediscussthoseinlatersections. 12.3 Examples Theconceptofendogeneitymaybeeasiesttounderstandbyexample.Wediscussthree.Ineachcase itisimportanttoseehowthestructuralparameterÎ²isdefinedindependentlyfromthelinearprojection model. Example: Measurement error in the regressor. Suppose that (Y,Z) are joint random variables, (cid:69)[Y |Z]=Z (cid:48)Î²islinear,andÎ²isthestructuralparameter. Z isnotobserved.InsteadweobserveX =Z+u whereu isakÃ1measurementerror, independentofe and Z.Thisisanexampleofalatentvariable model,whereâlatentâreferstoastructuralvariablewhichisunobserved. Themodel X =Z+u with Z andu independentand(cid:69)[u]=0isknownasclassicalmeasurement error.ThismeansthatX isanoisybutunbiasedmeasureofZ. BysubstitutionwecanexpressY asafunctionoftheobservedvariableX. Y =Z (cid:48)Î²+e=(Xâu) (cid:48)Î²+e=X (cid:48)Î²+v wherev=eâu (cid:48)Î².Thismeansthat(Y,X)satisfythelinearequation Y =X (cid:48)Î²+v withanerrorv.Butthiserrorisnotaprojectionerror.Indeed, (cid:69)[Xv]=(cid:69)(cid:163) (Z+u) (cid:161) eâu (cid:48)Î²(cid:162)(cid:164)=â(cid:69)(cid:163) uu (cid:48)(cid:164)Î²(cid:54)=0 ifÎ²(cid:54)=0and(cid:69)(cid:163) uu (cid:48)(cid:164)(cid:54)=0. Aswelearnedintheprevioussection,if(cid:69)[Xv](cid:54)=0thenleastsquaresestimation willbeinconsistent. We can calculate the form of the projection coefficient (which is consistently estimated by least squares).Forsimplicitysupposethatk=1.Wefind Î²â=Î²+ (cid:69)[Xv] =Î² (cid:195) 1â (cid:69)(cid:163) u2(cid:164)(cid:33) . (cid:69)(cid:163) X2 (cid:164) (cid:69)(cid:163) X2 (cid:164) Since(cid:69)(cid:163) u2(cid:164) /(cid:69)(cid:163) X2(cid:164)<1theprojectioncoefficientshrinksthestructuralparameterÎ²towardszero. This iscalledmeasurementerrorbiasorattenuationbias. Example: SupplyandDemand. ThevariablesQ andP (quantityandprice)aredeterminedjointly bythedemandequation Q=âÎ² P+e 1 1",
    "page": 353,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 334 andthesupplyequation Q=Î² P+e . 2 2 Assumethate=(e ,e )satisfies(cid:69)[e]=0and(cid:69)(cid:163) ee (cid:48)(cid:164)=I (thelatterforsimplicity). Thequestionis: ifwe 1 2 2 regressQ onP,whathappens? ItishelpfultosolveforQ andP intermsoftheerrors.Inmatrixnotation, (cid:183) 1 Î² (cid:184)(cid:181) Q (cid:182) (cid:181) e (cid:182) 1 = 1 1 âÎ² P e 2 2 so (cid:181) Q (cid:182) (cid:183) 1 Î² (cid:184)â1(cid:181) e (cid:182) = 1 1 P 1 âÎ² e 2 2 (cid:183) Î² Î² (cid:184)(cid:181) e (cid:182)(cid:181) 1 (cid:182) = 2 1 1 1 â1 e Î² +Î² 2 1 2 (cid:181) (cid:161)Î² e +Î² e (cid:162) /(Î² +Î² ) (cid:182) = 2 1 1 2 1 2 . (e âe )/(Î² +Î² ) 1 2 1 2 TheprojectionofQ onP yieldsQ=Î²â P+e â with(cid:69)[Pe â ]=0andtheprojectioncoefficientis Î²â= (cid:69)[PQ] = Î² 2 âÎ² 1 . (cid:69)(cid:163) P2 (cid:164) 2 TheprojectioncoefficientÎ²â equalsneitherthedemandslopeÎ² northesupplyslopeÎ² ,butequalsan 1 2 averageofthetwo.(Thefactthatitisasimpleaverageisanartifactofthecovariancestructure.) TheOLSestimatorsatisfiesÎ² (cid:98) ââÎ²â andthelimitdoesnotequaleitherÎ² 1 orÎ² 2 .Thefactthatthe p limitisneitherthesupplynordemandslopeiscalledsimultaneousequationsbias. Thisoccursgener- allywhenY andX arejointlydetermined,asinamarketequilibrium. Generally, when both the dependent variable and a regressor are simultaneously determined then thevariablesshouldbetreatedasendogenous. Example:ChoiceVariablesasRegressors.Taketheclassicwageequation log (cid:161) wage (cid:162)=Î²education+e withÎ²theaveragecausaleffectofeducationonwages. Ifwagesareaffectedbyunobservedabilityand individualswithhighabilityself-selectintohighereducationthene containsunobservedability,soed- ucation and e will be positively correlated. Hence education is endogenous. The positive correlation meansthatthelinearprojectioncoefficientÎ²â willbeupwardbiasedrelativetothestructuralcoefficient Î².Thusleastsquares(whichisestimatingtheprojectioncoefficient)willtendtoover-estimatethecausal effectofeducationonwages. This type of endogeneity occurs generally when Y and X are both choices made by an economic agent,eveniftheyaremadeatdifferentpointsintime. Generally,whenboththedependentvariableandaregressorarechoicevariablesmadebythesame agent,thevariablesshouldbetreatedasendogenous.",
    "page": 354,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 335 12.4 EndogenousRegressors Wehavedefinedendogeneityasthecontextwherearegressoriscorrelatedwiththeequationerror. Theconverseofendogeneityisexogeneity. Thatis, wesayaregressor X isexogenousforÎ²if(cid:69)[Xe]= 0. In general the distinction in an economic model is that a regressor X is endogenous if it is jointly determinedwithY,whilearegressorX isexogenousifitisdeterminedseparatelyfromY. Inmostapplicationsonlyasubsetoftheregressorsaretreatedasendogenous.PartitionX =(X ,X ) 1 2 withdimensions(k ,k )sothatX containstheexogenousregressorsandX containstheendogenous 1 2 1 2 regressors. AsthedependentvariableY isalsoendogenous,wesometimesdifferentiateX bycallingit 2 theendogenousright-hand-sidevariable. SimilarlypartitionÎ²=(Î² ,Î² ). Withthisnotationthestruc- 1 2 turalequationis Y =X (cid:48)Î² +X (cid:48)Î² +e. (12.3) 1 1 2 2 Analternativenotationisasfollows. LetY =X betheendogenousregressorsandrenamethedepen- 2 2 dentvariableY asY .Thenthestructuralequationis 1 Y =X (cid:48)Î² +Y (cid:48)Î² +e. (12.4) 1 1 1 2 2 Thisisespeciallyusefulsothatthenotationclarifieswhichvariablesareendogenousandwhichexoge- nous. WealsowriteY (cid:126) =(Y ,Y )asthesetofendogenousvariables. WeusethenotationY (cid:126) sothatthere 1 2 isnoconfusionwithY asdefinedin(12.3). Theassumptionsregardingtheregressorsandregressionerrorare (cid:69)[X e]=0 1 (cid:69)[Y e](cid:54)=0. 2 The endogenous regressors Y are the critical variables discussed in the examples of the previous 2 sectionâsimultaneousvariables,choicevariables,mis-measuredregressorsâthatarepotentiallycorre- latedwiththeequationerrore.Inmanyapplicationsk issmall(1or2).TheexogenousvariablesX are 2 1 theremainingregressors(includingtheequationintercept)andcanbeloworhighdimensional. 12.5 Instruments ToconsistentlyestimateÎ²werequireadditionalinformation.Onetypeofinformationwhichiscom- monlyusedineconomicapplicationsarewhatwecallinstruments. Definition12.1 The (cid:96)Ã1 random vector Z is an instrumental variable for (12.3)if (cid:69)[Ze]=0 (12.5) (cid:69)(cid:163) ZZ (cid:48)(cid:164)>0 (12.6) rank (cid:161)(cid:69)(cid:163) ZX (cid:48)(cid:164)(cid:162)=k. (12.7) There are three components to the definition as given. The first (12.5) is that the instruments are uncorrelated with the regression error. The second (12.6) is a normalization which excludes linearly",
    "page": 355,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 336 redundantinstruments. Thethird(12.7)isoftencalledtherelevanceconditionandisessentialforthe identificationofthemodel,aswediscusslater.Anecessaryconditionfor(12.7)isthat(cid:96)â¥k. Condition(12.5)âthattheinstrumentsareuncorrelatedwiththeequationerrorâisoftendescribed asthattheyareexogenousinthesensethattheyaredeterminedoutsidethemodelforY. Notice that the regressors X satisfy condition (12.5) and thus should be included as instrumental 1 variables.TheyarethereforeasubsetofthevariablesZ.Notationallywemakethepartition (cid:181) (cid:182) (cid:181) (cid:182) Z X k Z = 1 = 1 1 . (12.8) Z Z (cid:96) 2 2 2 Here,X =Z aretheincludedexogenousvariablesandZ aretheexcludedexogenousvariables.That 1 1 2 is,Z arevariableswhichcouldbeincludedintheequationforY (inthesensethattheyareuncorrelated 2 withe)yetcanbeexcludedastheyhavetruezerocoefficientsintheequation.Withthisnotationwecan alsowritethestructuralequation(12.4)as Y =Z (cid:48)Î² +Y (cid:48)Î² +e. (12.9) 1 1 1 2 2 ThisisusefulnotationasitclarifiesthatthevariableZ isexogenousandthevariableY isendogenous. 1 2 ManyauthorsdescribeZ astheâexogenousvariablesâ, Y astheâendogenousvariablesâ,andZ as 1 2 2 theâinstrumentalvariablesâ. Wesaythatthemodelisjust-identifiedif(cid:96)=k andover-identifiedif(cid:96)>k. Whatvariablescanbeusedasinstrumentalvariables?Fromthedefinition(cid:69)[Ze]=0theinstrument mustbeuncorrelatedwiththeequationerror,meaningthatitisexcludedfromthestructuralequationas mentionedabove. Fromtherankcondition(12.7)itisalsoimportantthattheinstrumentalvariablesbe correlatedwiththeendogenousvariablesY aftercontrollingfortheotherexogenousvariablesZ .These 2 1 tworequirementsaretypicallyinterpretedasrequiringthattheinstrumentsbedeterminedoutsidethe (cid:126) systemforY,causallydetermineY ,butdonotcausallydetermineY exceptthroughY . 2 1 2 Letâstakethethreeexamplesgivenabove. Measurementerrorintheregressor. WhenX isamis-measuredversionofZ acommonchoicefor aninstrumentZ isanalternativemeasurementofZ.ForthisZ tosatisfythepropertyofaninstrumen- 2 2 talvariablethemeasurementerrorinZ mustbeindependentofthatinX. 2 SupplyandDemand. AnappropriateinstrumentforpriceP inademandequationisavariable Z 2 whichinfluencessupplybutnotdemand. SuchavariableaffectstheequilibriumvaluesofP andQ but doesnotdirectlyaffectpriceexceptthroughquantity.Variableswhichaffectsupplybutnotdemandare typicallyrelatedtoproductioncosts. Anappropriateinstrumentforpriceinasupplyequationisavariablewhichinfluencesdemandbut not supply. Such a variable affects the equilibrium values of price and quantity but only affects price throughquantity. ChoiceVariableasRegressor. An ideal instrument affects the choice of the regressor (education) butdoesnotdirectlyinfluencethedependentvariable(wages)exceptthroughtheindirecteffectonthe regressor.Wewilldiscussanexampleinthenextsection",
    "page": 356,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Anappropriateinstrumentforpriceinasupplyequationisavariablewhichinfluencesdemandbut not supply. Such a variable affects the equilibrium values of price and quantity but only affects price throughquantity. ChoiceVariableasRegressor. An ideal instrument affects the choice of the regressor (education) butdoesnotdirectlyinfluencethedependentvariable(wages)exceptthroughtheindirecteffectonthe regressor.Wewilldiscussanexampleinthenextsection. 12.6 Example: CollegeProximity InainfluentialpaperDavidCard(1995)suggestedifapotentialstudentlivesclosetoacollegethis reduces the cost of attendence and thereby raises the likelihood that the student will attend college. However, college proximity does not directly affect a studentâs skills or abilities so should not have a directeffectonhisorhermarketwage.Theseconsiderationssuggestthatcollegeproximitycanbeused",
    "page": 356,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 337 asaninstrumentforeducationinawageregression.WeusethesimplestmodelreportedinCardâspaper toillustratetheconceptsofinstrumentalvariablesthroughoutthechapter. CarduseddatafromtheNationalLongitudinalSurveyofYoungMen(NLSYM)for1976. Abaseline leastsquareswageregressionforhisdatasetisreportedinthefirstcolumnofTable12.1.Thedependent variableisthelogofweeklyearnings.Theregressorsareeducation(yearsofschooling),experience(years ofworkexperience,calculatedasage(years)lesseducation+6),experience2/100,Black,south(anindica- torforresidenceinthesouthernregionoftheU.S.),andurban(anindicatorforresidenceinastandard metropolitanstatisticalarea). Wedropobservationsforwhichwageismissing. Theremainingsample has3,010observations.HisdataisthefileCard1995onthetextbookwebsite. The point estimate obtained by least squares suggests an 7% increase in earnings for each year of education. Table12.1:InstrumentalVariableWageRegressions OLS IV(a) IV(b) 2SLS(a) 2SLS(b) LIML education 0.074 0.132 0.133 0.161 0.160 0.164 (0.004) (0.049) (0.051) (0.040) (0.041) (0.042) experience 0.084 0.107 0.056 0.119 0.047 0.120 (0.007) (0.021) (0.026) (0.018) (0.025) (0.019) experience2/100 â0.224 â0.228 â0.080 â0.231 â0.032 â0.231 (0.032) (0.035) (0.133) (0.037) (0.127) (0.037) Black â0.190 â0.131 â0.103 â0.102 â0.064 â0.099 (0.017) (0.051) (0.075) (0.044) (0.061) (0.045) south â0.125 â0.105 â0.098 â0.095 â0.086 â0.094 (0.015) (0.023) (0.0284) (0.022) (0.026) (0.022) urban 0.161 0.131 0.108 0.116 0.083 0.115 (0.015) (0.030) (0.049) (0.026) (0.041) (0.027) Sargan 0.82 0.52 0.82 p-value 0.37 0.47 0.37 Notes: 1. IV(a)usescollegeasaninstrumentforeducation. 2. IV(b)usescollege,age,andage2/100asinstrumentsforeducation,experience,andexperience2/100. 3. 2SLS(a)usespublicandprivateasinstrumentsforeducation. 4. 2SLS(b) uses public, private, age, and age2 as instruments for education, experience, and experi- ence2/100. 5. LIMLusespublicandprivateasinstrumentsforeducation. Asdiscussedintheprevioussectionsitisreasonabletoviewyearsofeducationasachoicemadeby anindividualandthusislikelyendogenousforthestructuralreturntoeducation. Thismeansthatleast squares is an estimate of a linear projection but is inconsistent for coefficient of a structural equation representingthecausalimpactofyearsofeducationonexpectedwages. Laboreconomicspredictsthat ability,education,andwageswillbepositivelycorrelated. Thissuggeststhatthepopulationprojection coefficientestimatedbyleastsquareswillbehigherthanthestructuralparameter(andhenceupwards",
    "page": 357,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 338 biased).However,thesignofthebiasisuncertainsincetherearemultipleregressorsandthereareother potentialsourcesofendogeneity. To instrument for the endogeneity of education, Card suggested that a reasonable instrument is a dummyvariableindicatingiftheindividualgrewupnearacollege.Wewillconsiderthreemeasures: college Grewupinsamecountyasa4-yearcollege public Grewupinsamecountyasa4-yearpubliccollege private Grewupinsamecountyasa4-yearprivatecollege. 12.7 ReducedForm ThereducedformistherelationshipbetweentheendogenousregressorsY andtheinstrumentsZ. 2 AlinearreducedformmodelforY is 2 Y =Î(cid:48) Z+u =Î(cid:48) Z +Î(cid:48) Z +u (12.10) 2 2 12 1 22 2 2 ThisisamultivariateregressionasintroducedinChapter11. The(cid:96)Ãk coefficientmatrixÎisdefined 2 bylinearprojection: Î=(cid:69)(cid:163) ZZ (cid:48)(cid:164)â1(cid:69)(cid:163) ZY (cid:48)(cid:164) (12.11) 2 Thisimplies(cid:69)(cid:163) Zu (cid:48)(cid:164)=0.Theprojectioncoefficient(12.11)iswelldefinedanduniqueunder(12.6). 2 WealsoconstructthereducedformforY .Substitute(12.10)into(12.9)toobtain 1 Y =Z (cid:48)Î² +(cid:161)Î(cid:48) Z +Î(cid:48) Z +u (cid:162)(cid:48) Î² +e 1 1 1 12 1 22 2 2 2 =Z (cid:48)Î» +Z (cid:48)Î» +u (12.12) 1 1 2 2 1 =Z (cid:48)Î»+u (12.13) 1 where Î» =Î² +Î Î² (12.14) 1 1 12 2 Î» =Î Î² (12.15) 2 22 2 u =u (cid:48)Î² +e. 1 2 2 Wecanalsowrite Î»=ÎÎ² (12.16) where (cid:183) I Î (cid:184) (cid:183) I (cid:184) Î= k1 12 = k1 Î . 0 Î 0 22 Together,thereducedformequationsforthesystemare Y =Î»(cid:48) Z+u 1 1 Y =Î(cid:48) Z+u . 2 2 or (cid:183) Î»(cid:48) Î»(cid:48) (cid:184) Y (cid:126) = Î(cid:48) 1 Î(cid:48) 2 Z+u (12.17) 12 22 whereu=(u ,u ). 1 2",
    "page": 358,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 339 Therelationships(12.14)-(12.16)arecriticallyimportantforunderstandingtheidentificationofthe structuralparametersÎ² andÎ² ,aswediscussbelow. Theseequationsshowthetightrelationshipbe- 1 2 tweenthestructuralparameters(Î² andÎ² )andthereducedformparameters(ÎandÎ»). 1 2 The reduced form equations are projections so the coefficients may be estimated by least squares (seeChapter11).Theleastsquaresestimatorsof(12.11)and(12.13)are (cid:195) (cid:33)â1(cid:195) (cid:33) n n Î (cid:98) = (cid:88) Z i Z i (cid:48) (cid:88) Z i Y 2 (cid:48) i (12.18) i=1 i=1 (cid:195) (cid:33)â1(cid:195) (cid:33) n n Î» (cid:98) = (cid:88) Z i Z i (cid:48) (cid:88) Z i Y 1i (12.19) i=1 i=1 12.8 Identification Aparameterisidentifiedifitisauniquefunctionoftheprobabilitydistributionoftheobservables. One way to show that a parameter is identified is to write it as an explicit function of population mo- ments. For example, the reduced form coefficient matrices Î and Î» are identified since they can be writtenasexplicitfunctionsofthemomentsofthevariables(Y,X,Z).Thatis, Î=(cid:69)(cid:163) ZZ (cid:48)(cid:164)â1(cid:69)(cid:163) ZY (cid:48)(cid:164) (12.20) 2 Î»=(cid:69)(cid:163) ZZ (cid:48)(cid:164)â1(cid:69)[ZY ]. (12.21) 1 Theseareuniquelydeterminedbytheprobabilitydistributionof(Y ,Y ,Z)ifDefinition12.1holds,since 1 2 thisincludestherequirementthat(cid:69)(cid:163) ZZ (cid:48)(cid:164) isinvertible. WeareinterestedinthestructuralparameterÎ². Itrelatesto(Î»,Î)through(12.16). Î²isidentifiedif ituniquelydeterminedbythisrelation. Thisisasetof(cid:96)equationswithk unknownswith(cid:96)â¥k. From linearalgebraweknowthatthereisauniquesolutionifandonlyifÎhasfullrankk. (cid:179) (cid:180) rank Î =k. (12.22) Under(12.22)Î²canbeuniquelysolvedfrom(12.16).If(12.22)failsthen(12.16)hasfewerequationsthan coefficientssothereisnotauniquesolution. WecanwriteÎ=(cid:69)(cid:163) ZZ (cid:48)(cid:164)â1(cid:69)(cid:163) ZX (cid:48)(cid:164) .Combiningthiswith(12.16)weobtain (cid:69)(cid:163) ZZ (cid:48)(cid:164)â1(cid:69)[ZY ]=(cid:69)(cid:163) ZZ (cid:48)(cid:164)â1(cid:69)(cid:163) ZX (cid:48)(cid:164)Î² 1 or (cid:69)[ZY ]=(cid:69)(cid:163) ZX (cid:48)(cid:164)Î² 1 whichisasetof(cid:96)equationswithk unknowns.Thishasauniquesolutionif(andonlyif) rank (cid:161)(cid:69)(cid:163) ZX (cid:48)(cid:164)(cid:162)=k (12.23) which was listed in (12.7) as a condition of Definition 12.1. (Indeed, this is why it was listed as part ofthedefinition.) Wecanalsoseethat(12.22)and(12.23)areequivalentwaysofexpressingthesame requirement. IfthisconditionfailsthenÎ²willnotbeidentified. Thecondition(12.22)-(12.23)iscalled therelevancecondition. ItisusefultohaveexplicitexpressionsforthesolutionÎ².Theeasiestcaseiswhen(cid:96)=k.Then(12.22) impliesÎisinvertiblesothestructuralparameterequalsÎ²=Î â1Î».ItisauniquesolutionbecauseÎand Î»areuniqueandÎisinvertible.",
    "page": 359,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 340 When(cid:96)>k wecansolveforÎ²byapplyingleastsquarestothesystemofequationsÎ»=ÎÎ². Thisis(cid:96) (cid:179) (cid:48) (cid:180)â1 (cid:48) equationswithkunknownsandnoerror.TheleastsquaressolutionisÎ²= ÎÎ ÎÎ».Under(12.22)the (cid:48) matrixÎÎisinvertiblesothesolutionisunique. Î²isidentifiedifrank(Î)=k,whichistrueifandonlyifrank(Î )=k (bytheupper-diagonalstruc- 22 2 tureofÎ).Thusthekeytoidentificationofthemodelrestsonthe(cid:96) Ãk matrixÎ in(12.10).Toseethis, 2 2 22 recallthereducedformrelationships(12.14)-(12.15).WecanseethatÎ² isidentifiedfrom(12.15)alone, 2 andthenecessaryandsufficientconditionisrank(Î )=k . Ifthisissatisfiedthenthesolutioncanbe 22 2 writtenasÎ² =(cid:161)Î(cid:48) Î (cid:162)â1Î(cid:48) Î» . ThenÎ² isidentifiedfromthisand(12.14), withtheexplicitsolution 2 22 22 22 2 1 Î² =Î» âÎ (cid:161)Î(cid:48) Î (cid:162)â1Î(cid:48) Î» . Inthejust-identifiedcase((cid:96) =k )theseequationssimplifytotakethe 1 1 12 22 22 22 2 2 2 formÎ² =Îâ1Î» andÎ² =Î» âÎ Îâ1Î» . 2 22 2 1 1 12 22 2 12.9 InstrumentalVariablesEstimator Inthissectionweconsiderthespecialcasewherethemodelisjust-identifiedsothat(cid:96)=k. Theassumptionthat Z isaninstrumentalvariableimpliesthat(cid:69)[Ze]=0. Makingthesubstitution e=Y âX (cid:48)Î²wefind(cid:69)(cid:163) Z (cid:161) Y âX (cid:48)Î²(cid:162)(cid:164)=0.Expanding, 1 1 (cid:69)[ZY ]â(cid:69)(cid:163) ZX (cid:48)(cid:164)Î²=0. 1 Thisisasystemof(cid:96)=k equationsandk unknowns.SolvingforÎ²wefind Î²=(cid:161)(cid:69)(cid:163) ZX (cid:48)(cid:164)(cid:162)â1(cid:69)[ZY ]. 1 Thisrequiresthatthematrix(cid:69)(cid:163) ZX (cid:48)(cid:164) isinvertible,whichholdsunder(12.7)orequivalently(12.23). Theinstrumentalvariables(IV)estimatorÎ²replacespopulationbysamplemoments.Wefind (cid:195) (cid:33)â1(cid:195) (cid:33) Î² (cid:98)iv = n 1 (cid:88) n Z i X i (cid:48) n 1 (cid:88) n Z i Y 1i i=1 i=1 (cid:195) (cid:33)â1(cid:195) (cid:33) n n = (cid:88) Z X (cid:48) (cid:88) Z Y . (12.24) i i i 1i i=1 i=1 Moregenerally,givenanyvariableW â(cid:82)k itiscommontorefertotheestimator (cid:195) (cid:33)â1(cid:195) (cid:33) n n Î² (cid:98)iv = (cid:88) W i X i (cid:48) (cid:88) W i Y 1i i=1 i=1 astheIVestimatorforÎ²usingtheinstrumentW. Alternatively, recall that when (cid:96) = k the structural parameter can be written as a function of the reducedformparametersasÎ²=Î â1Î».ReplacingÎandÎ»bytheirleastsquaresestimators(12.18)-(12.19) we can construct what is called the Indirect Least Squares (ILS) estimator. Using the matrix algebra representations â1 Î² (cid:98)ils =Î(cid:98) Î» (cid:98) = (cid:179) (cid:161) Z (cid:48) Z (cid:162)â1(cid:161) Z (cid:48) X (cid:162) (cid:180)â1(cid:179) (cid:161) Z (cid:48) Z (cid:162)â1(cid:161) Z (cid:48) Y (cid:162) (cid:180) 1 =(cid:161) Z (cid:48) X (cid:162)â1(cid:161) Z (cid:48) Z (cid:162)(cid:161) Z (cid:48) Z (cid:162)â1(cid:161) Z (cid:48) Y (cid:162) 1 =(cid:161) Z (cid:48) X (cid:162)â1(cid:161) Z (cid:48) Y (cid:162) . 1",
    "page": 360,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 341 WeseethatthisequalstheIVestimator(12.24).ThustheILSandIVestimatorsareidentical. GiventheIVestimatorwedefinetheresiduale (cid:98)i =Y 1i âX i (cid:48)Î² (cid:98)iv .Itsatisfies Z (cid:48) e=Z (cid:48) Y âZ (cid:48) X (cid:161) Z (cid:48) X (cid:162)â1(cid:161) Z (cid:48) Y (cid:162)=0. (12.25) (cid:98) 1 1 Since Z includesaninterceptthismeansthattheresidualssumtozeroandareuncorrelatedwiththe includedandexcludedinstruments. ToillustrateIVregressionweestimatethereducedformequationsforcollegeproximity,nowtreating educationasendogenousandusingcollegeasaninstrumentalvariable.Thereducedformequationsfor log(wage)andeducationarereportedinthefirstandsecondcolumnsofTable12.2. Table12.2:ReducedFormRegressions log(wage) education education experience experience2/100 education experience 0.053 â0.410 â0.413 (0.007) (0.032) (0.032) experience2/100 â0.219 0.073 0.093 (0.033) (0.170) (0.171) black â0.264 â1.006 â1.468 1.468 0.282 â1.006 (0.018) (0.088) (0.115) (0.115) (0.026) (0.088) south â0.143 â0.291 â0.460 0.460 0.112 â0.267 (0.017) (0.078) (0.103) (0.103) (0.022) (0.079) urban 0.185 0.404 0.835 â0.835 â0.176 0.400 (0.017) (0.085) (0.112) (0.112) (0.025) (0.085) college 0.045 0.337 0.347 â0.347 â0.073 (0.016) (0.081) (0.109) (0.109) (0.023) public 0.430 (0.086) private 0.123 (0.101) age 1.061 â0.061 â0.555 (0.296) (0.296) (0.065) age2/100 â1.876 1.876 1.313 (0.516) (0.516) (0.116) F 17.51 8.22 1581 1112 13.87 Ofparticularinterestistheequationfortheendogenousregressoreducation,andthecoefficientsfor theexcludedinstrumentsâinthiscasecollege.Theestimatedcoefficientequals0.337withasmallstan- darderror. Thisimpliesthatgrowingupneara4-yearcollegeincreasesaverageeducationalattainment by0.3years.Thisseemstobeareasonablemagnitude. Since the structural equation is just-identified with one right-hand-side endogenous variable the ILS/IVestimatefortheeducationcoefficientistheratioofthecoefficientestimatesfortheinstrument college inthetwoequations, e.g. 0.045/0.337=0.13, implyinga13%returntoeachyearofeducation. Thisissubstantiallygreaterthanthe7%leastsquaresestimatefromthefirstcolumnofTable12.1. The IVestimatesofthefullequationarereportedinthesecondcolumnofTable12.1.Onefirstreactionissur- prisethattheIVestimateislargerthantheOLSestimate.Theendogeneityofeducationalchoiceshould leadtoupwardbiasintheOLSestimator,whichpredictsthattheIVestimateshouldhavebeensmaller than the OLS estimator. An alternative explanation may be needed. One possibility is heterogeneous educationeffects(whentheeducationcoefficientÎ²isheterogenousacrossindividuals).InSection12.34",
    "page": 361,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 342 weshowthatinthiscontexttheIVestimatorpicksupthistreatmenteffectforasubsetofthepopulation, andthismayexplainwhyIVestimationresultsinalargerestimatedcoefficientoneducation. Card(1995)alsopointsoutthatifeducationisendogenousthensoisourmeasureofexperiencesince itiscalculatedbysubtractingeducationfromage.Hesuggeststhatwecanusethevariablesageandage2 as instruments for experience and experience2. The age variables are exogenous (not choice variables) yethighlycorrelatedwithexperienceandexperience2. Noticethatthisapproachtreatsexperience2 asa variableseparatefromexperience.Indeed,thisisthecorrectapproach. Followingthisrecommendationwenowhavethreeendogenousregressorsandthreeinstruments. Wepresentthethreereducedformequationsforthethreeendogenousregressorsinthethirdthrough fifthcolumnsofTable12.2. Itisinterestingtocomparetheequationsforeducationandexperience. The twosetsofcoefficientsaresimplythesignchangeoftheotherwiththeexceptionofthecoefficienton age. Indeed this must be the case because the three variables are linearly related. Does this cause a problemfor2SLS?Fortunately,no.Thefactthatthecoefficientonageisnotsimplyasignchangemeans thattheequationsarenotlinearlysingular.HenceAssumption(12.22)isnotviolated. The IV estimates using the three instruments college, age, and age2 for the endogenous regressors education, experience, and experience2 is presented in the third column of Table 12.1. The estimate of thereturnstoschoolingisnotaffectedbythischangeintheinstrumentset,buttheestimatedreturnto experienceprofileflattens(thequadraticeffectdiminishes). TheIVestimatormaybecalculatedinStatausingtheivregress 2slscommand. 12.10 DemeanedRepresentation Does the well-known demeaned representation for linear regression (3.18) carry over to the IV es- timator? Toseethis, writethelinearprojectionequationintheformatY =X (cid:48)Î²+Î±+e whereÎ±isthe 1 interceptand X doesnotcontainaconstant. Similarly,partitiontheinstrumentas(1,Z)where Z does notcontainanintercept.WecanwritetheIVestimatorfortheith equationas Y 1i =X i (cid:48)Î² (cid:98)iv +Î± (cid:98)iv +e (cid:98)i . Theorthogonality(12.25)impliesthetwo-equationsystem n (cid:88)(cid:161) Y 1i âX i (cid:48)Î² (cid:98)iv âÎ± (cid:98)iv (cid:162)=0 i=1 n (cid:88) Z i (cid:161) Y 1i âX i (cid:48)Î² (cid:98)iv âÎ± (cid:98)iv (cid:162)=0. i=1 (cid:48) ThefirstequationimpliesÎ± (cid:98)iv =Y 1 âX Î² (cid:98)iv .Substitutingintothesecondequation (cid:88) n (cid:179)(cid:179) (cid:180) (cid:179) (cid:180)(cid:48) (cid:180) Z i Y 1i âY 1 â X i âX Î² (cid:98)iv i=1 andsolvingforÎ² (cid:98)iv wefind (cid:195) (cid:33)â1(cid:195) (cid:33) (cid:88) n (cid:179) (cid:180)(cid:48) (cid:88) n (cid:179) (cid:180) Î² (cid:98)iv = Z i X i âX Z i Y 1i âY 1 i=1 i=1 (cid:195) (cid:33)â1(cid:195) (cid:33) (cid:88) n (cid:179) (cid:180)(cid:179) (cid:180)(cid:48) (cid:88) n (cid:179) (cid:180)(cid:179) (cid:180) = Z âZ X âX Z âZ Y âY",
    "page": 362,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". (12.26) i i i 1i 1 i=1 i=1 ThusthedemeaningequationsforleastsquarescarryovertotheIVestimator. Thecoefficientesti- matorÎ² (cid:98)iv isafunctiononlyofthedemeaneddata.",
    "page": 362,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 343 12.11 WaldEstimator InmanycasesincludingtheCardproximityexampletheexcludedinstrumentisabinary(dummy) variable. Letâsfocusonthatcaseandsupposethatthemodelhasjustoneendogenousregressorandno otherregressorsbeyondtheintercept. ThemodelcanbewrittenasY =XÎ²+Î±+e with(cid:69)[e|Z]=0and Z binary. TakeexpectationsofthestructuralequationgivenZ =1andZ =0,respectively.Weobtain (cid:69)[Y |Z =1]=(cid:69)[X |Z =1]Î²+Î± (cid:69)[Y |Z =0]=(cid:69)[X |Z =0]Î²+Î±. Subtractinganddividingweobtainanexpressionfortheslopecoefficient (cid:69)[Y |Z =1]â(cid:69)[Y |Z =0] Î²= . (12.27) (cid:69)[X |Z =1]â(cid:69)[X |Z =0] Thenaturalmomentestimatorreplacestheexpectationsbytheaverageswithintheâgroupeddataâ whereZ =1andZ =0,respectively.Thatis,definethegroupmeans i i (cid:80)n Z Y (cid:80)n (1âZ )Y Y = i=1 i i , Y = i=1 i i 1 (cid:80)n Z 0 (cid:80)n (1âZ ) i=1 i i=1 i (cid:80)n Z X (cid:80)n (1âZ )X X = i=1 i i , X = i=1 i i 1 (cid:80)n Z 0 (cid:80)n (1âZ ) i=1 i i=1 i andthemomentestimator Y âY Î² (cid:98) = 1 0 . (12.28) X âX 1 0 ThisistheâWaldestimatorâofWald(1940). Theseexpressionsareratherinsightful. (12.27)showsthatthestructuralslopecoefficientistheex- pectedchangeinY duetochangingtheinstrumentdividedbytheexpectedchangeinX duetochanging theinstrument. Informally,itisthechangeinY (dueto Z)overthechangein X (dueto Z). Equation (12.28)showsthattheslopecoefficientcanbeestimatedbyatheratioofadifferenceinmeans. Theexpression(12.28)mayappearlikeadistinctestimatorfromtheIVestimatorÎ² (cid:98)iv butitturnsout thattheyarethesame.Thatis,Î² (cid:98) =Î² (cid:98)iv .Toseethis,use(12.26)tofind (cid:179) (cid:180) Î² (cid:98)iv = (cid:80)n i=1 Z i (cid:179) Y i âY (cid:180) = Y 1 âY . (cid:80)n i=1 Z i X i âX X 1 âX Thennotice (cid:195) (cid:33) 1 (cid:88) n 1 (cid:88) n (cid:179) (cid:180)(cid:179) (cid:180) Y âY =Y â Z Y + (1âZ )Y = 1âZ Y âY 1 1 i 1 i 0 1 0 n n i=1 i=1 andsimilarly (cid:179) (cid:180)(cid:179) (cid:180) X âX = 1âZ X âX 1 1 0 andhence (cid:179) (cid:180)(cid:179) (cid:180) 1âZ Y âY 1 0 Î² = =Î² (cid:98)iv (cid:179) (cid:180)(cid:179) (cid:180) (cid:98) 1âZ X âX 1 0 asdefinedin(12.28).ThustheWaldestimatorequalstheIVestimator.",
    "page": 363,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 344 WecanillustrateusingtheCardproximityexample.IfweestimateasimpleIVmodelwithnocovari- atesweobtaintheestimateÎ² (cid:98)iv =0.19.Ifweestimatethegroup-meanoflogwagesandeducationbased ontheinstrumentcollegewefind nearcollege notnearcollege difference log(wage) 6.311 6.156 0.155 education 13.527 12.698 0.829 ratio 0.19 BasedontheseestimatestheWaldestimatoroftheslopecoefficientis(6.311â6.156)/(13.527â12.698)= 0.155/0.829=0.19,thesameastheIVestimator. 12.12 Two-StageLeastSquares TheIVestimatordescribedintheprevioussectionpresumed(cid:96)=k. Nowweallowthegeneralcase of(cid:96)â¥k.Examiningthereduced-formequation(12.13)wesee Y =Z (cid:48)ÎÎ²+u 1 1 (cid:69)[Zu ]=0. 1 (cid:48) DefiningW =Î Z wecanwritethisas Y =W (cid:48)Î²+u 1 1 (cid:69)[Wu ]=0. 1 (cid:48) OnewayofthinkingaboutthisisthatZ issetofcandidateinstruments.TheinstrumentvectorW =Î Z isak-dimentionalsetoflinearcombinations. (cid:48) SupposethatÎwereknown.ThenwewouldestimateÎ²byleastsquaresofY onW =Î Z 1 Î² (cid:98) =(cid:161) W (cid:48) W (cid:162)â1(cid:161) W (cid:48) Y (cid:162)= (cid:179) Î (cid:48) Z (cid:48) ZÎ (cid:180)â1(cid:179) Î (cid:48) Z (cid:48) Y 1 (cid:180) . WhilethisisinfeasiblewecanestimateÎfromthereducedformregression.ReplacingÎwithitsestima- torÎ (cid:98) =(cid:161) Z (cid:48) Z (cid:162)â1(cid:161) Z (cid:48) X (cid:162) weobtain Î² (cid:98)2sls =(cid:161)Î (cid:98) (cid:48) Z (cid:48) ZÎ (cid:98) (cid:162)â1(cid:161)Î (cid:98) (cid:48) Z (cid:48) Y 1 (cid:162) = (cid:179) X (cid:48) Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) X (cid:180)â1 X (cid:48) Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) Y 1 = (cid:179) X (cid:48) Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) X (cid:180)â1 X (cid:48) Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) Y . (12.29) 1 This is called thetwo-stage-leastsquares(2SLS) estimator. It was originally proposed by Theil (1953) andBasmann(1957)andisastandardestimatorforlinearequationswithinstruments. Ifthemodelisjust-identified,sothatk=(cid:96),then2SLSsimplifiestotheIVestimatoroftheprevious (cid:48) (cid:48) section.SincethematricesX Z andZ X aresquarewecanfactor (cid:179) X (cid:48) Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) X (cid:180)â1 =(cid:161) Z (cid:48) X (cid:162)â1 (cid:179) (cid:161) Z (cid:48) Z (cid:162)â1 (cid:180)â1(cid:161) X (cid:48) Z (cid:162)â1 =(cid:161) Z (cid:48) X (cid:162)â1(cid:161) Z (cid:48) Z (cid:162)(cid:161) X (cid:48) Z (cid:162)â1 .",
    "page": 364,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 345 (Onceagain,thisonlyworkswhenk=(cid:96).)Then Î² (cid:98)2sls = (cid:179) X (cid:48) Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) X (cid:180)â1 X (cid:48) Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) Y 1 =(cid:161) Z (cid:48) X (cid:162)â1(cid:161) Z (cid:48) Z (cid:162)(cid:161) X (cid:48) Z (cid:162)â1 X (cid:48) Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) Y 1 =(cid:161) Z (cid:48) X (cid:162)â1(cid:161) Z (cid:48) Z (cid:162)(cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) Y 1 =(cid:161) Z (cid:48) X (cid:162)â1 Z (cid:48) Y 1 =Î² (cid:98)iv asclaimed.Thisshowsthatthe2SLSestimatorasdefinedin(12.29)isageneralizationoftheIVestimator definedin(12.24). There are several alternative representations of the 2SLS estimator which we now describe. First, definingtheprojectionmatrix P =Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) (12.30) Z wecanwritethe2SLSestimatormorecompactlyas Î² (cid:98)2sls =(cid:161) X (cid:48) P Z X (cid:162)â1 X (cid:48) P Z Y 1 . (12.31) ThisisusefulforrepresentationandderivationsbutisnotusefulforcomputationasthenÃnmatrixP Z istoolargetocomputewhennislarge. Second,definethefittedvaluesforX fromthereducedformX(cid:98) =P Z X =ZÎ (cid:98).Thenthe2SLSestimator canbewrittenas (cid:179) (cid:48) (cid:180)â1 (cid:48) Î² (cid:98)2sls = X(cid:98) X X(cid:98) Y 1 . ThisisanIVestimatorasdefinedintheprevioussectionusingX(cid:98) asaninstrumentforX. Third,sinceP isidempotentwecanalsowritethe2SLSestimatoras Z Î² (cid:98)2sls =(cid:161) X (cid:48) P Z P Z X (cid:162)â1 X (cid:48) P Z Y 1 = (cid:179) X(cid:98) (cid:48) X(cid:98) (cid:180)â1 X(cid:98) (cid:48) Y 1 whichistheleastsquaresestimatorobtainedbyregressingY 1 onthefittedvaluesX(cid:98). Thisisthesourceoftheâtwo-stageânameissinceitcanbecomputedasfollows. â¢ RegressX onZ toobtainthefittedX(cid:98):Î (cid:98) =(cid:161) Z (cid:48) Z (cid:162)â1(cid:161) Z (cid:48) X (cid:162) andX(cid:98) =ZÎ (cid:98) =P Z X. (cid:179) (cid:48) (cid:180)â1 (cid:48) â¢ RegressY 1 onX(cid:98):Î² (cid:98)2sls = X(cid:98) X(cid:98) X(cid:98) Y 1 . ItisusefultoscrutinizetheprojectionX(cid:98).Recall,X =[Z 1 ,Y 2 ]andZ =[Z 1 ,Z 2 ].NoticeX(cid:98)1 =P Z Z 1 = Z 1 sinceZ 1 liesinthespanofZ.ThenX(cid:98) =(cid:163) X(cid:98)1 ,Y(cid:98)2 (cid:164)=(cid:163) Z 1 ,Y(cid:98)2 (cid:164) . Thisshowsthatinthesecondstagewe regressY 1 on Z 1 andY(cid:98)2 .ThismeansthatonlytheendogenousvariablesY 2 arereplacedbytheirfitted values,Y(cid:98)2 =Î (cid:98) (cid:48) 12 Z 1 +Î (cid:98) (cid:48) 22 Z 2 . Afourthrepresentationof2SLScanbeobtainedusingtheFWLTheorem. Thethirdrepresentation andfollowingdiscussionshowedthat2SLSisobtainedasleastsquaresofY 1 onthefittedvalues(Z 1 ,Y(cid:98)2 ). Hence the coefficient Î² (cid:98)2 on the endogenous variables can be found by residual regression",
    "page": 365,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Thisshowsthatinthesecondstagewe regressY 1 on Z 1 andY(cid:98)2 .ThismeansthatonlytheendogenousvariablesY 2 arereplacedbytheirfitted values,Y(cid:98)2 =Î (cid:98) (cid:48) 12 Z 1 +Î (cid:98) (cid:48) 22 Z 2 . Afourthrepresentationof2SLScanbeobtainedusingtheFWLTheorem. Thethirdrepresentation andfollowingdiscussionshowedthat2SLSisobtainedasleastsquaresofY 1 onthefittedvalues(Z 1 ,Y(cid:98)2 ). Hence the coefficient Î² (cid:98)2 on the endogenous variables can be found by residual regression. Set P 1 = (cid:161) (cid:48) (cid:162)â1 (cid:48) Z Z Z Z .ApplyingtheFWLtheoremweobtain 1 1 1 1 (cid:179) (cid:48) (cid:180)â1(cid:179) (cid:48) (cid:180) Î² (cid:98)2 = Y(cid:98)2 (I n âP 1 )Y(cid:98)2 Y(cid:98)2 (I n âP 1 )Y 1 =(cid:161) Y (cid:48) P (I âP )P Y (cid:162)â1(cid:161) Y (cid:48) P (I âP )Y (cid:162) 2 Z n 1 Z 2 2 Z n 1 1 =(cid:161) Y (cid:48) (P âP )Y (cid:162)â1(cid:161) Y (cid:48) (P âP )Y (cid:162) 2 Z 1 2 2 Z 1 1",
    "page": 365,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 346 sinceP P =P . Z 1 1 A fifth representation can be obtained by a further projection. The projection matrix P can be Z replacedbytheprojectionontothepair[Z 1 ,Z(cid:101)2 ]where Z(cid:101)2 =(I n âP 1 )Z 2 is Z 2 projectedorthogonalto (cid:179) (cid:48) (cid:180)â1 (cid:48) Z 1 .SinceZ 1 andZ(cid:101)2 areorthogonal,P Z =P 1 +P 2 whereP 2 =Z(cid:101)2 Z(cid:101)2 Z(cid:101)2 Z(cid:101)2 .ThusP Z âP 1 =P 2 and Î² (cid:98)2 =(cid:161) Y (cid:48) 2 P 2 Y 2 (cid:162)â1(cid:161) Y (cid:48) 2 P 2 Y 1 (cid:162) = (cid:181) Y (cid:48) 2 Z(cid:101)2 (cid:179) Z(cid:101) (cid:48) 2 Z(cid:101)2 (cid:180)â1 Z(cid:101) (cid:48) 2 Y 2 (cid:182)â1(cid:181) Y (cid:48) 2 Z(cid:101)2 (cid:179) Z(cid:101) (cid:48) 2 Z(cid:101)2 (cid:180)â1 Z(cid:101) (cid:48) 2 Y 1 (cid:182) . (12.32) Giventhe2SLSestimatorwedefinetheresiduale (cid:98)i =Y 1i âX i (cid:48)Î² (cid:98)2sls . Whenthemodelisoveridentified theinstrumentsandresidualsarenotorthogonal.Thatis,Z (cid:48) e(cid:54)=0.Itdoes,however,satisfy (cid:98) X(cid:98) (cid:48) (cid:98) e=Î (cid:98) (cid:48) Z (cid:48) (cid:98) e =X (cid:48) Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) e (cid:98) =X (cid:48) Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) yâX (cid:48) Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) XÎ² (cid:98)2sls =0. ReturningtoCardâscollegeproximityexamplesupposethatwetreatexperienceasexogeneousbut thatinsteadofusingthesingleinstrumentcollege(grewupneara4-yearcollege)weusethetwoinstru- ments(public,private)(grewupnearapublic/private4-yearcollege,respectively). Inthiscasewehave oneendogenousvariable(education)andtwoinstruments(public,private).Theestimatedreducedform equationforeducationispresentedinthesixthcolumnofTable12.2.Inthisspecificationthecoefficient on public â growing up near a public 4-year college â is larger than that found for the variable college in the previous specification (column 2). Furthermore, the coefficient on private â growing up near a private4-yearcollegeâismuchsmaller. Thisindicatesthatthekeyimpactofproximityoneducationis viapubliccollegesratherthanprivatecolleges. The 2SLS estimates obtained using these two instruments are presented in the fourth column of Table12.1.Thecoefficientoneducationincreasesto0.161,indicatinga16%returntoayearofeducation. Thisisroughlytwiceaslargeastheestimateobtainedbyleastsquaresinthefirstcolumn. Additionally, if we follow Card and treat experience as endogenous and use age as an instrument wenowhavethreeendogenousvariables(education,experience,experience2/100)andfourinstruments (public,private,age,age2). Wepresentthe2SLSestimatesusingthisspecificationinthefifthcolumnof Table12.1.Theestimateofthereturntoeducationremains16%andthereturntoexperienceflattens. Youmightwonderifwecoulduseallthreeinstrumentsâcollege,public,andprivate. Theansweris no.Thisisbecausecollege=public+privatesothethreevariablesarecolinear.Sincetheinstrumentsare linearlyrelatedthethreetogetherwouldviolatethefull-rankcondition(12.6)",
    "page": 366,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Wepresentthe2SLSestimatesusingthisspecificationinthefifthcolumnof Table12.1.Theestimateofthereturntoeducationremains16%andthereturntoexperienceflattens. Youmightwonderifwecoulduseallthreeinstrumentsâcollege,public,andprivate. Theansweris no.Thisisbecausecollege=public+privatesothethreevariablesarecolinear.Sincetheinstrumentsare linearlyrelatedthethreetogetherwouldviolatethefull-rankcondition(12.6). The2SLSestimatormaybecalculatedinStatausingtheivregress 2slscommand. 12.13 LimitedInformationMaximumLikelihood Analternativemethodtoestimatetheparametersofthestructuralequationisbymaximumlikeli- hood. AndersonandRubin(1949)derivedthemaximumlikelihoodestimatorforthejointdistribution ofY (cid:126) =(Y ,Y ).Theestimatorisknownaslimitedinformationmaximumlikelihood(LIML). 1 2 This estimator is called âlimited informationâ because it is based on the structural equation for Y combinedwiththereducedformequationforX .Ifmaximumlikelihoodisderivedbasedonastructural 2 equationforX aswellthisleadstowhatisknownasfullinformationmaximumlikelihood(FIML).The 2 advantageofLIMLrelativetoFIMListhattheformerdoesnotrequireastructuralmodelforX andthus 2",
    "page": 366,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 347 allowstheresearchertofocusonthestructuralequationofinterestâthatforY. Wedonotdescribethe FIMLestimatorasitisnotcommonlyusedinappliedeconometrics. WhiletheLIMLestimatorislesswidelyusedamongeconomiststhan2SLSithasreceivedaresur- genceofattentionfromeconometrictheorists. ToderivetheLIMLestimatorrecallthedefinitionY (cid:126) =(Y ,Y )andthereducedform(12.17) 1 2 Y (cid:126) = (cid:183) Î Î» (cid:48) (cid:48) 1 Î Î» (cid:48) 2 (cid:184)(cid:181) Z Z 1 (cid:182) +u 12 22 2 =Î (cid:48) Z +Î (cid:48) Z +u (12.33) 1 1 2 2 whereÎ  =(cid:163) Î» Î (cid:164) andÎ  =(cid:163) Î» Î (cid:164) .TheLIMLestimatorisderivedundertheassumptionthat 1 1 12 2 2 22 uismultivariatenormal. DefineÎ³(cid:48)=(cid:163) 1 âÎ²(cid:48) (cid:164) .From(12.15)wefind 2 Î  Î³=Î» âÎ Î² =0. 2 2 22 2 Thusthe(cid:96) Ã(k +1)coefficientmatrixÎ  in(12.33)hasdeficientrank.Indeed,itsrankmustbek since 2 2 2 2 Î hasfullrank. 22 Thismeansthatthemodel(12.33)ispreciselythereducedrankregressionmodelofSection11.11. Theorem11.7presentsthemaximumlikelihoodestimatorsforthereducedrankparameters.Inparticu- lar,theMLEforÎ³is Î³(cid:48) Y (cid:126)(cid:48) M Y (cid:126)Î³ Î³=argmin 1 (12.34) (cid:98) Î³ Î³(cid:48) Y (cid:126)(cid:48) M Y (cid:126)Î³ Z where M =I âZ (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) and M =I âZ (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) . Theminimization(12.34)issometimes 1 n 1 1 1 1 Z n calledtheâleastvarianceratioâproblem. Theminimizationproblem(12.34)isinvarianttothescaleofÎ³(thatis,Î³c isequivalentlytheargmin (cid:98) foranyc)sonormalizationisrequired. AconvenientchoiceisÎ³(cid:48) Y (cid:126)(cid:48) M Y (cid:126)Î³=1. Usingthisnormalization Z and the theory of the minimum of quadratic forms (Section A.15) Î³ is the generalized eigenvector of (cid:98) (cid:126)(cid:48) (cid:126) (cid:126)(cid:48) (cid:126) Y M Y withrespecttoY M Y associatedwiththesmallestgeneralizedeigenvalue. (SeeSectionA.14 1 Z forthedefinitionofgeneralizedeigenvaluesandeigenvectors.)Computationallythisisstraightforward. Forexample,inMATLABthegeneralizedeigenvaluesandeigenvectorsofthematrix AwithrespecttoB isfoundbythecommandeig(A,B). OncethisÎ³isfoundanyothernormalizationcanbeobtainedby (cid:98) rescaling.Forexample,toobtaintheMLEforÎ² 2 makethepartitionÎ³ (cid:98) (cid:48)=(cid:163) Î³ (cid:98)1 Î³ (cid:98) (cid:48) 2 (cid:164) andsetÎ² (cid:98)2 =âÎ³ (cid:98)2 /Î³ (cid:98)1 . ToobtaintheMLEforÎ² recallthestructuralequationY =Z (cid:48)Î² +Y (cid:48)Î² +e.ReplaceÎ² withtheMLE 1 1 1 1 2 2 2 Î² (cid:98)2 andapplyregression.Thisyields Î² (cid:98)1 =(cid:161) Z (cid:48) 1 Z 1 (cid:162)â1 Z (cid:48) 1 (cid:161) Y 1 âY 2 Î² (cid:98)2 (cid:162) . (12.35) ThesesolutionsaretheMLEforthestructuralparametersÎ² andÎ² . 1 2 Many previous econometrics textbooks do not present a derivation of the LIML estimator as the originalderivationbyAndersonandRubin(1949)islengthyandnotparticularlyinsightful. Incontrast thederivationgivenherebasedonreducedrankregressionissimple",
    "page": 367,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". (12.35) ThesesolutionsaretheMLEforthestructuralparametersÎ² andÎ² . 1 2 Many previous econometrics textbooks do not present a derivation of the LIML estimator as the originalderivationbyAndersonandRubin(1949)islengthyandnotparticularlyinsightful. Incontrast thederivationgivenherebasedonreducedrankregressionissimple. There is an alternative (and traditional) expression for the LIML estimator. Define the minimum obtainedin(12.34) Î³(cid:48) Y (cid:126)(cid:48) M Y (cid:126)Î³ Îº=min 1 (12.36) (cid:98) Î³ Î³(cid:48) Y (cid:126)(cid:48) M Y (cid:126)Î³ Z (cid:126)(cid:48) (cid:126) (cid:126)(cid:48) (cid:126) whichisthesmallestgeneralizedeigenvalueofY M Y withrespecttoY M Y.TheLIMLestimatorcan 1 Z bewrittenas Î² (cid:98)liml =(cid:161) X (cid:48) (I n âÎº (cid:98) M Z )X (cid:162)â1(cid:161) X (cid:48) (I n âÎº (cid:98) M Z )Y 1 (cid:162) . (12.37)",
    "page": 367,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 348 We defer the derivation of (12.37) until the end of this section. Expression (12.37) does not simplify computation(sinceÎº (cid:98) requiressolvingthesameeigenvectorproblemthatyieldsÎ² (cid:98)2 ). However(12.37)is importantforthedistributiontheory. ItalsohelpsrevealthealgebraicconnectionbetweenLIML,least squares,and2SLS. Theestimator(12.37)witharbitraryÎºisknownasak-classestimatorofÎ².WhiletheLIMLestimator obtainsbysettingÎº=Îº,theleastsquaresestimatorisobtainedbysettingÎº=0and2SLSisobtainedby (cid:98) settingÎº=1.ItisworthobservingthattheLIMLsolutionsatisfiesÎºâ¥1. (cid:98) Whenthemodelisjust-identifiedtheLIMLestimatorisidenticaltotheIVand2SLSestimators.They areonlydifferentintheover-identifiedsetting. (Onecorollaryisthatunderjust-identificationandnor- malerrorstheIVestimatorisMLE.) Forinferenceitisusefultoobservethat(12.37)showsthatÎ² (cid:98)liml canbewrittenasanIVestimator (cid:179) (cid:48) (cid:180)â1(cid:179) (cid:48) (cid:180) Î² (cid:98)liml = X(cid:101) X X(cid:101) Y 1 usingtheinstrument (cid:181) (cid:182) X X(cid:101) =(I n âÎº (cid:98) M Z )X = X 2 âÎº 1 (cid:98) U(cid:98)2 whereU(cid:98)2 =M Z X 2 arethereduced-formresidualsfromthemultivariateregressionoftheendogenous regressorsY ontheinstrumentsZ.ExpressingLIMLusingthisIVformulaisusefulforvarianceestima- 2 tion. TheLIMLestimatorhasthesameasymptoticdistributionas2SLS.However,theyhavequitediffer- ent behaviors in finite samples. There is considerable evidence that the LIML estimator has reduced finitesamplebiasrelativeto2SLSwhentherearemanyinstrumentsorthereducedformisweak. (We reviewthesecasesinthefollowingsections.) However,ontheotherhandLIMLhaswiderfinitesample dispersion. Wenowderivetheexpression(12.37).UsethenormalizationÎ³(cid:48)=(cid:163) 1 âÎ²(cid:48) (cid:164) towrite(12.34)as 2 (cid:161) Y âY Î² (cid:162)(cid:48) M (cid:161) Y âY Î² (cid:162) Î² (cid:98)2 =arg Î² m 2 in (cid:161) Y 1 1 âY 2 Î² 2 2 (cid:162)(cid:48) M Z 1 (cid:161) Y 1 1 âY 2 2 Î² 2 2 (cid:162) . Thefirst-order-conditionforminimizationis2/ (cid:161) Y 1 âY 2 Î² (cid:98)2 (cid:162)(cid:48) M Z (cid:161) Y 1 âY 2 Î² (cid:98)2 (cid:162) times 0=Y (cid:48) 2 M 1 (cid:161) Y 1 âY 2 Î² (cid:98)2 (cid:162)â (cid:161) (cid:161) Y Y 1 1 â â Y Y 2 2 Î² Î² (cid:98) (cid:98) 2 2 (cid:162) (cid:162) (cid:48) (cid:48) M M Z 1 (cid:161) (cid:161) Y Y 1 1 â â Y Y 2 2 Î² Î² (cid:98) (cid:98) 2 2 (cid:162) (cid:162) X (cid:48) 2 M Z (cid:161) Y 1 âY 2 Î² (cid:98)2 (cid:162) =Y (cid:48) 2 M 1 (cid:161) Y 1 âY 2 Î² (cid:98)2 (cid:162)âÎº (cid:98) X (cid:48) 2 M Z (cid:161) Y 1 âY 2 Î² (cid:98)2 (cid:162) usingdefinition(12.36).Rewriting, Y (cid:48) 2 (M 1 âÎº (cid:98) M Z )X 2 Î² (cid:98)2 =X (cid:48) 2 (M 1 âÎº (cid:98) M Z )Y 1 . (12.38) Equation(12.37)isthesameasthetwoequationsystem Z (cid:48) 1 Z 1 Î² (cid:98)1 +Z (cid:48) 1 Y 2 Î² (cid:98)2 =Z (cid:48) 1 Y 1 Y (cid:48) 2 Z 1 Î² (cid:98)1 +(cid:161) Y (cid:48) 2 (I n âÎº (cid:98) M Z )Y 2 (cid:162)Î² (cid:98)2 =Y (cid:48) 2 (I n âÎº (cid:98) M Z )Y 1",
    "page": 368,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". (12.38) Equation(12.37)isthesameasthetwoequationsystem Z (cid:48) 1 Z 1 Î² (cid:98)1 +Z (cid:48) 1 Y 2 Î² (cid:98)2 =Z (cid:48) 1 Y 1 Y (cid:48) 2 Z 1 Î² (cid:98)1 +(cid:161) Y (cid:48) 2 (I n âÎº (cid:98) M Z )Y 2 (cid:162)Î² (cid:98)2 =Y (cid:48) 2 (I n âÎº (cid:98) M Z )Y 1 . Thefirstequationis(12.35).Using(12.35),thesecondis Y (cid:48) 2 Z 1 (cid:161) Z (cid:48) 1 Z 1 (cid:162)â1 Z (cid:48) 1 (cid:161) Y 1 âY 2 Î² (cid:98)2 (cid:162)+(cid:161) Y (cid:48) 2 (I n âÎº (cid:98) M Z )Y 2 (cid:162)Î² (cid:98)2 =Y (cid:48) 2 (I n âÎº (cid:98) M Z )Y 1",
    "page": 368,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 349 whichis(12.38)whenrearranged. Wehavethusshownthat(12.37)isequivalentto(12.35)and(12.38) andisthusavalidexpressionfortheLIMLestimator. ReturningtotheCardcollegeproximityexamplewenowpresenttheLIMLestimatesoftheequation withthetwoinstruments(public,private). TheyarereportedinthefinalcolumnofTable12.1. Theyare quitesimilartothe2SLSestimates. TheLIMLestimatormaybecalculatedinStatausingtheivregress limlcommand. TheodoreAnderson Theodore (Ted) Anderson (1918-2016) was a American statistician and econo- metrician,whomadefundamentalcontributionstomultivariatestatisticalthe- ory.ImportantcontributionsincludetheAnderson-Darlingdistributiontest,the Anderson-Rubinstatistic,themethodofreducedrankregression,andhismost famouseconometricscontributionâtheLIMLestimator.Hecontinuedworking throughouthislonglife,evenpublishingtheoreticalworkattheageof97! 12.14 Split-SampleIVandJIVE TheidealinstrumentforestimationofÎ²isW =Î(cid:48) Z.WecanwritetheidealIVestimatoras (cid:195) (cid:33)â1(cid:195) (cid:33) n n Î² (cid:98)ideal = (cid:88) W i X i (cid:48) (cid:88) W i Y i . i=1 i=1 ThisestimatorisnotfeasiblesinceÎisunknown. The2SLSestimatorreplacesÎwiththemultivariate leastsquaresestimatorÎ (cid:98)andW i withW(cid:99)i =Î (cid:98) (cid:48) Z i leadingtothefollowingrepresentationfor2SLS (cid:195) (cid:33)â1(cid:195) (cid:33) n n Î² (cid:98)2sls = (cid:88) W(cid:99)i X i (cid:48) (cid:88) W(cid:99)i Y i . i=1 i=1 SinceÎ (cid:98) isestimatedonthefullsampleincludingobservationi itisafunctionofthereducedform erroruwhichiscorrelatedwiththestructuralerrore.ItfollowsthatW(cid:99)andearecorrelated,whichmeans thatÎ² (cid:98)2sls isbiasedforÎ². Thiscorrelationandbiasdisappearsasymptoticallybutitcanbeimportantin applications. ApossiblesolutiontothisproblemistoreplaceW(cid:99)withapredictedvaluewhichisuncorrelatedwith theerrore. Onemethodisthesplit-sampleIV(SSIV)estimatorofAngristandKrueger(1995). Divide thesamplerandomlyintotwoindependenthalves A andB. Use A toestimatethereduceformandB to estimate the structural coefficient. Specifically, use sample A to construct Î (cid:98)A =(cid:161) Z (cid:48) A Z A (cid:162)â1(cid:161) Z (cid:48) A X A (cid:162) . Combine this with sample B to create the predicted valuesW(cid:99)B = Z B Î (cid:98)A . The SSIV estimator is Î² (cid:98)ssiv = (cid:179) (cid:48) (cid:180)â1(cid:179) (cid:48) (cid:180) W(cid:99)B X B W(cid:99)B Y B .ThishaslowerbiasthanÎ² (cid:98)2sls . AlimitationofSSIVisthattheresultswillbesensitivetothesamplespliting. Onesplitwillproduce oneestimator;anothersplitwillproduceadifferentestimator. Anyspecificsplitisarbitrary,sotheesti- matordependsonthespecificrandomsortingoftheobservationsintothesamples A andB. Asecond limitationofSSIVisthatitisunlikelytoworkwellwhenthesamplesizenissmall.",
    "page": 369,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 350 Amuchbettersolutionisobtainedbyaleave-one-outestimatorforÎ.Specifically,let Î (cid:98)(âi) =(cid:161) Z (cid:48) ZâZ i Z i (cid:48)(cid:162)â1(cid:161) Z (cid:48) X âZ i X i (cid:48)(cid:162) betheleastsquaresleave-one-outestimatorofthereducedformmatrixÎ, andletW(cid:99)i =Î (cid:98) (cid:48) (âi) Z i bethe reducedformpredictedvalues.UsingW(cid:99)i =Î (cid:98) (cid:48) (âi) Z i asaninstrumentweobtaintheestimator (cid:195) (cid:33)â1(cid:195) (cid:33) (cid:195) (cid:33)â1(cid:195) (cid:33) n n n n Î² (cid:98)jive1 = (cid:88) W(cid:99)i X i (cid:48) (cid:88) W(cid:99)i Y i = (cid:88) Î (cid:98) (cid:48) (âi) Z i X i (cid:48) (cid:88) Î (cid:98) (cid:48) (âi) Z i Y i . i=1 i=1 i=1 i=1 Thiswascalledthejackknifeinstrumentalvariables(JIVE1)estimatorbyAngrist,Imbens,andKrueger (1999).ItfirstappearedinPhillipsandHale(1977). Angrist,Imbens,andKrueger(1999)pointedoutthatasomewhatsimpleradjustmentalsoremoves thecorrelationandbias.Definetheestimatorandpredictedvalue Î (cid:101)(âi) =(cid:161) Z (cid:48) Z (cid:162)â1(cid:161) Z (cid:48) X âZ i X i (cid:48)(cid:162) W(cid:102)i =Î (cid:101) (cid:48) (âi) Z i (cid:48) whichonlyadjuststheZ X component.TheirJIVE2estimatoris (cid:195) (cid:33)â1(cid:195) (cid:33) (cid:195) (cid:33)â1(cid:195) (cid:33) n n n n Î² (cid:98)jive2 = (cid:88) W(cid:102)i X i (cid:48) (cid:88) W(cid:102)i Y i = (cid:88) Î (cid:101) (cid:48) (âi) Z i X i (cid:48) (cid:88) Î (cid:101) (cid:48) (âi) Z i Y i . i=1 i=1 i=1 i=1 Usingtheformulaforleave-one-outestimators(Theorem3.7), theJIVE1andJIVE2estimatorsuse twolinearoperations: thefirsttocreatethepredictedvaluesW(cid:99)i orW(cid:102)i ,andthesecondtocalculatethe IVestimator.Thustheestimatorsdonotrequiresignificantlymorecomputationthan2SLS. AnasymptoticdistributiontheoryfortheJIVE1andJIVE2estimatorswasdevelopedbyChao,Swan- son,Hausman,Newey,andWoutersen(2012). TheJIVE1andJIVE2estimatorsmaybecalculatedinStatausingthejive command.Itisnotapart ofthestandardpackagebutcanbeeasilyadded. 12.15 Consistencyof2SLS Wenowdemonstratetheconsistencyofthe2SLSestimatorforthestructuralparameter.Thefollow- ingisasetofregularityconditions. Assumption12.1 1. Thevariables(Y ,X ,Z ),i =1,...,n,areindependentandidenticallydis- 1i i i tributed. 2. (cid:69)(cid:163) Y2(cid:164)<â. 1 3. (cid:69)(cid:107)X(cid:107)2<â. 4. (cid:69)(cid:107)Z(cid:107)2<â. 5. (cid:69)(cid:163) ZZ (cid:48)(cid:164) ispositivedefinite. 6. (cid:69)(cid:163) ZX (cid:48)(cid:164) hasfullrankk. 7. (cid:69)[Ze]=0.",
    "page": 370,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 351 Assumptions12.1.2-4statethatallvariableshavefinitevariances. Assumption12.1.5statesthatthe instrumentvectorhasaninvertibledesignmatrix, whichisidenticaltothecoreassumptionaboutre- gressors in the linear regression model. This excludes linearly redundant instruments. Assumptions 12.1.6 and 12.1.7 are the key identification conditions for instrumental variables. Assumption 12.1.6 statesthattheinstrumentsandregressorshaveafull-rankcross-momentmatrix.Thisisoftencalledthe relevance condition. Assumption 12.1.7 states that the instrumental variables and structural error are uncorrelated.Assumptions12.1.5-7areidenticaltoDefinition12.1. Theorem12.1 UnderAssumption12.1,Î² (cid:98)2sls ââÎ²asnââ. p Theproofofthetheoremisprovidedbelow. Thistheoremshowsthatthe2SLSestimatorisconsistentforthestructuralcoefficientÎ²undersimilar moment conditions as the least squares estimator. The key differences are the instrumental variables assumption(cid:69)[Ze]=0andtherelevanceconditionrank (cid:161)(cid:69)(cid:163) ZX (cid:48)(cid:164)(cid:162)=k. TheresultincludestheIVestimator(when(cid:96)=k)asaspecialcase. Theproofofthisconsistencyresultissimilartothatforleastsquares. Takethestructuralequation Y =XÎ²+e inmatrixformatandsubstituteitintotheexpressionfortheestimator.Weobtain Î² (cid:98)2sls = (cid:179) X (cid:48) Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) X (cid:180)â1 X (cid:48) Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48)(cid:161) XÎ²+e (cid:162) =Î²+ (cid:179) X (cid:48) Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) X (cid:180)â1 X (cid:48) Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) e. (12.39) Thisseparatesoutthestochasticcomponent.Re-writingandapplyingtheWLLNandCMT Î² (cid:98)2sls âÎ²= (cid:181)(cid:181) 1 X (cid:48) Z (cid:182)(cid:181) 1 Z (cid:48) Z (cid:182)â1(cid:181) 1 Z (cid:48) X (cid:182)(cid:182)â1 n n n Ã (cid:181) 1 X (cid:48) Z (cid:182)(cid:181) 1 Z (cid:48) Z (cid:182)â1(cid:181) 1 Z (cid:48) e (cid:182) n n n ââ(cid:161) Q Q â1Q (cid:162)â1 Q Q â1 (cid:69)[Ze]=0 p XZ ZZ ZX XZ ZZ where Q =(cid:69)(cid:163) XZ (cid:48)(cid:164) XZ Q =(cid:69)(cid:163) ZZ (cid:48)(cid:164) ZZ Q =(cid:69)(cid:163) ZX (cid:48)(cid:164) . ZX TheWLLNholdsunderthei.i.d.Assumption12.1.1andthefinitesecondmomentAssumptions12.1.2-4. ThecontinuousmappingtheoremappliesifthematricesQ andQ Q â1Q areinvertible,which ZZ XZ ZZ ZX holdundertheidentificationAssumptions12.1.5and12.1.6.ThefinalequalityusesAssumption12.1.7. 12.16 AsymptoticDistributionof2SLS Wenowshowthatthe2SLSestimatorsatisfiesacentrallimittheorem.Wefirststateasetofsufficient regularityconditions.",
    "page": 371,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 352 Assumption12.2 InadditiontoAssumption12.1, 1. (cid:69)(cid:163) Y4(cid:164)<â. 1 2. (cid:69)(cid:107)X(cid:107)4<â. 3. (cid:69)(cid:107)Z(cid:107)4<â. 4. â¦=(cid:69)(cid:163) ZZ (cid:48) e2(cid:164) ispositivedefinite. Assumption12.2strengthensAssumption12.1byrequiringthatthedependentvariableandinstru- mentshavefinitefourthmoments.Thisisusedtoestablishthecentrallimittheorem. Theorem12.2 UnderAssumption12.2,asnââ. (cid:112) n (cid:161)Î² (cid:98)2sls âÎ²(cid:162)ââN (cid:161) 0,VÎ² (cid:162) d where VÎ² =(cid:161) Q XZ Q â Z 1 Z Q ZX (cid:162)â1(cid:161) Q XZ Q â Z 1 Z â¦Q â Z 1 Z Q ZX (cid:162)(cid:161) Q XZ Q â Z 1 Z Q ZX (cid:162)â1 . (cid:112) Thisshowsthatthe2SLSestimatorconvergesata n ratetoanormalrandomvector. Itshowsas welltheformofthecovariancematrix. Thelattertakesasubstantiallymorecomplicatedformthanthe leastsquaresestimator. Asinthecaseofleastsquaresestimationtheasymptoticvariancesimplifiesunderaconditionalho- moskedasticitycondition. For2SLSthesimplificationoccurswhen(cid:69)(cid:163) e2|Z (cid:164)=Ï2. Thisholdswhen Z ande areindependent. Itmaybereasonableinsomecontextstoconceivethattheerrore isindepen- dentoftheexcludedinstrumentsZ ,sincebyassumptiontheimpactofZ onY isonlythroughX,but 2 2 there is no reason to expect e to be independent of the included exogenous variables X . Hence het- 1 eroskedasticity should be equally expected in 2SLS and least squares regression. Nevertheless, under homoskedasticitywehavethesimplificationsâ¦=Q ZZ Ï2andVÎ² =V0 Î² d=ef(cid:161) Q XZ Q â Z 1 Z Q ZX (cid:162)â1Ï2. The derivation of the asymptotic distribution builds on the proof of consistency. Using equation (12.39)wehave (cid:112) n (cid:161)Î² (cid:98)2sls âÎ²(cid:162)= (cid:181)(cid:181) 1 X (cid:48) Z (cid:182)(cid:181) 1 Z (cid:48) Z (cid:182)â1(cid:181) 1 Z (cid:48) X (cid:182)(cid:182)â1(cid:181) 1 X (cid:48) Z (cid:182)(cid:181) 1 Z (cid:48) Z (cid:182)â1(cid:181) (cid:112) 1 Z (cid:48) e (cid:182) . n n n n n n WeapplytheWLLNandCMTforthemomentmatricesinvolving X and Z thesameasintheproofof consistency.Inaddition,bytheCLTfori.i.d.observations (cid:112) 1 Z (cid:48) e= (cid:112) 1 (cid:88) n Z e ââN(0,â¦) i i n n i=1 d because the vector Z e is i.i.d. and mean zero under Assumptions 12.1.1 and 12.1.7, and has a finite i i secondmomentasweverifybelow.",
    "page": 372,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 353 Weobtain (cid:112) n (cid:161)Î² (cid:98)2sls âÎ²(cid:162)= (cid:181)(cid:181) 1 X (cid:48) Z (cid:182)(cid:181) 1 Z (cid:48) Z (cid:182)â1(cid:181) 1 Z (cid:48) X (cid:182)(cid:182)â1(cid:181) 1 X (cid:48) Z (cid:182)(cid:181) 1 Z (cid:48) Z (cid:182)â1(cid:181) (cid:112) 1 Z (cid:48) e (cid:182) n n n n n n ââ(cid:161) Q XZ Q â Z 1 Z Q ZX (cid:162)â1 Q XZ Q â Z 1 Z N(0,â¦)=N (cid:161) 0,VÎ² (cid:162) d asstated. TocompletetheproofwedemonstratethatZe hasafinitesecondmomentunderAssumption12.2. Toseethis,notethatbyMinkowskiâsinequality(B.34) (cid:161)(cid:69)(cid:163) e4(cid:164)(cid:162)1/4= (cid:179) (cid:69) (cid:104) (cid:161) Y 1 âX (cid:48)Î²(cid:162)4 (cid:105)(cid:180)1/4 â¤(cid:161)(cid:69)(cid:163) Y 1 4(cid:164)(cid:162)1/4+(cid:176) (cid:176) Î²(cid:176) (cid:176) (cid:161)(cid:69)(cid:107)X(cid:107)4(cid:162)1/4<â underAssumptions12.2.1and12.2.2.ThenbytheCauchy-Schwarzinequality(B.32) (cid:69)(cid:107)Ze(cid:107)2â¤(cid:161)(cid:69)(cid:107)Z(cid:107)4(cid:162)1/2(cid:161)(cid:69)(cid:163) e4(cid:164)(cid:162)1/2<â usingAssumptions12.2.3. 12.17 Determinantsof2SLSVariance Itisinstructivetoexaminetheasymptoticvarianceofthe2SLSestimatortounderstandthefactors whichdeterminetheprecision(orlackthereof)oftheestimator. Asintheleastsquarescaseitismore transparenttoexaminethevarianceundertheassumptionofhomoskedasticity.Inthiscasetheasymp- toticvariancetakestheform V0 =(cid:161) Q Q â1Q (cid:162)â1Ï2 Î² XZ ZZ ZX = (cid:179) (cid:69)(cid:163) XZ (cid:48)(cid:164)(cid:161)(cid:69)(cid:163) ZZ (cid:48)(cid:164)(cid:162)â1(cid:69)(cid:163) ZX (cid:48)(cid:164) (cid:180)â1 (cid:69)(cid:163) e2(cid:164) . AsintheleastsquarescasewecanseethatthevarianceofÎ² (cid:98)2sls isincreasinginthevarianceoftheerror e anddecreasinginthevarianceofX. Whatisdifferentisthatthevarianceisdecreasinginthe(matrix- valued)correlationbetweenX andZ. It is also useful to observe that the variance expression is not affected by the variance structure of Z. Indeed, V0 is invariant to rotations of Z (if you replace Z withCZ for invertibleC the expression Î² doesnotchange). ThismeansthatthevarianceexpressionisnotaffectedbythescalingofZ andisnot directlyaffectedbycorrelationamongtheZ. Wecanalsousethisexpressiontoexaminetheimpactofincreasingtheinstrumentset.Supposewe partition Z =(Z a ,Z b )wheredim(Z a )â¥k sowecanconstructa2SLSestimatorusing Z a alone. LetÎ² (cid:98)a and Î² (cid:98)denote the 2SLS estimators constructed usingthe instrument sets Z a and (Z a ,Z b ), respectively. Without loss of generality we can assume that Z and Z are uncorrelated (if not, replace Z with the a b b projectionerrorafterprojectingonto Z )",
    "page": 373,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Wecanalsousethisexpressiontoexaminetheimpactofincreasingtheinstrumentset.Supposewe partition Z =(Z a ,Z b )wheredim(Z a )â¥k sowecanconstructa2SLSestimatorusing Z a alone. LetÎ² (cid:98)a and Î² (cid:98)denote the 2SLS estimators constructed usingthe instrument sets Z a and (Z a ,Z b ), respectively. Without loss of generality we can assume that Z and Z are uncorrelated (if not, replace Z with the a b b projectionerrorafterprojectingonto Z ). Inthiscaseboth(cid:69)(cid:163) ZZ (cid:48)(cid:164) and (cid:161)(cid:69)(cid:163) ZZ (cid:48)(cid:164)(cid:162)â1 areblockdiagonal a so avar (cid:163)Î² (cid:98) (cid:164)= (cid:179) (cid:69)(cid:163) XZ (cid:48)(cid:164)(cid:161)(cid:69)(cid:163) ZZ (cid:48)(cid:164)(cid:162)â1(cid:69)(cid:163) ZX (cid:48)(cid:164) (cid:180)â1 Ï2 = (cid:179) (cid:69)(cid:163) XZ (cid:48)(cid:164)(cid:161)(cid:69)(cid:163) Z Z (cid:48)(cid:164)(cid:162)â1(cid:69)(cid:163) Z X (cid:48)(cid:164)+(cid:69)(cid:163) XZ (cid:48)(cid:164)(cid:161)(cid:69)(cid:163) Z Z (cid:48)(cid:164)(cid:162)â1(cid:69)(cid:163) Z X (cid:48)(cid:164) (cid:180)â1 Ï2 a a a a b b b b â¤ (cid:179) (cid:69)(cid:163) XZ (cid:48)(cid:164)(cid:161)(cid:69)(cid:163) Z Z (cid:48)(cid:164)(cid:162)â1(cid:69)(cid:163) Z X (cid:48)(cid:164) (cid:180)â1 Ï2 a a a a =avar (cid:163)Î² (cid:98)a (cid:164)",
    "page": 373,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 354 withstrictinequalityif(cid:69)(cid:163) XZ (cid:48)(cid:164)(cid:54)=0. Thusthe2SLSestimatorwiththefullinstrumentsethasasmaller b asymptoticvariancethantheestimatorwiththesmallerinstrumentset. Whatwehaveshownisthattheasymptoticvarianceofthe2SLSestimatorisdecreasingasthenum- berofinstrumentsincreases. Fromtheviewpointofasymptoticefficiencythismeansthatitisbetterto usemoreinstruments(whentheyareavailableandareallknowntobevalidinstruments). Unfortunatelythereisacatch. Itturnsoutthatthefinitesamplebiasofthe2SLSestimator(which cannot be calculated exactly but can be approximated using asymptotic expansions) is generically in- creasinglinearilyasthenumberofinstrumentsincreases.Wewillseesomecalculationsillustratingthis phenomenoninSection12.37. Thusthechoiceofinstrumentsinpracticeinducesatrade-offbetween biasandvariance. 12.18 CovarianceMatrixEstimation Estimation of the asymptotic covariance matrix VÎ² is done using similar techniques as for least squaresestimation.Theestimatorisconstructedbyreplacingthepopulationmomentmatricesbysam- plecounterparts.Thus V(cid:98)Î² = (cid:179) Q(cid:98)XZ Q(cid:98) â Z 1 Z Q(cid:98)ZX (cid:180)â1(cid:179) Q(cid:98)XZ Q(cid:98) â Z 1 Z â¦ (cid:98)Q(cid:98) â Z 1 Z Q(cid:98)ZX (cid:180)(cid:179) Q(cid:98)XZ Q(cid:98) â Z 1 Z Q(cid:98)ZX (cid:180)â1 (12.40) where Q(cid:98)ZZ = n 1 (cid:88) n Z i Z i (cid:48)= n 1 Z (cid:48) Z i=1 Q(cid:98)XZ = n 1 (cid:88) n X i Z i (cid:48)= n 1 X (cid:48) Z i=1 â¦ (cid:98) = n 1 (cid:88) n Z i Z i (cid:48) e (cid:98)i 2 i=1 e (cid:98)i =Y i âX i (cid:48)Î² (cid:98)2sls . Thehomoskedasticcovariancematrixcanbeestimatedby V(cid:98) 0 Î² = (cid:179) Q(cid:98)XZ Q(cid:98) â Z 1 Z Q(cid:98)ZX (cid:180)â1 Ï (cid:98) 2 Ï2= 1 (cid:88) n e2. (cid:98) n (cid:98)i i=1 Standard errors for the coefficients are obtained as the square roots of the diagonal elements of n â1V(cid:98)Î². Confidence intervals, t-tests, and Wald tests may all be constructed from the coefficient esti- matesandcovariancematrixestimateexactlyasforleastsquaresregression. In Stata the ivregress command by default calculates the covariance matrix estimator using the homoskedasticcovariancematrix. Toobtaincovariancematrixestimationandstandarderrorswiththe robustestimatorV(cid:98)Î²,usetheâ,râoption. Theorem12.3 UnderAssumption12.2,asnââ,V(cid:98) 0 Î² ââV0 Î² andV(cid:98)Î² ââVÎ². p p",
    "page": 374,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 355 ToproveTheorem12.3thekeyistoshowâ¦ (cid:98) âââ¦astheotherconvergenceresultswereestablished p intheproofofconsistency.WedeferthistoExercise12.6. Itisimportantthatthecovariancematrixbeconstructedusingthecorrectresidualformulae =Y â (cid:98)i i X i (cid:48)Î² (cid:98)2sls .Thisisdifferentthanwhatwouldbeobtainediftheâtwo-stageâcomputationmethodwereused. Toseethisletâswalkthroughthetwo-stagemethod. First,weestimatethereducedform X i =Î (cid:98) (cid:48) Z i +u (cid:98)i toobtainthepredictedvalues X(cid:98)i =Î (cid:98) (cid:48) Z i . Second,weregressY on X(cid:98) toobtainthe2SLSestimatorÎ² (cid:98)2sls . Thislatterregressiontakestheform Y i =X(cid:98) i (cid:48)Î² (cid:98)2sls +v (cid:98)i (12.41) wherev areleastsquaresresiduals.Thecovariancematrix(andstandarderrors)reportedbythisregres- (cid:98)i sionareconstructedusingtheresidualv .Forexample,thehomoskedasticformulais (cid:98)i V(cid:98)Î² = (cid:181) n 1 X(cid:98) (cid:48) X(cid:98) (cid:182)â1 Ï (cid:98) 2 v = (cid:179) Q(cid:98)XZ Q(cid:98) â Z 1 Z Q(cid:98)ZX (cid:180)â1 Ï (cid:98) 2 v Ï2 = 1 (cid:88) n v2 (cid:98)v n (cid:98)i i=1 whichisproportionaltothevarianceestimatorÏ2 ratherthanÏ2.Thisisimportantbecausetheresidual (cid:98)v (cid:98) v (cid:98) differs from e (cid:98) . We can see this because the regression (12.41) uses the regressor X(cid:98) rather than X. Indeed,wecalculatethat v (cid:98)i =Y i âX i (cid:48)Î² (cid:98)2sls +(cid:161) X i âX(cid:98)i (cid:162)(cid:48) Î² (cid:98)2sls =e (cid:98)i +u (cid:98)i (cid:48)Î² (cid:98)2sls (cid:54)=e (cid:98)i . Thismeansthatstandarderrorsreportedbytheregression(12.41)willbeincorrect. Thisproblemisavoidedifthe2SLSestimatorisconstructeddirectlyandthestandarderrorscalcu- latedwiththecorrectformularatherthantakingtheâtwo-stepâshortcut. 12.19 LIMLAsymptoticDistribution InthissectionweshowthattheLIMLestimatorisasymptoticallyequivalenttothe2SLSestimator. Werecommend,however,adifferentcovariancematrixestimatorbasedontheIVrepresentation. Westartbyderivingtheasymptoticdistribution. RecallthattheLIMLestimatorhasseveralrepre- sentationsincluding Î² (cid:98)liml =(cid:161) X (cid:48) (I n âÎº (cid:98) M Z )X (cid:162)â1(cid:161) X (cid:48) (I n âÎº (cid:98) M Z )Y 1 (cid:162) where Î³(cid:48) Y (cid:126)(cid:48) M Y (cid:126)Î³ Îº=min 1 (cid:98) Î³ Î³(cid:48) Y (cid:126)(cid:48) M Y (cid:126)Î³ Z andÎ³=(1,âÎ²(cid:48) ) (cid:48) .Forthedistributiontheoryitisusefultorewritetheslopecoefficientas 2 Î² (cid:98)liml =(cid:161) X (cid:48) P Z X âÂµ (cid:98) X (cid:48) M Z X (cid:162)â1(cid:161) X (cid:48) P Z Y 1 âÂµ (cid:98) X (cid:48) M Z Y 1 (cid:162) where Î³(cid:48) Y (cid:126)(cid:48) M Z (cid:161) Z (cid:48) M Z (cid:162)â1 Z (cid:48) M Y (cid:126)Î³ Âµ=Îºâ1=min 1 2 2 1 2 2 1",
    "page": 375,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". (cid:98) (cid:98) Î³ Î³(cid:48) Y (cid:126)(cid:48) M Y (cid:126)Î³ Z ThissecondequalityholdssincethespanofZ =[Z ,Z ]equalsthespanof[Z ,M Z ].Thisimplies 1 2 1 1 2 P =Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48)=Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) +M Z (cid:161) Z (cid:48) M Z (cid:162)â1 Z (cid:48) M . Z 1 1 1 1 1 2 2 1 2 2 1",
    "page": 375,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 356 WenowshowthatnÂµ=O (1).Thereducedform(12.33)impliesthat (cid:98) p Y =Z Î  +Z Î  +e. 1 1 2 2 Itwillbeimportanttonotethat Î  =[Î» ,Î ]=(cid:163)Î Î² ,Î (cid:164) 2 2 22 22 2 22 using(12.15).ItfollowsthatÎ  Î³=0.NoteUÎ³=e.ThenM YÎ³=M e andM YÎ³=M e.Hence 2 Z Z 1 1 Î³(cid:48) Y (cid:126)(cid:48) M Z (cid:161) Z (cid:48) M Z (cid:162)â1 Z (cid:48) M Y (cid:126)Î³ nÂµ=min 1 2 2 1 2 2 1 (cid:98) Î³ Î³(cid:48)1Y (cid:126)(cid:48) M Y (cid:126)Î³ n Z (cid:179) (cid:112)1 e (cid:48) M Z (cid:180) (cid:161)1Z (cid:48) M Z (cid:162)â1 (cid:179) (cid:112)1 Z (cid:48) M e (cid:180) â¤ n 1 2 n 2 1 2 n 2 1 1e (cid:48) M e n Z =O (1). p Itfollowsthat (cid:112) n (cid:161)Î² (cid:98)liml âÎ²(cid:162)= (cid:181) 1 X (cid:48) P Z X âÂµ (cid:98) 1 X (cid:48) M Z X (cid:182)â1(cid:181) (cid:112) 1 X (cid:48) P Z eâ (cid:112) nÂµ (cid:98) 1 X (cid:48) M Z e (cid:182) n n n n = (cid:181) 1 X (cid:48) P X âo (1) (cid:182)â1(cid:181) (cid:112) 1 X (cid:48) P eâo (1) (cid:182) Z p Z p n n (cid:112) = n (cid:161)Î² (cid:98)2sls âÎ²(cid:162)+o p (1) which means that LIML and 2SLS have the same asymptotic distribution. This holds under the same assumptionsasfor2SLS. Consequently,onemethodtoobtainanasymptoticallyvalidcovarianceestimatorforLIMListouse the2SLSformula.However,thisisnotthebestchoice.Rather,considertheIVrepresentationforLIML (cid:179) (cid:48) (cid:180)â1(cid:179) (cid:48) (cid:180) Î² (cid:98)liml = X(cid:101) X X(cid:101) Y 1 where (cid:181) (cid:182) X X(cid:101) = X 2 âÎº 1 (cid:98) U(cid:98)2 andU(cid:98)2 =M Z X 2 .TheasymptoticcovariancematrixformulaforanIVestimatoris V(cid:98)Î² = (cid:181) 1 X(cid:101) (cid:48) X (cid:182)â1 â¦ (cid:98) (cid:181) 1 X (cid:48) X(cid:101) (cid:182)â1 (12.42) n n where â¦ (cid:98) = n 1 (cid:88) n X(cid:101)i X(cid:101)i e (cid:98)i 2 i=1 e (cid:98)i =Y 1i âX i (cid:48)Î² (cid:98)liml . Thissimplifiestothe2SLS formulawhenÎº=1butotherwisediffers. The estimator(12.42)isabetter (cid:98) choicethanthe2SLSformulaforcovariancematrixestimationasittakesadvantageoftheLIMLestima- torstructure.",
    "page": 376,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 357 12.20 FunctionsofParameters GiventhedistributiontheoryinTheorems12.2and12.3itisstraightforwardtoderivetheasymptotic distributionofsmoothnonlinearfunctionsofthecoefficientestimators. Specifically, given a function r (cid:161)Î²(cid:162) :(cid:82)k âÎâ(cid:82)q we define the parameter Î¸ =r (cid:161)Î²(cid:162) . Given Î² (cid:98)2sls a naturalestimatorofÎ¸isÎ¸ (cid:98)2sls =r (cid:161)Î² (cid:98)2sls (cid:162) . ConsistencyfollowsfromTheorem12.1andthecontinuousmappingtheorem. Theorem12.4 UnderAssumptions12.1and7.3,asnââ,Î¸ (cid:98)2sls ââÎ¸. p Ifr (cid:161)Î²(cid:162) isdifferentiablethenanestimatoroftheasymptoticcovariancematrixforÎ¸ (cid:98)2sls is (cid:48) V(cid:98)Î¸ =R(cid:98) V(cid:98)Î²R(cid:98) â R(cid:98) = âÎ² r(Î² (cid:98)2sls ) (cid:48) . WesimilarlydefinethehomoskedasticvarianceestimatorasV(cid:98) 0 Î¸ =R(cid:98) (cid:48) V(cid:98) 0 Î²R(cid:98). TheasymptoticdistributiontheoryfollowsfromTheorems12.2and12.3andthedeltamethod. Theorem12.5 UnderAssumptions12.2and7.3,asnââ, (cid:112) n (cid:161)Î¸ (cid:98)2sls âÎ¸(cid:162)ââN(0,VÎ¸) d â andV(cid:98)Î¸ ââVÎ¸ whereVÎ¸ =R (cid:48) VÎ²R andR= r(Î²) (cid:48) . p âÎ² (cid:113) Whenq=1,astandarderrorforÎ¸ (cid:98)2sls iss(Î¸ (cid:98)2sls )= n â1V(cid:98)Î¸ . For example, letâs take the parameter estimates from the fifth column of Table 12.1, which are the 2SLS estimates with three endogenous regressors and four excluded instruments. Suppose we are in- terested in the return to experience, which depends on the level of experience. The estimated return atexperience=10is0.047â0.032Ã2Ã10/100=0.041anditsstandarderroris0.003. Thisimpliesa4% increaseinwagesperyearofexperienceandispreciselyestimated. Orsupposeweareinterestedinthe level of experience at which the function maximizes. The estimate is 50Ã0.047/0.032=73. This has a standard error of 249. The large standard error implies that the estimate (73 years of experience) is withoutprecisionandisthusuninformative. 12.21 HypothesisTests Asintheprevioussection,foragivenfunctionr (cid:161)Î²(cid:162) :(cid:82)k âÎâ(cid:82)q wedefinetheparameterÎ¸=r (cid:161)Î²(cid:162) andconsidertestsofhypothesesoftheform(cid:72) :Î¸=Î¸ against(cid:72) :Î¸(cid:54)=Î¸ .TheWaldstatisticfor(cid:72) is 0 0 1 0 0 W =n (cid:161)Î¸ (cid:98) âÎ¸ 0 (cid:162)(cid:48) V(cid:98) â Î¸(cid:98) 1(cid:161)Î¸ (cid:98) âÎ¸ 0 (cid:162) .",
    "page": 377,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 358 FromTheorem12.5wededucethatW isasymptoticallychi-squaredistributed.LetG (u)denotetheÏ2 q q distributionfunction. Theorem12.6 UnderAssumption12.2,Assumption7.3,and(cid:72) ,thenasnâ 0 â, W ââ Ï2. For c satisfying Î± = 1âG (c), (cid:80)[W >c|(cid:72) ] ââ Î± so the test q q 0 d âReject(cid:72) ifW >câhasasymptoticsizeÎ±. 0 In linear regression we often report the F version of the Wald statistic (by dividing by degrees of freedom)andusetheF distributionforinferenceasthisisjustifiedinthenormalsamplingmodel. For 2SLSestimation,however,thisisnotdoneasthereisnofinitesampleF justificationfortheF versionof theWaldstatistic. To illustrate, once again letâs take the parameter estimates from the fifth column of Table 12.1 and againconsiderthereturntoexperiencewhichisdeterminedbythecoefficientsonexperienceandexpe- rience2/100. Neithercoefficientisstatisticiallysignificantatthe5%levelanditisuncleariftheoverall effectisstatisticallysignificant.Wecanassessthisbytestingthejointhypothesisthatbothcoefficients arezero. TheWaldstatisticforthishypothesisisW =244whichishighlysignificantwithanasymptotic p-valueof0.0000. Thusbyexaminingthejointtestincontrasttotheindividualtestsisquiteclearthat experiencehasanon-zeroeffect. 12.22 FiniteSampleTheory InChapter5wereviewedtherichexactdistributionavailableforthelinearregressionmodelunder theassumptionofnormalinnovations. ThereisasimilarlyrichliteratureineconometricsforIV,2SLS andLIMLestimators. Anexcellentreviewofthetheory,mostlydevelopedinthe1970sandearly1980s, isprovidedbyPeterPhillips(1983). Thistheorywasdevelopedundertheassumptionthatthestructuralerrorvectoreandreducedform error u are multivariate normally distributed. Even though the errors are normal, IV-type estimators 2 arenonlinearfunctionsoftheseerrorsandarethusnon-normallydistributed. Formulaefortheexact distributionshavebeenderivedbutareunfortunatelyfunctionsofmodelparametersandhencearenot directlyusefulforfinitesampleinference. Oneimportantimplicationofthisliteratureisthateveninthisoptimalcontextofexactnormalinno- vationsthefinitesampledistributionsoftheIVestimatorsarenon-normalandthefinitesampledistri- butionsofteststatisticsarenotchi-squared. Thenormalandchi-squaredapproximationsholdasymp- toticallybutthereisnoreasontoexpecttheseapproximationstobeaccurateinfinitesamples. Asecondimportantresultisthatundertheassumptionofnormalerrorsmostoftheestimatorsdo nothavefinitemomentsinanyfinitesample. Acleanstatementconcerningtheexistenceofmoments forthe2SLSestimatorwasobtainedbyKinal(1980)forthecaseofjointnormality.LetÎ² (cid:98)2sls,2 bethe2SLS estimatorsofthecoefficientsontheendogeneousregressors. Theorem12.7 If(Y,X,Z)arejointlynormal,thenforanyr,(cid:69)(cid:176) (cid:176) Î² (cid:98)2sls,2 (cid:176) (cid:176) r <âif andonlyifr <(cid:96) âk +1. 2 2",
    "page": 378,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 359 Thisresultstatesthatinthejust-identifiedcasetheIVestimatordoesnothaveanyfiniteorderin- tegermoments. Intheover-identifiedcasethenumberoffinitemomentscorrespondstothenumber ofoveridentifyingrestrictions((cid:96) âk ). Thusifthereisoneover-identifyingrestriction2SLShasafinite 2 2 meanandiftherearetwoover-identifyingrestrictionsthenthe2SLSestimatorhasafinitevariance. TheLIMLestimatorhasamoreseveremomentproblemasithasnofiniteintegermoments(Mari- ano,1982)regardlessofthenumberofover-identifyingrestrictions. DuetothislackofmomentsFuller (1977)proposedthefollowingmodificationofLIML.Hisestimatoris Î² (cid:98)Fuller =(cid:161) X (cid:48) (I n âKM Z )X (cid:162)â1(cid:161) X (cid:48) (I n âKM Z )Y 1 (cid:162) C K =Îºâ (cid:98) nâk forsomeC â¥1.Fullershowedthathisestimatorhasallmomentsfiniteundersuitableconditions. Hausman,Newey,Woutersen,ChaoandSwanson(2012)proposeanestimatortheycallHFULwhich combinestheideasofJIVEandFullerwhichhasexcellentfinitesampleproperties. 12.23 Bootstrapfor2SLS ThestandardbootstrapalgorithmforIV,2SLS,andGMMgeneratesbootstrapsamplesbysampling thetriplets(Y â ,X â ,Z â )independentlyandwithreplacementfromtheoriginalsample{(Y ,X ,Z ):i = 1i i i 1i i i â â â 1,...,n}. Sampling n such observations and stacking into observation matrices (Y ,X ,Z ), the boot- 1 strap2SLSestimatoris Î² (cid:98) â = (cid:179) X â(cid:48) Z â(cid:161) Z â(cid:48) Z â(cid:162)â1 Z â(cid:48) X â (cid:180)â1 X â(cid:48) Z â(cid:161) Z â(cid:48) Z â(cid:162)â1 Z â(cid:48) Y â . 2sls 1 ThisisrepeatedB timestocreateasampleofB bootstrapdraws. Giventhesedrawsbootstrapstatistics can be calculated. This includes the bootstrap estimate of variance, standard errors, and confidence intervals,includingpercentile,BCpercentile,BC andpercentile-t. a Wenowshowthatthebootstrapestimatorhasthesameasymptoticdistributionasthesampleesti- mator. Foroveridentifiedcasesthisdemonstrationrequiresabitofextracare. Thiswasfirstshownby Hahn(1996). ThesampleobservationssatisfythemodelY = X (cid:48)Î²+e with(cid:69)[Ze]=0. ThetruevalueofÎ²inthe 1 populationcanbewrittenas Î²= (cid:179) (cid:69)(cid:163) XZ (cid:48)(cid:164)(cid:69)(cid:163) ZZ (cid:48)(cid:164)â1(cid:69)(cid:163) ZX (cid:48)(cid:164) (cid:180)â1 (cid:69)(cid:163) XZ (cid:48)(cid:164)(cid:69)(cid:163) ZZ (cid:48)(cid:164)â1(cid:69)[ZY ]. 1 Thetruevalueinthebootstrapuniverseisobtainedbyreplacingthepopulationmomentsbythesample moments,whichequalsthe2SLSestimator (cid:179) (cid:69)â(cid:163) X â Z â(cid:48)(cid:164)(cid:69)â(cid:163) Z â Z â(cid:48)(cid:164)â1(cid:69)â(cid:163) Z â X â(cid:48)(cid:164) (cid:180)â1 (cid:69)â(cid:163) X â Z â(cid:48)(cid:164)(cid:69)â(cid:163) Z â Z â(cid:48)(cid:164)â1(cid:69)â(cid:163) Z â Y â(cid:164) 1 = (cid:181)(cid:181) 1 X (cid:48) Z (cid:182)(cid:181) 1 Z (cid:48) Z (cid:182)â1(cid:181) 1 Z (cid:48) X (cid:182)(cid:182)â1(cid:181) 1 X (cid:48) Z (cid:182)(cid:181) 1 Z (cid:48) Z (cid:182)â1(cid:183) 1 Z (cid:48) Y (cid:184) 1 n n n n n n =Î² (cid:98)2sls",
    "page": 379,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". ThebootstrapobservationsthussatisfytheequationY 1 â i =X i â(cid:48)Î² (cid:98)2sls +e i â . Inmatrixnotationforthe samplethisis Y â 1 =X â(cid:48)Î² (cid:98)2sls +e â . (12.43)",
    "page": 379,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 360 Givenabootstraptriple(Y â ,X â ,Z â )=(Y ,X ,Z )forsomeobservation j thetruebootstraperroris 1i i i 1j j j e i â=Y 1j âX j (cid:48)Î² (cid:98)2sls =e (cid:98)j . Itfollowsthat (cid:69)â(cid:163) Z â e â(cid:164)=n â1Z (cid:48) e. (12.44) (cid:98) Thisisgenerallynotequaltozerointheover-identifiedcase. Thisananimportantcomplication. Inover-identifiedmodelsthetrueobservationssatisfythepop- ulation condition (cid:69)[Ze]=0 but in the bootstrap sample (cid:69)â [Z â e â ](cid:54)=0. This means that to apply the centrallimittheoremtothebootstrapestimatorwefirsthavetorecenterthemomentcondition.Thatis, (12.44)andthebootstrapCLTimply (cid:112) 1 (cid:161) Z â(cid:48) e ââZ (cid:48) e (cid:162)= (cid:112) 1 (cid:88) n (cid:161) Z â e ââ(cid:69)â(cid:163) Z â e â(cid:164)(cid:162)ââN(0,â¦) (12.45) n (cid:98) n i=1 i i dâ where â¦=(cid:69)(cid:163) ZZ (cid:48) e2(cid:164) . Using(12.43)wecannormalizethebootstrapestimatoras (cid:112) n (cid:179) Î² (cid:98) â âÎ² (cid:98) (cid:180) = (cid:112) n (cid:179) X â(cid:48) Z â(cid:161) Z â(cid:48) Z â(cid:162)â1 Z â(cid:48) X â (cid:180)â1 X â(cid:48) Z â(cid:161) Z â(cid:48) Z â(cid:162)â1 Z â(cid:48) e â 2sls 2sls = (cid:181)(cid:181) 1 X â(cid:48) Z â (cid:182)(cid:181) 1 Z â(cid:48) Z â (cid:182)â1(cid:181) 1 Z â(cid:48) X â (cid:182)(cid:182)â1 n n n Ã (cid:181) 1 X â(cid:48) Z â (cid:182)(cid:181) 1 Z â(cid:48) Z â (cid:182)â1 (cid:112) 1 (cid:161) Z â(cid:48) e ââZ (cid:48) e (cid:162) (12.46) (cid:98) n n n + (cid:181)(cid:181) 1 X â(cid:48) Z â (cid:182)(cid:181) 1 Z â(cid:48) Z â (cid:182)â1(cid:181) 1 Z â(cid:48) X â (cid:182)(cid:182)â1 n n n Ã (cid:181) 1 X â(cid:48) Z â (cid:182)(cid:181) 1 Z â(cid:48) Z â (cid:182)â1(cid:181) (cid:112) 1 Z (cid:48) e (cid:182) . (12.47) (cid:98) n n n UsingthebootstrapWLLN, 1 X â(cid:48) Z â= 1 X (cid:48) Z+o (1) p n n 1 Z â(cid:48) Z â= 1 Z (cid:48) Z+o (1). p n n Thisimplies(12.47)isequalto (cid:112) n (cid:179) X (cid:48) Z (cid:161) Z (cid:48) Z (cid:162)â1(cid:161) Z (cid:48) X (cid:162) (cid:180)â1 X (cid:48) Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) e+o (1)=0+o (1). (cid:98) p p Theequalityholdsbecausethe2SLSfirst-orderconditionimpliesX (cid:48) Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) e=0. Also,combined (cid:98) with(12.45)weseethat(12.46)convergesinbootstrapdistributionto (cid:161) Q XZ Q â Z 1 Z Q ZX (cid:162)â1 Q XZ Q â Z 1 Z N(0,â¦)=N (cid:161) 0,VÎ² (cid:162) w (cid:112) here VÎ² is the 2SLS asymptotic variance from Theorem 12.2. This is the asymptotic distribution of n (cid:161)Î² (cid:98) â 2sls âÎ² (cid:98)2sls (cid:162) . Bystandardcalculationswecanalsoshowthatbootstrapt-ratiosareasymptoticallynormal.",
    "page": 380,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 361 Theorem12.8 UnderAssumption12.2,asnââ (cid:112) n (cid:161)Î² (cid:98) â 2sls âÎ² (cid:98)2sls (cid:162)â d â â N (cid:161) 0,VÎ² (cid:162) whereVÎ²isthe2SLSasymptoticvariancefromTheorem12.2.Furthermore, (cid:112) T â= n (cid:161)Î² (cid:98) â 2sls âÎ² (cid:98)2sls (cid:162) ââN(0,1). s (cid:161)Î² (cid:98) â (cid:162) dâ 2sls Thisshowsthatpercentile-typeandpercentile-tconfidenceintervalsareasymptoticallyvalid. One might expect that the asymptotic refinement arguments extend to the BC and percentile-t (cid:112) (cid:112) a s m a e m th e o a d s s ym bu p t to th ti i c s d d i o st e r s ib n u o ti t o a n p t p h e e a y r d to iff b er e i t n h fi e n c i a te se s . am W p h l i e le sby n a (cid:161) n Î² (cid:98) â 2 O sls â (cid:161) n Î² (cid:98) â 2 1 s / l 2 s (cid:162) (cid:162) t a e n r d m. T n h (cid:161) i Î² (cid:98) s 2 m sls e â an Î² s (cid:162) t h h a a v t e th th e e y p havedistinctEdgeworthexpansions. Consequently,unadjustedbootstrapmethodswillnotachievean asymptoticrefinement. AnalternativesuggestedbyHallandHorowitz(1996)istorecenterthebootstrap2SLSestimatorso thatitsatisfiesthecorrectorthogonalitycondition.Define Î² (cid:98) â 2s â ls = (cid:179) X â(cid:48) Z â(cid:161) Z â(cid:48) Z â(cid:162)â1 Z â(cid:48) X â (cid:180)â1 X â(cid:48) Z â(cid:161) Z â(cid:48) Z â(cid:162)â1(cid:161) Z â(cid:48) Y â 1 âZ (cid:48) (cid:98) e (cid:162) . Wecanseethat (cid:112) n (cid:161)Î² (cid:98) â 2s â ls âÎ² (cid:98)2sls (cid:162)= (cid:181) n 1 X â(cid:48) Z â (cid:181) n 1 Z â(cid:48) Z â (cid:182)â1 n 1 Z â(cid:48) X â (cid:182)â1 Ã (cid:181) 1 X â(cid:48) Z â (cid:182)(cid:181) 1 Z â(cid:48) Z â (cid:182)â1 (cid:195) (cid:112) 1 (cid:88) n (cid:161) Z â e ââ(cid:69)â(cid:163) Z â e â(cid:164)(cid:162) (cid:33) n n n i=1 i i (cid:161) (cid:162) whichconvergestotheN 0,VÎ² distributionwithoutspecialhandling. HallandHorowitz(1996)show that percentile-t methods applied to Î² (cid:98) ââ achieve an asymptotic refinement and are thus preferred to 2sls theunadjustedbootstrapestimator. Thisrecenteredestimator,however,isnotthestandardimplementationofthebootstrapfor2SLSas usedinempiricalpractice. 12.24 ThePerilofBootstrap2SLSStandardErrors Itistemptingtousethebootstrapalgorithmtoestimatevariancematricesandstandarderrorsforthe 2SLSestimator.Infactthisisoneofthemostcommonusesofbootstrapmethodsincurrenteconometric practice. Unfortunately this is an unjustified and ill-conceived idea and should not be done. In finite samples the 2SLS estimator may not have a finite second moment, meaning that bootstrap variance estimatesareunstableandunreliable. Theorem 12.7 shows that under jointly normality the 2SLS estimator will have a finite variance if and only if the number of overidentifying restrictions is two or larger. Thus for just-identified IV, and 2SLSwithonedegreeofoveridentification,thefinitesamplevarianceisinfinite. Thebootstrapwillbe attemptingtoestimatethisvalueâinfinityâandwillyieldnonsensicalanswers",
    "page": 381,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Theorem 12.7 shows that under jointly normality the 2SLS estimator will have a finite variance if and only if the number of overidentifying restrictions is two or larger. Thus for just-identified IV, and 2SLSwithonedegreeofoveridentification,thefinitesamplevarianceisinfinite. Thebootstrapwillbe attemptingtoestimatethisvalueâinfinityâandwillyieldnonsensicalanswers. Whentheobservations arenotjointlynormalthereisnofinitesampletheory(soitispossiblethatthefinitesamplevarianceis actuallyfinite)butthisisunknownandunverifiable.",
    "page": 381,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 362 Inoveridentifiedsettingswhenthenumberofoveridentifyingrestrictionsistwoorlargertheboot- strapcanbeappliedforstandarderrorestimation.Howeverthisisnotthemostcommonapplicationof IVmethodsineconometricpracticeandthusshouldbeviewedastheexceptionratherthanthenorm. To understand what is going on consider the simplest case of a just-identified model with a single endogenousregressorandnoincludedexogenousregressors. Inthiscasetheestimatorcanbewritten asaratioofmeans (cid:80)n Z e Î² (cid:98)iv âÎ²= (cid:80)n i=1 Z i X i . i=1 i i Under joint normality of (e ,X ) this has a Cauchy-like distribution which does not possess any finite i i integermoments. Thetroubleisthatthedenominatorcanbeeitherpositiveornegative,andarbitrarily closetozero.Thismeansthattheratiocantakearbitrarilylargevalues. To illustrate let us return to the basic Card IV wage regression from column 2 of Table 12.1 which usescollegeasaninstrumentforeducation. WeestimatethisequationforthesubsampleofBlackmen whichhasn=703observations,andfocusonthecoefficientforthereturntoeducation.Thecoefficient estimateisreportedinTable12.3,alongwithasymptotic,jackknife,andtwobootstrapstandarderrors eachcalculatedwith10,000bootstrapreplications. Table12.3:InstrumentalVariableReturntoEducationforBlackMen Estimate 0.11 Asymptotics.e. (0.11) Jackknifes.e. (0.11) Bootstraps.e.(standard) (1.42) Bootstraps.e.(repeat) (4.79) Thebootstrapstandarderrorsareanorderofmagnitudelargerthantheasymptoticstandarderrors, andvarysubstantiallyacrossthebootstraprunsdespiteusing10,000bootstrapreplications. Thisindi- catesmomentfailureandunreliabilityofthebootstrapstandarderrors. ThisisastrongmessagethatbootstrapstandarderrorsshouldnotbecomputedforIVestimators. Instead,reportpercentile-typeconfidenceintervals.Afurthercautionaryremarkisthatinfinitesamples percentile confidence intervals also may have poor coverage rates, especially in contexts such as the resultsofTable12.3. 12.25 ClusteredDependence InSection4.23weintroducedclustereddependence. Wecanalsousethemethodsofclusteredde- pendence for 2SLS estimation. Recall, the gth cluster has the observations Y =(Y ,...,Y ) (cid:48) , X = g 1g ngg g (X ,...,X ) (cid:48) ,andZ =(Z ,...,Z ) (cid:48) .Thestructuralequationforthegth clustercanbewrittenasthe 1g ngg g 1g ngg matrixsystemY =X Î²+e .Usingthisnotationthecentered2SLSestimatorcanbewrittenas g g g Î² (cid:98)2sls âÎ²= (cid:179) X (cid:48) Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) X (cid:180)â1 X (cid:48) Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) e (cid:195) (cid:33) = (cid:179) X (cid:48) Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) X (cid:180)â1 X (cid:48) Z (cid:161) Z (cid:48) Z (cid:162)â1 (cid:88) G Z (cid:48) e",
    "page": 382,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". g g g=1 Thecluster-robustcovariancematrixestimatorforÎ² (cid:98)2sls thustakestheform V(cid:98)Î² = (cid:179) X (cid:48) Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) X (cid:180)â1 X (cid:48) Z (cid:161) Z (cid:48) Z (cid:162)â1 S(cid:98) (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) X (cid:179) X (cid:48) Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) X (cid:180)â1",
    "page": 382,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 363 with G S(cid:98) = (cid:88) Z (cid:48) g(cid:98) e g(cid:98) e (cid:48) g Z g g=1 andtheclusteredresiduals (cid:98) e g =Y g âX g Î² (cid:98)2sls . Thedifferencebetweentheheteroskedasticity-robustestimatorandthecluster-robustestimatoris thecovarianceestimatorS(cid:98). 12.26 GeneratedRegressors Theâtwo-stageâformofthe2SLSestimatorisanexampleofwhatiscalledâestimationwithgenerated regressorsâ. We say a regressor is a generated if it is an estimate of an idealized regressor or if it is a functionofestimatedparameters. Typically, ageneratedregressorW(cid:99) isanestimateofanunobserved idealregressorW.Asanestimate,W(cid:99)i isafunctionofthefullsamplenotjustobservationi.Henceitisnot âi.i.d.âasitisdependentacrossobservationswhichinvalidatestheconventionalregressionassumptions. Consequently,thesamplingdistributionofregressionestimatesisaffected. Unlessthisisincorporated intoourinferencemethods,covariancematrixestimatesandstandarderrorswillbeincorrect. TheeconometrictheoryofgeneratedregressorswasdevelopedbyPagan(1984)forlinearmodelsand extendedtononlinearmodels andmore general two-stepestimatorsby Pagan(1986). Independently, similarresultswereobtainedbyMurphyandTopel(1985).Herewefocusonthelinearmodel: Y =W (cid:48)Î²+v (12.48) W =A (cid:48) Z (cid:69)[Zv]=0. Theobservablesare(Y,Z).Wealsohaveanestimate A(cid:98) of A. (cid:48) GivenA(cid:98) weconstructtheestimateW(cid:99)i =A(cid:98) Z i ofW i ,replaceW i in(12.48)withW(cid:99)i ,andthenestimate Î²byleastsquares,resultingintheestimator (cid:195) (cid:33)â1(cid:195) (cid:33) n n Î² (cid:98) = (cid:88) W(cid:99)i W(cid:99) i (cid:48) (cid:88) W(cid:99)i Y i . (12.49) i=1 i=1 TheregressorsW(cid:99)i arecalledgeneratedregressors. ThepropertiesofÎ² (cid:98)aredifferentthanleastsquares withi.i.d.observationssincethegeneratedregressorsarethemselvesestimates. Thisframeworkincludes2SLSaswellasothercommonestimators. The2SLSmodelcanbewritten as(12.48)bylookingatthereducedformequation(12.13),withW =Î(cid:48) Z, A=Î,and A(cid:98) =Î (cid:98). TheexampleswhichmotivatedPagan(1984)andMurphyandTopel(1985)emergedfromthemacroe- conomicsliterature,inparticulartheworkofBarro(1977)whichexaminedtheimpactofinflationexpec- tationsandexpectationerrorsoneconomicoutput. LetÏdenoterealizedinflationand Z bevariables availabletoeconomicagents. AmodelofinflationexpectationssetsW =(cid:69)[Ï|Z]=Î³(cid:48) Z andamodelof expectationerrorsetsW =Ïâ(cid:69)[Ï|Z]=ÏâÎ³(cid:48) Z. Sinceexpectationsanderrorsarenotobservedthey arereplacedinapplicationswiththefittedvaluesW(cid:99)i =Î³ (cid:98) (cid:48) Z i andresidualsW(cid:99)i =Ï i âÎ³ (cid:98) (cid:48) Z i whereÎ³ (cid:98) isthe coefficientfromaregressionofÏonZ. Thegeneratedregressorframeworkincludesalloftheseexamples",
    "page": 383,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". LetÏdenoterealizedinflationand Z bevariables availabletoeconomicagents. AmodelofinflationexpectationssetsW =(cid:69)[Ï|Z]=Î³(cid:48) Z andamodelof expectationerrorsetsW =Ïâ(cid:69)[Ï|Z]=ÏâÎ³(cid:48) Z. Sinceexpectationsanderrorsarenotobservedthey arereplacedinapplicationswiththefittedvaluesW(cid:99)i =Î³ (cid:98) (cid:48) Z i andresidualsW(cid:99)i =Ï i âÎ³ (cid:98) (cid:48) Z i whereÎ³ (cid:98) isthe coefficientfromaregressionofÏonZ. Thegeneratedregressorframeworkincludesalloftheseexamples. ThegoalistoobtainadistributionalapproximationforÎ² (cid:98)inordertoconstructstandarderrors,con- fidenceintervals,andtests.Startbysubstitutingequation(12.48)into(12.49).Weobtain (cid:195) (cid:33)â1(cid:195) (cid:33) n n Î² (cid:98) = (cid:88) W(cid:99)i W(cid:99) i (cid:48) (cid:88) W(cid:99)i (cid:161) W i (cid:48)Î²+v i (cid:162) . i=1 i=1",
    "page": 383,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 364 Next,substituteW i (cid:48)Î²=W(cid:99) i (cid:48)Î²+(cid:161) W i âW(cid:99)i (cid:162)(cid:48) Î².Weobtain (cid:195) (cid:33)â1(cid:195) (cid:33) Î² (cid:98) âÎ²= (cid:88) n W(cid:99)i W(cid:99) i (cid:48) (cid:88) n W(cid:99)i (cid:179) (cid:161) W i âW(cid:99)i (cid:162)(cid:48) Î²+v i (cid:180) . (12.50) i=1 i=1 Effectively, thisshowsthatthedistributionofÎ² (cid:98) âÎ²hastworandomcomponents, oneduetothecon- ventionalregressioncomponentandthesecondduetothegeneratedregressor. Conventionalvariance estimatorsdonotaddressthissecondcomponentandthuswillbebiased. Interestingly, the distribution in (12.50) dramatically simplifies in the special case that the âgener- atedregressortermâ (cid:161) W i âW(cid:99)i (cid:162)(cid:48) Î²disappears. Thisoccurswhentheslopecoefficientsonthegenerated regressors are zero. To be specific, partitionW i =(W 1i ,W 2i ), W(cid:99)i =(cid:161) W 1i ,W(cid:99)2i (cid:162) , and Î²=(cid:161)Î² 1 ,Î² 2 (cid:162) so that W 1i aretheconventionalobservedregressorsandW(cid:99)2i arethegeneratedregressors.Then (cid:161) W i âW(cid:99)i (cid:162)(cid:48) Î²= (cid:161) W 2i âW(cid:99)2i (cid:162)(cid:48) Î² 2 .ThusifÎ² 2 =0thistermdisappears.Inthiscase(12.50)equals (cid:195) (cid:33)â1(cid:195) (cid:33) n n Î² (cid:98) âÎ² (cid:98) = (cid:88) W(cid:99)i W(cid:99) i (cid:48) (cid:88) W(cid:99)i v i . i=1 i=1 Thisisadramaticsimplification. (cid:48) Furthermore,sinceW(cid:99)i =A(cid:98) Z i wecanwritetheestimatorasafunctionofsamplemoments: (cid:112) n (cid:161)Î² (cid:98) âÎ²(cid:162)= (cid:195) A(cid:98) (cid:48) (cid:195) n 1 i (cid:88) = n 1 Z i Z i (cid:48) (cid:33) A(cid:98) (cid:33)â1 A(cid:98) (cid:48) (cid:195) (cid:112) 1 n i (cid:88) = n 1 Z i v i (cid:33) . (cid:112) If A(cid:98) ââAwefindfromstandardmanipulationsthat n (cid:161)Î² (cid:98) âÎ²(cid:162)ââN (cid:161) 0,VÎ² (cid:162) where p d VÎ² =(cid:161) A (cid:48)(cid:69)(cid:163) ZZ (cid:48)(cid:164) A (cid:162)â1(cid:161) A (cid:48)(cid:69)(cid:163) ZZ (cid:48) v2(cid:164) A (cid:162)(cid:161) A (cid:48)(cid:69)(cid:163) ZZ (cid:48)(cid:164) A (cid:162)â1 . (12.51) TheconventionalasymptoticcovariancematrixestimatorforÎ² (cid:98)takestheform (cid:195) (cid:33)â1(cid:195) (cid:33)(cid:195) (cid:33)â1 V(cid:98)Î² = n 1 (cid:88) n W(cid:99)i W(cid:99) i (cid:48) n 1 (cid:88) n W(cid:99)i W(cid:99) i (cid:48) v (cid:98)i 2 n 1 (cid:88) n W(cid:99)i W(cid:99) i (cid:48) (12.52) i=1 i=1 i=1 where v (cid:98)i =Y i âW(cid:99) i (cid:48)Î² (cid:98). Underthegivenassumptions,V(cid:98)Î² â p âVÎ². ThusinferenceusingV(cid:98)Î² isasymptot- ically valid. This is useful when we are interested in tests of Î² =0. Often this is of major interest in 2 applications. Totest(cid:72) 0 :Î² 2 =0wepartitionÎ² (cid:98) =(cid:161)Î² (cid:98)1 ,Î² (cid:98)2 (cid:162) andconstructaconventionalWaldstatistic W =nÎ² (cid:98) (cid:48) 2 (cid:161)(cid:163) V(cid:98)Î² (cid:164) 22 (cid:162)â1Î² (cid:98)2",
    "page": 384,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Underthegivenassumptions,V(cid:98)Î² â p âVÎ². ThusinferenceusingV(cid:98)Î² isasymptot- ically valid. This is useful when we are interested in tests of Î² =0. Often this is of major interest in 2 applications. Totest(cid:72) 0 :Î² 2 =0wepartitionÎ² (cid:98) =(cid:161)Î² (cid:98)1 ,Î² (cid:98)2 (cid:162) andconstructaconventionalWaldstatistic W =nÎ² (cid:98) (cid:48) 2 (cid:161)(cid:163) V(cid:98)Î² (cid:164) 22 (cid:162)â1Î² (cid:98)2 . Theorem12.9 Takemodel(12.48)with(cid:69)(cid:163) Y4(cid:164)<â,(cid:69)(cid:107)Z(cid:107)4<â, A (cid:48)(cid:69)(cid:163) ZZ (cid:48)(cid:164) A> (cid:112) 0, A(cid:98) ââ A, andW(cid:99)i =(cid:161) W 1i ,W(cid:99)2i (cid:162) . Under(cid:72) 0 :Î² 2 =0, asn ââ, n (cid:161)Î² (cid:98) âÎ²(cid:162)ââ p d N (cid:161) 0,VÎ² (cid:162) where VÎ² is given in (12.51). For V(cid:98)Î² given in (12.52), V(cid:98)Î² ââ VÎ². p Furthermore, W ââ Ï2 where q = dim(Î² ). For c satisfying Î± = 1âG (c), q 2 q d (cid:80)[W >c|(cid:72) ]âÎ±,sothetestâReject(cid:72) ifW >câhasasymptoticsizeÎ±. 0 0",
    "page": 384,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 365 InthespecialcasethatA(cid:98) =A(X,Z)andv|X,Z â¼N (cid:161) 0,Ï2(cid:162) thereisafinitesampleversionofthepre- viousresult.LetW0betheWaldstatisticconstructedwithahomoskedasticcovariancematrixestimator, andlet F =W/q (12.53) bethetheF statistic,whereq=dim(Î² ). 2 Theorem12.10 Takemodel(12.48)with A(cid:98) = A(X,Z), v |X,Z â¼N (cid:161) 0,Ï2(cid:162) and W(cid:99) =(cid:161) W 1 ,W(cid:99)2 (cid:162) . Under (cid:72) 0 : Î² 2 =0, t-statistics have exact N(0,1) distributions, andtheF statistic(12.53)hasanexactF q,nâk distributionwhere q =dim(Î² 2 ) andk=dim(Î²). To summarize, in the model Y =W (cid:48)Î² +W (cid:48)Î² +v whereW is not observed but replaced with an 1 1 2 2 2 estimateW(cid:99)2 ,conventionalsignificancetestsfor(cid:72) 0 :Î² 2 =0areasymptoticallyvalidwithoutadjustment. Whilethistheoryallowstestsof(cid:72) :Î² =0itunfortunatelydoesnotjustifyconventionalstandard 0 2 errorsorconfidenceintervals. Forthis,weneedtoworkoutthedistributionwithoutimposingthesim- plification Î² =0. This often needs to be worked out case-by-case or by using methods based on the 2 generalizedmethodofmomentstobeintroducedinChapter13. However,inoneimportantsetofex- amplesitisstraightforwardtoworkouttheasymptoticdistribution. Fortheremainderofthissectionweexaminethesettingwheretheestimators A(cid:98) takealeastsquares formsoforsomeX canbewrittenasA(cid:98) =(cid:161) Z (cid:48) Z (cid:162)â1(cid:161) Z (cid:48) X (cid:162) .Suchestimatorscorrespondtothemultivariate projectionmodel X =A (cid:48) Z+u (12.54) (cid:69)(cid:163) Zu (cid:48)(cid:164)=0. This class of estimators includes 2SLS and the expectation model described above. We can write the matrixofgeneratedregressorsasW(cid:99) =ZA(cid:98) andthen(12.50)as Î² (cid:98) âÎ²= (cid:179) W(cid:99) (cid:48) W(cid:99) (cid:180)â1(cid:179) W(cid:99) (cid:48)(cid:161)(cid:161) W âW(cid:99) (cid:162)Î²+v (cid:162) (cid:180) = (cid:179) A(cid:98) (cid:48) Z (cid:48) ZA(cid:98) (cid:180)â1(cid:179) A(cid:98) (cid:48) Z (cid:48) (cid:179) âZ (cid:161) Z (cid:48) Z (cid:162)â1(cid:161) Z (cid:48) U (cid:162)Î²+v (cid:180)(cid:180) = (cid:179) A(cid:98) (cid:48) Z (cid:48) ZA(cid:98) (cid:180)â1(cid:179) A(cid:98) (cid:48) Z (cid:48)(cid:161)âUÎ²+v (cid:162) (cid:180) = (cid:179) A(cid:98) (cid:48) Z (cid:48) ZA(cid:98) (cid:180)â1(cid:179) A(cid:98) (cid:48) Z (cid:48) e (cid:180) where e=vâu (cid:48)Î²=Y âX (cid:48)Î². (12.55) (cid:112) Thisestimatorhastheasymptoticdistribution n (cid:161)Î² (cid:98) âÎ²(cid:162)ââN (cid:161) 0,VÎ² (cid:162) where d VÎ² =(cid:161) A (cid:48)(cid:69)(cid:163) ZZ (cid:48)(cid:164) A (cid:162)â1(cid:161) A (cid:48)(cid:69)(cid:163) ZZ (cid:48) e2(cid:164) A (cid:162)(cid:161) A (cid:48)(cid:69)(cid:163) ZZ (cid:48)(cid:164) A (cid:162)â1 . (12.56) Underconditionalhomoskedasticitythecovariancematrixsimplifiesto VÎ² =(cid:161) A (cid:48)(cid:69)(cid:163) ZZ (cid:48)(cid:164) A (cid:162)â1(cid:69)(cid:163) e2(cid:164) .",
    "page": 385,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 366 AnappropriateestimatorofVÎ²is V(cid:98)Î² = (cid:181) n 1 W(cid:99) (cid:48) W(cid:99) (cid:182)â1 (cid:195) n 1 (cid:88) n W(cid:99)i W(cid:99) i (cid:48) e (cid:98)i 2 (cid:33) (cid:181) n 1 W(cid:99) (cid:48) W(cid:99) (cid:182)â1 (12.57) i=1 e (cid:98)i =Y i âX i (cid:48)Î² (cid:98). Undertheassumptionofconditionalhomoskedasticitythiscanbesimplifiedasusual. Thisappearstobetheusualcovariancematrixestimator,butitisnotbecausetheleastsquaresresid- ualsv (cid:98)i =Y i âW(cid:99) i (cid:48)Î² (cid:98)havebeenreplacedwithe (cid:98)i . Thisisexactlythesubstitutionmadebythe2SLScovari- ancematrixformula.Indeed,thecovariancematrixestimatorV(cid:98)Î²preciselyequals(12.40). Theorem12.11 Takemodel(12.48)and(12.54)with(cid:69)(cid:163) Y4(cid:164)<â, (cid:69)(cid:107)Z(cid:107)4 <â, (cid:112) A (cid:48)(cid:69)(cid:163) ZZ (cid:48)(cid:164) A >0, and A(cid:98) =(cid:161) Z (cid:48) Z (cid:162)â1(cid:161) Z (cid:48) X (cid:162) . As n ââ, n (cid:161)Î² (cid:98) âÎ²(cid:162)ââN (cid:161) 0,VÎ² (cid:162) d whereVÎ² isgivenin(12.56)withe definedin(12.55). ForV(cid:98)Î² givenin(12.57), V(cid:98)Î² ââVÎ². p Sincetheparameterestimatorsareasymptoticallynormalandthecovariancematrixisconsistently estimated,standarderrorsandteststatisticsconstructedfromV(cid:98)Î²areasymptoticallyvalidwithconven- tionalinterpretations. Wenowsummarizetheresultsofthissection. Ingeneral,careneedstobeexercisedwhenestimat- ingmodelswithgeneratedregressors. Asageneralrule, generatedregressorsandtwo-stepestimation affectssamplingdistributionsandvariancematrices.Animportantsimplicationoccursforteststhatthe generatedregressorshavezeroslopes. Inthiscaseconventionaltestshaveconventionaldistributions, bothasymptoticallyandinfinitesamples. Anotherimportantspecialcaseoccurswhenthegenerated regressorsareleastsquaresfittedvalues. Inthiscasetheasymptoticdistributiontakesaconventional formbuttheconventionalresidualneedstobereplacedbyoneconstructedwiththeforecastedvariable. Withthisonemodificationasymptoticinferenceusingthegeneratedregressorsisconventional. 12.27 RegressionwithExpectationErrors In this section we examine a generated regressor model which includes expectation errors in the regression. Thisisanimportantclassofgeneratedregressormodelsandisrelativelystraightforwardto characterize.Themodelis Y =X (cid:48)Î²+u (cid:48)Î±+Î½ W =A (cid:48) Z X =W +u (cid:69)[ZÎ½]=0 (cid:69)[uÎ½]=0 (cid:69)(cid:163) Zu (cid:48)(cid:164)=0. Theobservablesare(Y,X,Z). ThismodelstatesthatW istheexpectationof X (ormoregenerally,the projectionofX onZ)anduisitsexpectationerror.Themodelallowsforexogenousregressorsasinthe",
    "page": 386,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 367 standardIVmodeliftheyarelistedinW, X,andZ. Thismodelisused,forexample,todecomposethe effectofexpectationsfromexpectationerrors.Insomecasesitisdesiredtoincludeonlytheexpectation erroru,nottheexpectationW.Thisdoesnotchangetheresultsdescribedhere. The model is estimated as follows. First, A is estimated by multivariate least squares of X on Z, A(cid:98) =(cid:161) Z (cid:48) Z (cid:162)â1(cid:161) Z (cid:48) X (cid:162) ,whichyieldsasby-productsthefittedvaluesW(cid:99)i = A(cid:98) (cid:48) Z i andresidualsu (cid:98)i =X(cid:98)i âW(cid:99)i . Second,thecoefficientsareestimatedbyleastsquaresofY onthefittedvaluesW(cid:99)andresidualsu (cid:98) Y i =W(cid:99) i (cid:48)Î² (cid:98) +u (cid:98)i (cid:48)Î± (cid:98) +Î½ (cid:98)i . Wenowexaminetheasymptoticdistributionsoftheseestimators. Bythefirst-stepregressionZ (cid:48) U(cid:98) =0,W(cid:99) (cid:48) U(cid:98) =0andW (cid:48) U(cid:98) =0.ThismeansthatÎ² (cid:98)andÎ± (cid:98) canbecomputed separately.Noticethat (cid:179) (cid:48) (cid:180)â1 (cid:48) Î² (cid:98) = W(cid:99)W(cid:99) W(cid:99) Y and Y =W(cid:99) Î²+UÎ±+(cid:161) W âW(cid:99) (cid:162)Î²+Î½. Substituting,usingW(cid:99) (cid:48) U(cid:98) =0andW âW(cid:99) =âZ (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) U wefind Î² (cid:98) âÎ²= (cid:179) W(cid:99) (cid:48) W(cid:99) (cid:180)â1 W(cid:99) (cid:48)(cid:161) UÎ±+(cid:161) W âW(cid:99) (cid:162)Î²+Î½(cid:162) = (cid:179) A(cid:98) (cid:48) Z (cid:48) ZA(cid:98) (cid:180)â1 A(cid:98) (cid:48) Z (cid:48)(cid:161) UÎ±âUÎ²+Î½(cid:162) = (cid:179) A(cid:98) (cid:48) Z (cid:48) ZA(cid:98) (cid:180)â1 A(cid:98) (cid:48) Z (cid:48) e where e =v +u (cid:48)(cid:161)Î±âÎ²(cid:162)=Y âX (cid:48)Î². i i i i i Wealsofind (cid:179) (cid:48) (cid:180)â1 (cid:48) Î± (cid:98) = U(cid:98) U(cid:98) U(cid:98) Y. SinceU(cid:98) (cid:48) W =0,UâU(cid:98) =Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) U andU(cid:98) (cid:48) Z =0then Î± (cid:98) âÎ±= (cid:179) U(cid:98) (cid:48) U(cid:98) (cid:180)â1 U(cid:98) (cid:48)(cid:161) WÎ²+(cid:161) UâU(cid:98) (cid:162)Î±+Î½(cid:162) (cid:179) (cid:48) (cid:180)â1 (cid:48) = U(cid:98) U(cid:98) U(cid:98) Î½. Together,weestablishthefollowingdistributionalresult.",
    "page": 387,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 368 Theorem12.12 For the model and estimators described in this section, with (cid:69)(cid:163) Y4(cid:164)<â,(cid:69)(cid:107)Z(cid:107)4<â,(cid:69)(cid:107)X(cid:107)4<â,A (cid:48)(cid:69)(cid:163) ZZ (cid:48)(cid:164) A>0,and(cid:69)(cid:163) uu (cid:48)(cid:164)>0,asnââ (cid:112) (cid:181) Î²âÎ² (cid:182) (cid:98) n ââN(0,V) (12.58) Î±âÎ± (cid:98) d where (cid:181) (cid:182) V = VÎ²Î² VÎ²Î± VÎ±Î² VÎ±Î± and VÎ²Î² =(cid:161) A (cid:48)(cid:69)(cid:163) ZZ (cid:48)(cid:164) A (cid:162)â1(cid:161) A (cid:48)(cid:69)(cid:163) ZZ (cid:48) e2(cid:164) A (cid:162)(cid:161) A (cid:48)(cid:69)(cid:163) ZZ (cid:48)(cid:164) A (cid:162)â1 VÎ±Î² =(cid:161)(cid:69)(cid:163) uu (cid:48)(cid:164)(cid:162)â1(cid:161)(cid:69)(cid:163) uZ (cid:48) eÎ½(cid:164) A (cid:162)(cid:161) A (cid:48)(cid:69)(cid:163) ZZ (cid:48)(cid:164) A (cid:162)â1 VÎ±Î± =(cid:161)(cid:69)(cid:163) uu (cid:48)(cid:164)(cid:162)â1(cid:69)(cid:163) uu (cid:48)Î½2(cid:164)(cid:161)(cid:69)(cid:163) uu (cid:48)(cid:164)(cid:162)â1 . Theasymptoticcovariancematrixisestimatedby V(cid:98)Î²Î² = (cid:181) n 1 W(cid:99) (cid:48) W(cid:99) (cid:182)â1 (cid:195) n 1 (cid:88) n W(cid:99)i W(cid:99) i (cid:48) e (cid:98)i 2 (cid:33) (cid:181) n 1 W(cid:99) (cid:48) W(cid:99) (cid:182)â1 i=1 V(cid:98)Î±Î² = (cid:181) n 1 U(cid:98) (cid:48) U(cid:98) (cid:182)â1 (cid:195) n 1 (cid:88) n u (cid:98)i W(cid:99) i (cid:48) e (cid:98)i Î½ (cid:98)i (cid:33) (cid:181) n 1 W(cid:99) (cid:48) W(cid:99) (cid:182)â1 i=1 V(cid:98)Î±Î± = (cid:181) n 1 U(cid:98) (cid:48) U(cid:98) (cid:182)â1 (cid:195) n 1 (cid:88) n U(cid:98)i U(cid:98) i (cid:48)Î½ (cid:98) 2 i (cid:33) (cid:181) n 1 U(cid:98) (cid:48) U(cid:98) (cid:182)â1 i=1 where (cid:48) W(cid:99)i =A(cid:98) Z i u (cid:98)i =X(cid:98)i âW(cid:99)i e (cid:98)i =Y i âX i (cid:48)Î² (cid:98) Î½ (cid:98)i =Y i âW(cid:99) i (cid:48)Î² (cid:98) âu (cid:98)i (cid:48)Î± (cid:98) . Underconditionalhomoskedasticity,specifically (cid:69) (cid:183)(cid:181) e e Î½ i 2 e v i Î½ 2 i (cid:182)(cid:175) (cid:175) (cid:175) (cid:175) Z i (cid:184) =C i i i thenVÎ±Î² =0andthecoefficientestimatesÎ² (cid:98)andÎ± (cid:98) areasymptoticallyindependent. Thevariancecom- ponentsalsosimplifyto VÎ²Î² =(cid:161) A (cid:48)(cid:69)(cid:163) ZZ (cid:48)(cid:164) A (cid:162)â1(cid:69)(cid:163) e2(cid:164) i VÎ±Î± =(cid:161)(cid:69)(cid:163) uu (cid:48)(cid:164)(cid:162)â1(cid:69)(cid:163)Î½2(cid:164) . Inthiscasewehavethecovariancematrixestimators V(cid:98) 0 Î²Î² = (cid:181) n 1 W(cid:99) (cid:48) W(cid:99) (cid:182)â1 (cid:195) n 1 (cid:88) n e (cid:98)i 2 (cid:33) i=1 V(cid:98) 0 Î±Î± = (cid:181) n 1 U(cid:98) (cid:48) U(cid:98) (cid:182)â1 (cid:195) n 1 (cid:88) n Î½ (cid:98) 2 i (cid:33) i=1",
    "page": 388,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 369 andV(cid:98) 0 Î±Î² =0. 12.28 ControlFunctionRegression Inthissectionwepresentanalternativewayofcomputingthe2SLSestimatorbyleastsquares. Itis usefulinnonlinearcontexts,andalsointhelinearmodeltoconstructtestsforendogeneity. ThestructuralandreducedformequationsforthestandardIVmodelare Y =X (cid:48)Î² +X (cid:48)Î² +e 1 1 2 2 X =Î(cid:48) Z +Î(cid:48) Z +u . 2 12 1 22 2 2 Sincetheinstrumentalvariableassumptionspecifiesthat(cid:69)[Ze]=0, X isendogenous(correlatedwith 2 e)ifu andearecorrelated.Wecanthereforeconsiderthelinearprojectionofeonu 2 2 e=u (cid:48)Î±+Î½ 2 Î±=(cid:161)(cid:69)(cid:163) u u (cid:48)(cid:164)(cid:162)â1(cid:69)[u e] 2 2 2 (cid:69)[u Î½]=0. 2 Substitutingthisintothestructuralformequationwefind Y =X (cid:48)Î² +X (cid:48)Î² +u (cid:48)Î±+Î½ (12.59) 1 1 2 2 2 (cid:69)[X Î½]=0 1 (cid:69)[X Î½]=0 2 (cid:69)[u Î½]=0. 2 Noticethat X isuncorrelatedwithÎ½. Thisisbecause X iscorrelatedwithe onlythroughu ,andÎ½is 2 2 2 theerrorafterehasbeenprojectedorthogonaltou . 2 If u were observed we could then estimate (12.59) by least squares. Since it is not observed we 2 estimateitbythereduced-formresidualu (cid:98)2i =X 2i âÎ (cid:98) (cid:48) 12 Z 1i âÎ (cid:98) (cid:48) 22 Z 2i .Thenthecoefficients(Î² 1 ,Î² 2 ,Î±)can beestimatedbyleastsquaresofY on(X ,X ,u ).Wecanwritethisas 1 2 (cid:98)2 Y i =X i (cid:48)Î² (cid:98) +u (cid:98)2 (cid:48) i Î± (cid:98) +Î½ (cid:98)i (12.60) orinmatrixnotationas Y =XÎ² (cid:98) +U(cid:98)2 Î± (cid:98) +Î½ (cid:98) . Thisturnsouttobeanalternativealgebraicexpressionforthe2SLSestimator. Indeed,wenowshowthatÎ² (cid:98) =Î² (cid:98)2sls .First,notethatthereducedformresidualcanbewrittenas U(cid:98)2 =(I n âP Z )X 2 whereP isdefinedin(12.30).BytheFWLrepresentation Z (cid:179) (cid:48) (cid:180)â1(cid:179) (cid:48) (cid:180) Î² (cid:98) = X(cid:101) X(cid:101) X(cid:101) Y (12.61) whereX(cid:101) =(cid:163) X(cid:101)1 ,X(cid:101)2 (cid:164) with (cid:179) (cid:48) (cid:180)â1 (cid:48) X(cid:101)1 =X 1 âU(cid:98)2 U(cid:98)2 U(cid:98)2 U(cid:98)2 X 1 =X 1",
    "page": 389,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 370 (cid:48) (sinceU(cid:98)2 X 1 =0)and (cid:179) (cid:48) (cid:180)â1 (cid:48) X(cid:101)2 =X 2 âU(cid:98)2 U(cid:98)2 U(cid:98)2 U(cid:98)2 X 2 =X 2 âU(cid:98)2 (cid:161) X (cid:48) 2 (I n âP Z )X 2 (cid:162)â1 X (cid:48) 2 (I n âP Z )X 2 =X 2 âU(cid:98)2 =P X . Z 2 ThusX(cid:101) =[X 1 ,P Z X 2 ]=P Z X.Substitutedinto(12.61)wefind Î² (cid:98) =(cid:161) X (cid:48) P Z X (cid:162)â1(cid:161) X (cid:48) P Z Y (cid:162)=Î² (cid:98)2sls whichis(12.31)asclaimed. Again, whatwehavefoundisthatOLSestimationofequation(12.60)yieldsalgebraicallythe2SLS estimatorÎ² (cid:98)2sls . Wenowconsiderthedistributionofthecontrolfunctionestimator (cid:161)Î² (cid:98),Î± (cid:98) (cid:162) .Itisageneratedregression model, andinfactiscoveredbythemodelexaminedinSection12.27afteraslightreparametrization. LetW =Î (cid:48) Z.Noteu=XâW.Thenthemainequation(12.59)canbewrittenasY =W (cid:48)Î²+u (cid:48)Î³+Î½where 2 Î³=Î±+Î² .ThisisthemodelinSection12.27. 2 SetÎ³ (cid:98) =Î± (cid:98) +Î² (cid:98)2 .Itfollowsfrom(12.58)thatasnââwehavethejointdistribution (cid:112) (cid:181) Î² âÎ² (cid:182) n (cid:98)2 2 ââN(0,V) Î³âÎ³ (cid:98) d where (cid:181) (cid:182) V = V 22 V 2Î³ VÎ³2 VÎ³Î³ V = (cid:183) (cid:179) Î (cid:48) (cid:69)(cid:163) ZZ (cid:48)(cid:164)Î (cid:180)â1 Î (cid:48) (cid:69)(cid:163) ZZ (cid:48) e2(cid:164)Î (cid:179) Î (cid:48) (cid:69)(cid:163) ZZ (cid:48)(cid:164)Î (cid:180)â1 (cid:184) 22 22 VÎ³2 = (cid:183) (cid:161)(cid:69)(cid:163) u 2 u 2 (cid:48)(cid:164)(cid:162)â1(cid:69)(cid:163) uZ (cid:48) eÎ½(cid:164)Î (cid:179) Î (cid:48) (cid:69)(cid:163) ZZ (cid:48)(cid:164)Î (cid:180)â1 (cid:184) Â·2 VÎ³Î³ =(cid:161)(cid:69)(cid:163) u 2 u 2 (cid:48)(cid:164)(cid:162)â1(cid:69)(cid:163) u 2 u 2 (cid:48)Î½2(cid:164)(cid:161)(cid:69)(cid:163) u 2 u 2 (cid:48)(cid:164)(cid:162)â1 e=Y âX (cid:48)Î². TheasymptoticdistributionofÎ³ (cid:98) =Î± (cid:98) âÎ² (cid:98)2 canbededuced. Theorem12.13 If (cid:69)(cid:163) Y4(cid:164)<â, (cid:69)(cid:107)Z(cid:107)4 <â, (cid:69)(cid:107)X(cid:107)4 <â, A (cid:48)(cid:69)(cid:163) ZZ (cid:48)(cid:164) A >0, and (cid:69)(cid:163) uu (cid:48)(cid:164)>0,asnââ (cid:112) n(Î± (cid:98) âÎ±)ââN(0,VÎ±) d where VÎ± =V 22 +VÎ³Î³ âVÎ³2 âV (cid:48) Î³2 .",
    "page": 390,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 371 Underconditionalhomoskedasticitywehavetheimportantsimplifications V = (cid:183) (cid:179) Î (cid:48) (cid:69)(cid:163) ZZ (cid:48)(cid:164)Î (cid:180)â1 (cid:184) (cid:69)(cid:163) e2(cid:164) 22 22 VÎ³Î³ =(cid:161)(cid:69)(cid:163) u 2 u 2 (cid:48)(cid:164)(cid:162)â1(cid:69)(cid:163)Î½2(cid:164) VÎ³2 =0 VÎ± =V 22 +VÎ³Î³. AnestimatorforVÎ±inthegeneralcaseis (cid:48) V(cid:98)Î± =V(cid:98)22 +V(cid:98)Î³Î³ âV(cid:98)Î³2 âV(cid:98)Î³2 (12.62) where (cid:34) (cid:195) (cid:33) (cid:35) V(cid:98)22 = n 1(cid:161) X (cid:48) P Z X (cid:162)â1 X (cid:48) Z (cid:161) Z (cid:48) Z (cid:162)â1 (cid:88) n Z i Z i (cid:48) e (cid:98)i 2 (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) X (cid:161) X (cid:48) P Z X (cid:162)â1 i=1 22 (cid:34) (cid:195) (cid:33) (cid:35) V(cid:98)Î³2 = n 1(cid:179) U(cid:98) (cid:48) U(cid:98) (cid:180)â1 (cid:88) n u (cid:98)i W(cid:99) i (cid:48) e (cid:98)i Î½ (cid:98)i (cid:161) X (cid:48) P Z X (cid:162)â1 i=1 Â·2 e (cid:98)i =Y i âX i (cid:48)Î² (cid:98) Î½ (cid:98)i =Y i âX i (cid:48)Î² (cid:98) âu (cid:98)2 (cid:48) i Î³ (cid:98) . Undertheassumptionofconditionalhomoskedasticitywehavetheestimator V(cid:98) 0 Î± =V(cid:98) 0 Î²Î² +V(cid:98) 0 Î³Î³ (cid:195) (cid:33) V(cid:98)Î²Î² = (cid:104) (cid:161) X (cid:48) P Z X (cid:162)â1 (cid:105) (cid:88) n e (cid:98)i 2 22 i=1 (cid:195) (cid:33) V(cid:98)Î³Î³ = (cid:179) U(cid:98) (cid:48) U(cid:98) (cid:180)â1 (cid:88) n Î½ (cid:98) 2 i . i=1 12.29 EndogeneityTests The 2SLS estimator allows the regressor X to be endogenous, meaning that X is correlated with 2 2 thestructuralerrore. IfthiscorrelationiszerothenX isexogenousandthestructuralequationcanbe 2 estimatedbyleastsquares.Thisisatestablerestriction.Effectively,thenullhypothesisis (cid:72) :(cid:69)[X e]=0 0 2 withthealternative (cid:72) :(cid:69)[X e](cid:54)=0. 1 2 Themaintainedhypothesisis(cid:69)[Ze]=0. Since X isacomponentof Z thisimplies(cid:69)[X e]=0. Conse- 1 1 quentlywecouldalternativelywritethenullas(cid:72) :(cid:69)[Xe]=0(andsomeauthorsdoso). 0 Recallthecontrolfunctionregression(12.59) Y =X (cid:48)Î² +X (cid:48)Î² +u (cid:48)Î±+Î½ 1 1 2 2 2 Î±=(cid:161)(cid:69)(cid:163) u u (cid:48)(cid:164)(cid:162)â1(cid:69)[u e]. 2 2 2",
    "page": 391,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 372 Noticethat(cid:69)[X e]=0ifandonlyif(cid:69)[u e]=0, sothehypothesiscanberestatedas(cid:72) :Î±=0against 2 2 0 (cid:72) :Î±(cid:54)=0.ThusanaturaltestisbasedontheWaldstatisticW forÎ±=0inthecontrolfunctionregression 1 (12.28). UnderTheorem12.9, Theorem12.10, and(cid:72) ,W isasymptoticallychi-squarewithk degrees 0 2 offreedom. Inaddition,underthenormalregressionassumptiontheF statistichasanexactF(k ,nâ 2 k â2k )distribution. WeacceptthenullhypothesisthatX isexogenousifW (orF)issmallerthanthe 1 2 2 criticalvalue,andrejectinfavorofthehypothesisthatX isendogenousifthestatisticislargerthanthe 2 criticalvalue. Specifically,estimatethereducedformbyleastsquares X 2i =Î (cid:98) (cid:48) 12 Z 1i +Î (cid:98) (cid:48) 22 Z 2i +u (cid:98)2i toobtaintheresiduals.Thenestimatethecontrolfunctionbyleastsquares Y i =X i (cid:48)Î² (cid:98) +u (cid:98)2 (cid:48) i Î± (cid:98) +Î½ (cid:98)i . (12.63) LetW,W0andF =W0/k denotetheWald,homoskedasticWald,andF statisticsforÎ±=0. 2 (cid:104) (cid:105) Theorem12.14 Under(cid:72) 0 ,W â d âÏ2 k2 .Letc 1âÎ±solve(cid:80) Ï2 k2 â¤c 1âÎ± =1âÎ±.The testâReject(cid:72) 0 ifW >c 1âÎ±âhasasymptoticsizeÎ±. Theorem12.15 Supposee|X,Z â¼N (cid:161) 0,Ï2(cid:162) . Under(cid:72) ,Fâ¼F(k ,nâk â2k ). 0 2 1 2 Let c 1âÎ± solve (cid:80)[F(k 2 ,nâk 1 â2k 2 )â¤c 1âÎ±]=1âÎ±. The test âReject (cid:72) 0 if F> c 1âÎ±âhasexactsizeÎ±. Sinceingeneralwedonotwanttoimposehomoskedasticitytheseresultssuggestthatthemostap- propriatetestistheWaldstatisticconstructedwiththerobustheteroskedasticcovariancematrix. This canbecomputedinStatausingthecommandestat endogenousafterivregresswhenthelatteruses arobustcovarianceoption. StatareportstheWaldstatisticinF form(andthususestheF distribution tocalculatethep-value)asâRobustregressionFâ. UsingtheF ratherthantheÏ2 isnotformallyjusti- fiedbutisareasonablefinitesampleadjustment. Ifthecommandestat endogenousisappliedafter ivregresswithoutarobustcovarianceoptionStatareportstheF statisticasâWu-HausmanFâ. There is an alternative (and traditional) way to derive a test for endogeneity. Under (cid:72) , both OLS 0 and2SLSareconsistentestimators. Butunder(cid:72) theyconvergetodifferentvalues. Thusthedifference 1 between the OLS and 2SLS estimators is a valid test statistic for endogeneity. It also measures what weoftencaremostaboutâtheimpactofendogeneityontheparameterestimates. Thisliteraturewas developedundertheassumptionofconditionalhomoskedasticity(anditisimportantfortheseresults) soweassumethisconditionforthedevelopmentofthestatistic. LetÎ² (cid:98) =(cid:161)Î² (cid:98)1 ,Î² (cid:98)2 (cid:162) betheOLSestimatorandletÎ² (cid:101) =(cid:161)Î² (cid:101)1 ,Î² (cid:101)2 (cid:162) bethe2SLSestimator",
    "page": 392,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". It also measures what weoftencaremostaboutâtheimpactofendogeneityontheparameterestimates. Thisliteraturewas developedundertheassumptionofconditionalhomoskedasticity(anditisimportantfortheseresults) soweassumethisconditionforthedevelopmentofthestatistic. LetÎ² (cid:98) =(cid:161)Î² (cid:98)1 ,Î² (cid:98)2 (cid:162) betheOLSestimatorandletÎ² (cid:101) =(cid:161)Î² (cid:101)1 ,Î² (cid:101)2 (cid:162) bethe2SLSestimator. Under(cid:72) 0 andho- moskedasticitytheOLSestimatorisGauss-MarkovefficientsobytheHausmanequality var (cid:163)Î² (cid:98)2 âÎ² (cid:101)2 (cid:164)=var (cid:163)Î² (cid:101)2 (cid:164)âvar (cid:163)Î² (cid:98)2 (cid:164) = (cid:179) (cid:161) X (cid:48) (P âP )X (cid:162)â1â(cid:161) X (cid:48) M X (cid:162)â1 (cid:180) Ï2 2 Z 1 2 2 1 2",
    "page": 392,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 373 whereP =Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) ,P =X (cid:161) X (cid:48) X (cid:162)â1 X (cid:48) ,andM =I âP .Thusavalidteststatisticfor(cid:72) is Z 1 1 1 1 1 1 n 1 0 (cid:161)Î² (cid:98)2 âÎ² (cid:101)2 (cid:162)(cid:48)(cid:179) (cid:161) X (cid:48) 2 (P Z âP 1 )X 2 (cid:162)â1â(cid:161) X (cid:48) 2 M 1 X 2 (cid:162)â1 (cid:180)â1(cid:161)Î² (cid:98)2 âÎ² (cid:101)2 (cid:162) T = (12.64) Ï2 (cid:98) forsomeestimatorÏ2 ofÏ2. Durbin(1954)firstproposedT asatestforendogeneityinthecontextof (cid:98) IVestimationsettingÏ2 tobetheleastsquaresestimatorofÏ2. Wu(1973)proposedT asatestforen- (cid:98) dogeneity in the context of 2SLS estimation, considering a set of possible estimators Ï2 including the (cid:98) regressionestimatorfrom(12.63). Hausman(1978)proposedaversionofT basedonthefullcontrast Î² (cid:98) âÎ² (cid:101),andobservedthatitequalstheregressionWaldstatisticW0 describedearlier. Infact,whenÏ (cid:98) 2 is theregressionestimatorfrom(12.63)thestatistic(12.64)algebraicallyequalsbothW0andtheversionof (12.64)basedonthefullcontrastÎ² (cid:98) âÎ² (cid:101). Weshowtheseequalitiesbelow. Thusthesethreeapproaches yieldexactlythesamestatisticexceptforpossibledifferencesregardingthechoiceofÏ2. Sincethere- (cid:98) gression F test described earlier has an exact F distribution in the normal sampling model and thus canexactlycontroltestsize,thisisthepreferredversionofthetest. Thegeneralclassoftestsarecalled Durbin-Wu-Hausmantests,Wu-Hausmantests,orHausmantests,dependingontheauthor. Whenk =1(thereisoneright-hand-sideendogenousvariable),whichisquitecommoninapplica- 2 tions,theendogeneitytestcanbeequivalentlyexpressedatthet-statisticforÎ±intheestimatedcontrol (cid:98) function. Thusitissufficienttoestimatethecontrolfunctionregressionandcheckthet-statisticforÎ±. (cid:98) If|Î±|>2thenwecanrejectthehypothesisthatX isexogenousforÎ². (cid:98) 2 WeillustrateusingtheCardproximityexampleusingthetwoinstrumentspublicandprivate.Wefirst estimate the reduced form for education, obtain the residual, and then estimate the control function regression. The residual has a coefficient â0.088 with a standard error of 0.037 and a t-statistic of 2.4. Sincethelatterexceedsthe5%criticalvalue(itsp-valueis0.017)werejectexogeneity. Thismeansthat the2SLSestimatesarestatisticallydifferentfromtheleastsquaresestimatesofthestructuralequation andsupportsourdecisiontotreateducationasanendogenousvariable. (Alternatively,theF statisticis 2.42=5.7withthesamep-value). Wenowshowtheequalityofthevariousstatistics. Wefirstshowthatthestatistic(12.64)isnotalteredifbasedonthefullcontrastÎ² (cid:98) âÎ² (cid:101).Indeed,Î² (cid:98)1 âÎ² (cid:101)1 isalinearfunctionofÎ² (cid:98)2 âÎ² (cid:101)2 , sothereisnoextrainformationinthefullcontrast",
    "page": 393,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Thismeansthat the2SLSestimatesarestatisticallydifferentfromtheleastsquaresestimatesofthestructuralequation andsupportsourdecisiontotreateducationasanendogenousvariable. (Alternatively,theF statisticis 2.42=5.7withthesamep-value). Wenowshowtheequalityofthevariousstatistics. Wefirstshowthatthestatistic(12.64)isnotalteredifbasedonthefullcontrastÎ² (cid:98) âÎ² (cid:101).Indeed,Î² (cid:98)1 âÎ² (cid:101)1 isalinearfunctionofÎ² (cid:98)2 âÎ² (cid:101)2 , sothereisnoextrainformationinthefullcontrast. Toseethis, observe thatgivenÎ² (cid:98)2 wecansolvebyleastsquarestofind Î² (cid:98)1 =(cid:161) X (cid:48) 1 X 1 (cid:162)â1(cid:161) X (cid:48) 1 (cid:161) Y âX 2 Î² (cid:98)2 (cid:162)(cid:162) andsimilarly Î² (cid:101)1 =(cid:161) X (cid:48) 1 X 1 (cid:162)â1(cid:161) X (cid:48) 1 (cid:161) Y âP Z X 2 Î² (cid:101) (cid:162)(cid:162)=(cid:161) X (cid:48) 1 X 1 (cid:162)â1(cid:161) X (cid:48) 1 (cid:161) Y âX 2 Î² (cid:101) (cid:162)(cid:162) thesecondequalitysinceP X =X .Thus Z 1 1 Î² (cid:98)1 âÎ² (cid:101)1 =(cid:161) X (cid:48) 1 X 1 (cid:162)â1 X (cid:48) 1 (cid:161) Y âX 2 Î² (cid:98)2 (cid:162)â(cid:161) X (cid:48) 1 X 1 (cid:162)â1 X (cid:48) 1 (cid:161) Y âP Z X 2 Î² (cid:101) (cid:162) =(cid:161) X (cid:48) 1 X 1 (cid:162)â1 X (cid:48) 1 X 2 (cid:161)Î² (cid:101)2 âÎ² (cid:98)2 (cid:162) asclaimed. We next show that T in (12.64) equals the homoskedastic Wald statisticW0 for Î± from the regres- (cid:98) sion (12.63). Consider the latter regression. Since X is contained in X the coefficient estimate Î± is 2 (cid:98) invariant to replacing U(cid:98)2 = X 2 âX(cid:98)2 with âX(cid:98)2 = âP Z X 2 . By the FWL representation, setting M X = I âX (cid:161) X (cid:48) X (cid:162)â1 X (cid:48) , n Î± (cid:98) =â (cid:179) X(cid:98) (cid:48) 2 M X X(cid:98)2 (cid:180)â1 X(cid:98) (cid:48) 2 M X Y =â(cid:161) X (cid:48) 2 P Z M X P Z X 2 (cid:162)â1 X (cid:48) 2 P Z M X Y.",
    "page": 393,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 374 Itfollowsthat (cid:48) (cid:161) (cid:48) (cid:162)â1 (cid:48) Y M P X X P M P X X P M Y W0= X Z 2 2 Z X Z 2 2 Z X . Ï2 (cid:98) (cid:179) (cid:48) (cid:180)â1 (cid:48) Our goal is to show that T = W0. Define X(cid:101)2 = (I n âP 1 )X 2 so Î² (cid:98)2 = X(cid:101)2 X(cid:101)2 X(cid:101)2 Y. Then using (cid:179) (cid:48) (cid:180)â1 (cid:48) (P Z âP 1 )(I n âP 1 )=(P Z âP 1 )anddefiningQ=X(cid:101)2 X(cid:101)2 X(cid:101)2 X(cid:101)2 wefind âd=ef(cid:161) X (cid:48) 2 (P Z âP 1 )X 2 (cid:162)(cid:161)Î² (cid:101)2 âÎ² (cid:98)2 (cid:162) =X (cid:48) 2 (P Z âP 1 )Y â(cid:161) X (cid:48) 2 (P Z âP 1 )X 2 (cid:162) (cid:179) X(cid:101) (cid:48) 2 X(cid:101)2 (cid:180)â1 X(cid:101) (cid:48) 2 Y =X (cid:48) (P âP )(I âQ)Y 2 Z 1 n =X (cid:48) (P âP âP Q)Y 2 Z 1 Z =X (cid:48) P (I âP âQ)Y 2 Z n 1 =X (cid:48) P M Y. 2 Z X Thethird-to-lastequalityisP Q=0andthefinalusesM =I âP âQ.Wealsocalculatethat 1 X n 1 Q âd=ef(cid:161) X (cid:48) (P âP )X (cid:162) (cid:179) (cid:161) X (cid:48) (P âP )X (cid:162)â1â(cid:161) X (cid:48) M X (cid:162)â1 (cid:180) (cid:161) X (cid:48) (P âP )X (cid:162) 2 Z 1 2 2 Z 1 2 2 1 2 2 Z 1 2 =X (cid:48) (P âP â(P âP )Q(P âP ))X 2 Z 1 Z 1 Z 1 2 =X (cid:48) (cid:161) P âP âP QP (cid:162) X 2 Z 1 Z Z 2 =X (cid:48) P M P X . 2 Z X Z 2 Thus â(cid:48) Q ââ1â T = Ï2 (cid:98) (cid:48) (cid:161) (cid:48) (cid:162)â1 (cid:48) Y M P X X P M P X X P M Y = X Z 2 2 Z X Z 2 2 Z X Ï2 (cid:98) =W0 asclaimed. 12.30 SubsetEndogeneityTests Insomecaseswemayonlywishtotesttheendogeneityofasubsetofthevariables.IntheCardprox- imityexamplewemaywishtesttheexogeneityofeducationseparatelyfromexperienceanditssquare. Toexecuteasubsetendogeneitytestitisusefultopartitiontheregressorsintothreegroupssothatthe structuralmodelis Y =X (cid:48)Î² +X (cid:48)Î² +X (cid:48)Î² +e 1 1 2 2 3 3 (cid:69)[Ze]=0. Asbefore,theinstrumentvectorZ includesX .ThevectorX istreatedasendogenousandX istreated 1 3 2 as potentially endogenous. The hypothesis to test is that X is exogenous, or (cid:72) :(cid:69)[X e]=0 against 2 0 2 (cid:72) :(cid:69)[X e](cid:54)=0. 1 2 UnderhomoskedasticityastraightfowardtestcanbeconstructedbytheDurbin-Wu-Hausmanprin- ciple.Under(cid:72) theappropriateestimatoris2SLSusingtheinstruments(Z,X ).LetthisestimatorofÎ² 0 2 2",
    "page": 394,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 375 bedenotedÎ² (cid:98)2 . Under(cid:72) 1 theappropriateestimatoris2SLSusingthesmallerinstrumentsetZ. Letthis estimatorofÎ² 2 bedenotedÎ² (cid:101)2 .ADurbin-Wu-Hausmanstatisticfor(cid:72) 0 against(cid:72) 1 is T =(cid:161)Î² (cid:98)2 âÎ² (cid:101)2 (cid:162)(cid:48)(cid:161) v (cid:99) ar (cid:163)Î² (cid:101)2 (cid:164)âv (cid:99) ar (cid:163)Î² (cid:98)2 (cid:164)(cid:162)â1(cid:161)Î² (cid:98)2 âÎ² (cid:101)2 (cid:162) . The asymptotic distribution under (cid:72) is Ï2 where k =dim(X ), so we reject the hypothesis that the 0 k2 2 2 variablesX areexogenousifT exceedsanuppercriticalvaluefromtheÏ2 distribution. 2 k2 InsteadofusingtheWaldstatisticonecouldusetheF versionofthetestbydividingbyk andusing 2 theF distributionforcriticalvalues.Thereisnofinitesamplejustificationforthismodification,however, sinceX isendogenousunderthenullhypothesis. 3 InStata, thecommandestat endogenous(addingthevariablenametospecifywhichvariableto test for exogeneity) after ivregress without a robust covariance option reports the F version of this statistic as âWu-Hausman Fâ. For example, in the Card proximity example using the four instruments public,private,age,andage2,ifweestimatetheequationby2SLSwithanon-robustcovariancematrix andthencomputetheendogeneitytestforeducationwefindF =272withap-valueof0.0000,butifwe computethetestforexperienceanditssquarewefindF =2.98withap-valueof0.051. Inthismodel, theassumptionofexogeneitywithhomogenouscoefficientsisrejectedforeducationbuttheresultfor experienceisunclear. Aheteroskedasticityorcluster-robusttestcannotbeconstructedeasilybytheDurbin-Wu-Hausman approach since the covariance matrix does not take a simple form. To allow for non-homoskedastic errorsitisrecommendedtouseGMMestimation.SeeSection13.24. 12.31 OverIdentificationTests When(cid:96)>kthemodelisoveridentifiedmeaningthattherearemoremomentsthanfreeparameters. Thisisarestrictionandistestable.Suchtestsarecalledoveridentificationtests. Theinstrumentalvariablesmodelspecifies(cid:69)[Ze]=0.Equivalently,sincee=Y âX (cid:48)Î²thisis (cid:69)[ZY]â(cid:69)(cid:163) ZX (cid:48)(cid:164)Î²=0. Thisisan(cid:96)Ã1vectorofrestrictionsonthemomentmatrices(cid:69)[ZY]and(cid:69)(cid:163) ZX (cid:48)(cid:164) .YetsinceÎ²isofdimen- sionk whichislessthan(cid:96)itisnotcertainifindeedsuchaÎ²exists. Tomakethingsabitmoreconcrete,supposethereisasingleendogenousregressor X ,no X ,and 2 1 twoinstrumentsZ andZ .Thenthemodelspecifiesthat 1 2 (cid:69)([Z Y]=(cid:69)[Z X ]Î² 1 1 2 and (cid:69)[Z Y]=(cid:69)[Z X ]Î². 2 2 2 ThusÎ²solvesbothequations.Thisisratherspecial. AnotherwayofthinkingaboutthisiswecouldsolveforÎ²usingeitheroneequationortheother. In termsofestimationthisisequivalenttoestimatingbyIVusingjusttheinstrumentZ orinsteadjustus- 1 ingtheinstrumentZ .Thesetwoestimators(infinitesamples)aredifferent.Butiftheoveridentification 2 hypothesisiscorrectbothareestimatingthesameparameterandbothareconsistentforÎ²",
    "page": 395,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". 2 2 2 ThusÎ²solvesbothequations.Thisisratherspecial. AnotherwayofthinkingaboutthisiswecouldsolveforÎ²usingeitheroneequationortheother. In termsofestimationthisisequivalenttoestimatingbyIVusingjusttheinstrumentZ orinsteadjustus- 1 ingtheinstrumentZ .Thesetwoestimators(infinitesamples)aredifferent.Butiftheoveridentification 2 hypothesisiscorrectbothareestimatingthesameparameterandbothareconsistentforÎ². Incontrast, iftheoveridentificationhypothesisisfalsethenthetwoestimatorswillconvergetodifferentprobability limitsanditisunclearifeitherprobabilitylimitisinteresting. Forexample,takethe2SLSestimatesinthefourthcolumnofTable12.1whichusepublicandprivate asinstrumentsforeducation. SupposeweinsteadestimatebyIVusingjustpublicasaninstrumentand",
    "page": 395,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 376 thenrepeatusingprivate. TheIVcoefficientforeducationinthefirstcaseis0.16andinthesecondcase 0.27. Theseappeartobequitedifferent. However,thesecondestimatehasalargestandarderror(0.16) sothedifferencemaybesamplingvariation.Anoveridentificationtestaddressesthisquestion. For a general overidentification test the null and alternative hypotheses are (cid:72) :(cid:69)[Ze]=0 against 0 (cid:72) :(cid:69)[Ze](cid:54)=0.Wewillalsoaddtheconditionalhomoskedasticityassumption 1 (cid:69)(cid:163) e2|Z (cid:164)=Ï2. (12.65) Toavoid(12.65)itisbesttotakeaGMMapproachwhichwedeferuntilChapter13. Toimplementatestof(cid:72) consideralinearregressionoftheerroreontheinstrumentsZ 0 e=Z (cid:48)Î±+Î½ (12.66) withÎ±=(cid:161)(cid:69)(cid:163) ZZ (cid:48)(cid:164)(cid:162)â1(cid:69)[Ze].Wecanrewrite(cid:72) asÎ±=0.Whileeisnotobservedwecanreplaceitwiththe 0 2SLSresiduale andestimateÎ±byleastsquaresregression,e.g.Î±=(cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) e.Sargan(1958)proposed (cid:98)i (cid:98) (cid:98) testing(cid:72) viaascoretest,whichequals 0 (cid:48) (cid:161) (cid:48) (cid:162)â1 (cid:48) e Z Z Z Z e S=Î±(cid:48) (var[Î±]) âÎ±= (cid:98) (cid:98) . (12.67) (cid:98) (cid:99) (cid:98) (cid:98) Ï2 (cid:98) where Ï2 = 1e (cid:48) e. Basmann (1960) independently proposed a Wald statistic for (cid:72) , which is S with Ï2 (cid:98) n(cid:98)(cid:98) 0 (cid:98) replacedwithÏ2=n â1Î½(cid:48)Î½whereÎ½=eâZÎ±. BytheequivalenceofhomoskedasticscoreandWaldtests (cid:101) (cid:98) (cid:98) (cid:98) (cid:98) (cid:98) (seeSection9.16)BasmannâsstatisticisamonotonicfunctionofSarganâsstatisticandhencetheyyield equivalenttests.Sarganâsversionismoretypicallyreported. TheSargantestrejects(cid:72) infavorof(cid:72) ifS>c forsomecriticalvaluec. Anasymptotictestsetsc as 0 1 the1âÎ±quantileoftheÏ2 distribution.ThisisjustifiedbytheasymptoticnulldistributionofSwhich (cid:96)âk wenowderive. Theorem12.16 Under Assumption 12.2 and (cid:69)(cid:163) e2|Z (cid:164) = Ï2, then as n â â, SââÏ2 (cid:96)âk .ForcsatisfyingÎ±=1âG(cid:96)âk (c),(cid:80)[S>c|(cid:72) 0 ]âÎ±sothetestâReject d (cid:72) ifS>câhasasymptoticsizeÎ±. 0 WeproveTheorem12.16below. TheSarganstatisticS isanasymptotictestoftheoveridentifyingrestrictionsundertheassumption ofconditionalhomoskedasticity.Ithassomelimitations.First,itisanasymptotictestanddoesnothave afinitesample(e.g. F)counterpart. Simulationevidencesuggeststhatthetestcanbeoversized(reject toofrequently)insmallandmoderatesamplesizes. Consequently,p-valuesshouldbeinterpretedcau- tiously. Second,theassumptionofconditionalhomoskedasticityisunrealisticinapplications",
    "page": 396,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". 0 WeproveTheorem12.16below. TheSarganstatisticS isanasymptotictestoftheoveridentifyingrestrictionsundertheassumption ofconditionalhomoskedasticity.Ithassomelimitations.First,itisanasymptotictestanddoesnothave afinitesample(e.g. F)counterpart. Simulationevidencesuggeststhatthetestcanbeoversized(reject toofrequently)insmallandmoderatesamplesizes. Consequently,p-valuesshouldbeinterpretedcau- tiously. Second,theassumptionofconditionalhomoskedasticityisunrealisticinapplications. Thebest way to generalize the Sargan statistic to allow heteroskedasticity is to use the GMM overidentification statisticâwhichwewillexamineinChapter13.For2SLS,Wooldrige(1995)suggestedarobustscoretest, butBaum,SchafferandStillman(2003)pointoutthatitisnumericallyequivalenttotheGMMoveriden- tificationstatistic.Hencethebottomlineappearstobethattoallowheteroskedasticityorclusteringitis besttouseaGMMapproach. Inoveridentifiedapplicationsitisalwaysprudenttoreportanoveridentificationtest. Ifthetestis insignificant it means that the overidentifying restrictions are not rejected, supporting the estimated model.Iftheoveridentifyingteststatisticishighlysignificant(ifthep-valueisverysmall)thisisevidence",
    "page": 396,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 377 thattheoveridentifyingrestrictionsareviolated. Inthiscaseweshouldbeconcernedthatthemodelis misspecifiedandinterpretingtheparameterestimatesshouldbedonecautiously. When reporting the results of an overidentification test it seems reasonable to focus on very small significancelevelssuchas1%. ThismeansthatweshouldonlytreatamodelasârejectedâiftheSargan p-valueisverysmall,e.g. lessthan0.01. Thereasontofocusonverysmallsignificancelevelsisbecause it is very difficult to interpret the result âThe model is rejectedâ. Stepping back a bit it does not seem crediblethatanyoveridentifiedmodelisliterallytrue;ratherwhatseemspotentiallycredibleisthatan overidentified model is a reasonable approximation. A test is asking the question âIs there evidence thatamodelisnottrueâwhenwereallywanttoknowtheanswertoâIsthereevidencethatthemodel is a poor approximationâ. Consequently it seems reasonable to require strong evidence to lead to the conclusionâLetâsrejectthismodelâ. Therecommendationisthatmildrejections(p-valuesbetween1% and5%)shouldbeviewedasmildlyworrisomebutnotcriticalevidenceagainstamodel. Theresultsof anoveridentificationtestshouldbeintegratedwithotherinformationbeforemakingastrongdecision. We illustrate the methods with the Card college proximity example. We have estimated two overi- dentified models by 2SLS in columns 4 & 5 of Table 12.1. In each case the number of overidentifying restrictionsis1. WereporttheSarganstatisticanditsasymptoticp-value(calculatedusingtheÏ2 dis- 1 tribution)inthetable. Bothp-values(0.37and0.47)arefarfromsignificantindicatingthatthereisno evidencethatthemodelsaremisspecified. WenowproveTheorem12.16. ThestatisticS isinvarianttorotationsofZ (replacingZ withZC)so withoutlossofgeneralityweassume(cid:69)(cid:163) ZZ (cid:48)(cid:164)=I(cid:96). Asnââ,n â1/2Z (cid:48) eââÏZ whereZ â¼N(0,I(cid:96)). Also d 1Z (cid:48) Z ââI(cid:96)and 1Z (cid:48) X ââQ,say.Then n p n p n â1/2Z (cid:48) (cid:98) e= (cid:181) I(cid:96) â (cid:181) 1 Z (cid:48) X (cid:182)(cid:181) 1 X (cid:48) P Z X (cid:182)â1(cid:181) 1 X (cid:48) Z (cid:182)(cid:181) 1 Z (cid:48) Z (cid:182)â1(cid:182) n â1/2Z (cid:48) e n n n n ââÏ (cid:179) I(cid:96) âQ (cid:161) Q (cid:48) Q (cid:162)â1 Q (cid:48) (cid:180) Z. d SinceÏ2ââÏ2itfollowsthat (cid:98) p SââZ (cid:48) (cid:179) I(cid:96) âQ (cid:161) Q (cid:48) Q (cid:162)â1 Q (cid:48) (cid:180) Zâ¼Ï2 (cid:96)âk . d ThedistributionisÏ2 (cid:96)âk sinceI(cid:96) âQ (cid:161) Q (cid:48) Q (cid:162)â1 Q (cid:48) isidempotentwithrank(cid:96)âk. TheSarganstatistictestcanbeimplementedinStatausingthecommandestat overidafterivregress 2slsorivregres limlifastandard(non-robust)covariancematrixhasbeenspecified(thatis,without theâ,râoption),orbythecommandestat overid, forcenonrobustotherwise. DenisSargan TheBritisheconometricianJohnDenisSargan(1924-1996)wasapioneerinthe fieldofeconometrics.Hemadearangeoffundamentalcontributionsincluding theoveridentificationtest,Edgeworthexpansions,andunitroottheory",
    "page": 397,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". TheSarganstatistictestcanbeimplementedinStatausingthecommandestat overidafterivregress 2slsorivregres limlifastandard(non-robust)covariancematrixhasbeenspecified(thatis,without theâ,râoption),orbythecommandestat overid, forcenonrobustotherwise. DenisSargan TheBritisheconometricianJohnDenisSargan(1924-1996)wasapioneerinthe fieldofeconometrics.Hemadearangeoffundamentalcontributionsincluding theoveridentificationtest,Edgeworthexpansions,andunitroottheory. Hewas also influential in his role as dissertation advisor for many LSE-trained econo- metricians.",
    "page": 397,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 378 12.32 SubsetOverIdentificationTests Testsof(cid:72) :(cid:69)[Ze]=0aretypicallyinterpretedastestsofmodelspecification. Thealternative(cid:72) : 0 1 (cid:69)[Ze](cid:54)=0meansthatatleastoneelementofZ iscorrelatedwiththeerroreandisthusaninvalidinstru- mentalvariable.Insomecasesitmaybereasonabletotestonlyasubsetofthemomentconditions. Asintheprevioussectionwerestrictattentiontothehomoskedasticcase(cid:69)(cid:163) e2|Z (cid:164)=Ï2. Partition Z =(Z ,Z )withdimensions(cid:96) and(cid:96) , respectively, where Z containstheinstruments a b a b a whicharebelievedtobeuncorrelatedwithe andZ containstheinstrumentswhichmaybecorrelated b with e. It is necessary to select this partition so that (cid:96) > k, or equivalently (cid:96) < (cid:96)âk. This means a b thatthemodelwithjusttheinstruments Z isover-identified,orthat(cid:96) issmallerthanthenumberof a b overidentifyingrestrictions.(If(cid:96) =k thenthetestsdescribedhereexistbutreducetotheSargantestso a arenotinteresting.)Hencethetestsrequirethat(cid:96)âk>1,thatthenumberofoveridentifyingrestrictions exceedsone. Given this partition the maintained hypothesis is (cid:69)[Z e]=0. The null and alternative hypotheses a are (cid:72) :(cid:69)[Z e]=0 against (cid:72) :(cid:69)[Z e](cid:54)=0. That is, the null hypothesis is that the full set of moment 0 b 1 b conditionsarevalidwhilethealternativehypothesisisthattheinstrumentsubsetZ iscorrelatedwithe b andthusaninvalidinstrument. Rejectionof(cid:72) infavorof(cid:72) istheninterpretedasevidencethatZ is 0 1 b misspecifiedasaninstrument. Basedonthesamereasoningasdescribedintheprevioussection,totest(cid:72) against(cid:72) weconsider 0 1 apartitionedversionoftheregression(12.66) e=Z (cid:48)Î± +Z (cid:48)Î± +Î½ a a b b but now focus on the coefficient Î± . Given (cid:69)[Z e] = 0, (cid:72) is equivalent to Î± = 0. The equation is b a 0 b estimatedbyleastsquaresreplacingtheunobservede withthe2SLSresiduale .TheestimateofÎ± is i (cid:98)i b Î± =(cid:161) Z (cid:48) M Z (cid:162)â1 Z (cid:48) M e (cid:98)b b a b b a(cid:98) whereM =I âZ (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) .Newey(1985)showedthatanoptimal(asymptoticallymostpowerful) a n a a a a testof(cid:72) against(cid:72) istorejectforlargevaluesofthescorestatistic 0 1 (cid:98) e (cid:48) R (cid:181) R (cid:48) RâR (cid:48) X(cid:98) (cid:179) X(cid:98) (cid:48) X(cid:98) (cid:180)â1 X(cid:98) (cid:48) R (cid:182)â1 R (cid:48) (cid:98) e N =Î±(cid:48) (var[Î± ]) âÎ± = (cid:98)b (cid:99) (cid:98)b (cid:98)b Ï2 (cid:98) whereX(cid:98) =PX,P=Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) ,R=M a Z b ,andÏ (cid:98) 2= n 1 (cid:98) e (cid:48) (cid:98) e. Independently from Newey (1985), Eichenbaum, L. Hansen, and Singleton (1988) proposed a test based on the difference of Sargan statistics. Let S be the Sargan test statistic (12.67) based on the full instrument set and S be the Sargan statistic based on the instrument set Z . The Sargan difference a a statistic is C = SâS a",
    "page": 398,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Independently from Newey (1985), Eichenbaum, L. Hansen, and Singleton (1988) proposed a test based on the difference of Sargan statistics. Let S be the Sargan test statistic (12.67) based on the full instrument set and S be the Sargan statistic based on the instrument set Z . The Sargan difference a a statistic is C = SâS a . Specifically, let Î² (cid:101)2sls be the 2SLS estimator using the instruments Z a only, set e (cid:101)i =Y i âX i (cid:48)Î² (cid:101)2sls ,andsetÏ (cid:101) 2= n 1 (cid:101) e (cid:48) (cid:101) e.Then (cid:48) (cid:161) (cid:48) (cid:162)â1 (cid:48) e Z Z Z Z e S = (cid:101) a a a a(cid:101) . a Ï2 (cid:101) AnadvantageoftheC statisticisthatitisquitesimpletocalculatefromthestandardregressionoutput. Atthispointitisusefultoreflectonourstatedrequirementthat(cid:96) >k.Indeed,if(cid:96) <kthenZ fails a a a theorderconditionforidentificationandÎ² (cid:101)2sls cannotbecalculated.Thus(cid:96) a â¥kisnecessarytocompute S a andhenceS. Furthermore,if(cid:96) a =k thenmodela isjustidentifiedsowhileÎ² (cid:101)2sls canbecalculated,",
    "page": 398,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 379 thestatisticS =0soC =S. Thuswhen(cid:96) =k thesubsettestequalsthefulloveridentificationtestso a a thereisnogainfromconsideringsubsettests. TheC statisticS isasymptoticallyequivalenttoreplacingÏ2inS withÏ2,yieldingthestatistic a (cid:101) a (cid:98) (cid:48) (cid:161) (cid:48) (cid:162)â1 (cid:48) (cid:48) (cid:161) (cid:48) (cid:162)â1 (cid:48) e Z Z Z Z e e Z Z Z Z e C â= (cid:98) (cid:98)â(cid:101) a a a a(cid:101) . Ï2 Ï2 (cid:98) (cid:98) ItturnsoutthatthisisNeweyâsstatisticN.Thesetestshavechi-squareasymptoticdistributions. Letc satisfyÎ±=1âG(cid:96) (c). b Theorem12.17 Algebraically,N =C â .UnderAssumption12.2and(cid:69)(cid:163) e2|Z (cid:164)= Ï2,asnââ,N ââÏ2 andC ââÏ2 . ThusthetestsâReject(cid:72) ifN >câand (cid:96) (cid:96) 0 d b d b âReject(cid:72) ifC >câareasymptoticallyequivalentandhaveasymptoticsizeÎ±. 0 â Theorem12.17showsthatN andC areidenticalandarenearequivalentstotheconvenientstatistic C. TheappropriateasymptoticdistributionisÏ2 . Computationally,theeasiestmethodtoimplement (cid:96) b a subset overidentification test is to estimate the model twice by 2SLS, first using the full instrument set Z andthesecondusingthepartialinstrumentset Z . ComputetheSarganstatisticsforboth2SLS a regressionsandcomputeC asthedifferenceintheSarganstatistics. InStata,forexample,thisissimple toimplementwithafewlinesofcode. WeillustrateusingtheCardcollegeproximityexample. Ourreported2SLSestimateshave(cid:96)âk =1 sothereisnoroleforasubsetoveridentificationtest. (Recall,thenumberofoveridentifyingrestrictions must exceed one.) To illustrate we add extra instruments to the estimates in column 5 of Table 12.1 (the 2SLS estimates using public, private, age, and age2 as instruments for education, experience, and experience2/100). Weaddtwoinstruments: theyearsofeducationofthefather andthemother ofthe worker.ThesevariableshadbeenusedintheearlierlaboreconomicsliteratureasinstrumentsbutCard didnot. (Heusedthemasregressioncontrolsinsomespecifications.) Themotivationforusingparentâs education as instruments is the hypothesis that parental education influences childrenâs educational attainment but does not directly influence their ability. The more modern labor economics literature hasdisputedthisidea,arguingthatchildrenareeducatedinpartathomeandthusparentâseducation hasadirectimpactontheskillattainmentofchildren(andnotjustanindirectimpactviaeducational attainment).Theolderviewwasthatparentâseducationisavalidinstrument,themodernviewisthatit isnotvalid.Wecantestthisdisputeusingaoveridentificationsubsettest. We do this by estimating the wage equation by 2SLS using public, private, age, age2, father, and mother, as instruments for education, experience, and experience2/100). We do not report the param- eterestimatesherebutobservethatthismodelisoveridentifiedwith3overidentifyingrestrictions. We calculate the Sargan overidentification statistic. It is 7.9 with an asymptotic p-value (calculated using Ï2)of0.048",
    "page": 399,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". We do this by estimating the wage equation by 2SLS using public, private, age, age2, father, and mother, as instruments for education, experience, and experience2/100). We do not report the param- eterestimatesherebutobservethatthismodelisoveridentifiedwith3overidentifyingrestrictions. We calculate the Sargan overidentification statistic. It is 7.9 with an asymptotic p-value (calculated using Ï2)of0.048. Thisisamildrejectionofthenullhypothesisofcorrectspecification. Aswearguedinthe 3 previous section this by itself is not reason to reject the model. Now we consider a subset overidenti- ficationtest. Weareinterestedintestingthevalidityofthetwoinstrumentsfather andmother,notthe instrumentspublic,private,age,age2.Totestthehypothesisthatthesetwoinstrumentsareuncorrelated withthestructuralerrorwecomputethedifferenceinSarganstatistic,C =7.9â0.5=7.4, whichhasa p-value(calculatedusingÏ2)of0.025. Thisismarginallystatisticallysignificant, meaningthatthereis 2 evidencethatfatherandmotherarenotvalidinstrumentsforthewageequation.Sincethep-valueisnot smallerthan1%itisnotoverwhelmingevidencebutitstillsupportsCardâsdecisiontonotuseparental educationasinstrumentsforthewageequation.",
    "page": 399,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 380 WenowprovetheresultsinTheorem12.17. WefirstshowthatN =C â . DefineP =Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) andP =R (cid:161) R (cid:48) R (cid:162)â1 R (cid:48) . Since[Z ,R]span a a a a a R a Z wefindP=P +P andP P =0.Itwillbeusefultonotethat R a R a P R X(cid:98) =P R PX =P R X X(cid:98) (cid:48) X(cid:98) âX(cid:98) (cid:48) P R X(cid:98) =X (cid:48) (PâP R )X =X (cid:48) P a X. ThefactthatX (cid:48) P (cid:98) e=X(cid:98) (cid:48) (cid:98) e=0impliesX (cid:48) P R(cid:98) e=âX (cid:48) P a(cid:98) e.Finally,sinceY =XÎ² (cid:98) + (cid:98) e, e= (cid:179) I âX (cid:161) X (cid:48) P X (cid:162)â1 X (cid:48) P (cid:180) e (cid:101) n a a (cid:98) so e (cid:48) P e=e (cid:48) (cid:179) P âP X (cid:161) X (cid:48) P X (cid:162)â1 X (cid:48) P (cid:180) e. (cid:101) a(cid:101) (cid:98) a a a a (cid:98) ApplyingtheWoodburymatrixequalitytothedefinitionofN andtheabovealgebraicrelationships, (cid:98) e (cid:48) P R(cid:98) e+ (cid:98) e (cid:48) P R X(cid:98) (cid:179) X(cid:98) (cid:48) X(cid:98) âX(cid:98) (cid:48) P R X(cid:98) (cid:180)â1 X(cid:98) (cid:48) P R(cid:98) e N = Ï2 (cid:98) e (cid:48) Peâe (cid:48) P e+e (cid:48) P X (cid:161) X (cid:48) P X (cid:162)â1 X (cid:48) P e = (cid:98) (cid:98) (cid:98) a(cid:98) (cid:98) a a a(cid:98) Ï2 (cid:98) e (cid:48) Peâe (cid:48) P e = (cid:98) (cid:98) (cid:101) a(cid:101) Ï2 (cid:98) =C â asclaimed. Wenextestablishtheasymptoticdistribution. SinceZ isasubsetofZ,PM =M P,thusPR =R a a a andR (cid:48) X =R (cid:48) X(cid:98).Consequently (cid:112) 1 R (cid:48) (cid:98) e= (cid:112) 1 R (cid:48)(cid:161) Y âXÎ² (cid:98) (cid:162) n n = (cid:112) 1 R (cid:48) (cid:181) I n âX (cid:179) X(cid:98) (cid:48) X(cid:98) (cid:180)â1 X(cid:98) (cid:48) (cid:182) e n = (cid:112) 1 R (cid:48) (cid:181) I n âX(cid:98) (cid:179) X(cid:98) (cid:48) X(cid:98) (cid:180)â1 X(cid:98) (cid:48) (cid:182) e n ââN(0,V ) 2 d where V 2 =plim (cid:181) 1 R (cid:48) Râ 1 R (cid:48) X(cid:98) (cid:181) 1 X(cid:98) (cid:48) X(cid:98) (cid:182)â1 1 X(cid:98) (cid:48) R (cid:182) . nââ n n n n ItfollowsthatN =C âââÏ2 asclaimed.SinceC =C â+o (1)ithasthesamelimitingdistribution. (cid:96) p d b 12.33 BootstrapOveridentificationTests Insmalltomoderatesamplesizestheoveridentificationtestsarenotwellapproximatedbytheasymp- toticchi-squaredistributions. Forimprovedaccuracyitisadvisedtousebootstrapcriticalvalues. The bootstrapfor2SLS(Section12.23)canbeusedforthispurposebutthebootstrapversionoftheoveriden- tificationstatisticmustbeadjusted.Thisisbecauseinthebootstrapuniversetheoveridentifiedmoment conditionsarenotsatisfied.Onesolutionistocenterthemomentconditions.",
    "page": 400,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 381 Forthe2SLSestimatorthestandardoveridentificationtestisbasedontheSarganstatistic (cid:48) (cid:161) (cid:48) (cid:162)â1 (cid:48) e Z Z Z Z e S=n (cid:98) (cid:98) (cid:48) e e (cid:98)(cid:98) (cid:98) e=Y âXÎ² (cid:98)2sls . Therecenteredbootstrapanalogis (cid:161) e â(cid:48) Z ââZ (cid:48) e (cid:162)(cid:161) Z â(cid:48) Z â(cid:162)â1(cid:161) Z â(cid:48) e ââZ (cid:48) e (cid:162) S ââ=n (cid:98) (cid:98) (cid:98) (cid:98) â(cid:48) â e e (cid:98) (cid:98) (cid:98) e â=Y ââX âÎ² (cid:98) â 2sls . ââ OneachbootstrapsampleS (b)iscalculatedandstored.Thebootstrapp-valueis p â= 1 (cid:88) B 1(cid:169) S ââ (b)>S (cid:170) . B b=1 ââ Thisbootstrapp-valueisvalidbecausethestatisticS satisfiestheoveridentifiedmomentcondi- tions. 12.34 LocalAverageTreatmentEffects InapairofinfluentialpapersImbensandAngrist(1994)andAngrist,ImbensandRubin(1996)pro- posedannewinterpretationoftheinstrumentalvariablesestimatorusingthepotentialoutcomesmodel introducedinSection2.30. WewillrestrictattentiontothecasethattheendogenousregressorX andexcludedinstrumentZ are binaryvariables.Wewritethemodelasapairofpotentialoutcomefunctions.ThedependentvariableY isafunctionoftheregressorandanunobservablevectorU,Y =h(X,U),andtheendogenousregressor X isafunctionoftheinstrument Z andU, X =g(Z,U). ByspecifyingU asavectorthereisnolossof generalityinlettingbothequationsdependonU. InthisframeworktheoutcomesaredeterminedbytherandomvectorU andtheexogenousinstru- ment Z. This determines X which determines Y. To put this in the context of the college proximity examplethevariableU iseverythingspecificaboutanindividual. GivencollegeproximityZ theperson decidestoattendcollegeornot. ThepersonâswageisdeterminedbytheindividualattributesU aswell ascollegeattendenceX butisnotdirectlyaffectedbycollegeproximityZ. WecanomittherandomvariableU fromthenotationasfollows. AnindividualhasarealizationU. WethensetY(x)=h(x,U)andX(z)=g(z,U). Also,givenarealizationZ theobservablesareX =X(Z) andY =Y(X). InthismodelthecausaleffectofcollegeforanindividualisC =Y(1)âY(0). AsdiscussedinSection 2.30,thisisindividual-specificandrandom. Wewouldliketolearnaboutthedistributionofthecausaleffects,oratleastfeaturesofthedistribu- tion.Acommonfeatureofinterestistheaveragetreatmenteffect(ATE) ATE=(cid:69)[C]=(cid:69)[Y(1)âY(0)]. This, however, it typically not feasible to estimate allowing for endogenous X without strong assump- tions(suchasthatthecausaleffectC isconstantacrossindividuals). Thetreatmenteffectliteraturehas exploredwhatfeaturesofthedistributionofC canbeestimated.",
    "page": 401,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 382 One particular feature of interest, and emphasized by Imbens and Angrist (1994), is known as the localaveragetreatmenteffect(LATE)andisroughlytheaverageeffectuponthoseeffectedbytheinstru- mentalvariable. TounderstandLATEitishelpfultoconsiderthecollegeproximityexampleusingthe potentialoutcomesframework. Inthisframework,eachpersonisfullycharacterizedbytheirindividual unobservableU.GivenU,theirdecisiontoattendcollegeisafunctionoftheproximityindicatorZ.For somestudentsproximityhasnoeffectontheirdecision. Forotherstudentsithasaneffectinthespe- cificsensethatgiven Z =1theychoosetoattendcollegewhileif Z =0theychoosetonotattend. We cansummarizethepossibiliteswiththefollowingchartwhichisbasedonlabelsdevelopedbyAngrist, ImbensandRubin(1996). X(0)=0 X(0)=1 X(1)=0 NeverTakers Deniers X(1)=1 Compliers AlwaysTakers ThecolumnsindicatethecollegeattendencedecisiongivenZ =0(notclosetoacollege). Therows indicatethecollegeattendencedecisiongivenZ =1(closetoacollege).Thefourentriesarelabelsforthe fourtypesofindividualsbasedonthesedecisions. Theupper-leftentryaretheindividualswhodonot attendcollegeregardlessofZ. TheyarecalledâNeverTakersâ. Thelower-rightentryaretheindividuals whoconverselyattendcollegeregardlessof Z. TheyarecalledâAlwaysTakersâ. Thebottomleftarethe individuals who only attend college if they live close to one. They are called âCompliersâ. The upper rightentryisabitofachallenge. Theseareindividualswhoattendcollegeonlyiftheydonotliveclose to one. They are called âDeniersâ. Imbens and Angrist discovered that to identify the parameters of interestweneedtoassumethattherearenoDeniers,orequivalentlythat X(1)â¥X(0). Theycallthisa âmonotonicityâconditionâincreasingtheinstrumentdoesnotdecreaseX foranyindividual. Asanotherexamplesupposeweareinterestedintheeffectofwearingafacemask X duringavirus pandemiconhealthY. Wearingafacemaskisachoicemadebytheindividualsoshouldbeviewedas endogenous. ForaninstrumentZ consideragovernmentpolicythatrequiresfacemaskstobewornin public. TheâCompliersâarethosewhowearafacemaskifthereisapolicybutotherwisedonot. The âDeniersâarethosewhodotheconverse. Thatis,theseindividualswouldhavewornafacemaskbased ontheevidenceofapandemicbutrebelagainstagovernmentpolicy.Onceagain,identificationrequires thattherearenoDeniers. WecandistinguishthetypesinthetablebytherelativevaluesofX(1)âX(0). ForNever-Takersand Always-TakersX(1)âX(0)=0,whileforCompliersX(1)âX(0)=1. WeareinterestedinthecausaleffectC =h(1,U)âh(0,U)ofcollegeonwages. Theaveragecausal effect(ACE)isitsexpectation(cid:69)[Y(1)âY(0)].ToestimatetheACEweneedobservationsofbothY(0)and Y(1)whichmeansweneedtoobservesomeindividualswhoattendcollegeandsomewhodonotattend college.ConsiderthegroupâNever-Takersâ.TheyneverattendcollegesoweonlyobserveY(0).Itisthus impossibletoestimatetheACEofcollegeforthisgroup. SimilarlyconsiderthegroupâAlways-Takersâ",
    "page": 402,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". WeareinterestedinthecausaleffectC =h(1,U)âh(0,U)ofcollegeonwages. Theaveragecausal effect(ACE)isitsexpectation(cid:69)[Y(1)âY(0)].ToestimatetheACEweneedobservationsofbothY(0)and Y(1)whichmeansweneedtoobservesomeindividualswhoattendcollegeandsomewhodonotattend college.ConsiderthegroupâNever-Takersâ.TheyneverattendcollegesoweonlyobserveY(0).Itisthus impossibletoestimatetheACEofcollegeforthisgroup. SimilarlyconsiderthegroupâAlways-Takersâ. TheyalwaysattendcollegesoweonlyobserveY(1)andagainwecannotestimatetheACEofcollegefor thisgroup.ThegroupforwhichwecanestimatetheACEaretheâCompliersâ.TheACEforthisgroupis LATE=(cid:69)[Y(1)âY(0)|X(1)>X(0)]. Imbens and Angrist call this the local average treatment effect (LATE) as it is the average treatment effectforthesub-populationwhoseendogenousregressorisaffectedbytheinstrument. Examiningthe definition, the LATE is the average causal effect of college attendence on wages for the sub-sample of individualswhochoosetoattendcollegeif(andonlyif)theyliveclosetoone. Interestingly,weshowbelowthat (cid:69)[Y |Z =1]â(cid:69)[Y |Z =0] LATE= . (12.68) (cid:69)[X |Z =1]â(cid:69)[X |Z =0]",
    "page": 402,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 383 That is, LATE equals the Wald expression (12.27) for the slope coefficient in the IV regression model. This means that the standard IV estimator is an estimator of LATE. Thus when treatment effects are potentiallyheterogeneouswecaninterpretIVasanestimatorofLATE.Theequality(12.68)occursunder thefollowingconditions. Assumption12.3 U andZ areindependentand(cid:80)[X(1)âX(0)<0]=0. One interesting feature about LATE is that its value can depend on the instrument Z and the dis- tributionofcausaleffectsC inthepopulation. TomakethisconcretesupposethatinsteadoftheCard proximityinstrumentweconsideraninstrumentbasedonthefinancialcostoflocalcollegeattendence. Itisreasonabletoexpectthatwhilethesetofstudentsaffectedbythesetwoinstrumentsaresimilarthe two sets of students will not be the same. That is, some students may be responsive to proximity but not finances, and conversely. If the causal effectC has a different average in these two groups of stu- dentsthenLATEwillbedifferentwhencalculatedwiththesetwoinstruments. ThusLATEcanvaryby thechoiceofinstrument. Howcanthatbe?Howcanawell-definedparameterdependonthechoiceofinstrument?Doesnâtthis contradictthebasicIVregressionmodel?TheansweristhatthebasicIVregressionmodelisrestrictiveâ itspecifiesthatthecausaleffectÎ²iscommonacrossallindividuals.Thusitsvalueisthesameregardless ofthechoiceofspecificinstrument(solongasitsatisfiestheinstrumentalvariablesassumptions). In contrast,thepotentialoutcomesframeworkismoregeneralallowingforthecausaleffecttovaryacross individuals. WhatthisanalysisshowsusisthatinthiscontextisquitepossiblefortheLATEcoefficient tovarybyinstrument.Thisoccurswhencausaleffectsareheterogeneous. OneimplicationoftheLATEframeworkisthatIVestimatesshouldbeinterpretedascausaleffects onlyforthepopulationofcompliers.Interpretationshouldfocusonthepopulationofpotentialcompli- ersandextensiontootherpopulationsshouldbedonewithcaution.Forexample,intheCardproximity modeltheIVestimatesofthecausalreturntoschoolingpresentedinTable12.1shouldbeinterpretedas applyingtothepopulationofstudentswhoareincentivizedtoattendcollegebythepresenceofacollege withintheirhomecounty.Theestimatesshouldnotbeappliedtootherstudents. Formally,theanalysisofthissectionexaminedthecaseofabinaryinstrumentandendogenousre- gressor. Howdoesthisgeneralize? Supposethattheregressor X isdiscrete,taking J+1discretevalues. WecanthenrewritethemodelasonewithJ binaryendogenousregressors. Ifwethenhave J binaryin- strumentswearebackintheImbens-Angristframework(assumingtheinstrumentshaveamonotonic impact on the endogenous regressors). A benefit is that with a larger set of instruments it is plausible thatthesetofcompliersinthepopulationisexpanded. We close this section by showing (12.68) under Assumption 12.3. The realized value of X can be writtenas X =(1âZ)X(0)+ZX(1)=X(0)+Z(X(1)âX(0)). Similarly Y =Y(0)+X(Y(1)âY(0))=Y(0)+XC. Combining, Y =Y(0)+X(0)C+Z(X(1)âY(0))C",
    "page": 403,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Ifwethenhave J binaryin- strumentswearebackintheImbens-Angristframework(assumingtheinstrumentshaveamonotonic impact on the endogenous regressors). A benefit is that with a larger set of instruments it is plausible thatthesetofcompliersinthepopulationisexpanded. We close this section by showing (12.68) under Assumption 12.3. The realized value of X can be writtenas X =(1âZ)X(0)+ZX(1)=X(0)+Z(X(1)âX(0)). Similarly Y =Y(0)+X(Y(1)âY(0))=Y(0)+XC. Combining, Y =Y(0)+X(0)C+Z(X(1)âY(0))C. TheindependenceofuandZ impliesindependenceof(Y(0),Y(1),X(0),X(1),C)andZ.Thus (cid:69)[Y |Z =1]=(cid:69)[Y(0)]+(cid:69)[X(0)C]+(cid:69)[(X(1)âX(0))C]",
    "page": 403,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 384 and (cid:69)[Y |Z =0]=(cid:69)[Y(0)]+(cid:69)[X(0)C]. Subtractingweobtain (cid:69)[Y |Z =1]â(cid:69)[Y |Z =0]=(cid:69)[(X(1)âX(0))C] =1Ã(cid:69)[C |X(1)âX(0)=1](cid:80)[X(1)âX(0)=1] +0Ã(cid:69)[C |X(1)âX(0)=0](cid:80)[X(1)âX(0)=0] +(â1)Ã(cid:69)[C |X(1)âX(0)=â1](cid:80)[X(1)âX(0)=â1] =(cid:69)[C |X(1)âX(0)=1]((cid:69)[X |X =1]â(cid:69)[X |Z =0]) wherethefinalequalityuses(cid:80)[X(1)âX(0)<0]=0and (cid:80)[X(1)âX(0)=1]=(cid:69)[X(1)âX(0)]=(cid:69)[X |Z =1]â(cid:69)[X |Z =0]. Rearranging (cid:69)[Y |Z =1]â(cid:69)[Y |Z =0] LATE=(cid:69)[C |X(1)âX(0)=1]= (cid:69)[X |Z =1]â(cid:69)[X |Z =0] asclaimed. 12.35 IdentificationFailure Recallthereducedformequation X =Î(cid:48) Z +Î(cid:48) Z +u . 2 12 1 22 2 2 TheparameterÎ²failstobeidentifiedifÎ hasdeficientrank.Theconsequencesofidentificationfailure 22 forinferencearequitesevere. Takethesimplestcasewherek =0andk =(cid:96) =1.Thenthemodelmaybewrittenas 1 2 2 Y =XÎ²+e (12.69) X =ZÎ³+u andÎ =Î³=(cid:69)[ZX]/(cid:69)(cid:163) Z2(cid:164) .WeseethatÎ²isidentifiedifandonlyifÎ³(cid:54)=0,whichoccurswhen(cid:69)[XZ](cid:54)=0. 22 Thusidentificationhingesontheexistenceofcorrelationbetweentheexcludedexogenousvariableand theincludedendogenousvariable. Suppose this condition fails. In this case Î³=0 and (cid:69)[XZ]=0. We now analyze the distribution of the least squares and IV estimators of Î². For simplicity we assume conditional homoskedasticity and normalizethevariancesofe,u,andZ tounity.Thus var (cid:183)(cid:181) e (cid:182)(cid:175) (cid:175) (cid:175)Z (cid:184) = (cid:181) 1 Ï (cid:182) . (12.70) u (cid:175) Ï 1 Theerrorshavenon-zerocorrelationÏ(cid:54)=0whenthevariablesareendogenous. BytheCLTwehavethejointconvergence (cid:112) 1 (cid:88) n (cid:181) Z i e i (cid:182) ââ (cid:181) Î¾ 1 (cid:182) â¼N (cid:181) 0, (cid:181) 1 Ï (cid:182)(cid:182) . n i=1 Z i u i d Î¾ 2 Ï 1 ItisconvenienttodefineÎ¾ =Î¾ âÏÎ¾ whichisnormalandindependentof Î¾ . 0 1 2 2",
    "page": 404,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 385 AsabenchmarkitisusefultoobservethattheleastsquaresestimatorofÎ²satisfies n â1(cid:80)n u e Î² (cid:98)ols âÎ²= n â1(cid:80) i= n 1 u i 2 i â p âÏ(cid:54)=0 i=1 i soendogeneitycausesÎ² (cid:98)ols tobeinconsistentforÎ². UnderidentificationfailureÎ³=0theasymptoticdistributionoftheIVestimatoris Î² (cid:98)iv âÎ²= (cid:112) (cid:112) 1 1 n n (cid:80) (cid:80) n i n i = = 1 1 Z Z i i X e i i â d â Î¾ Î¾ 2 1 =Ï+ Î¾ Î¾ 0 2 . Thisasymptoticconvergenceresultusesthecontinuousmappingtheoremwhichappliessincethefunc- tionÎ¾ /Î¾ iscontinuouseverywhereexceptatÎ¾ =0,whichoccurswithprobabilityequaltozero. 1 2 2 Thislimitingdistributionhasseveralnotablefeatures. First,Î² (cid:98)iv doesnotconvergeinprobabilitytoalimit,ratheritconvergesindistributiontoarandom variable. Thus the IV estimator is inconsistent. Indeed, it is not possible to consistently estimate an unidentifiedparameterandÎ²isnotidentifiedwhenÎ³=0. Second,theratioÎ¾ /Î¾ issymmetricallydistributedaboutzerosothemedianofthelimitingdistri- 0 2 butionofÎ² (cid:98)iv isÎ²+Ï.ThismeansthattheIVestimatorismedianbiasedunderendogeneity.Thusunder identificationfailuretheIVestimatordoesnotcorrectthecentering(medianbias)ofleastsquares. Third, the ratio Î¾ /Î¾ of two independent normal random variables is Cauchy distributed. This is 0 2 particularly nasty as the Cauchy distribution does not have a finite mean. The distribution has thick tailsmeaningthatextremevaluesoccurwithhigherfrequencythanthenormal.Inferencesbasedonthe normaldistributioncanbequiteincorrect. Together, these results show that Î³=0 renders the IV estimator particularly poorly behaved â it is inconsistent,medianbiased,andnon-normallydistributed. Wecanalsoexaminethebehaviorofthet-statistic.Forsimplicityconsidertheclassical(homoskedas- tic)t-statistic.Theerrorvarianceestimatehastheasymptoticdistribution Ï (cid:98) 2= 1 (cid:88) n (cid:161) Y i âX i Î² (cid:98)iv (cid:162)2 n i=1 = n 1 (cid:88) n e i 2â n 2 (cid:88) n e i X i (cid:161)Î² (cid:98)iv âÎ²(cid:162)+ n 1 (cid:88) n X i 2(cid:161)Î² (cid:98)iv âÎ²(cid:162)2 i=1 i=1 i=1 Î¾ (cid:181)Î¾ (cid:182)2 ââ1â2Ï 1 + 1 . Î¾ Î¾ d 2 2 Thusthet-statistichastheasymptoticdistribution T = Î² (cid:98)iv âÎ² ââ Î¾ 1 /Î¾ 2 . (cid:113) Ï (cid:98) 2(cid:80)n i=1 Z i 2/ (cid:175) (cid:175) (cid:80)n i=1 Z i X i (cid:175) (cid:175) d (cid:114) 1â2ÏÎ¾ Î¾ 1 + (cid:179) Î¾ Î¾ 1 (cid:180)2 2 2 The limiting distribution is non-normal, meaning that inference using the normal distribution will be (considerably) incorrect. This distribution depends on the correlation Ï. The distortion is increasing inÏ. IndeedasÏâ1wehaveÎ¾ /Î¾ â 1andtheunexpectedfindingÏ2â 0. Thelattermeansthat 1 2 p (cid:98) p theconventionalstandarderrors(Î² (cid:98)iv )forÎ² (cid:98)iv alsoconvergesinprobabilitytozero. Thisimpliesthatthe t-statisticdivergesinthesense|T|â â. Inthissituationsusersmayincorrectlyinterpretestimatesas p precisedespitethefactthattheyarehighlyimprecise.",
    "page": 405,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 386 12.36 WeakInstruments Intheprevioussectionweexaminedtheextremeconsequencesoffullidentificationfailure. Similar problemsoccurwhenidentificationisweakinthesensethatthereducedformcoefficientsareofsmall magnitude. InthissectionwederivetheasymptoticdistributionoftheOLS,2SLS,andLIMLestimators whenthereducedformcoefficientsaretreatedasweak. Weshowthattheestimatorsareinconsistent andthe2SLSandLIMLestimatorsremainrandominlargesamples. Tosimplifytheexpositionweassumethattherearenoincludedexogenousvariables(no X )sowe 1 writeX ,Z ,andÎ² simplyasX,Z,andÎ².Themodelis 2 2 2 Y =X (cid:48)Î²+e X =Î(cid:48) Z+u . 2 Recallthereducedformerrorvectoru=(u ,u )anditscovariancematrix 1 2 (cid:183) Î£ Î£ (cid:184) (cid:69)(cid:163) uu (cid:48)(cid:164)=Î£= 11 12 . Î£ Î£ 21 22 Recall that the structural error is e =u âÎ²(cid:48) u =Î³(cid:48) u where Î³=(cid:161) 1,âÎ²(cid:162) which has variance (cid:69)(cid:163) e2|Z (cid:164)= 1 2 Î³(cid:48)Î£Î³.AlsodefinethecovarianceÎ£ =(cid:69)[u e|Z]=Î£ âÎ£ Î². 2e 2 21 22 In Section 12.35 we assumed complete identification failure in the sense that Î=0. We now want to assume that identification does not completely fail but is weak in the sense that Î is small. A rich asymptoticdistributiontheoryhasbeendevelopedtounderstandthissettingbymodelingÎasâlocal- to-zeroâ. The seminal contribution is Staiger and Stock (1997). The theory was extended to nonlinear GMMestimationbyStockandWright(2000). ThetechnicaldeviceintroducedbyStaigerandStock(1997)istoassumethatthereducedformpa- rameterislocal-to-zero,specifically Î=n â1/2C (12.71) whereC isafreematrix. Then â1/2 scalingispickedbecauseitprovidesjusttherightbalancetoallow ausefuldistributiontheory. Thelocal-to-zeroassumption(12.71)isnotmeanttobetakenliterallybut rather is meant to be a useful distributional approximation. The parameter C indexes the degree of identification.Larger(cid:107)C(cid:107)impliesstrongeridentification;smaller(cid:107)C(cid:107)impliesweakeridentification. Wenowderivetheasymptoticdistributionoftheleastsquares,2SLS,andLIMLestimatorsunderthe local-to-unityassumption(12.71). Theleastsquaresestimatorsatisfies Î² (cid:98)ols âÎ²=(cid:161) n â1X (cid:48) X (cid:162)â1(cid:161) n â1X (cid:48) e (cid:162) =(cid:161) n â1U (cid:48) U (cid:162)â1(cid:161) n â1U (cid:48) e (cid:162)+o (1) 2 2 2 p ââÎ£â1Î£ . 22 2e p ThustheleastsquaresestimatorisinconsistentforÎ². Toexaminethe2SLSestimator,bythecentrallimittheorem (cid:112) 1 (cid:88) n Z u (cid:48) ââÎ¾=[Î¾ ,Î¾ ] n i=1 i i d 1 2 where vec(Î¾)â¼N (cid:161) 0,(cid:69)(cid:163) uu (cid:48)âZZ (cid:48)(cid:164)(cid:162) .",
    "page": 406,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 387 Thisimplies (cid:112) 1 Z (cid:48) eââÎ¾ =Î¾Î³. e n d Wealsofindthat (cid:112) 1 Z (cid:48) X = 1 Z (cid:48) ZC+(cid:112) 1 Z (cid:48) U ââQ C+Î¾ . 2 Z 2 n n n d Thus X (cid:48) P X = (cid:181) (cid:112) 1 X (cid:48) Z (cid:182)(cid:181) 1 Z (cid:48) Z (cid:182)â1(cid:181) (cid:112) 1 Z (cid:48) X (cid:182) ââ(cid:161) Q C+Î¾ (cid:162)(cid:48) Q â1(cid:161) Q C+Î¾ (cid:162) Z n n n d Z 2 Z Z 2 and X (cid:48) P e= (cid:181) (cid:112) 1 X (cid:48) Z (cid:182)(cid:181) 1 Z (cid:48) Z (cid:182)â1(cid:181) (cid:112) 1 Z (cid:48) e (cid:182) ââ(cid:161) Q C+Î¾ (cid:162)(cid:48) Q â1Î¾ . Z n n n d Z 2 Z e Wefindthatthe2SLSestimatorhastheasymptoticdistribution Î² (cid:98)2sls âÎ²=(cid:161) X (cid:48) P Z X (cid:162)â1(cid:161) X (cid:48) P Z e (cid:162) ââ (cid:179) (cid:161) Q C+Î¾ (cid:162)(cid:48) Q â1(cid:161) Q C+Î¾ (cid:162) (cid:180)â1(cid:161) Q C+Î¾ (cid:162)(cid:48) Q â1Î¾ . (12.72) Z 2 Z Z 2 Z 2 Z e d AsinthecaseofcompleteidentificationfailurewefindthatÎ² (cid:98)2sls isinconsistentforÎ²,itisasymptotically random,anditsasymptoticdistributionisnon-normal. ThedistortionisaffectedbythecoefficientC. As(cid:107)C(cid:107)ââthedistributionin(12.72)convergesinprobabilitytozerosuggestingthatÎ² (cid:98)2sls isconsistent forÎ².Thiscorrespondstotheclassicâstrongidentificationâcontext. NowconsidertheLIMLestimator. ThereducedformisY (cid:126) =ZÎ +U. ThisimpliesM Y (cid:126) =M U and Z Z bystandardasymptotictheory 1 Y (cid:126)(cid:48) M Y (cid:126) = 1 U (cid:48) M U ââÎ£=(cid:69)(cid:163) uu (cid:48)(cid:164) . Z Z n n p DefineÎ²=(cid:163)Î²,I (cid:164) sothatthereducedformcoefficientsequalÎ =(cid:163)ÎÎ²,Î(cid:164)=n â1/2CÎ².Then k (cid:112) 1 Z (cid:48) Y (cid:126) = 1 Z (cid:48) ZCÎ²+(cid:112) 1 Z (cid:48) U ââQ CÎ²+Î¾ Z n n n d and Y (cid:126)(cid:48) Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) Y (cid:126) ââ (cid:179) Q CÎ²+Î¾ (cid:180)(cid:48) Q â1 (cid:179) Q CÎ²+Î¾ (cid:180) . Z Z Z d Thisallowsustocalculatethatbythecontinuousmappingtheorem Î³(cid:48) Y (cid:126)(cid:48) Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) Y (cid:126)Î³ nÂµ=min (cid:98) Î³ Î³(cid:48)1Y (cid:126)(cid:48) M Y (cid:126)Î³ n Z (cid:179) (cid:180)(cid:48) (cid:179) (cid:180) Î³(cid:48) Q CÎ²+Î¾ Q â1 Q CÎ²+Î¾ Î³ Z Z Z ââmin Î³ Î³(cid:48)Î£Î³ d =Âµâ say,whichisafunctionofÎ¾andthusrandom. WededucethattheasymptoticdistributionoftheLIML estimatoris",
    "page": 407,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 388 Î² (cid:98)liml âÎ²= (cid:181) X (cid:48) P Z X ânÂµ (cid:98) 1 X (cid:48) M Z X (cid:182)â1(cid:181) X (cid:48) P Z eânÂµ (cid:98) 1 X (cid:48) M Z e (cid:182) n n ââ (cid:179) (cid:161) Q C+Î¾ (cid:162)(cid:48) Q â1(cid:161) Q C+Î¾ (cid:162)âÂµâÎ£ (cid:180)â1(cid:179) (cid:161) Q C+Î¾ (cid:162)(cid:48) Q â1Î¾ âÂµâÎ£ (cid:180) . Z 2 Z Z 2 22 Z 2 Z e 2e d Similarlyto2SLS,theLIMLestimatorisinconsistentforÎ²,isasymptoticallyrandom,andnon-normally distributed. Wesummarize. Theorem12.18 Under(12.71), Î² âÎ²ââÎ£â1Î£ (cid:98)ols 22 2e p Î² (cid:98)2sls âÎ²ââ (cid:179) (cid:161) Q Z C+Î¾ 2 (cid:162)(cid:48) Q â Z 1(cid:161) Q Z C+Î¾ 2 (cid:162) (cid:180)â1(cid:161) Q Z C+Î¾ 2 (cid:162)(cid:48) Q â Z 1Î¾ e d and Î² (cid:98)liml âÎ²ââ (cid:179) (cid:161) Q Z C+Î¾ 2 (cid:162)(cid:48) Q â Z 1(cid:161) Q Z C+Î¾ 2 (cid:162)âÂµâÎ£ 22 (cid:180)â1 d Ã (cid:179) (cid:161) Q C+Î¾ (cid:162)(cid:48) Q â1Î¾ âÂµâÎ£ (cid:180) Z 2 Z e 2e where (cid:179) (cid:180)(cid:48) (cid:179) (cid:180) Î³(cid:48) Q CÎ²+Î¾ Q â1 Q CÎ²+Î¾ Î³ Âµâ=min Z Z Z Î³ Î³(cid:48)Î£Î³ andÎ²=(cid:163)Î²,I (cid:164) . k Allthreeestimatorsareinconsistent.The2SLSandLIMLestimatorsareasymptoticallyrandomwith non-standard distributions, similar to the asymptotic distribution of the IV estimator under complete identification failure explored in the previous section. The difference under weak identification is the presenceofthecoefficientmatrixC. 12.37 ManyInstruments Someapplicationshaveavailablealargenumber(cid:96)ofinstruments. Iftheyareallvalidusingalarge numbershouldreducetheasymptoticvariancerelativetoestimationwithasmallernumberofinstru- ments. Is it then good practice to use many instruments? Or is there a cost to this practice? Bekker (1994) initiated a large literature investigating this question by formalizing the idea of âmany instru- mentsâ. Bekker proposed an asymptotic approximation which treats the number of instruments (cid:96) as proportionaltothesamplesize,thatis(cid:96)=Î±n,orequivalentlythat(cid:96)/nâÎ±â[0,1). Thedistributional theoryobtainedissimilarinmanyrespectstotheweakinstrumenttheoryoutlinedintheprevioussec- tion.Consequentlytheimpactofâweakâandâmanyâinstrumentsissimilar. Againforsimplicityweassumethattherearenoincludedexogenousregressorssothatthemodelis Y =X (cid:48)Î²+e (12.73) X =Î(cid:48) Z+u 2",
    "page": 408,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 389 with Z (cid:96)Ã1. Wealsomakethesimplifyingassumptionthatthereducedformerrorsareconditionally homoskedastic.Specifically, (cid:183) Î£ Î£ (cid:184) (cid:69)(cid:163) uu (cid:48)|Z (cid:164)=Î£= 11 12 . (12.74) Î£ Î£ 21 22 Inadditionweassumethattheconditionalfourthmomentsarebounded (cid:69)(cid:163)(cid:107)u(cid:107)4|Z (cid:164)â¤B<â. (12.75) Theideathatthereareâmanyinstrumentsâisformalizedbytheassumptionthatthenumberofin- strumentsisincreasingproportionatelywiththesamplesize (cid:96) ââÎ±. (12.76) n ThebestwaytothinkaboutthisistoviewÎ±astheratioof(cid:96)toninagivensample.Thusifanapplication hasn=100observationsand(cid:96)=10instruments,thenweshouldtreatÎ±=0.10. SupposethatthereisasingleendogenousregressorX.Calculateitsvarianceusingthereducedform: var[X]=var (cid:163) Z (cid:48)Î(cid:164)+var[u]. Supposeaswellthatvar[X]andvar[u]areunchangingas(cid:96)increases. This impliesthatvar (cid:163) Z (cid:48)Î(cid:164) isunchangingeventhoughthedimension(cid:96)isincreasing.Thisisausefulassump- tionasitimpliesthatthepopulationR2 ofthereducedformisnotchangingwith(cid:96). Wedonâtneedthis exact condition, rather we simply assume that the sample version converges in probability to a fixed constant.Specifically,weassumethat 1 (cid:88) n Î(cid:48) Z Z (cid:48)ÎââH (12.77) n i=1 i i p for some matrix H >0. Again, this essentially implies that the R2 of the reduced form regressions for eachcomponentofX convergetoconstants. AsabaselineitisusefultoexaminethebehavioroftheleastsquaresestimatorofÎ². First,observe thatthevarianceofvec (cid:161) n â1(cid:80)n Î(cid:48) Z u (cid:48)(cid:162) ,conditionalonZ,is i=1 i i n Î£ân â2(cid:88) Î(cid:48) Z Z (cid:48)Îââ0 i=1 i i p by(12.77).Thusitconvergesinprobabilitytozero: n n â1(cid:88) Î(cid:48) Z u (cid:48) ââ0. (12.78) i=1 i i p Combinedwith(12.77)andtheWLLNwefind 1 (cid:88) n X e = 1 (cid:88) n Î(cid:48) Z e + 1 (cid:88) n u e ââÎ£ i i i i 2i i 2e n i=1 n i=1 n i=1 p and 1 (cid:88) n X X (cid:48)= 1 (cid:88) n Î(cid:48) Z Z (cid:48)Î+ 1 (cid:88) n Î(cid:48) Z u (cid:48) + 1 (cid:88) n u Z (cid:48)Î+ 1 (cid:88) n u u (cid:48) ââH+Î£ . n i=1 i i n i=1 i i n i=1 i 2i n i=1 2i i n i=1 2i 2i p 22 Hence (cid:195) (cid:33)â1(cid:195) (cid:33) Î² (cid:98)ols =Î²+ n 1 i (cid:88) = n 1 X i X i (cid:48) n 1 i (cid:88) = n 1 X i e i â p âÎ²+(H+Î£ 22 ) â1Î£ 2e .",
    "page": 409,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 390 ThusleastsquaresisinconsistentforÎ². Nowconsiderthe2SLSestimator.Inmatrixnotation,settingP =Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) , Z Î² (cid:98)2sls âÎ²= (cid:181) 1 X (cid:48) P Z X (cid:182)â1(cid:181) 1 X (cid:48) P Z e (cid:182) n n = (cid:181) 1 Î (cid:48) Z (cid:48) ZÎ+ 1 Î (cid:48) Z (cid:48) u + 1 u (cid:48) ZÎ+ 1 u (cid:48) P u (cid:182)â1(cid:181) 1 Î(cid:48) Z (cid:48) e+ 1 u (cid:48) P e (cid:182) . (12.79) n n 2 n 2 n 2 Z 2 n n 2 Z Intheexpressionontheright-sideof(12.79)severalofthecomponentshavebeenexaminedin(12.77) and(12.78).Wenowexaminetheremainingcomponents 1u (cid:48) P eand 1u (cid:48) P u whicharesub-components n 2 Z n 2 Z 2 ofthematrix 1u (cid:48) P u.Takethe jkth element 1u (cid:48) P u . n Z n j Z k First,takeitsexpectation.Wehave(givenundertheconditionalhomoskedasticityassumption(12.74)) (cid:69) (cid:183) n 1 u (cid:48) j P Z u k (cid:175) (cid:175) (cid:175) (cid:175) Z (cid:184) = n 1 tr (cid:179) (cid:69) (cid:104) P Z u k u (cid:48) j (cid:175) (cid:175) (cid:175) Z (cid:105)(cid:180) = n 1 tr(P Z )Î£ jk = n (cid:96) Î£ jk âÎ±Î£ jk (12.80) usingtr(P )=(cid:96). Z Second, we calculate its variance which is a more cumbersome exercise. Let P = Z (cid:48)(cid:161) Z (cid:48) Z (cid:162)â1 Z im i m be the imth element of P . Then u (cid:48) P u =(cid:80)n (cid:80)n u u P . The matrix P is idempotent. It Z j Z k i=1 m=1 ji km im Z thereforehastheproperties (cid:80)n P =tr(P )=(cid:96)and0â¤P â¤1.ThepropertyP P =P alsoimplies i=1 ii Z ii Z Z Z (cid:80)n P2 =P .Then m=1 im ii var (cid:183) n 1 u (cid:48) j P Z u k (cid:175) (cid:175) (cid:175) (cid:175) Z (cid:184) = n 1 2 (cid:69) (cid:34) i (cid:88) = n 1m (cid:88) n =1 (cid:161) u ji u km â(cid:69)(cid:163) u ji u km (cid:164)1 {i =m} (cid:162) P im (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) Z (cid:35)2 (cid:34) (cid:35) = 1 (cid:69) (cid:88) n (cid:88) n (cid:88) n (cid:88) n (cid:161) u u âÎ£ 1 {i =m} (cid:162) P (cid:161) u u âÎ£ 1(cid:169) q=r (cid:170)(cid:162) P n2 i=1m=1q=1r=1 ji km jk im jq kr jk qr = 1 (cid:88) n (cid:69) (cid:104) (cid:161) u u âÎ£ (cid:162)2 (cid:105) P2 n2 ji ki jk ii i=1 + 1 (cid:88) n (cid:88) (cid:69) (cid:104) u2 u2 (cid:105) P2 + 1 (cid:88) n (cid:88) (cid:69)(cid:163) u u u u (cid:164) P2 n2 ji km im n2 ji km jm ki im i=1m(cid:54)=i i=1m(cid:54)=i (cid:195) (cid:33) â¤ B (cid:88) n P2 +2 (cid:88) n (cid:88) n P2 n2 i=1 ii i=1m=1 im 3B (cid:88) n â¤ P n2 ii i=1 (cid:96) =3B â0. n2 Thethirdequalityholdsbecausetheremainingcross-productshavezeroexpectationsincetheobser- vationsareindependentandtheerrorshavezeromean. Thefirstinequalityis(12.75). Theseconduses P2 â¤P and (cid:80)n P2 =P .Thefinalequalityis (cid:80)n P =(cid:96). ii ii m=1 im ii i=1 ii Using(12.76),(12.80),Markovâsinequality(B.36),andcombiningacrossall j andk wededucethat 1 u (cid:48) P uââÎ±Î£. (12.81) Z n p Returningtothe2SLSestimator(12.79)andcombining(12.77),(12.78),and(12.81),wefind Î² (cid:98)2sls âÎ²ââ(H+Î±Î£ 22 ) â1Î±Î£ 2e . p",
    "page": 410,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 391 Thus2SLSisalsoinconsistentforÎ².Thelimit,however,dependsonthemagnitudeofÎ±. WefinallyexaminetheLIMLestimator.(12.81)implies 1 Y (cid:48) M Y = 1 u (cid:48) uâ 1 u (cid:48) P uââ(1âÎ±)Î£. Z Z n n n p Similarly (cid:181) (cid:182) (cid:181) (cid:182) (cid:181) (cid:182) 1 Y (cid:48) Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) Y =Î² (cid:48) Î(cid:48) 1 Z (cid:48) Z ÎÎ²+Î² (cid:48) Î(cid:48) 1 Z (cid:48) u + 1 u (cid:48) Z ÎÎ²+ 1 u (cid:48) P u Z n n n n n (cid:48) ââÎ² HÎ²+Î±Î£. d Hence (cid:179) (cid:48) (cid:180) Î³(cid:48) Y (cid:48) Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) YÎ³ Î³(cid:48) Î² HÎ²+Î±Î£ Î³ Î± Âµ=min ââmin = (cid:98) Î³ Î³(cid:48) Y (cid:48) M Z YÎ³ d Î³ Î³(cid:48) (1âÎ±)Î£Î³ 1âÎ± and Î² (cid:98)liml âÎ²= (cid:181) 1 X (cid:48) P Z X âÂµ (cid:98) 1 X (cid:48) M Z X (cid:182)â1(cid:181) 1 X (cid:48) P Z eâÂµ (cid:98) 1 X (cid:48) M Z e (cid:182) n n n n (cid:179) Î± (cid:180)â1(cid:179) Î± (cid:180) ââ H+Î±Î£ â (1âÎ±)Î£ Î±Î£ â (1âÎ±)Î£ d 22 1âÎ± 22 2e 1âÎ± 2e =H â10 =0. ThusLIMLisconsistentforÎ²,unlike2SLS. Westatetheseresultsformally. Theorem12.19 In model (12.73), under assumptions (12.74), (12.75) and (12.76),thenasnââ. Î² (cid:98)ols ââÎ²+(H+Î£ 22 ) â1Î£ 2e p Î² (cid:98)2sls ââÎ²+(H+Î±Î£ 22 ) â1Î±Î£ 2e p Î² (cid:98)liml ââÎ². p This result is quite insightful. It shows that while endogeneity (Î£ (cid:54)=0) renders the least squares 2e estimator inconsistent, the 2SLS estimator is also inconsistent if the number of instruments diverges proportionatelywithn. ThelimitinTheorem12.19showsacontinuitybetweenleastsquaresand2SLS. Theprobabilitylimitofthe2SLSestimatoriscontinuousinÎ±,withtheextremecase(Î±=1)implyingthat 2SLSandleastsquareshavethesameprobabilitylimit.Thegeneralimplicationisthattheinconsistency of2SLSisincreasinginÎ±. Thetheoremalsoshowsthatunlike2SLStheLIMLestimatorisconsistentunderthemanyinstru- mentsassumption.Effectively,LIMLmakesabias-correction. Theorems12.18(weakinstruments)and12.19(manyinstruments)tellacautionarytale. Theyshow thatwheninstrumentsareweakand/ormanythe2SLSestimatorisinconsistent. Thedegreeofincon- sistencydependsontheweaknessoftheinstruments(themagnitudeofthematrixC inTheorem12.18)",
    "page": 411,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 392 and the degree of overidentification (the ratio Î± in Theorem 12.19). The Theorems also show that the LIMLestimatorisinconsistentundertheweakinstrumentassumptionbutwithabias-correction,and isconsistentunderthemanyinstrumentassumption.ThissuggeststhatLIMLismorerobustthan2SLS toweakandmanyinstruments. AnimportantlimitationoftheresultsinTheorem12.19istheassumptionofconditionalhomoskedas- ticity. ItappearslikelythattheconsistencyofLIMLfailsinthemanyinstrumentsettingiftheerrorsare heteroskedastic. Inapplicationsusersshouldbeawareofthepotentialconsequencesofthemanyinstrumentframe- work. It is useful to calculate the âmany instrument ratioâ Î±=(cid:96)/n. While there is no specific rule-of- thumbforÎ±whichleadstoacceptableinferenceaminimumcriterionisthatifÎ±â¥0.05youshouldbe seriouslyconcernedaboutthemany-instrumentproblem.Ingeneral,whenÎ±islargeitseemspreferable touseLIMLinsteadof2SLS. 12.38 TestingforWeakInstruments Intheprevioussectionswefoundthatweakinstrumentsresultsinnon-standardasymptoticdistri- butionsforthe2SLSandLIMLestimators.Inpracticehowdoweknowifthisisaproblem?Isthereaway tocheckiftheinstrumentsareweak? This question was addressed in an influential paper by Stock and Yogo (2005) as an extension of StaigerandStock(1997).Stock-Yogofocusontwoimplicationsofweakinstruments:(1)estimationbias and(2)inferencedistortion.Theyshowhowtotestthehypothesisthatthesedistortionsarenotâtoobigâ. TheyproposeF testsfortheexcludedinstrumentsinthereducedformregressionswithnon-standard criticalvalues.Inparticular,whenthereisoneendogenousregressorandasingleinstrumenttheStock- Yogo test rejects the null of weak instruments when this F statistic exceeds 10. While Stock and Yogo exploretwotypesofdistortions,wefocusexclusivelyoninferenceasthatisthemorechallengingprob- lem. InthissectionwedescribetheStock-Yogotheoryandtestsforthecaseofasingleendogenousre- gressor(k =1).Inthefollowingsectionwedescribetheirmethodforthecaseofmultipleendogeneous 2 regressors. WhilethetheoryinStockandYogoallowsforanarbitrarynumberofexogenousregressorsandin- struments,forthesakeofclearexpositionwewillfocusontheverysimplecaseofnoincludedexogenous variables(k =0)andjustoneexogenousinstrument((cid:96) =1)whichismodel(12.69)fromSection12.35. 1 2 Y =XÎ²+e X =ZÎ+u. Furthermore,asinSection12.35weassumeconditionalhomoskedasticityandnormalizethevariances asin(12.70).Sincethemodelisjust-identifiedthe2SLS,LIM,LandIVestimatorsareallequivalent. ThequestionofprimaryinterestistodetermineconditionsonthereducedformunderwhichtheIV estimatorofthestructuralequationiswellbehaved,andsecondly,whatstatisticaltestscanbeusedto learniftheseconditionsaresatisfied.AsinSection12.36weassumethatthereducedformcoefficientÎ islocal-to-zero,specificallyÎ=n â1/2Âµ. TheasymptoticdistributionoftheIVestimatorispresentedin Theorem12.18.Giventhesimplifyingassumptionstheresultis Î¾ Î² âÎ²ââ e (cid:98)iv Âµ+Î¾ d 2 where (Î¾ ,Î¾ ) are bivariate normal",
    "page": 412,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". ThequestionofprimaryinterestistodetermineconditionsonthereducedformunderwhichtheIV estimatorofthestructuralequationiswellbehaved,andsecondly,whatstatisticaltestscanbeusedto learniftheseconditionsaresatisfied.AsinSection12.36weassumethatthereducedformcoefficientÎ islocal-to-zero,specificallyÎ=n â1/2Âµ. TheasymptoticdistributionoftheIVestimatorispresentedin Theorem12.18.Giventhesimplifyingassumptionstheresultis Î¾ Î² âÎ²ââ e (cid:98)iv Âµ+Î¾ d 2 where (Î¾ ,Î¾ ) are bivariate normal. For inference we also examine the behavior of the classical (ho- e 2",
    "page": 412,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 393 moskedastic)t-statisticfortheIVestimator.Note Ï (cid:98) 2= 1 (cid:88) n (cid:161) Y i âX i Î² (cid:98)iv (cid:162)2 n i=1 = n 1 (cid:88) n e i 2â n 2 (cid:88) n e i X i (cid:161)Î² (cid:98)iv âÎ²(cid:162)+ n 1 (cid:88) n X i 2(cid:161)Î² (cid:98)iv âÎ²(cid:162)2 i=1 i=1 i=1 Î¾ (cid:181) Î¾ (cid:182)2 ââ1â2Ï e + e . Âµ+Î¾ Âµ+Î¾ d 2 2 Thus Î² âÎ² Î¾ T = (cid:98)iv ââ 1 d=ef S. (12.82) (cid:113) Ï (cid:98) 2(cid:80)n i=1 z i 2/ (cid:175) (cid:175) (cid:80)n i=1 z i x i (cid:175) (cid:175) d (cid:114) 1â2Ï Âµ+ Î¾ 1 Î¾ + (cid:179) Âµ+ Î¾ 1 Î¾ (cid:180)2 2 2 Ingeneral,Sisnon-normalanditsdistributiondependsontheparametersÏandÂµ. CanweusethedistributionSforinferenceonÎ²?Thedistributiondependsontwounknownparam- eters and neither is consistently estimable. This means we cannot use the distribution in (12.82) with Ï andÂµreplacedwithestimates. ToeliminatethedependenceonÏ onepossibilityistousetheâworst caseâ value which turns out to be Ï =1. By worst-case we mean the value which causes the greatest distortionawayfromnormalcriticalvalues.SettingÏ=1wehavetheconsiderablesimplification (cid:175) Î¾(cid:175) S=S 1 =Î¾(cid:175) (cid:175) (cid:175) 1+ Âµ (cid:175) (cid:175) (cid:175) (12.83) where Î¾â¼N(0,1). When the model is strongly identified (so (cid:175) (cid:175) Âµ(cid:175) (cid:175) is very large) then S 1 âÎ¾ is standard normal,consistentwithclassicaltheory. Howeverwhen (cid:175) (cid:175) Âµ(cid:175) (cid:175)isverysmall(butnon-zero)|S 1 |âÎ¾2/Âµ(in thesensethatthistermdominates),whichisascaledÏ2 andquitefarfromnormal. As (cid:175) (cid:175) Âµ(cid:175) (cid:175) â0wefind 1 theextremecase|S |â â. 1 p While (12.83) is a convenient simplification it does not yield a useful approximation for inference sincethedistributionin(12.83)ishighlydependentontheunknownÂµ. Ifwetaketheworst-casevalue ofÂµ,whichisÂµ=0,wefindthat|S |divergesandalldistributionalapproximationsfail. 1 TobreakthisimpasseStockandYogo(2005)recommendedaconstructivealternative. Ratherthan usingtheworst-caseÂµtheysuggestedfindingathresholdsuchthatifÂµexceedsthisthresholdthenthe distribution(12.83)isnotâtoobadlyâdistortedfromthenormaldistribution. Specifically,theStock-Yogorecommendationcanbesummarizedbytwosteps.First,thedistribution result(12.83)canbeusedtofindathresholdvalueÏ2 suchthatifÂµ2â¥Ï2 thenthesizeofthenominal1 5% test âReject if |T|â¥1.96â has asymptotic size (cid:80)[|S |â¥1.96]â¤0.15. This means that while the goal 1 istoobtainatestwithsize5%,werecognizethattheremaybesizedistortionduetoweakinstruments andarewillingtotolerateaspecificdistortion.Forexample,a10%distortionmeansweallowtheactual size to be up to 15%. Second, they use the asymptotic distribution of the reduced-form (first stage) F statistictotestiftheactualunknownvalueofÂµ2exceedsthethresholdÏ2.Thesetwostepstogethergive risetotherule-of-thumbthatthefirst-stageF statisticshouldexceed10inordertoachievereliableIV inference. (Thisisforthecaseofoneinstrumentalvariable",
    "page": 413,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Second, they use the asymptotic distribution of the reduced-form (first stage) F statistictotestiftheactualunknownvalueofÂµ2exceedsthethresholdÏ2.Thesetwostepstogethergive risetotherule-of-thumbthatthefirst-stageF statisticshouldexceed10inordertoachievereliableIV inference. (Thisisforthecaseofoneinstrumentalvariable. Ifthereismorethanoneinstrumentthen therule-of-thumbchanges.)Wenowdescribethestepsbehindthisreasoninginmoredetail. The first step is to use the distribution (12.82) to determine the threshold Ï2. Formally, the goal is tofindthevalueofÏ2=Âµ2 atwhichtheasymptoticsizeofanominal5%testisactuallyagivenr (e.g. 1Thetermânominalsizeâofatestistheofficialintendedsizeâthesizewhichwouldobtainunderidealcircumstances. In thiscontextthetestâRejectif|T|â¥1.96âhasnominalsize0.05asthiswouldbetheasymptoticrejectionprobabilityintheideal contextofstronginstruments.",
    "page": 413,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 394 r =0.15),thus(cid:80)[|S 1 |â¥1.96]â¤r.Bysomealgebraandthequadraticformulatheevent (cid:175) (cid:175) Î¾(cid:161) 1+Î¾/Âµ(cid:162)(cid:175) (cid:175) <xis thesameas Âµ2 (cid:179) Âµ(cid:180)2 Âµ2 âxÂµ< Î¾+ < +xÂµ. 4 2 4 TherandomvariablebetweentheinequalitiesisdistributedÏ2(Âµ2/4),anoncentralchi-squarewithone 1 degreeoffreedomandnoncentralityparameterÂµ2/4.Thus (cid:183) (cid:181)Âµ2(cid:182) Âµ2 (cid:184) (cid:183) (cid:181)Âµ2(cid:182) Âµ2 (cid:184) (cid:80)[|S |â¥x]=(cid:80) Ï2 â¥ +xÂµ +(cid:80) Ï2 â¤ âxÂµ 1 1 4 4 1 4 4 (cid:181)Âµ2 Âµ2(cid:182) (cid:181)Âµ2 Âµ2(cid:182) =1âG +xÂµ, +G âxÂµ, (12.84) 4 4 4 4 whereG(u,Î»)isthedistributionfunctionofÏ2(Î»).HencethedesiredthresholdÏ2solves 1 (cid:181)Ï2 Ï2(cid:182) (cid:181)Ï2 Ï2(cid:182) 1âG +1.96Ï, +G â1.96Ï, =r 4 4 4 4 oreffectively (cid:181)Ï2 Ï2(cid:182) G +1.96Ï, =1âr 4 4 sinceÏ2/4â1.96Ï<0forrelevantvaluesofÏ. Thenumericalsolution(computedwiththenon-central chi-square distribution function, e.g. ncx2cdf in MATLAB) is Ï2 = 1.70 when r = 0.15. (That is, the command ncx2cdf(1.7/4+1.96âsqrt(1.7),1,1.7/4) yields the answer 0.8500. Stock and Yogo (2005) approximate the same calculation using simulation methodsandreportÏ2=1.82.) ThiscalculationmeansthatifthereducedformsatisfiesÂµ2â¥1.7,orequivalentlyifÎ2â¥1.7/n,then theasymptoticsizeofanominal5%testonthestructuralparameterisnolargerthan15%. TosummarizetheStock-Yogofirststep,wecalculatetheminimumvalueÏ2forÂµ2sufficienttoensure thattheasymptoticsizeofanominal5%t-testdoesnotexceedr,andfindthatÏ2=1.70forr =0.15. TheStock-Yogosecondstepistofindacriticalvalueforthefirst-stageF statisticsufficienttoreject thehypothesisthat(cid:72) :Âµ2=Ï2against(cid:72) :Âµ2>Ï2.Wenowdescribethisprocedure. 0 1 Theysuggesttesting(cid:72) :Âµ2=Ï2atthe5%sizeusingthefirststageF statistic.IftheF statisticissmall 0 sothatthetestdoesnotrejectthenweshouldbeworriedthatthetruevalueofÂµ2issmallandthereisa weakinstrumentproblem.OntheotherhandiftheF statisticislargesothatthetestrejectsthenwecan havesomeconfidencethatthetruevalueofÂµ2issufficientlylargethattheweakinstrumentproblemis nottoosevere. To implement the test we need to calculate an appropriate critical value. It should be calculated underthenullhypothesis(cid:72) :Âµ2=Ï2. ThisisdifferentfromaconventionalF testwhichiscalculated 0 under(cid:72) :Âµ2=0. 0 WestartbycalculatingtheasymptoticdistributionofF.Sincethereisoneregressorandoneinstru- mentinoursimplifiedsettingthefirst-stageF statisticisthesquaredt-statisticfromthereducedform. Givenourpreviouscalculationsithastheasymptoticdistribution Î³2 (cid:161)(cid:80)n Z X (cid:162)2 F= (cid:98) = i=1 i i ââ(cid:161)Âµ+Î¾ (cid:162)2â¼Ï2(cid:161)Âµ2(cid:162) . s (cid:161)Î³ (cid:98) (cid:162)2 (cid:161)(cid:80)n i=1 X i 2(cid:162)Ï (cid:98) 2 u d 2 1 Thisisanon-centralchi-squaredistributionwithonedegreeoffreedomandnon-centralityparameter Âµ2.ThedistributionfunctionofthelatterisG(u,Âµ2).",
    "page": 414,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 395 Totest(cid:72) :Âµ2=Ï2 against(cid:72) :Âµ2>Ï2 werejectforFâ¥c wherec isselectedsothattheasymptotic 0 1 rejectionprobabilitysatisfies (cid:80)(cid:163) Fâ¥c|Âµ2=Ï2(cid:164)â(cid:80)(cid:163)Ï2(cid:161)Ï2(cid:162)â¥c (cid:164)=1âG (cid:161) c,Ï2(cid:162)=0.05 1 forÏ2=1.70,orequivalentlyG(c,1.7)=0.95.Thisisfoundbyinvertingthenon-centralchi-squarequan- tilefunction,e.g. thefunctionQ(p,d)whichsolvesG(Q(p,d),d)=p. Wefindthatc=Q(0.95,1.7)=8.7. In MATLAB, this can be computed by ncx2inv(.95,1.7). Stock and Yogo (2005) report c =9.0 since theyusedÏ2=1.82. ThismeansthatifF>8.7wecanreject(cid:72) :Âµ2=1.7against(cid:72) :Âµ2>1.7withanasymptotic5%test. 0 1 InthiscontextweshouldexpecttheIVestimatorandteststobereasonablywellbehaved. However,if F<8.7thenweshouldbecautiousabouttheIVestimator,confidenceintervals,andtests. Thisfinding ledStaigerandStock(1997)toproposetheinformalâruleofthumbâthatthefirststageF statisticshould exceed10.NoticethatFexceeding8.7(or10)isequivalenttothereducedformt-statisticexceeding2.94 (or3.16),whichisconsiderablylargerthanaconventionalcheckifthet-statisticisâsignificantâ. Equiv- alently,therecommendedrule-of-thumbforthecaseofasingleinstrumentistoestimatethereduced formandverifythatthet-statisticforexclusionoftheinstrumentalvariableexceeds3inabsolutevalue. Doestheproposedprocedurecontroltheasymptoticsizeofa2SLStest?Thefirststephasasymptotic sizeboundedbelowr (e.g.15%).Thesecondstephasasymptoticsize5%.BytheBonferronibound(see Section9.20)thetwostepstogetherhaveasymptoticsizeboundedbelowr+0.05(e.g.20%).Wecanthus calltheStock-Yogoprocedurearigoroustestwithasymptoticsizer+0.05(or20%). Ouranalysishasbeenconfinedtothecasek =(cid:96) =1. StockandYogo(2005)alsoexaminethecase 2 2 (cid:96) >1 (which requires numerical simulation to solve) and both the 2SLS and LIML estimators. They 2 showthattheF statisticcriticalvaluesdependonthenumberofinstruments(cid:96) aswellastheestimator. 2 WereporttheircalculationsinTable12.4. Table12.4:5%CriticalValueforWeakInstruments,k =1 2 MaximalSizer 2SLS LIML (cid:96) 0.10 0.15 0.20 0.25 0.10 0.15 0.20 0.25 2 1 16.4 9.0 6.7 5.5 16.4 9.0 6.7 5.5 2 19.9 11.6 8.7 7.2 8.7 5.3 4.4 3.9 3 22.3 12.8 9.5 7.8 6.5 4.4 3.7 3.3 4 24.6 14.0 10.3 8.3 5.4 3.9 3.3 3.0 5 26.9 15.1 11.0 8.8 4.8 3.6 3.0 2.8 6 29.2 16.2 11.7 9.4 4.4 3.3 2.9 2.6 7 31.5 17.4 12.5 9.9 4.2 3.2 2.7 2.5 8 33.8 18.5 13.2 10.5 4.0 3.0 2.6 2.4 9 36.2 19.7 14.0 11.1 3.8 2.9 2.5 2.3 10 38.5 20.9 14.8 11.6 3.7 2.8 2.5 2.2 15 50.4 26.8 18.7 12.2 3.3 2.5 2.2 2.0 20 62.3 32.8 22.7 17.6 3.2 2.3 2.1 1.9 25 74.2 38.8 26.7 20.6 3.8 2.2 2.0 1.8 30 86.2 44.8 30.7 23.6 3.9 2.2 1.9 1.7 Onestrikingfeatureaboutthesecriticalvaluesisthatthoseforthe2SLSestimatorarestronglyin- creasingin(cid:96) whilethosefortheLIMLestimatoraredecreasingin(cid:96) .Thismeansthatwhenthenumber 2 2 ofinstruments(cid:96) islarge,2SLSrequiresamuchstrongerreducedform(largerÂµ2)inorderforinference 2",
    "page": 415,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 396 tobereliable,butthisisnotthecaseforLIML.ThisisdirectevidencethatLIMLinferenceislesssensitive toweakinstrumentsthan2SLS.ThismakesastrongcaseforLIMLover2SLS,especiallywhen(cid:96) islarge 2 ortheinstrumentsarepotentiallyweak. We now summarize the recommended Staiger-Stock/Stock-Yogo procedure for k â¥1, k =1, and 1 2 (cid:96) â¥1.Thestructuralequationandreducedformequationsare 2 Y =Z (cid:48)Î² +Y Î² +e 1 1 1 2 2 Y =Z (cid:48)Î³ +Z (cid:48)Î³ +u. 2 1 1 2 2 Thestructuralequationisestimatedbyeither2SLSorLIML.LetFbetheF statisticfor(cid:72) :Î³ =0inthe 0 2 reducedformequation.Lets(Î² (cid:98)2 )beastandarderrorforÎ² 2 inthestructuralequation.Theprocedureis: 1. CompareFwiththecriticalvaluesc inTable12.4withtherowselectedtomatchthenumberof excluded instruments (cid:96) and the columns to match the estimation method (2SLS or LIML) and 2 thedesiredsizer. 2. IfF>c thenreportthe2SLSorLIMLestimateswithconventionalinference. TheStock-YogotestcanbeimplementedinStatausingthecommandestat firststageafterivregress 2slsorivregres limlifastandard(non-robust)covariancematrixhasbeenspecified(thatis,without theâ,râoption). TherearepossibleextensionstotheStock-Yogoprocedure. Onemodestextensionistousetheinformationtoconveythedegreeofconfidenceintheaccuracy of a confidence interval. Suppose in an application you have (cid:96) = 5 excluded instruments and have 2 estimatedyourequationby2SLS.NowsupposethatyourreducedformF statisticequals12. Youcheck Table12.4andfindthatF=12issignificantwithr =0.20. Thuswecaninterprettheconventional2SLS confidence interval as having coverage of 80% (or 75% if we make the Bonferroni correction). On the otherhandifF=27wewouldconcludethatthetestforweakinstrumentsissignificantwithr =0.10, meaningthattheconventional2SLSconfidenceintervalcanbeinterpretedashavingcoverageof90%(or 85%afterBonferronicorrection). ThusthevalueoftheFstatisticcanbeusedtocalibratethecoverage accuracy. Amoresubstantiveextension,whichwenowdiscuss,reversesthesteps. Unfortunatelythisdiscus- sionwillbelimitedtothecase(cid:96) =1. First,usethereducedformF statistictofindaone-sidedconfi- 2 denceintervalforÂµ2 oftheform[Âµ2,â). Second,usethelowerboundÂµ2 tocalculateacriticalvaluec L L forS suchthatthe2SLStesthasasymptoticsizeboundedbelow0.05. Thisproducesbettersizecon- 1 trolthantheStock-YogoprocedureandproducesmoreinformativeconfidenceintervalsforÎ² .Wenow 2 describethestepsindetail. Thefirstgoalistofindaone-sidedconfidenceintervalforÂµ2. Thisisfoundbytestinversion. Aswe describedearlier, foranyÏ2 wereject(cid:72) :Âµ2=Ï2 infavorof(cid:72) :Âµ2>Ï2 ifF>c whereG(c,Ï2)=0.95. 0 1 Equivalently, werejectifG(F,Ï2)>0.95. Bythetestinversionprincipleanasymptotic95%confidence interval[Âµ2,â)isthesetofallvaluesofÏ2 whicharenotrejected. SinceG(F,Ï2)â¥0.95forallÏ2 inthis L set, thelowerboundÂµ2 satisfiesG(F,Âµ2)=0.95, andisfoundnumerically. InMATLAB,thesolutionis L L mu2whenncx2cdf(F,1,mu2)returns0.95",
    "page": 416,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Thisisfoundbytestinversion. Aswe describedearlier, foranyÏ2 wereject(cid:72) :Âµ2=Ï2 infavorof(cid:72) :Âµ2>Ï2 ifF>c whereG(c,Ï2)=0.95. 0 1 Equivalently, werejectifG(F,Ï2)>0.95. Bythetestinversionprincipleanasymptotic95%confidence interval[Âµ2,â)isthesetofallvaluesofÏ2 whicharenotrejected. SinceG(F,Ï2)â¥0.95forallÏ2 inthis L set, thelowerboundÂµ2 satisfiesG(F,Âµ2)=0.95, andisfoundnumerically. InMATLAB,thesolutionis L L mu2whenncx2cdf(F,1,mu2)returns0.95. Thesecondgoalistofindthecriticalvaluec suchthat(cid:80)(|S |â¥c)=0.05whenÂµ2=Âµ2. From(12.84) 1 L thisisachievedwhen (cid:195)Âµ2 Âµ2 (cid:33) (cid:195)Âµ2 Âµ2 (cid:33) 1âG L +cÂµ , L +G L âcÂµ , L =0.05. (12.85) L L 4 4 4 4 Thiscanbesolvedas (cid:195)Âµ2 Âµ2 (cid:33) G L +cÂµ , L =0.95. L 4 4",
    "page": 416,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 397 (The third term on the left-hand-side of (12.85) is zero for all solutions so can be ignored.) Using the non-centralchi-squarequantilefunctionQ(p,d),thisC equals (cid:179) Âµ2(cid:180) Âµ2 Q 0.95, L â L 4 4 c= . Âµ L Forexample,inMATLABthisisfoundasc=(ncx2inv(.95,1,mu2/4)-mu2/4)/sqrt(mu2). 95%confi- denceintervalsforÎ² 2 arethencalculatedasÎ² (cid:98)iv Â±cs(Î² (cid:98)iv ). Wecanalsocalculateap-valueforthet-statisticT forÎ² .Thisis 2 (cid:195)Âµ2 Âµ2 (cid:33) (cid:195)Âµ2 Âµ2 (cid:33) p=1âG L +|T|Âµ , L +G L â|T|Âµ , L L L 4 4 4 4 where the third term equals zero if |T|â¥Âµ /4. In MATLAB, for example, this can be calculated by the L commands T1=mu2/4+abs(T)âsqrt(mu2); T2=mu2/4âabs(T)âsqrt(mu2); p=âncx2cdf(T1,1,mu2/4)+ncx2cdf(T2,1,mu2/4); Theseconfidenceintervalsandp-valueswillbelargerthantheconventionalintervalsandp-values, reflectingtheincorporationofinformationaboutthestrengthoftheinstrumentsthroughthefirst-stage F statistic. Also,bytheBonferroniboundthesetestshaveasymptoticsizeboundedbelow10%andthe confidence intervals have asymptotic converage exceeding 90%, unlike the Stock-Yogo method which hassizeof20%andcoverageof80%. Theaugmentedproceduresuggestedhere,onlyforthe(cid:96) =1case,is 2 1. FindÂµ2 whichsolvesG (cid:161) F,Âµ2(cid:162)=0.95. InMATLAB,thesolutionismu2whenncx2cdf(F,1,mu2) L L returns0.95. 2. Findc whichsolvesG (cid:161)Âµ2/4+cÂµ ,Âµ2/4 (cid:162)=0.95.InMATLAB,thecommandis L L L c=(ncx2inv(.95,1,mu2/4)-mu2/4)/sqrt(mu2) 3. ReporttheconfidenceintervalÎ² (cid:98)2 Â±cs(Î² (cid:98)2 )forÎ² 2 . 4. ForthetstatisticT =(cid:161)Î² (cid:98)2 âÎ² 2 (cid:162) /s(Î² (cid:98)2 )theasymptoticp-valueis (cid:195)Âµ2 Âµ2 (cid:33) (cid:195)Âµ2 Âµ2 (cid:33) p=1âG L +|T|Âµ , L +G L â|T|Âµ , L L L 4 4 4 4 whichiscomputedinMATLABbyT1=mu2/4+abs(T)*sqrt(mu2);T2=mu2/4-abs(T)*sqrt(mu2); andp=1-ncx2cdf(T1,1,mu2/4)+ncx2cdf(T2,1,mu2/4). WehavedescribedanextensiontotheStock-Yogoprocedureforthecaseofoneinstrumentalvari- able(cid:96) =1.Thisrestrictionwasduetotheuseoftheanalyticformula(12.85)fortheasymptoticdistribu- 2 tionwhichisonlyavailablewhen(cid:96) =1.Inprincipletheprocedurecouldbeextendedusingsimulation 2 orbootstrapmethodsbutthishasnotbeendonetomyknowledge. To illustrate the Stock-Yogo and extended procedures let us return to the Card proximity example. Take the IV estimates reported in the second column of Table 12.1 which used college proximity as a singleinstrument. Thereducedformestimatesfortheendogenousvariableeducationarereportedin thesecondcolumnofTable12.2. Theexcludedinstrumentcollegehasat-ratioof4.2whichimpliesan F statistic of 17.8. The F statistic exceeds the rule-of thumb of 10 so the structural estimates pass the",
    "page": 417,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 398 Stock-Yogothreshold. Basedontheirrecommendationthismeansthatwecaninterprettheestimates conventionally. However,theconventionalconfidenceinterval,e.g. forthereturnstoeducation0.132Â± 0.049Ã1.96=[0.04,0.23],hasanasymptoticcoverageof80%ratherthanthenominal95%rate. Nowconsidertheextendedprocedure. GivenF=17.8wecalculatethelowerboundÂµ2 =6.6. This L impliesacriticalvalueofC =2.7. Henceanimprovedconfidenceintervalforthereturnstoeducation inthisequationis0.132Â±0.049Ã2.7=[0.01,0.26]. Thisisawiderconfidenceintervalbuthasimproved asymptoticcoverageof90%.Thep-valueforÎ² =0isp=0.012. 2 Next,takethe2SLSestimatesreportedinthefourthcolumnofTable11.1whichusethetwoinstru- ments public and private. The reduced form equation is reported in column six of Table 12.2. An F statisticforexclusionofthetwoinstrumentsisF=13.9whichexceedsthe15%sizethresholdfor2SLS andallthresholdsforLIML,indicatingthatthestructuralestimatespasstheStock-Yogothresholdtest andcanbeinterpretedconventionally. Theweakinstrumentmethodsdescribedhereareimportantforappliedeconometricsastheydis- ciplineresearcherstoassessthequalityoftheirreducedformrelationshipsbeforereportingstructural estimates.Thetheory,however,haslimitationsandshortcomings,inparticularthestrongassumptionof conditionalhomoskedasticity. Despitethislimitation,inpracticeresearchersapplytheStock-Yogorec- ommendationstoestimatescomputedwithheteroskedasticity-robuststandarderrors. Thisisanactive areaofresearchsotherecommendedmethodsmaychangeintheyearsahead. > 12.39 WeakInstrumentswithk 1 2 Whenthereismorethanoneendogenousregressor(k >1)itisbettertoexaminethereducedform 2 as a system. Staiger and Stock (1997) and Stock and Yogo (2005) provided an analysis of this case and constructedatestforweakinstruments. Thetheoryisconsiderablymoreinvolvedthanthek =1case 2 sowebrieflysummarizeithereexcludingmanydetails,emphasizingtheirsuggestedmethods. Thestructuralequationandreducedformequationsare Y =Z (cid:48)Î² +Y (cid:48)Î² +e 1 1 1 2 2 Y =Î(cid:48) Z +Î(cid:48) Z +u . 2 12 1 22 2 2 Asintheprevioussectionweassumethattheerrorsareconditionallyhomoskedastic. IdentificationofÎ² requiresthematrixÎ tobefullrank. Anecessaryconditionisthateachrowof 2 22 Î(cid:48) isnon-zerobutthisisnotsufficient. 22 WefocusonthesizeperformanceofthehomoskedasticWaldstatisticforthe2SLSestimatorofÎ² . 2 For simplicity assume that the variance of e is known and normalized to one. Using representation (12.32),theWaldstatisticcanbewrittenas W =e (cid:48) Z(cid:101)2 (cid:179) Z(cid:101) (cid:48) 2 Z(cid:101)2 (cid:180)â1 Z(cid:101) (cid:48) 2 Y 2 (cid:181) Y (cid:48) 2 Z(cid:101)2 (cid:179) Z(cid:101) (cid:48) 2 Z(cid:101)2 (cid:180)â1 Z(cid:101) (cid:48) 2 Y 2 (cid:182)â1(cid:181) Y (cid:48) 2 Z(cid:101)2 (cid:179) Z(cid:101) (cid:48) 2 Z(cid:101)2 (cid:180)â1 Z(cid:101) (cid:48) 2 e (cid:182) whereZ(cid:101)2 =(I n âP 1 )Z 2 andP 1 =Z 1 (cid:161) Z (cid:48) 1 Z 1 (cid:162)â1 Z (cid:48) 1",
    "page": 418,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". RecallfromSection12.36thatStockandStaigermodeltheexcludedinstrumentsZ asweakbyset- 2 tingÎ =n â1/2C forsomematrixC.Inthisframeworkwehavetheasymptoticdistributionresults 22 n 1 Z(cid:101) (cid:48) 2 Z(cid:101)2 â p âQ=(cid:69)(cid:163) Z 2 Z 2 (cid:48)(cid:164)â(cid:69)(cid:163) Z 2 Z 1 (cid:48)(cid:164)(cid:161)(cid:69)(cid:163) Z 1 Z 1 (cid:48)(cid:164)(cid:162)â1(cid:69)(cid:163) Z 1 Z 2 (cid:48)(cid:164) and 1 (cid:48) (cid:112) Z(cid:101)2 eââQ1/2Î¾ 0 n d",
    "page": 418,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 399 whereÎ¾ isamatrixnormalvariatewhosecolumnsareindependentN(0,I). Furthermore, settingÎ£= 0 (cid:69)(cid:163) u u (cid:48)(cid:164) andC =Q1/2CÎ£â1/2, 2 2 1 (cid:48) 1 (cid:48) 1 (cid:48) (cid:112) n Z(cid:101)2 Y 2 = n Z(cid:101)2 Z(cid:101)2 C+(cid:112) n Z(cid:101)2 U 2 â d âQ1/2CÎ£1/2+Q1/2Î¾ 2 Î£1/2 whereÎ¾ isamatrixnormalvariatewhosecolumnsareindependentN(0,I). ThevariablesÎ¾ andÎ¾ are 2 0 2 correlated.TogetherweobtaintheasymptoticdistributionoftheWaldstatistic W ââS=Î¾(cid:48) (cid:179) C+Î¾ (cid:180)(cid:179) C (cid:48) C (cid:180)â1(cid:179) C+Î¾ (cid:180)(cid:48) Î¾ . 0 2 2 0 d Usingthespectraldecomposition,C (cid:48) C =H (cid:48)ÎH where H (cid:48) H =I andÎisdiagonal. Thuswecanwrite (cid:48) S =Î¾(cid:48)Î¾ Îâ1Î¾ Î¾ where Î¾ =CH (cid:48)+Î¾ H (cid:48) . The matrix Î¾â =(Î¾ ,Î¾ ) is multivariate normal, so Î¾â(cid:48)Î¾â has 0 2 2 0 2 2 0 2 whatiscalledanon-centralWishartdistribution.ItonlydependsonthematrixC throughHC (cid:48) CH (cid:48)=Î whicharetheeigenvaluesofC (cid:48) C. SinceS isafunctionofÎ¾â onlythroughÎ¾ (cid:48) Î¾ weconcludethatS isa 2 0 functionofC onlythroughtheseeigenvalues. ThisisaveryquickderivationofaratherinvolvedderivationbuttheconclusiondrawnbyStockand YogoisthattheasymptoticdistributionoftheWaldstatisticisnon-standardandafunctionofthemodel (cid:48) parametersonlythroughtheeigenvaluesofC C andthecorrelationsbetweenthenormalvariatesÎ¾ and 0 Î¾ . Theworst-casecanbesummarizedbythemaximalcorrelationbetweenÎ¾ andÎ¾ andthesmallest 2 0 2 (cid:48) eigenvalue ofC C. For convenience they rescale the latter by dividing by the number of endogenous variables.Define G=C (cid:48) C/k =Î£â1/2C (cid:48) QCÎ£â1/2/k 2 2 and g =Î» (G)=Î» (cid:161)Î£â1/2C (cid:48) QCÎ£â1/2(cid:162) /k . min min 2 Thiscanbeestimatedfromthereduced-formregression X 2i =Î (cid:98) (cid:48) 12 Z 1i +Î (cid:98) (cid:48) 22 Z 2i +u (cid:98)2i . Theestimatoris G(cid:98) =Î£ (cid:98) â1/2Î (cid:98) (cid:48) 22 (cid:179) Z(cid:101) (cid:48) 2 Z(cid:101)2 (cid:180) Î (cid:98)22 Î£ (cid:98) â1/2/k 2 =Î£ (cid:98) â1/2 (cid:181) X (cid:48) 2 Z(cid:101)2 (cid:179) Z(cid:101) (cid:48) 2 Z(cid:101)2 (cid:180)â1 Z(cid:101) (cid:48) 2 X 2 (cid:182) Î£ (cid:98) â1/2/k 2 Î£ (cid:98) = nâ 1 k (cid:88) n u (cid:98)2i u (cid:98)2 (cid:48) i i=1 g (cid:98) =Î» min (cid:161) G(cid:98) (cid:162) . G(cid:98) isamatrixF-typestatisticforthecoefficientmatrixÎ (cid:98)22 . Thestatisticg wasproposedbyCraggandDonald(1993)asatestforunderidentification.Stockand (cid:98) Yogo (2005) use it as a test for weak instruments. Using simulation methods they determined critical values for g similar to those for k =1. For given size r >0.05 there is a critical value c (reported in (cid:98) 2 thetablebelow)suchthatif g (cid:98) >c thenthe2SLS(orLIML)WaldstatisticW forÎ² (cid:98)2 hasasymptoticsize boundedbelowr.Ontheotherhand,ifg â¤cthenwecannotboundtheasymptoticsizebelowr andwe (cid:98) cannotrejectthehypothesisofweakinstruments",
    "page": 419,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Using simulation methods they determined critical values for g similar to those for k =1. For given size r >0.05 there is a critical value c (reported in (cid:98) 2 thetablebelow)suchthatif g (cid:98) >c thenthe2SLS(orLIML)WaldstatisticW forÎ² (cid:98)2 hasasymptoticsize boundedbelowr.Ontheotherhand,ifg â¤cthenwecannotboundtheasymptoticsizebelowr andwe (cid:98) cannotrejectthehypothesisofweakinstruments. TheStock-Yogocriticalvaluesfork =2arepresentedinTable12.5.Themethodsandtheoryapplies 2 to the cases k > 2 as well but those critical values have not been calculated. As for the k = 1 case 2 2 the critical values for 2SLS are dramatically increasing in (cid:96) . Thus when the model is over-identified, 2",
    "page": 419,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 400 we need a large value of g to reject the hypothesis of weak instruments. This is a strong cautionary (cid:98) messagetochecktheg statisticinapplications. Furthermore,thecriticalvaluesforLIMLaregenerally (cid:98) decreasingin(cid:96) (exceptforr =0.10wherethecriticalvaluesareincreasingforlarge(cid:96) ).Thismeansthat 2 2 forover-identifiedmodelsLIMLinferenceislesssensitivetoweakinstrumentsthan2SLSandmaybe thepreferredestimationmethod. TheStock-YogotestcanbeimplementedinStatausingthecommandestat firststageafterivregress 2slsorivregres limlifastandard(non-robust)covariancematrixhasbeenspecified(thatis,without theâ,râoption). Criticalvalueswhichcontrolforsizeareonlyavailablefork â¤2. Forfork >2critical 2 2 valueswhichcontrolforrelativebiasarereported. RobustversionsofthetesthavebeenproposedbyKleibergenandPaap(2006). Thesecanbeimple- mentedinStatausingthedownloadablecommandivreg2. Table12.5:5%CriticalValueforWeakInstruments,k =2 2 MaximalSizer 2SLS LIML (cid:96) 0.10 0.15 0.20 0.25 0.10 0.15 0.20 0.25 2 2 7.0 4.6 3.9 3.6 7.0 4.6 3.9 3.6 3 13.4 8.2 6.4 5.4 5.4 3.8 3.3 3.1 4 16.9 9.9 7.5 6.3 4.7 3.4 3.0 2.8 5 19.4 11.2 8.4 6.9 4.3 3.1 2.8 2.6 6 21.7 12.3 9.1 7.4 4.1 2.9 2.6 2.5 7 23.7 13.3 9.8 7.9 3.9 2.8 2.5 2.4 8 25.6 14.3 10.4 8.4 3.8 2.7 2.4 2.3 9 27.5 15.2 11.0 8.8 3.7 2.7 2.4 2.2 10 29.3 16.2 11.6 9.3 3.6 2.6 2.3 2.1 15 38.0 20.6 14.6 11.6 3.5 2.4 2.1 2.0 20 46.6 25.0 17.6 13.8 3.6 2.4 2.0 1.9 25 55.1 29.3 20.6 16.1 3.6 2.4 1.97 1.8 30 63.5 33.6 23.5 18.3 4.1 2.4 1.95 1.7 12.40 Example: Acemoglu,JohnsonandRobinson(2001) Oneparticularlywell-citedinstrumentalvariableregressionisinAcemoglu,JohnsonandRobinson (2001) with additional details published in (2012). They are interested in the effect of political insti- tutions on economic performance. The theory is that good institutions (rule-of-law, property rights) shouldresultinacountryhavinghigherlong-termeconomicoutputthanifthesamecountryhadpoor institutions. Toinvestigatethisquestiontheyfocusonasampleof64formerEuropeancolonies. Their dataisinthefileAJR2001onthetextbookwebsite. Theauthorsâpremiseisthatmodernpoliticalinstitutionshavebeeninfluencedbycolonization. In particulartheyarguethatcolonizingcountriestendedtosetupcoloniesaseitheranâextractivestateâor asaâmigrantcolonyâ.Anextractivestatewasusedbythecolonizertoextractresourcesforthecolonizing countrybutwasnotlargelysettledbytheEuropeancolonists.Inthiscasethecolonistshadnoincentive tosetupgoodpoliticalinstitutions. Incontrast,ifacolonywassetupasaâmigrantcolonyâthenlarge numbersofEuropeansettlersmigratedtothecolonytolive. Thesesettlersdesiredinstitutionssimilar to those in their home country and hence had an incentive to set up good political institutions. The natureofinstitutionsisquitepersistentovertimesothese19th-centuryfoundationsaffectthenatureof",
    "page": 420,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 401 moderninstitutions.Theauthorsconcludethatthe19th-centurynatureofthecolonyispredictiveofthe natureofmoderninstitutionsandhencemoderneconomicgrowth. TostarttheinvestigationtheyreportanOLSregressionoflogGDPpercapitain1995onameasure ofpoliticalinstitutionstheycallriskwhichisameasureoflegalprotectionagainstexpropriation. This variablerangesfrom0to10,with0thelowestprotectionagainstappropriationand10thehighest. For eachcountrytheauthorstaketheaveragevalueoftheindexover1985to1995(themeanis6.5witha standarddeviationof1.5).TheirreportedOLSestimates(interceptomitted)are log(GD(cid:225)PperCapita)= 0.52 risk. (12.86) (0.06) Theseestimatesimplya52%differenceinGDPbetweencountrieswitha1-unitdifferenceinrisk. Theauthorsarguethattheriskisendogenoussinceeconomicoutputinfluencespoliticalinstitutions andbecausethevariableriskisundoubtedlymeasuredwitherror.Theseissuesinduceleast-squarebias indifferentdirectionsandthustheoverallbiaseffectisunclear. Tocorrectforendogeneitybiastheauthorsarguetheneedforaninstrumentalvariablewhichdoes notdirectlyaffecteconomicperformanceyetisassociatedwithpoliticalinstitutions. Theirinnovative suggestion was to use the mortality rate which faced potential European settlers in the 19th century. ColonieswithhighexpectedmortalitywerelessattractivetoEuropeansettlersresultinginlowerlevels ofEuropeanmigrants. Asaconsequencetheauthorsexpectsuchcoloniestobemorelikelystructured asanextractivestateratherthanamigrantcolony. Tomeasuretheexpectedmortalityratetheauthors useestimatesprovidedbyhistoricalresearchoftheannualizeddeathsper1000soldiers,labeledmortal- ity. (Theyusedmilitarymortalityratesasthemilitarymaintainedhigh-qualityrecords.) Thefirst-stage regressionis risk= â0.61 log(mortality)+u. (12.87) (cid:98) (0.13) Theseestimatesconfirmthat19th-centuryhighmortalityratesareassociatedwithlowerqualitymodern institutions. Usinglog(mortality)asaninstrumentforrisk,theyestimatethestructuralequationusing 2SLSandreport log(GD(cid:225)PperCapita)= 0.94 risk. (12.88) (0.16) ThisestimateismuchhigherthantheOLSestimatefrom(12.86). Theestimateisconsistentwithanear doublingofGDPduetoa1-unitdifferenceintheriskindex. Thesearesimpleregressionsinvolvingjustoneright-hand-sidevariable. Theauthorsconsidereda rangeofothermodels. Includedintheseresultsareareversalofatraditionalfinding. Inaconventional (leastsquares)regressiontworelevantvariablesforoutputarelatitude(distancefromtheequator)and africa(adummyvariableforcountriesfromAfrica)bothofwhicharedifficulttointerpretcausally. But intheproposedinstrumentalvariablesregressionthevariableslatitudeandafricahavemuchsmallerâ andstatisticallyinsignificantâcoefficients. ToassessthespecificationwecanusetheStock-Yogoandendogeneitytests. TheStock-Yogotestis fromthereducedform(12.87). Theinstrumenthasat-ratioof4.8(orF =23)whichexceedstheStock- Yogocriticalvalueandhencecanbetreatedasstrong",
    "page": 421,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". But intheproposedinstrumentalvariablesregressionthevariableslatitudeandafricahavemuchsmallerâ andstatisticallyinsignificantâcoefficients. ToassessthespecificationwecanusetheStock-Yogoandendogeneitytests. TheStock-Yogotestis fromthereducedform(12.87). Theinstrumenthasat-ratioof4.8(orF =23)whichexceedstheStock- Yogocriticalvalueandhencecanbetreatedasstrong. Foranendogeneitytestwetaketheleastsquares residualufromthisequationandincludeitinthestructuralequationandestimatebyleastsquares.We (cid:98) findacoefficientonuofâ0.57withat-ratioof4.7whichishighlysignificant.Weconcludethattheleast (cid:98)",
    "page": 421,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 402 squares and 2SLS estimates are statistically different and reject the hypothesis that the variable risk is exogenousfortheGDPstructuralequation. InExercise12.23youwillreplicateandextendtheseresultsusingtheauthorsâdata. Thispaperisacreativeandcarefuluseofinstrumentalvariables. Thecreativitystemsfromthehis- toricalanalysiswhichleadtothefocusonmortalityasapotentialpredictorofmigrationchoices. The carecomesintheimplementationastheauthorsneededtogathercountry-leveldataonpoliticalinsti- tutionsandmortalityfromdistinctsources.Puttingthesepiecestogetheristheartoftheproject. 12.41 Example: AngristandKrueger(1991) AnotherinfluentialinstrumentvariableregressionisAngristandKrueger(1991).Theirconcern,sim- ilartoCard(1995),isestimationofthestructuralreturnstoeducationwhiletreatingeducationalattain- mentasendogenous.LikeCard,theirgoalistofindaninstrumentwhichisexogenousforwagesyethas animpactoneducation.AsubsetoftheirdatainthefileAK1991onthetextbookwebsite. Theircreativesuggestionwastofocusoncompulsoryschoolattendancepoliciesandtheirinterac- tion with birthdates. Compulsory schooling laws vary across states in the United States, but typically requirethatyouthremaininschooluntiltheirsixteenthorseventeenthbirthday. AngristandKrueger arguethatcompulsoryschoolinghasacausaleffectonwagesâyouthwhowouldhavechosentodrop outofschoolstayinschoolformoreyearsâandthushavemoreeducationwhichcausallyimpactstheir earningsasadults. AngristandKruegerobservethatthesepolicieshavedifferentialimpactonyouthwhoarebornearly orlateintheschoolyear. Studentswhoarebornearlyinthecalendaryeararetypicallyolderwhenthey enterschool. Consequentlywhentheyattainthelegaldropoutagetheyhaveattendedlessschoolthan thosebornneartheendoftheyear. Thismeansthatbirthdate(earlyinthecalendaryearversuslate) exogenouslyimpactseducationalattainmentandthuswagesthrougheducation. Yetbirthdatemustbe exogenous for the structural wage equation as there is no reason to believe that birthdate itself has a causalimpactonapersonâsabilityorwages. Theseconsiderationstogethersuggestthatbirthdateisa validinstrumentalvariableforeducationinacausalwageequation. Typical wage datasets include age but not birthdates. To obtain information on birthdate, Angrist andKruegerusedU.S.Censusdatawhichincludesanindividualâsquarterofbirth(January-March,April- June,etc.).Theyusethisvariabletoconstruct2SLSestimatesofthereturntoeducation. Theirpapercarefullydocumentsthateducationalattainmentvariesbyquarterofbirth(aspredicted bytheabovediscussion), andreportsalargesetofleastsquaresand2SLSestimates",
    "page": 422,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Typical wage datasets include age but not birthdates. To obtain information on birthdate, Angrist andKruegerusedU.S.Censusdatawhichincludesanindividualâsquarterofbirth(January-March,April- June,etc.).Theyusethisvariabletoconstruct2SLSestimatesofthereturntoeducation. Theirpapercarefullydocumentsthateducationalattainmentvariesbyquarterofbirth(aspredicted bytheabovediscussion), andreportsalargesetofleastsquaresand2SLSestimates. Wefocusontwo estimatesatthecoreoftheiranalysisreportedincolumn(6)oftheirTablesVandVII.Thisinvolvesdata fromthe1980censuswithmenbornin1930-1939,with329,509observations.Thefirstequationis lo(cid:225)g(wage)= 0.081 eduâ 0.230 Black+ 0.158 urban+ 0.244 married (12.89) (0.016) (0.026) (0.017) (0.005) where edu is years of education and Black, urban, and married are dummy variables indicating race (1ifBlack,0otherwise),livesinametropolitanarea,andifmarried. Inadditiontothereportedcoeffi- cientstheequationalsoincludesasregressorsnineyear-of-birthdummiesandeightregion-of-residence dummies.Theequationisestimatedby2SLS.Theinstrumentalvariablesarethe30interactionsofthree quarter-of-birthtimestenyear-of-birthdummyvariables. Thisequationindicatesan8%increaseinwagesduetoeachyearofeducation. Angrist and Krueger observe that the effect of compulsory education laws are likely to vary across states, soexpandtheinstrumentsettoincludeinteractionswithstate-of-birth. Theyestimatethefol-",
    "page": 422,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 403 lowingequationby2SLS lo(cid:225)g(wage)= 0.083 eduâ 0.233 Black+ 0.151 urban+ 0.244 married. (12.90) (0.009) (0.011) (0.009) (0.004) This equation also adds fifty state-of-birth dummy variables as regressors. The instrumental variables arethe180interactionsofquarter-of-birthtimesyear-of-birthdummyvariables, plusquarter-of-birth timesstate-of-birthinteractions. This equation shows a similar estimated causal effect of education on wages as in (12.89). More notably,thestandarderrorissmallerin(12.90)suggestingimprovedprecisionbytheexpandedinstru- mentalvariableset. However, these estimates seem excellent candidates for weak instruments and many instruments. Indeed, this paper (published in 1991) helped spark these two literatures. We can use the Stock-Yogo toolstoexploretheinstrumentstrengthandtheimplicationsfortheAngrist-Kruegerestimates. Wefirsttakeequation(12.89). UsingtheoriginalAngrist-Kruegerdataweestimatethecorrespond- ingreducedformandcalculatetheF statisticforthe30excludedinstruments.WefindF=4.8.Ithasan asymptoticp-valueof0.000suggestingthatwecanreject(atanysignificancelevel)thehypothesisthat the coefficients on the excluded instruments are zero. Thus Angrist and Krueger appear to be correct thatquarterofbirthhelpstoexplaineducationalattainmentandarethusavalidinstrumentalvariable set. However, usingtheStock-YogotestF=4.8isnothighenoughtorejectthehypothesisthatthein- strumentsareweak.Specifically,for(cid:96) =30thecriticalvaluefortheF statisticis45toboundsizebelow 2 15%. Theactualvalueof4.8isfarbelow45. Sincewecannotrejectthattheinstrumentsareweakthis indicatesthatwecannotinterpretthe2SLSestimatesandteststatisticsin(12.89)asreliable. Second, take (12.90) with the expanded regressor and instrument set. Estimating the correspond- ingreducedformwefindtheF statisticforthe180excludedinstrumentsisF=2.43whichalsohasan asymptotic p-value of 0.000 indicating that we can reject at any significance level the hypothesis that theexcludedinstrumentshavenoeffectoneducationalattainment.However,usingtheStock-Yogotest wealsocannotrejectthehypothesisthattheinstrumentsareweak. WhileStockandYogodidnotcal- culate the critical values for (cid:96) =180, the 2SLS critical values are increasing in (cid:96) so we can use those 2 2 for (cid:96) =30 as a lower bound. The observed value of F=2.43 is far below the level needed for signifi- 2 cance. Consequently the results in (12.90) cannot be viewed as reliable. In particular, the observation thatthestandarderrorsin(12.90)aresmallerthanthosein(12.89)shouldnotbeinterpretedasevidence ofgreaterprecision.Rather,theyshouldbeviewedasevidenceofunreliabilityduetoweakinstruments. WheninstrumentsareweakoneconstructivesuggestionistouseLIMLestimationratherthan2SLS. Anotherconstructivesuggestionistoaltertheinstrumentset. WhileAngristandKruegerusedalarge number of instrumental variables we can consider using a smaller set. Take equation (12.89)",
    "page": 423,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". In particular, the observation thatthestandarderrorsin(12.90)aresmallerthanthosein(12.89)shouldnotbeinterpretedasevidence ofgreaterprecision.Rather,theyshouldbeviewedasevidenceofunreliabilityduetoweakinstruments. WheninstrumentsareweakoneconstructivesuggestionistouseLIMLestimationratherthan2SLS. Anotherconstructivesuggestionistoaltertheinstrumentset. WhileAngristandKruegerusedalarge number of instrumental variables we can consider using a smaller set. Take equation (12.89). Rather than estimating it using the 30 interaction instruments consider using only the three quarter-of-birth dummyvariables.Wereportthereducedformestimateshere: e(cid:100)du=â 1.57 Black+ 1.05 urban+ 0.225 married+ 0.050 Q 2 + 0.101 Q 3 + 0.142 Q 4 (0.02) (0.01) (0.016) (0.016) (0.016) (0.016) (12.91) whereQ ,Q andQ aredummyvariablesforbirthinthe2nd,3rd,and4th quarter. Theregressionalso 2 3 4 includesnineyear-of-birthandeightregion-of-residencedummyvariables. Thereducedformcoefficientsin(12.91)onthequarter-of-birthdummiesareinstructive.Thecoeffi- cientsarepositiveandincreasing,consistentwiththeAngrist-Kruegerhypothesisthatindividualsborn laterintheyearachievehigheraverageeducation.FocusingontheweakinstrumentproblemtheF test",
    "page": 423,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 404 forexclusionofthesethreevariablesisF=31.TheStock-Yogocriticalvalueis12.8for(cid:96) =3andasizeof 2 15%,andis22.3forasizeof10%.SinceF=31exceedsboththesethresholdswecanrejectthehypothesis thatthisreducedformisweak.Estimatingthemodelby2SLSwiththesethreeinstrumentswefind lo(cid:225)g(wage)= 0.099 eduâ 0.201 Black+ 0.139 urban+ 0.240 married. (12.92) (0.021) (0.033) (0.022) (0.006) These estimates indicate a slightly larger (10%) causal impact of education on wages but with a larger standard error. The Stock-Yogo analysis indicates that we can interpret the confidence intervals from theseestimatesashavingasymptoticcoverage85%. WhiletheoriginalAngrist-Kruegerestimatessufferduetoweakinstrumentstheirpaperisaverycre- ativeandthoughtfulapplicationofthenaturalexperimentmethodology.Theydiscoveredacompletely exogenous variation present in the world â birthdate â and showed how this has a small but measur- ableeffectoneducationalattainmentandtherebyonearnings.Theircraftingofthisnaturalexperiment regression is extremely clever and demonstrates a style of analysis which can successfully underlie an effectiveinstrumentalvariablesempiricalanalysis. 12.42 Programming WenowpresentStatacodeforsomeoftheempiricalworkreportedinthischapter. StatadoFileforCardExample useCard1995.dta,clear setmoreoff genexp=age76-ed76-6 genexp2=(exp^2)/100 *Dropobservationswithmissingwage dropiflwage76==. *Table12.1regressions reglwage76ed76expexp2blackreg76rsmsa76r,r ivregress2slslwage76expexp2blackreg76rsmsa76r(ed76=nearc4),r ivregress2slslwage76blackreg76rsmsa76r(ed76expexp2=nearc4age76age2),rperfect ivregress2slslwage76expexp2blackreg76rsmsa76r(ed76=nearc4anearc4b),r ivregress 2sls lwage76 black reg76r smsa76r (ed76 exp exp2 = nearc4a nearc4b age76 age2), r perfect ivregresslimllwage76expexp2blackreg76rsmsa76r(ed76=nearc4anearc4b),r *Table12.2regressions reglwage76expexp2blackreg76rsmsa76rnearc4,r reged76expexp2blackreg76rsmsa76rnearc4,r reged76blackreg76rsmsa76rnearc4age76age2,r regexpblackreg76rsmsa76rnearc4age76age2,r regexp2blackreg76rsmsa76rnearc4age76age2,r reged76expexp2blackreg76rsmsa76rnearc4anearc4b,r reglwage76ed76expexp2smsa76rreg76r,r reglwage76nearc4expexp2smsa76rreg76r,r reged76nearc4expexp2smsa76rreg76r,r",
    "page": 424,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 405 StatadoFileforAcemoglu-Johnson-RobinsonExample useAJR2001.dta,clear regloggdprisk regrisklogmort0 predictu,residual ivregress2slsloggdp(risk=logmort0) regloggdprisku StatadoFileforAngrist-KruegerExample useAK1991.dta,clear ivregress2slslogwageblacksmsamarriedi.yobi.region(edu=i.qob#i.yob) ivregress 2sls logwage black smsa married i.yob i.region i.state (edu = i.qob#i.yobi.qob#i.state) regedublacksmsamarriedi.yobi.regioni.qob#i.yob testparmi.qob#i.yob regedublacksmsamarriedi.yobi.regioni.statei.qob#i.yobi.qob#i.state testparmi.qob#i.yobi.qob#i.state regedublacksmsamarriedi.yobi.regioni.qob testparmi.qob ivregress2slslogwageblacksmsamarriedi.yobi.region(edu=i.qob) _____________________________________________________________________________________________ 12.43 Exercises Exercise12.1 ConsiderthesingleequationmodelY =ZÎ²+ewhereY andZ arebothreal-valued(1Ã1). LetÎ² (cid:98)denotetheIVestimatorofÎ²usingasaninstrumentadummyvariableD (takesonlythevalues0 and1).FindasimpleexpressionfortheIVestimatorinthiscontext. Exercise12.2 Take the linear model Y = X (cid:48)Î²+e with (cid:69)[e|X] = 0. Suppose Ï2(x) = (cid:69)(cid:163) e2|X =x (cid:164) is known. Show that the GLS estimator of Î² can be written as an IV estimator using some instrument Z.(FindanexpressionforZ.) Exercise12.3 TakethelinearmodelY =X (cid:48)Î²+e. LettheOLSestimatorforÎ²beÎ² (cid:98)withOLSresiduale (cid:98)i . LettheIVestimatorforÎ²usingsomeinstrument Z beÎ² (cid:101)withIVresiduale (cid:101)i =Y i âX i (cid:48)Î² (cid:101). If X isindeed endogenous,willIVâfitâbetterthanOLSinthesensethat (cid:80)n e2<(cid:80)n e2,atleastinlargesamples? i=1(cid:101)i i=1(cid:98)i Exercise12.4 ThereducedformbetweentheregressorsX andinstrumentsZ takestheformX =Î(cid:48) Z+u whereX iskÃ1,Z is(cid:96)Ã1,andÎis(cid:96)Ãk.TheparameterÎisdefinedbythepopulationmomentcondition (cid:69)(cid:163) Zu (cid:48)(cid:164)=0.ShowthatthemethodofmomentsestimatorforÎisÎ (cid:98) =(cid:161) Z (cid:48) Z (cid:162)â1(cid:161) Z (cid:48) X (cid:162) . Exercise12.5 In the structural model Y = X (cid:48)Î²+e with X =Î(cid:48) Z +u and Î (cid:96)Ãk, (cid:96)â¥k, we claim that a necessary condition for Î² to be identified (can be recovered from the reduced form) is rank(Î)=k. Explainwhythisistrue.Thatis,showthatifrank(Î)<k thenÎ²isnotidentified.",
    "page": 425,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 406 Exercise12.6 ForTheorem12.3establishthatV(cid:98)Î² ââVÎ². p Exercise12.7 TakethelinearmodelY =X (cid:48)Î²+ewith(cid:69)[e|X]=0whereX andÎ²are1Ã1. (a) Showthat(cid:69)[Xe]=0and(cid:69)(cid:163) X2e (cid:164)=0.IsZ =(X X2) (cid:48) avalidinstrumentforestimationofÎ²? (b) Definethe2SLSestimatorofÎ²usingZ asaninstrumentforX.HowdoesthisdifferfromOLS? Exercise12.8 Supposethatpriceandquantityaredeterminedbytheintersectionofthelineardemand andsupplycurves Demand: Q=a +a P+a Y +e 0 1 2 1 Supply: Q=b +b P+b W +e 0 1 2 2 whereincome(Y)andwage(W)aredeterminedoutsidethemarket. Inthismodelaretheparameters identified? Exercise12.9 Consider the model Y = X (cid:48)Î²+e with (cid:69)[e|Z] =0 with Y scalar and X and Z each a k vector.Youhavearandomsample(Y ,X ,Z :i =1,...,n). i i i (a) AssumethatX isexogenousinthesensethat(cid:69)[e|Z,X]=0.IstheIVestimatorÎ² (cid:98)iv unbiased? (b) ContinuingtoassumethatX isexogenous,findtheconditionalcovariancematrixvar (cid:163)Î² (cid:98)iv |X,Z (cid:164) . Exercise12.10 Considerthemodel Y =X (cid:48)Î²+e X =Î(cid:48) Z+u (cid:69)[Ze]=0 (cid:69)(cid:163) Zu (cid:48)(cid:164)=0 with Y scalar and X and Z each a k vector. You have a random sample (Y ,X ,Z : i = 1,...,n). Take i i i thecontrolfunctionequatione =u (cid:48)Î³+Î½with(cid:69)[uÎ½]=0andassumeforsimplicitythatu isobserved. InsertingintothestructuralequationwefindY =Z (cid:48)Î²+u (cid:48)Î³+Î½. Thecontrolfunctionestimator(Î² (cid:98),Î³ (cid:98) )is OLSestimationofthisequation. (a) Showthat(cid:69)[XÎ½]=0(algebraically). (b) Derivetheasymptoticdistributionof(Î² (cid:98),Î³ (cid:98) ). Exercise12.11 Considerthestructuralequation Y =Î² +Î² X+Î² X2+e (12.93) 0 1 2 with X â (cid:82) treated as endogenous so that (cid:69)[Xe] (cid:54)= 0. We have an instrument Z â (cid:82) which satisfies (cid:69)[e|Z]=0soinparticular(cid:69)[e]=0,(cid:69)[Ze]=0and(cid:69)(cid:163) Z2e (cid:164)=0. (a) ShouldX2betreatedasendogenousorexogenous?",
    "page": 426,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 407 (b) SupposewehaveascalarinstrumentZ whichsatisfies X =Î³ +Î³ Z+u (12.94) 0 1 withuindependentofZ andmeanzero. Consider using (1,Z,Z2) as instruments. Is this a sufficient number of instruments? Is (12.93) just-identified,over-identified,orunder-identified? (c) WriteoutthereducedformequationforX2. Underwhatconditiononthereducedformparame- ters(12.94)aretheparametersin(12.93)identified? Exercise12.12 Considerthestructuralequationandreducedform Y =Î²X2+e X =Î³Z+u (cid:69)[Ze]=0 (cid:69)[Zu]=0 withX2treatedasendogenoussothat(cid:69)(cid:163) X2e (cid:164)(cid:54)=0. Forsimplicityassumenointercepts. Y,Z,andX are scalar.AssumeÎ³(cid:54)=0.Considerthefollowingestimator.First,estimateÎ³byOLSofX onZ andconstruct thefittedvaluesX(cid:98)i =Î³ (cid:98) Z i .Second,estimateÎ²byOLSofY i on (cid:161) X(cid:98)i (cid:162)2 . (a) WriteoutthisestimatorÎ² (cid:98)explicitlyasafunctionofthesample. (b) Finditsprobabilitylimitasnââ. (c) Ingeneral,isÎ² (cid:98)consistentforÎ²?IsthereareasonableconditionunderwhichÎ² (cid:98)isconsistent? Exercise12.13 ConsiderthestructuralequationY =Z (cid:48)Î² +Y (cid:48)Î² +e with(cid:69)[Ze]=0whereY isk Ã1 1 1 1 2 2 2 2 andtreatedasendogenous. Thevariables Z =(Z ,Z )aretreatedasexogenouswhere Z is(cid:96) Ã1and 1 2 2 2 (cid:96) â¥k .Youareinterestedintestingthehypothesis(cid:72) :Î² =0. 2 2 0 2 ConsiderthereducedformequationforY 1 Y =Z (cid:48)Î» +Z (cid:48)Î» +u . (12.95) 1 1 1 2 2 1 Showhowtotest(cid:72) usingonlytheOLSestimatesof(12.95). 0 Hint: Thiswillrequireananalysisofthereducedformequationsandtheirrelationtothestructural equation. Exercise12.14 TakethelinearinstrumentalvariablesequationY =Z (cid:48)Î² +Y (cid:48)Î² +ewith(cid:69)[Ze]=0where 1 1 1 2 2 Z isk Ã1, Y isk Ã1, and Z is(cid:96)Ã1, with(cid:96)â¥k =k +k .Thesamplesizeisn. AssumethatQ = 1 1 2 2 1 2 ZZ (cid:69)(cid:163) ZZ (cid:48)(cid:164)>0andQ =(cid:69)(cid:163) ZX (cid:48)(cid:164) hasfullrankk. ZX Supposethatonly(Y ,Z ,Z )areavailableandY ismissingfromthedataset. 1 1 2 2 Considerthe2SLSestimatorÎ² (cid:98)1 ofÎ² 1 obtainedfromthemisspecifiedIVregressionofY 1 onZ 1 only, usingZ asaninstrumentforZ . 2 1 (a) FindastochasticdecompositionÎ² (cid:98)1 =Î² 1 +b 1n +r 1n wherer 1n dependsontheerroreandb 1n does notdependontheerrore. (b) Showthatr â 0asnââ. 1n p",
    "page": 427,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 408 (c) Findtheprobabilitylimitofb 1n andÎ² (cid:98)1 asnââ. (d) DoesÎ² (cid:98)1 sufferfromâomittedvariablesbiasâ? Explain. Underwhatconditionsistherenoomitted variablesbias? (cid:112) (e) Findtheasymptoticdistributionasnââof n (cid:161)Î² (cid:98)1 âÎ² 1 âb 1n (cid:162) . Exercise12.15 Take the linear instrumental variables equation Y = ZÎ² +Y Î² +e with (cid:69)[e|Z] = 0 1 1 2 2 wherebothX andZ arescalar1Ã1. (a) Canthecoefficients(Î² ,Î² )beestimatedby2SLSusingZ asaninstrumentforY ? 1 2 2 Whyorwhynot? (b) Canthecoefficients(Î² ,Î² )beestimatedby2SLSusingZ andZ2asinstruments? 1 2 (c) Forthe2SLSestimatorsuggestedin(b),whatistheimplicitexclusionrestriction? (d) In(b)whatistheimplicitassumptionaboutinstrumentrelevance? [Hint:WritedowntheimpliedreducedformequationforY .] 2 (e) Inagenericapplicationwouldyoubecomfortablewiththeassumptionsin(c)and(d)? Exercise12.16 Takealinearequationwithendogeneityandajust-identifiedlinearreducedform Y = XÎ²+ewithX =Î³Z+u wherebothX andZ arescalar1Ã1.Assumethat(cid:69)[Ze]=0and(cid:69)[Zu ]=0. 2 2 (a) DerivethereducedformequationY =ZÎ»+u .ShowthatÎ²=Î»/Î³ifÎ³(cid:54)=0,andthat(cid:69)[Zu]=0. 1 (b) LetÎ» (cid:98)denotetheOLSestimatefromlinearregressionofY onZ,andletÎ³ (cid:98) denotetheOLSestimate (cid:112) from linear regression of X on Z. Write Î¸ =(Î»,Î³) (cid:48) and let Î¸ (cid:98) =(Î» (cid:98),Î³ (cid:98) ) (cid:48) . Define u =(u 1 ,u 2 ). Write n (cid:161)Î¸ (cid:98) âÎ¸(cid:162) usingasingleexpressionasafunctionoftheerroru. (c) Showthat(cid:69)[Zu]=0. (cid:112) (d) Derivethejointasymptoticdistributionof n (cid:161)Î¸ (cid:98) âÎ¸(cid:162) asnââ.Hint:Defineâ¦ u =(cid:69)(cid:163) Z2uu (cid:48)(cid:164) . (e) Using the previous result and the Delta Method find the asymptotic distribution of the Indirect LeastSquaresestimatorÎ² (cid:98) =Î» (cid:98)/Î³ (cid:98) . (f) Istheanswerin(e)thesameastheasymptoticdistributionofthe2SLSestimatorinTheorem12.2? (cid:181) (cid:182) Hint:Showthat (cid:161) 1 âÎ² (cid:162) u=eand (cid:161) 1 âÎ² (cid:162)â¦ 1 =(cid:69)(cid:163) Z2e2(cid:164) . u âÎ² Exercise12.17 Take the model Y = X (cid:48)Î²+e with (cid:69)[Ze] = 0 and consider the two-stage least squares estimator. The first-stage estimate is least squares of X on Z with least squares fitted values X(cid:98). The second-stage is least squares of Y on X(cid:98) with coefficient estimator Î² (cid:98)and least squares residuals e (cid:98)i = Y i âX(cid:98)i Î² (cid:98). ConsiderÏ (cid:98) 2= n 1 (cid:80)n i=1 e (cid:98)i 2asanestimatorforÏ2=(cid:69)(cid:163) e i 2(cid:164) .Isthisappropriate? Ifnot,proposean alternativeestimator.",
    "page": 428,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 409 Exercise12.18 Youhavetwoindependenti.i.d. samples(Y ,X ,Z :i =1,...,n)and(Y ,X ,Z :i = 1i 1i 1i 2i 2i 2i 1,...,n).ThedependentvariablesY andY arereal-valued. Theregressors X and X andinstruments 1 2 1 2 Z andZ arek-vectors.Themodelisstandardjust-identifiedlinearinstrumentalvariables 1 2 Y =X (cid:48)Î² +e 1 1 1 1 (cid:69)[Z e ]=0 1 1 Y =X (cid:48)Î² +e 2 2 2 2 (cid:69)[Z e ]=0. 2 2 Forconcreteness,sample1arewomenandsample2aremen. Youwanttotest(cid:72) :Î² =Î² ,thatthe 0 1 2 twosampleshavethesamecoefficients. (a) Developateststatisticfor(cid:72) . 0 (b) Derivetheasymptoticdistributionoftheteststatistic. (c) Describe(inbrief)thetestingprocedure. Exercise12.19 YouwanttousehouseholddatatoestimateÎ²inthemodelY =XÎ²+ewithX scalarand endogenous,usingasaninstrumentthestateofresidence. (a) Whataretheassumptionsneededtojustifythischoiceofinstrument? (b) Isthemodeljustidentifiedoroveridentified? Exercise12.20 ThemodelisY =X (cid:48)Î²+e with(cid:69)[Ze]=0. Aneconomistwantstoobtainthe2SLSesti- matesandstandarderrorsforÎ².Heusesthefollowingsteps â¢ RegressesX onZ,obtainsthepredictedvaluesX(cid:98). â¢ RegressesY onX(cid:98),obtainsthecoefficientestimateÎ² (cid:98)andstandarderrors(Î² (cid:98))fromthisregression. Isthiscorrect?Doesthisproducethe2SLSestimatesandstandarderrors? Exercise12.21 LetY =X 1 (cid:48)Î² 1 +X 2 (cid:48)Î² 2 +e.Let(Î² (cid:98)1 ,Î² (cid:98)2 )denotethe2SLSestimatesof(Î² 1 ,Î² 2 )whenZ 2 isused asaninstrumentforX 2 andtheyarethesamedimension(sothemodelisjustidentified).Let(Î» (cid:98)1 ,Î» (cid:98)2 )be theOLSestimatesfromtheregressionofY onX 1 andZ 2 .ShowthatÎ² (cid:98)1 =Î» (cid:98)1 . Exercise12.22 InthelinearmodelY =XÎ²+e withX â(cid:82)supposeÏ2(x)=(cid:69)(cid:163) e2|X =x (cid:164) isknown. Show thattheGLSestimatorofÎ²canbewrittenasaninstrumentalvariablesestimatorusingsomeinstrument Z.(FindanexpressionforZ.) Exercise12.23 You will replicate and extend the work reported in Acemoglu, Johnson and Robinson (2001).Theauthorsprovidedanexpandedsetofcontrolswhentheypublishedtheir2012extensionand postedthedataontheAERwebsite.ThisdatasetisAJR2001onthetextbookwebsite. (a) EstimatetheOLSregression(12.86),thereducedformregression(12.87),andthe2SLSregression (12.88). (Which point estimate is different by 0.01 from the reported values? This is a common phenomenoninempiricalreplication).",
    "page": 429,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 410 (b) Fortheaboveestimatescalculatebothhomoskedasticandheteroskedastic-robuststandarderrors. Whichwereusedbytheauthors(asreportedin(12.86)-(12.87)-(12.88)?) (c) Calculatethe2SLSestimatesbytheIndirectLeastSquaresformula.Aretheythesame? (d) Calculatethe2SLSestimatesbythetwo-stageapproach.Aretheythesame? (e) Calculatethe2SLSestimatesbythecontrolvariableapproach.Aretheythesame? (f) Acemoglu, Johnson and Robinson (2001) reported many specifications including alternative re- gressorcontrols, forexamplelatitude andafrica. Estimatebyleastsquarestheequationforlog- GDPaddinglatitudeandafricaasregressors.Doesthisregressionsuggestthatlatitudeandafrica arepredictiveofthelevelofGDP? (g) Now estimate the same equation as in (f) but by 2SLS using log(mortality) as an instrument for risk.Howdoestheinterpretationoftheeffectoflatitudeandafricachange? (h) Returntoourbaselinemodel(withoutincludinglatitudeandafrica). Theauthorsâreducedform equation uses log(mortality) as the instrument, rather than, say, the level of mortality. Estimate thereducedformforriskwithmortality astheinstrument. (Thisvariableisnotprovidedinthe datasetsoyouneedtotaketheexponentialoflog(mortality).) Canyouexplainwhytheauthors preferredtheequationwithlog(mortality)? (i) Try an alternative reduced form including both log(mortality) and the square of log(mortality). Interprettheresults. Re-estimatethestructuralequationby 2SLSusingbothlog(mortality)and itssquareasinstruments.Howdotheresultschange? (j) Fortheestimatesin(i)aretheinstrumentsstrongorweakusingtheStock-Yogotest? (k) Calculateandinterpretatestforexogeneityoftheinstruments. (l) EstimatetheequationbyLIMLusingtheinstrumentslog(mortality)andthesquareoflog(mortality). Exercise12.24 InExercise12.23youextendedtheworkreportedinAcemoglu,JohnsonandRobinson (2001). Considerthe2SLSregression(12.88). Computethestandarderrorsbothbytheasymptoticfor- mula and by the bootstrap using a large number (10,000) of bootstrap replications. Re-calculate the bootstrapstandarderrors.CommentonthereliabilityofbootstrapstandarderrorsforIVregression. Exercise12.25 You will replicate and extend the work reported in the chapter relating to Card (1995). The data is from the authorâs website and is posted as Card1995. The model we focus on is labeled 2SLS(a)inTable12.1whichusespublicandprivateasinstrumentsforedu. Thevariablesyouwillneed forthisexerciseincludelwage76,ed76,age76,smsa76r,reg76r,nearc2,nearc4,nearc4a,nearc4b.Seethe descriptionfilefordefinitions.Experienceisnotinthedataset,soneedstobegeneratedasageâeduâ6. (a) First, replicate the reduced form regression presented in the final column of Table 12.2, and the 2SLS regression described above (using public and private as instruments for edu) to verify that youhavethesamevariabledefintions. (b) Try a different reduced form model. The variable nearc2 means âgrew up near a 2-year collegeâ. Seeifaddingittothereducedformequationisuseful.",
    "page": 430,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER12. INSTRUMENTALVARIABLES 411 (c) Trymoreinteractionsinthereducedform.Createtheinteractionsnearc4a*age76andnearc4a*age762/100, andaddthemtothereducedformequation. Estimatethisbyleastsquares. Interpretthecoeffi- cientsonthetwonewvariables. (d) Estimatethestructuralequationby2SLSusingtheexpandedinstrumentset {nearc4a,nearc4b,nearc4a*age76,nearc4a*age762/100}. Whatistheimpactonthestructuralestimateofthereturntoschooling? (e) UsingtheStock-Yogotestaretheinstrumentsstrongorweak? (f) Testthehypothesisthateduisexogenousforthestructuralreturntoschooling. (g) Re-estimatethelastequationbyLIML.Dotheresultschangemeaningfully? Exercise12.26 InExercise12.25youextendedtheworkreportedinCard(1995). Now, estimatetheIV equationcorrespondingtotheIV(a)columnofTable12.1whichisthebaselinespecificationconsidered inCard. UsethebootstraptocalculateaBCpercentileconfidenceinterval. Inthisexampleshouldwe alsoreportthebootstrapstandarderror? Exercise12.27 You will extend Angrist and Krueger (1991) using the data file AK1991 on the textbook website.. Their Table VIII reports estimates of an analog of (12.90) for the subsample of 26,913 Black men.Usethissub-sampleforthefollowinganalysis. (a) Estimate an equation which is identical in form to (12.90) with the same additional regressors (year-of-birth,region-of-residence,andstate-of-birthdummyvariables)and180excludedinstru- mental variables (the interactions of quarter-of-birth times year-of-birth dummy variables and quarter-of-birth times state-of-birth interactions) but use the subsample of Black men. One re- gressormustbeomittedtoachieveidentification.Whichvariableisthis? (b) Estimatethereducedformfortheaboveequationbyleastsquares.CalculatetheF statisticforthe excludedinstruments.Whatdoyouconcludeaboutthestrengthoftheinstruments? (c) Repeat,estimatingthereducedformfortheanalogof(12.89)whichhas30excludedinstrumental variablesanddoesnotincludethestate-of-birthdummyvariablesintheregression. Whatdoyou concludeaboutthestrengthoftheinstruments? (d) Repeat, estimating the reduced form for the analog of (12.92) which has only 3 excluded instru- mentalvariables. Aretheinstrumentssufficientlystrongfor2SLSestimation? ForLIMLestima- tion? (e) Estimate the structural wage equation using what you believe is the most appropriate set of re- gressors,instruments,andthemostappropriateestimationmethod.Whatistheestimatedreturn toeducation(forthesubsampleofBlackmen)anditsstandarderror? Withoutdoingaformalhy- pothesistest,dotheseresults(orinwhichway?)appearmeaningfullydifferentfromtheresultsfor thefullsample? Exercise12.28 In Exercise 12.27 you extended the work reported in Angrist and Krueger (1991) by es- timatingwageequationsforthe subsampleofBlack men. Re-estimateequation(12.92) for this group usingasinstrumentsonlythethreequarter-of-birthdummyvariables. Calculatethestandarderrorfor thereturntoeducationbyasymptoticandbootstrapmethods.CalculateaBCpercentileinterval.Inthis applicationof2SLSisitappropriatetoreportthebootstrapstandarderror?",
    "page": 431,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Chapter 13 Generalized Method of Moments 13.1 Introduction One of the most popular estimation methods in applied econometrics is the Generalized Method of Moments (GMM). GMM generalizes classical method of moments by allowing for more equations than unknown parameters (so are overidentified) and by allowing general nonlinear functions of the observations and parameters. Together this allows for a fairly rich and flexible estimation framework. GMM includes as special cases OLS, IV, multivariate regression, and 2SLS. It includes both linear and nonlinearmodels.Inthischapterwefocusprimarilyonlinearmodels. TheGMMlabelandmethodswereintroducedtoeconometricsinaseminalpaperbyLarsHansen (1982). The ideas and methods build on the work of Amemiya (1974, 1977), Gallant (1977), and Gal- lantandJorgenson(1979). TheideasarecloselyrelatedtothecontemporeneousworkofHalbertWhite (1980,1982)andWhiteandDomowitz(1984).Themethodsarealsorelatedtowhatarecalledestimating equationsinthestatisticsliterature.ForareviewofthelatterseeGodambe(1991). 13.2 MomentEquationModels Allofthemodelsthathavebeenintroducedsofarcanbewrittenasmomentequationmodelswhere thepopulationparameterssolveasystemofmomentequations. Momentequationmodelsarebroader thanthemodelssofarconsideredandunderstandingtheircommonstructureopensupstraightforward techniquestohandleneweconometricmodels. Momentequationmodelstakethefollowingform.Letg (Î²)beaknown(cid:96)Ã1functionoftheith ob- i servationandakÃ1parameterÎ².Amomentequationmodelissummarizedbythemomentequations (cid:69)(cid:163) g (Î²) (cid:164)=0 (13.1) i andaparameterspaceÎ²âB.Forexample,intheinstrumentalvariablesmodelg (cid:161)Î²(cid:162)=Z (cid:161) Y âX (cid:48)Î²(cid:162) . i i i i Ingeneral,wesaythataparameterÎ²isidentifiedifthereisauniquemappingfromthedatadistri- butiontoÎ².Inthecontextofthemodel(13.1)thismeansthatthereisauniqueÎ²satisfying(13.1).Since (13.1)isasystemof(cid:96)equationswithkunknowns,thenitisnecessarythat(cid:96)â¥kfortheretobeaunique solution. If(cid:96)=k wesaythatthemodelisjustidentified,meaningthatthereisjustenoughinformation toidentifytheparameters. If(cid:96)>k wesaythatthemodelisoveridentified,meaningthatthereisexcess information. If(cid:96)<k wesaythatthemodelisunderidentified,meaningthatthereisinsufficientinfor- mationtoidentifytheparameters.Ingeneral,weassumethat(cid:96)â¥k sothemodeliseitherjustidentified oroveridentified. 412",
    "page": 432,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER13. GENERALIZEDMETHODOFMOMENTS 413 13.3 MethodofMomentsEstimators Inthissectionweconsiderthejust-identifiedcase(cid:96)=k. Definethesampleanalogof(13.5) 1 (cid:88) n g (Î²)= g (Î²). (13.2) n n i i=1 Themethodofmomentsestimator(MME)Î² (cid:98)mm istheparametervaluewhichsetsg n (Î²)=0.Thus 1 (cid:88) n g n (Î² (cid:98)mm )= n g i (Î² (cid:98)mm )=0. (13.3) i=1 Theequations(13.3)areknownastheestimatingequationsastheyaretheequationswhichdetermine theestimatorÎ² (cid:98)mm . In some contexts (such as those discussed in the examples below) there is an explicit solution for Î² (cid:98)mm .Inothercasesthesolutionmustbefoundnumerically. Wenowshowhowmostoftheestimatorsdiscussedsofarinthetextbookcanbewrittenasmethod ofmomentsestimators. Mean:Setg (cid:161)Âµ(cid:162)=Y âÂµ.TheMMEisÂµ= 1(cid:80)n Y . i i (cid:98) n i=1 i MeanandVariance:Set (cid:195) (cid:33) g i (cid:161)Âµ,Ï2(cid:162)= (cid:161) Y â Y i Âµ â (cid:162)2 Âµ âÏ2 . i TheMMEareÂµ= 1(cid:80)n Y andÏ2= 1(cid:80)n (cid:161) Y âÂµ(cid:162)2 . (cid:98) n i=1 i (cid:98) n i=1 i (cid:98) OLS:Setg i (cid:161)Î²(cid:162)=X i (cid:161) Y i âX i (cid:48)Î²(cid:162) .TheMMEisÎ² (cid:98) =(cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48) Y (cid:162) . OLSandVariance:Set g i (cid:161)Î²,Ï2(cid:162)= (cid:195) (cid:161) Y X i â (cid:161) Y X i (cid:48) â Î²(cid:162) X 2 i (cid:48) â Î² Ï (cid:162) 2 (cid:33) . i i TheMMEisÎ² (cid:98) =(cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48) Y (cid:162) andÏ (cid:98) 2= n 1(cid:80)n i=1 (cid:161) Y i âX i (cid:48)Î² (cid:98) (cid:162)2 . MultivariateLeastSquares,vectorform:Setg i (cid:161)Î²(cid:162)=X (cid:48) i (cid:179) Y i âX i Î² (cid:180) .TheMMEisÎ² (cid:98) = (cid:179) (cid:80)n i=1 X (cid:48) i X i (cid:180)â1(cid:179) (cid:80)n i=1 X i Y i (cid:180) whichis(11.4). MultivariateLeastSquares,matrixform:Setg i (B)=vec (cid:161) X i (cid:161) Y i (cid:48)âX i (cid:48) B (cid:162)(cid:162) .TheMMEisB(cid:98) =(cid:161)(cid:80)n i=1 X i X i (cid:48)(cid:162)â1(cid:161)(cid:80)n i=1 X i Y i (cid:48)(cid:162) whichis(11.6). SeeminglyUnrelatedRegression:Set ï£« X Î£â1 (cid:179) Y âX (cid:48) Î² (cid:180) ï£¶ g i (cid:161)Î²,Î£(cid:162)= ï£­ (cid:179) (cid:179) i (cid:48) i (cid:180)(cid:179) i (cid:48) (cid:180)(cid:48)(cid:180) ï£¸. vec Î£â Y âX Î² Y âX Î² i i i i TheMMEisÎ² (cid:98) = (cid:179) (cid:80)n i=1 X i Î£ (cid:98) â1X (cid:48) i (cid:180)â1(cid:179) (cid:80)n i=1 X i Î£ (cid:98) â1Y i (cid:180) andÎ£ (cid:98) =n â1(cid:80)n i=1 (cid:179) Y i âX (cid:48) i Î² (cid:98) (cid:180)(cid:179) Y i âX (cid:48) i Î² (cid:98) (cid:180)(cid:48) . IV:Setg i (cid:161)Î²(cid:162)=Z i (cid:161) Y i âX i (cid:48)Î²(cid:162) .TheMMEisÎ² (cid:98) =(cid:161) Z (cid:48) X (cid:162)â1(cid:161) Z (cid:48) Y (cid:162) .",
    "page": 433,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER13. GENERALIZEDMETHODOFMOMENTS 414 GeneratedRegressors:Set g i (cid:161)Î²,A (cid:162)= (cid:181) v A ec (cid:48) Z (cid:161) Z i (cid:161) Y (cid:161) X i â (cid:48)â Z i Z (cid:48) A (cid:48) Î² A (cid:162) (cid:162)(cid:162) (cid:182) . i i i TheMMEis A(cid:98) =(cid:161)(cid:80)n i=1 Z i Z i (cid:48)(cid:162)â1(cid:161)(cid:80)n i=1 Z i X i (cid:48)(cid:162) andÎ² (cid:98) = (cid:179) A(cid:98) (cid:48) Z (cid:48) ZA(cid:98) (cid:180)â1(cid:179) A(cid:98) (cid:48) Z (cid:48) Y (cid:180) . Acommonfeatureoftheseexamplesisthattheestimatorcanbewrittenasthesolutiontoasetof estimatingequations(13.3). Thisprovidesacommonframeworkwhichenablesaconvenientdevelop- mentofaunifieddistributiontheory. 13.4 OveridentifiedMomentEquations Intheinstrumentalvariablesmodelg (Î²)=Z (cid:161) Y âX (cid:48)Î²(cid:162) .Thus(13.2)is i i i i g (Î²)= 1 (cid:88) n g (Î²)= 1 (cid:88) n Z (cid:161) Y âX (cid:48)Î²(cid:162)= 1(cid:161) Z (cid:48) Y âZ (cid:48) XÎ²(cid:162) . (13.4) n n i n i i i n i=1 i=1 We have defined the method of moments estimator for Î² as the parameter value which sets g (Î²) = n 0. However, when the model is overidentified (if (cid:96)>k) this is generally impossible as there are more equationsthanfreeparameters. Equivalently,thereisnochoiceofÎ²whichsets(13.4)tozero. Thusthe methodofmomentsestimatorisnotdefinedfortheoveridentifiedcase. While we cannot find an estimator which sets g (Î²) equal to zero we can try to find an estimator n whichmakesg (Î²)asclosetozeroaspossible. n One way to think about this is to define the vector Âµ = Z (cid:48) Y, the matrix G = Z (cid:48) X and the âerrorâ Î·=ÂµâGÎ². Thenwecanwrite(13.4)asÎ²=GÎ²+Î·. Thislookslikearegressionequationwiththe(cid:96)Ã1 dependentvariableÂµ,the(cid:96)Ãk regressormatrixG,andthe(cid:96)Ã1errorvectorÎ·. Thegoalistomakethe errorvectorÎ·assmallaspossible.Recallingourknowledgeaboutleastsquareswedeductthatasimple methodistoregressÂµonG,obtainingÎ² (cid:98) =(cid:161) G (cid:48) G (cid:162)â1(cid:161) G (cid:48)Âµ(cid:162) . Thisminimizesthesum-of-squaresÎ·(cid:48)Î·. This iscertainlyonewaytomakeÎ·âsmallâ. Moregenerallyweknowthatwhenerrorsarenon-homogeneousitcanbemoreefficienttoestimate byweightedleastsquares.ThusforsomeweightmatrixW considertheestimator Î² (cid:98) =(cid:161) G (cid:48) WG (cid:162)â1(cid:161) G (cid:48) WÂµ(cid:162)=(cid:161) X (cid:48) ZWZ (cid:48) X (cid:162)â1(cid:161) X (cid:48) ZWZ (cid:48) Y (cid:162) . ThisminimizestheweightedsumofsquaresÎ·(cid:48) WÎ·. Thissolutionisknownasthegeneralizedmethod ofmoments(GMM). The estimator is typically defined as follows. Given a set of moment equations (13.2) and an (cid:96)Ã(cid:96) weightmatrixW >0theGMMcriterionfunctionisdefinedas J(Î²)=ng (Î²) (cid:48) W g (Î²). n n Thefactorânâisnotimportantforthedefinitionoftheestimatorbutisconvenientforthedistribution theory. Thecriterion J(Î²)istheweightedsumofsquaredmomentequationerrors. WhenW =I(cid:96) then J(Î²)=ng (Î²) (cid:48) g (Î²)=n (cid:176) (cid:176)g (Î²) (cid:176) (cid:176) 2 , the square of the Euclidean length",
    "page": 434,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". The estimator is typically defined as follows. Given a set of moment equations (13.2) and an (cid:96)Ã(cid:96) weightmatrixW >0theGMMcriterionfunctionisdefinedas J(Î²)=ng (Î²) (cid:48) W g (Î²). n n Thefactorânâisnotimportantforthedefinitionoftheestimatorbutisconvenientforthedistribution theory. Thecriterion J(Î²)istheweightedsumofsquaredmomentequationerrors. WhenW =I(cid:96) then J(Î²)=ng (Î²) (cid:48) g (Î²)=n (cid:176) (cid:176)g (Î²) (cid:176) (cid:176) 2 , the square of the Euclidean length. Since we restrict attention to n n n positivedefiniteweightmatricesW thecriterionJ(Î²)isnon-negative. Definition13.1 TheGeneralizedMethodofMoments(GMM)estimatoris Î² (cid:98)gmm =argminJ (cid:161)Î²(cid:162) . Î²",
    "page": 434,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER13. GENERALIZEDMETHODOFMOMENTS 415 Recallthatinthejust-identifiedcasek=(cid:96)themethodofmomentsestimatorÎ² (cid:98)mm solvesg n (Î² (cid:98)mm )= 0. Henceinthiscase J (cid:161)Î² (cid:98)mm (cid:162)=0whichmeansthatÎ² (cid:98)mm minimizes J (cid:161)Î²(cid:162) andequalsÎ² (cid:98)gmm =Î² (cid:98)mm . This meansthatGMMincludesMMEasaspecialcase. ThisimpliesthatallofourresultsforGMMapplyto anymethodofmomentsestimator. Intheover-identifiedcasetheGMMestimatordependsonthechoiceofweightmatrixW andsothis isanimportantfocusofthetheory.Inthejust-identifiedcasetheGMMestimatorsimplifiestotheMME whichdoesnotdependonW. Themethodandtheoryofthegeneralizedmethodofmomentswasdevelopedinaninfluentialpaper byLarsHansen(1982). Thispaperintroducedthemethod,itsasymptoticdistribution,theformofthe efficientweightmatrix,andtestsforoveridentification. 13.5 LinearMomentModels Oneofthegreatadvantagesofthemomentequationframeworkisthatitallowsbothlinearandnon- linearmodels.However,whenthemomentequationsarelinearintheparametersthenwehaveexplicit solutionsfortheestimatesandastraightforwardasymptoticdistributiontheory.Hencewestartbycon- fining attention to linear moment equations and return to nonlinear moment equations later. In the exampleslistedearliertheestimatorswhichhavelinearmomentequationsincludethesamplemean, OLS, multivariate least squares, IV, and 2SLS. The estimates which have nonlinear moment equations includethesamplevariance,SUR,andgeneratedregressors. Inparticular,wefocusontheoveridentifiedIVmodelwithmomentequations g (Î²)=Z (Y âX (cid:48)Î²) (13.5) i i i i whereZ is(cid:96)Ã1andX iskÃ1. i i 13.6 GMMEstimator Given(13.5)thesamplemomentequationsare(13.4).TheGMMcriterioncanbewrittenas J(Î²)=n (cid:161) Z (cid:48) Y âZ (cid:48) XÎ²(cid:162)(cid:48) W (cid:161) Z (cid:48) Y âZ (cid:48) XÎ²(cid:162) . TheGMMestimatorminimizesJ(Î²).Thefirstorderconditionsare â 0= J(Î² (cid:98)) âÎ² â =2 g (Î² (cid:98)) (cid:48) Wg (Î² (cid:98)) âÎ² n n (cid:181) (cid:182) (cid:181) (cid:182) =â2 1 X (cid:48) Z W 1 Z (cid:48)(cid:161) Y âXÎ² (cid:98) (cid:162) . n n Thesolutionisgivenasfollows. Theorem13.1 FortheoveridentifiedIVmodel Î² (cid:98)gmm =(cid:161) X (cid:48) ZWZ (cid:48) X (cid:162)â1(cid:161) X (cid:48) ZWZ (cid:48) Y (cid:162) . (13.6)",
    "page": 435,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER13. GENERALIZEDMETHODOFMOMENTS 416 While theestimatordependsonW thedependenceisonly uptoscale. ThisisbecauseifW isre- placedbycW forsomec>0,Î² (cid:98)gmm doesnotchange.WhenW isfixedbytheuserwecallÎ² (cid:98)gmm aone-step GMMestimator. Theformula(13.6)appliesfortheover-identified((cid:96)>k)andthejust-identified((cid:96)=k) case.Whenthemodelisjust-identifiedthenX (cid:48) Z iskÃk soexpression(13.6)simplifiesto Î² (cid:98)gmm =(cid:161) Z (cid:48) X (cid:162)â1 W â1(cid:161) X (cid:48) Z (cid:162)â1(cid:161) X (cid:48) ZWZ (cid:48) Y (cid:162)=(cid:161) Z (cid:48) X (cid:162)â1(cid:161) Z (cid:48) Y (cid:162)=Î² (cid:98)iv theIVestimator. TheGMMestimator(13.6)resemblesthe2SLSestimator(12.29). InfacttheyareequalwhenW = (cid:161) (cid:48) (cid:162)â1 Z Z .Thismeansthatthe2SLSestimatorisaone-stepGMMestimatorforthelinearmodel. Theorem13.2 IfW =(cid:161) Z (cid:48) Z (cid:162)â1 then Î² (cid:98)gmm =Î² (cid:98)2sls . Furthermore, ifk =(cid:96) then Î² (cid:98)gmm =Î² (cid:98)iv . 13.7 DistributionofGMMEstimator LetQ=(cid:69)(cid:163) ZX (cid:48)(cid:164) andâ¦=(cid:69)(cid:163) ZZ (cid:48) e2(cid:164) .Then (cid:181) (cid:182) (cid:181) (cid:182) 1 X (cid:48) Z W 1 Z (cid:48) X ââQ (cid:48) WQ n n p and (cid:181) (cid:182) (cid:181) (cid:182) 1 X (cid:48) Z W (cid:112) 1 Z (cid:48) e ââQ (cid:48) W N(0,â¦). n n d Weconclude: Theorem13.3 AsymptoticDistributionofGMMEstimator. UnderAssump- (cid:112) tion12.2,asnââ, n (cid:161)Î² (cid:98)gmm âÎ²(cid:162)ââN (cid:161) 0,VÎ² (cid:162) where d VÎ² =(cid:161) Q (cid:48) WQ (cid:162)â1(cid:161) Q (cid:48) Wâ¦WQ (cid:162)(cid:161) Q (cid:48) WQ (cid:162)â1 . (13.7) TheGMMestimatorisasymptoticallynormalwithaâsandwichformâasymptoticvariance. OurderivationtreatedtheweightmatrixW asifitisnon-randombutTheorem13.3appliestothe random weight matrix case so long asW(cid:99) converges in probability to a positive definite limitW. This mayrequirescalingtheweightmatrix,forexamplereplacingW(cid:99) =(cid:161) Z (cid:48) Z (cid:162)â1 withW(cid:99) =(cid:161) n â1Z (cid:48) Z (cid:162)â1 . Since rescalingtheweightmatrixdoesnotaffecttheestimatorthisisignoredinimplementation.",
    "page": 436,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER13. GENERALIZEDMETHODOFMOMENTS 417 13.8 EfficientGMM TheasymptoticdistributionoftheGMMestimatorÎ² (cid:98)gmm dependsontheweightmatrixW through theasymptoticvarianceVÎ². TheasymptoticallyoptimalweightmatrixW 0 isthatwhichminimizesVÎ². ThisturnsouttobeW =â¦â1.TheproofislefttoExercise13.4. 0 WhentheGMMestimatorÎ² (cid:98)isconstructedwithW =W 0 =â¦â1 (oraweightmatrixwhichisacon- sistentestimatorofW )wecallittheEfficientGMMestimator: 0 Î² (cid:98)gmm =(cid:161) X (cid:48) Zâ¦â1Z (cid:48) X (cid:162)â1(cid:161) X (cid:48) Zâ¦â1Z (cid:48) Y (cid:162) . ItsasymptoticdistributiontakesasimplerformthaninTheorem13.3. BysubstitutingW =W =â¦â1 0 into(13.7)wefind VÎ² =(cid:161) Q (cid:48)â¦â1Q (cid:162)â1(cid:161) Q (cid:48)â¦â1â¦â¦â1Q (cid:162)(cid:161) Q (cid:48)â¦â1Q (cid:162)â1=(cid:161) Q (cid:48)â¦â1Q (cid:162)â1 . ThisistheasymptoticvarianceoftheefficientGMMestimator. Theorem13.4 Asymptotic Distribution of GMM with Efficient Weight Ma- (cid:112) trix. Under Assumption 12.2 and W = â¦â1, as n â â, n (cid:161)Î² (cid:98)gmm âÎ²(cid:162) ââ d N (cid:161) 0,VÎ² (cid:162) whereVÎ² =(cid:161) Q (cid:48)â¦â1Q (cid:162)â1 . Theorem13.5 EfficientGMM.UnderAssumption12.2,foranyW >0, (cid:161) Q (cid:48) WQ (cid:162)â1(cid:161) Q (cid:48) Wâ¦WQ (cid:162)(cid:161) Q (cid:48) WQ (cid:162)â1â(cid:161) Q (cid:48)â¦â1Q (cid:162)â1â¥0. The inequality ââ¥âcanbe replaced withâ>âifW (cid:54)=â¦â1. ThusifÎ² (cid:98)gmm is the efficientGMMestimatorandÎ² (cid:101)gmm isanotherGMMestimator,then avar (cid:163)Î² (cid:98)gmm (cid:164)â¤avar (cid:163)Î² (cid:101)gmm (cid:164) . ForaproofseeExercise13.4. This means that the smallest possible GMM covariance matrix (in the positive definite sense) is achievedbytheefficientGMMweightmatrix. W =â¦â1isnotknowninpracticebutitcanbeestimatedconsistentlyaswediscussinSection13.10. 0 ForanyW(cid:99) ââW 0 theasymptoticdistributioninTheorem13.4isunaffected. Consequentlywecallany p Î² (cid:98)gmm constructedwithanestimateoftheefficientweightmatrixanefficientGMMestimator. ByâefficientâwemeanthatthisestimatorhasthesmallestasymptoticvarianceintheclassofGMM estimatorswiththissetofmomentconditions. Thisisaweakconceptofoptimalityasweareonlycon- sideringalternativeweightmatricesW(cid:99).However,itturnsoutthattheGMMestimatorissemiparamet- ricallyefficientasshownbyGaryChamberlain(1987). Ifitisknownthat(cid:69)(cid:163) g (Î²) (cid:164)=0andthisisallthat i is known this is a semi-parametric problem as the distribution of the data is unknown. Chamberlain showedthatinthiscontextnosemiparametricestimator(onewhichisconsistentgloballyfortheclass",
    "page": 437,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER13. GENERALIZEDMETHODOFMOMENTS 418 ofmodelsconsidered)canhaveasmallerasymptoticvariancethan (cid:161) G (cid:48)â¦â1G (cid:162)â1 whereG=(cid:69) (cid:104) â g (Î²) (cid:105) . âÎ²(cid:48) i SincetheGMMestimatorhasthisasymptoticvarianceitissemiparametricallyefficient. Theresultsinthissectionshowthatinthelinearmodelnoestimatorhasbetterasymptoticefficiency thantheefficientlinearGMMestimator.Noestimatorcandobetter(inthisfirst-orderasymptoticsense) withoutimposingadditionalassumptions. 13.9 EfficientGMMversus2SLS For the linear model we introduced 2SLS as a standard estimator for Î². Now we have introduced GMMwhichincludes2SLSasaspecialcase.Isthereacontextwhere2SLSisefficient? To answer this question recall that 2SLS is GMM given the weight matrix W(cid:99) = (cid:161) Z (cid:48) Z (cid:162)â1 or equiv- alently W(cid:99) = (cid:161) n â1Z (cid:48) Z (cid:162)â1 since scaling doesnât matter. Since W(cid:99) ââ (cid:161)(cid:69)(cid:163) ZZ (cid:48)(cid:164)(cid:162)â1 this is asymptotically p equivalenttotheweightmatrixW =(cid:161)(cid:69)(cid:163) ZZ (cid:48)(cid:164)(cid:162)â1 . Incontrast,theefficientweightmatrixtakestheform (cid:161)(cid:69)(cid:163) ZZ (cid:48) e2(cid:164)(cid:162)â1 . Nowsupposethatthestructuralequationerrore isconditionallyhomoskedasticinthe sensethat(cid:69)(cid:163) e2|Z (cid:164)=Ï2. ThentheefficientweightmatrixequalsW =(cid:161)(cid:69)(cid:163) ZZ (cid:48)(cid:164)(cid:162)â1Ïâ2 orequivalently W =(cid:161)(cid:69)(cid:163) ZZ (cid:48)(cid:164)(cid:162)â1 sincescalingdoesnâtmatter.Thelatterweightmatrixisthesameasthe2SLSasymptotic weightmatrix. Thisshowsthatthe2SLSweightmatrixistheefficientweightmatrixunderconditional homoskedasticity. Theorem13.6 Under Assumption 12.2 and (cid:69)(cid:163) e2|Z (cid:164) = Ï2, Î² (cid:98)2sls is efficient GMM. Thisshowsthat2SLSisefficientunderhomoskedasticity. Whenhomoskedasticityholdsthereisno reasontouseefficientGMMover2SLS.Morebroadly,whenhomoskedasticityisareasonableapproxi- mationthen2SLSwillbeareasonableestimator.However,thisresultalsoshowsthatinthegeneralcase wheretheerrorisconditionallyheteroskedastic,2SLSisinefficientrelativetoefficientGMM. 13.10 EstimationoftheEfficientWeightMatrix ToconstructtheefficientGMMestimatorweneedaconsistentestimatorW(cid:99)ofW 0 =â¦â1. Thecon- ventionistoformanestimatorâ¦ (cid:98) ofâ¦andthensetW(cid:99) =â¦ (cid:98) â1. Thetwo-stepGMMestimatorproceedsbyusingaone-stepconsistentestimatorofÎ²toconstruct the weight matrix estimator W(cid:99). In the linear model the natural one-step estimator for Î² is 2SLS. Set e (cid:101)i =Y i âX i (cid:48)Î² (cid:98)2sls ,g (cid:101)i =g i (Î² (cid:101))=Z i e (cid:101)i ,andg n =n â1(cid:80)n i=1 g (cid:101)i .Twomomentestimatorsofâ¦are â¦ (cid:98) = n 1 (cid:88) n g (cid:101)i g (cid:101)i (cid:48) (13.8) i=1 and â¦ (cid:98) â= n 1 (cid:88) n (cid:161) g (cid:101)i âg n (cid:162)(cid:161) g (cid:101)i âg n (cid:162)(cid:48)",
    "page": 438,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". In the linear model the natural one-step estimator for Î² is 2SLS. Set e (cid:101)i =Y i âX i (cid:48)Î² (cid:98)2sls ,g (cid:101)i =g i (Î² (cid:101))=Z i e (cid:101)i ,andg n =n â1(cid:80)n i=1 g (cid:101)i .Twomomentestimatorsofâ¦are â¦ (cid:98) = n 1 (cid:88) n g (cid:101)i g (cid:101)i (cid:48) (13.8) i=1 and â¦ (cid:98) â= n 1 (cid:88) n (cid:161) g (cid:101)i âg n (cid:162)(cid:161) g (cid:101)i âg n (cid:162)(cid:48) . (13.9) i=1 The estimator (13.8) is an uncentered covariance matrix estimator while the estimator (13.9) is a centeredversion.Eitherisconsistentwhen(cid:69)[Ze]=0whichholdsundercorrectspecification.However undermisspecificationwemayhave(cid:69)[Ze](cid:54)=0.Inthelattercontextâ¦ (cid:98) â remainsanestimatorofvar[Ze]",
    "page": 438,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER13. GENERALIZEDMETHODOFMOMENTS 419 whileâ¦ (cid:98) isanestimatorof(cid:69)(cid:163) ZZ (cid:48) e2(cid:164) . Inthissenseâ¦ (cid:98) â isarobustvarianceestimator. Forsometesting problemsitturnsouttobepreferabletouseacovariancematrixestimatorwhichisrobusttothealter- nativehypothesis. Forthesereasonsestimator(13.9)isgenerallypreferred. Theuncenteredestimator (13.8)ismorecommonlyseeninpracticesinceitisthedefaultchoicebymostpackages.Itisalsoworth observingthatwhenthemodelisjustidentifiedtheng =0sothetwoarealgebraicallyidentical. The n choiceofweightmatrixmayalsoimpactcovariancematrixestimationasdiscussedinSection13.12. Given the choice of covariance matrix estimator we set W(cid:99) =â¦ (cid:98) â1 or W(cid:99) =â¦ (cid:98) ââ1. Given this weight matrixweconstructthetwo-stepGMMestimatoras(13.6)usingtheweightmatrixW(cid:99). Sincethe2SLSestimatorisconsistentforÎ²,byargumentsnearlyidenticaltothoseusedforcovari- ancematrixestimationwecanshowthatâ¦ (cid:98) andâ¦ (cid:98) â areconsistentforâ¦andthusW(cid:99)isconsistentforâ¦â1. SeeExercise13.3. Thisalsomeansthatthetwo-stepGMMestimatorsatisfiestheconditionsforTheorem13.4. Theorem13.7 Under Assumption 12.2 and â¦ > 0, if W(cid:99) = â¦ (cid:98) â1 or W(cid:99) = (cid:112) â¦ (cid:98) ââ1 where the latter are defined in (13.8) and (13.9) then as n â â, n (cid:161)Î² (cid:98)gmm âÎ²(cid:162)ââN (cid:161) 0,VÎ² (cid:162) whereVÎ² =(cid:161) Q (cid:48)â¦â1Q (cid:162)â1 . d Thisshowsthatthetwo-stepGMMestimatorisasymptoticallyefficient. The two-step GMM estimator of the IV regression equation can be computed in Stata using the ivregress gmm command. By default it uses formula (13.8). The centered version (13.9) may be se- lectedusingthecenteroption. 13.11 IteratedGMM Theasymptoticdistributionofthetwo-stepGMMestimatordoesnotdependonthechoiceofthe preliminaryone-stepestimator.However,theactualvalueoftheestimatordependsonthischoiceandso willthefinitesampledistribution. Thisisundesirableandlikelyinefficient. Toremovethisdependence we can iterate the estimation sequence. Specifically, given Î² (cid:98)gmm we can construct an updated weight matrixestimateW(cid:99) andthenre-estimateÎ² (cid:98)gmm . Thisupdatingcanbeiterateduntilconvergence1. The resultiscalledtheiteratedGMMestimatorandisacommonimplementationofefficientGMM. Interestingly,B.E.HansenandLee(2020)showthattheiteratedGMMestimatorisunaffectedifthe weightmatrixiscomputedwithorwithoutcentering. Standarderrorsandteststatistics,however,will beaffectedbythechoice. TheiteratedGMMestimatoroftheIVregressionequationcanbecomputedinStatausingtheivregress gmmcommandusingtheigmmoption. 13.12 CovarianceMatrixEstimation An estimator of the asymptotic variance of Î² (cid:98)gmm can be obtained by replacing the matrices in the asymptoticvarianceformulabyconsistentestimators. 1Inpractice,âconvergenceâobtainswhenthedifferencebetweentheestimatesatsubsequentstepsissmallerthanapre- specifiedtolerance. Asufficientconditionforconvergenceisthatthesequenceisacontractionmapping",
    "page": 439,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". 13.12 CovarianceMatrixEstimation An estimator of the asymptotic variance of Î² (cid:98)gmm can be obtained by replacing the matrices in the asymptoticvarianceformulabyconsistentestimators. 1Inpractice,âconvergenceâobtainswhenthedifferencebetweentheestimatesatsubsequentstepsissmallerthanapre- specifiedtolerance. Asufficientconditionforconvergenceisthatthesequenceisacontractionmapping. Indeed,B.Hansen andLee(2020)haveshownthattheiteratedGMMestimatorgenerallysatisfiesthisconditioninlargesamples.",
    "page": 439,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER13. GENERALIZEDMETHODOFMOMENTS 420 Fortheone-steportwo-stepGMMestimatorthecovariancematrixestimatoris (cid:179) (cid:48) (cid:180)â1(cid:179) (cid:48) (cid:180)(cid:179) (cid:48) (cid:180)â1 V(cid:98)Î² = Q(cid:98) W(cid:99)Q(cid:98) Q(cid:98) W(cid:99) â¦ (cid:98)W(cid:99)Q(cid:98) Q(cid:98) W(cid:99)Q(cid:98) (13.10) whereQ(cid:98) = n 1(cid:80)n i=1 Z i X i (cid:48) .Theweightmatrixisconstructedusingeithertheuncenteredestimator(13.8)or centeredestimator(13.9)withtheresidualse (cid:98)i =Y i âX i (cid:48)Î² (cid:98)gmm . FortheefficientiteratedGMMestimatorthecovariancematrixestimatoris V(cid:98)Î² = (cid:179) Q(cid:98) (cid:48) â¦ (cid:98) â1Q(cid:98) (cid:180)â1 = (cid:181)(cid:181) 1 X (cid:48) Z (cid:182) â¦ (cid:98) â1 (cid:181) 1 Z (cid:48) X (cid:182)(cid:182)â1 . (13.11) n n â¦ (cid:98) canbecomputedusingeithertheuncenteredestimator(13.8)orcenteredestimator(13.9). Basedon theasymptoticapproximationtheestimator(13.11)canbeusedaswellforthetwo-stepestimatorbut shouldusethefinalresidualse (cid:98)i =Y i âX i (cid:48)Î² (cid:98)gmm . Asymptoticstandarderrorsaregivenbythesquarerootsofthediagonalelementsofn â1V(cid:98)Î². Itisunclearifitispreferredtousethecovariancematrixestimatorbasedonthecenteredoruncen- teredestimatorofâ¦toconstructthecovariancematrixestimator. Usingthecenteredestimatorresults inasmallercovariancematrixandstandarderrorsandthusmoreâsignificantâtestsbasedonasymptotic criticalvalues.Incontrasttheuncenteredestimatorofâ¦willresultinlargerstandarderrorsandwillthus bemoreâconservativeâ. In Stata, the default covariance matrix estimation method is determined by the choice of weight matrix.Thusifthecenteredestimator(13.9)isusedfortheweightmatrixitisalsousedforthecovariance matrixestimator. 13.13 ClusteredDependence InSection4.23weintroducedclustereddependenceandinSection12.25describedcovariancema- trixestimationfor2SLS.ThemethodsextendnaturallytoGMMbutwiththeadditionalcomplicationof potentiallyalteringweightmatrixcalculation. Thestructuralequationforthegth clustercanbewrittenasthematrixsystemY =X Î²+e .Using g g g thisnotationthecenteredGMMestimatorwithweightmatrixW canbewrittenas (cid:195) (cid:33) G Î² (cid:98)gmm âÎ²=(cid:161) X (cid:48) ZWZ (cid:48) X (cid:162)â1 X (cid:48) ZW (cid:88) Z (cid:48) g e g . g=1 Thecluster-robustcovariancematrixestimatorforÎ² (cid:98)gmm is V(cid:98)Î² =(cid:161) X (cid:48) ZWZ (cid:48) X (cid:162)â1 X (cid:48) ZWS(cid:98)WZ (cid:48) X (cid:161) X (cid:48) ZWZ (cid:48) X (cid:162)â1 (13.12) with G S(cid:98) = (cid:88) Z (cid:48) g(cid:98) e g(cid:98) e (cid:48) g Z g (13.13) g=1 andtheclusteredresiduals (cid:98) e g =Y g âX g Î² (cid:98)gmm . (13.14) Thecluster-robustestimator(13.12)isappropriatefortheone-steportwo-stepGMMestimator.Itis alsoappropriatefortheiteratedestimatorwhenthelatterusesaconventional(non-clustered)efficient weightmatrix.Howeverintheclusteringcontextitismorenaturaltouseacluster-robustweightmatrix",
    "page": 440,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER13. GENERALIZEDMETHODOFMOMENTS 421 suchasW =S(cid:98) â1 whereS(cid:98) isacluster-robustcovarianceestimatorasin(13.13)basedonaone-stepor iteratedresidual.Thisgivesrisetothecluster-robustGMMestimator Î² (cid:98)gmm = (cid:179) X (cid:48) ZS(cid:98) â1 Z (cid:48) X (cid:180)â1 X (cid:48) ZS(cid:98) â1 Z (cid:48) Y. (13.15) Anappropriatecluster-robustcovariancematrixestimatoris V(cid:98)Î² = (cid:179) X (cid:48) ZS(cid:98) â1 Z (cid:48) X (cid:180)â1 whereS(cid:98)iscalculatedusingthefinalresiduals. Toimplementacluster-robustweightmatrixusethe2SLSestimatorforfirststep.Computetheclus- terresiduals(13.14)andcovariancematrix(13.13).Then(13.15)isthetwo-stepGMMestimator.Iterating theresidualsandcovariancematrixuntilconvergenceweobtaintheiteratedGMMestimator. In Stata, using the ivregress gmm command with the cluster option implements the two-step GMMestimatorusingthecluster-robustweightmatrixandcluster-robustcovariancematrixestimator. TousethecenteredcovariancematrixusethecenteroptionandtoimplementtheiteratedGMMesti- matorusetheigmmoption.Alternatively,youcanusethewmatrixandvceoptionstoseparatelyspecify theweightmatrixandcovariancematrixestimationmethods. 13.14 WaldTest For a given function r (cid:161)Î²(cid:162) :(cid:82)k âÎâ(cid:82)q we define the parameter Î¸ =r (cid:161)Î²(cid:162) . The GMM estimator of Î¸ is Î¸ (cid:98)gmm =r (cid:161)Î² (cid:98)gmm (cid:162) . By the delta method it is asymptotically normal with covariance matrixVÎ¸ = â R (cid:48) VÎ²R where R = r(Î²) (cid:48) . An estimator of the asymptotic covariance matrix is V(cid:98)Î¸ = R(cid:98) (cid:48) V(cid:98)Î²R(cid:98) where âÎ² â (cid:113) R(cid:98) = âÎ² r(Î² (cid:98)gmm ) (cid:48) .WhenÎ¸isscalarthenanasymptoticstandarderrorforÎ¸ (cid:98)gmm isformedas n â1V(cid:98)Î¸. Astandardtestofthehypothesis(cid:72) :Î¸=Î¸ against(cid:72) :Î¸(cid:54)=Î¸ isbasedontheWaldstatistic 0 0 1 0 W =n (cid:161)Î¸ (cid:98) âÎ¸ 0 (cid:162)(cid:48) V(cid:98) â Î¸(cid:98) 1(cid:161)Î¸ (cid:98) âÎ¸ 0 (cid:162) . LetG (u)denotetheÏ2 distributionfunction. q q Theorem13.8 Under Assumption 12.2, Assumption 7.3, and (cid:72) , as n â â, 0 W ââÏ2. Forc satisfyingÎ±=1âG (c),(cid:80)[W >c|(cid:72) ]ââÎ±sothetestâReject q q 0 d (cid:72) ifW >câhasasymptoticsizeÎ±. 0 ForaproofseeExercise13.5. InStata,thecommandstestandtestparmcanbeusedafterivregress gmmtoimplementWald testsoflinearhypotheses. Thecommandsnlcomandtestnlcanbeusedafterivregress gmmtoim- plementWaldtestsofnonlinearhypotheses.",
    "page": 441,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER13. GENERALIZEDMETHODOFMOMENTS 422 13.15 RestrictedGMM Itisoftendesirabletoimposerestrictionsonthecoefficients. Inthissectionweconsiderestimation subjecttothelinearconstraintsR (cid:48)Î²=c.Inthefollowingsectionweconsidernonlinearconstraints. TheconstrainedGMMestimatorminimizestheGMMcriterionsubjecttotheconstraint.Itis Î² (cid:98)cgmm =argminJ(Î²). R(cid:48)Î²=c This is the parameter vector which makes the estimating equations as close to zero as possible with respecttotheweightedquadraticdistancewhileimposingtherestrictionontheparameters. SupposetheweightmatrixW isfixed. UsingthemethodsofChapter8itisstraightforwardtoderive thattheconstrainedGMMestimatoris Î² (cid:98)cgmm =Î² (cid:98)gmm â(cid:161) X (cid:48) ZWZ (cid:48) X (cid:162)â1 R (cid:179) R (cid:48)(cid:161) X (cid:48) ZWZ (cid:48) X (cid:162)â1 R (cid:180)â1(cid:161) R (cid:48)Î² (cid:98)gmm âc (cid:162) . (13.16) (Fordetails,seeExercise13.6.) We derive the asymptotic distribution under the assumption that the restriction is true. Make the substitutionc=R (cid:48)Î²in(13.16)andreorganizetofind (cid:112) n (cid:161)Î² (cid:98)cgmm âÎ²(cid:162)= (cid:181) I k â(cid:161) X (cid:48) ZWZ (cid:48) X (cid:162)â1 R (cid:179) R (cid:48)(cid:161) X (cid:48) ZWZ (cid:48) X (cid:162)â1 R (cid:180)â1 R (cid:48) (cid:182)(cid:112) n (cid:161)Î² (cid:98)gmm âÎ²(cid:162) . (13.17) (cid:112) Thisisalinearfunctionof (cid:112) n (cid:161)Î² (cid:98)gmm âÎ²(cid:162) . Sincetheasymptoticdistributionofthelatterisknownthe asymptoticdistributionof n (cid:161)Î² (cid:98)cgmm âÎ²(cid:162) isalinearfunctionoftheformer. Theorem13.9 UnderAssumptions12.2and8.3,fortheconstrainedGMMes- (cid:112) timator(13.16), n (cid:161)Î² (cid:98)cgmm âÎ²(cid:162)ââN (cid:161) 0,V cgmm (cid:162) asnââ,where d V cgmm =VÎ² â(cid:161) Q (cid:48) WQ (cid:162)â1 R (cid:179) R (cid:48)(cid:161) Q (cid:48) WQ (cid:162)â1 R (cid:180)â1 R (cid:48) VÎ² (13.18) âVÎ²R (cid:179) R (cid:48)(cid:161) Q (cid:48) WQ (cid:162)â1 R (cid:180)â1 R (cid:48)(cid:161) Q (cid:48) WQ (cid:162)â1 +(cid:161) Q (cid:48) WQ (cid:162)â1 R (cid:179) R (cid:48)(cid:161) Q (cid:48) WQ (cid:162)â1 R (cid:180)â1 R (cid:48) VÎ²R (cid:179) R (cid:48)(cid:161) Q (cid:48) WQ (cid:162)â1 R (cid:180)â1 R (cid:48)(cid:161) Q (cid:48) WQ (cid:162)â1 . For a proof, see Exercise 13.8. Unfortunately the asymptotic covariance matrix formula (13.18) is quitetedious! NowsupposethatthetheweightmatrixissetasW =â¦ (cid:98) â1,theefficientweightmatrixfromuncon- strainedestimation.InthiscasetheconstrainedGMMestimatorcanbewrittenas Î² (cid:98)cgmm =Î² (cid:98)gmm âV(cid:98)Î²R (cid:161) R (cid:48) V(cid:98)Î²R (cid:162)â1(cid:161) R (cid:48)Î² (cid:98)gmm âc (cid:162) (13.19) whichisthesameformula(8.25)asefficientminimumdistance.(Fordetails,seeExercise13.7.)Wefind thattheasymptoticcovariancematrixsimplifiesconsiderably.",
    "page": 442,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER13. GENERALIZEDMETHODOFMOMENTS 423 Theorem13.10 UnderAssumptions12.2and8.3,fortheefficientconstrained (cid:112) GMMestimator(13.19), n (cid:161)Î² (cid:98)cgmm âÎ²(cid:162)ââN (cid:161) 0,V cgmm (cid:162) asnââ,where d V cgmm =VÎ² âVÎ²R (cid:161) R (cid:48) VÎ²R (cid:162)â1 R (cid:48) VÎ². (13.20) Foraproof,seeExercise13.9. Theasymptoticcovariancematrix(13.20)canbeestimatedby V(cid:98)cgmm =V(cid:101)Î² âV(cid:101)Î²R (cid:161) R (cid:48) V(cid:101)Î²R (cid:162)â1 R (cid:48) V(cid:101)Î². (13.21) V(cid:101)Î² = (cid:179) Q(cid:98) (cid:48) â¦ (cid:101) â1Q(cid:98) (cid:180)â1 â¦ (cid:101) = n 1 (cid:88) n Z i Z i (cid:48) e (cid:101)i 2 (13.22) i=1 e (cid:101)i =Y i âX i (cid:48)Î² (cid:98)cgmm . Thecovariancematrix(13.18)canbeestimatedsimilarly,thoughusing(13.10)toestimateVÎ². Theco- variancematrixestimatorâ¦ (cid:101) canalsobereplacedwithacenteredversion. AconstrainediteratedGMMestimatorcanbeimplementedbysettingW =â¦ (cid:101) â1 whereâ¦ (cid:101) isdefined in(13.22)andtheniteratinguntilconvergence.Thisisanaturalestimatorasitistheappropriateimple- mentationofiteratedGMM. Sincebothâ¦ (cid:98) andâ¦ (cid:101) convergetothesamelimitâ¦undertheassumptionthattheconstraintistrue theconstrainediteratedGMMestimatorhastheasymptoticdistributiongiveninTheorem13.10. 13.16 NonlinearRestrictedGMM Nonlinearconstraintsontheparameterscanbewrittenasr (cid:161)Î²(cid:162)=0forsomefunctionr :(cid:82)k â(cid:82)q. The constraint is nonlinear if r (cid:161)Î²(cid:162) cannot be written as a linear function of Î². Least squares estima- tion subject to nonlinear constraints was explored in Section 8.14. In this section we introduce GMM estimationsubjecttononlinearconstraints. TheconstrainedGMMestimatorminimizestheGMMcriterionsubjecttotheconstraint.Itis Î² (cid:98)cgmm =argminJ(Î²). (13.23) r(Î²)=0 This is the parameter vector which makes the estimating equations as close to zero as possible with respecttotheweightedquadraticdistancewhileimposingtherestrictionontheparameters. IngeneralthereisnoexplicitsolutionforÎ² (cid:98)cgmm . Insteadthesolutionisfoundnumerically. Fortu- natelythereareexcellentnonlinearconstrainedoptimizationsolversimplementedinstandardsoftware packages. Fortheasymptoticdistributionassumethattherestrictionr (cid:161)Î²(cid:162)=0istrue.Usingthesamemethods asintheproofofTheorem8.10wecanshowthat(13.17)approximatelyholdsinthesensethat (cid:112) n (cid:161)Î² (cid:98)cgmm âÎ²(cid:162)= (cid:181) I k â(cid:161) X (cid:48) ZWZ (cid:48) X (cid:162)â1 R (cid:179) R (cid:48)(cid:161) X (cid:48) ZWZ (cid:48) X (cid:162)â1 R (cid:180)â1 R (cid:48) (cid:182)(cid:112) n (cid:161)Î² (cid:98)gmm âÎ²(cid:162)+o p (1) whereR = â r (cid:161)Î²(cid:162)(cid:48) . Thustheasymptoticdistributionoftheconstrainedestimatortakesthesameform âÎ² asinthelinearcase.",
    "page": 443,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER13. GENERALIZEDMETHODOFMOMENTS 424 Theorem13.11 Under Assumptions 12.2 and 8.3, for the constrained GMM (cid:112) estimator (13.23), n (cid:161)Î² (cid:98)cgmm âÎ²(cid:162) ââ N (cid:161) 0,V cgmm (cid:162) as n â â, where V cgmm d equals(13.18).IfW =â¦ (cid:98) â1,thenV cgmm equals(13.20). Theasymptoticcovariancematrixintheefficientcaseisestimatedby(13.21)withR replacedwith R(cid:98) = â â Î² r (cid:161)Î² (cid:98)cgmm (cid:162)(cid:48) .Theasymptoticcovariancematrix(13.18)inthegeneralcaseisestimatedsimilarly. ToimplementaniteratedrestrictedGMMestimatortheweightmatrixmaybesetasW =â¦ (cid:101) â1where â¦ (cid:101) isdefinedin(13.22),andtheniterateduntilconvergence. 13.17 ConstrainedRegression TaketheconventionalprojectionmodelY =X (cid:48)Î²+ewith(cid:69)[Xe]=0.ThisisaspecialcaseofGMMas itismodel(13.5)withZ =X.Thejust-identifiedGMMestimatorequalsleastsquaresÎ² (cid:98)gmm =Î² (cid:98)ols . InChapter8wediscussedestimationoftheprojectionmodelsubjecttolinearconstraintsR (cid:48)Î²=c, which includes exclusion restrictions. Since the projection model is a special case of GMM the con- strainedprojectionmodelisalsoconstrainedGMM.FromtheresultsofSection13.15wefindthatthe efficientconstrainedGMMestimatoris Î² (cid:98)cgmm =Î² (cid:98)ols âV(cid:98)Î²R (cid:161) R (cid:48) V(cid:98)Î²R (cid:162)â1(cid:161) R (cid:48)Î² (cid:98)ols âc (cid:162)=Î² (cid:98)emd , the efficient minimum distance estimator. Thus for linear constraints on the linear projection model efficientGMMequalsefficientminimumdistance.Thusoneconvenientmethodtoimplementefficient minimumdistanceisGMM. 13.18 MultivariateRegression GMMmethodscansimplifyestimationandinferenceformultivariateregressionssuchasthosein- troducedinChapter11. Thegeneralmultivariateregression(projection)modelis Y =X (cid:48)Î² +e j j j j (cid:69)(cid:163) X e (cid:164)=0 j j for j =1,...,m. UsingthenotationfromSection11.2theequationscanbewrittenjointlyasY =XÎ²+e andforthefullsampleasY =XÎ²+e.Thek momentconditionsare (cid:104) (cid:48)(cid:179) (cid:180)(cid:105) (cid:69) X Y âXÎ² =0. (13.24) GivenakÃk weightmatrixW theGMMcriterionis (cid:179) (cid:180)(cid:48) (cid:48)(cid:179) (cid:180) J(Î²)=n Y âXÎ² XWX Y âXÎ² . TheGMMestimatorÎ² (cid:98)gmm minimizes J(Î²). Sincethisisajust-identifiedmodeltheestimatorsolves thesampleequations (cid:48)(cid:179) (cid:180) X Y âXÎ² (cid:98)gmm =0.",
    "page": 444,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER13. GENERALIZEDMETHODOFMOMENTS 425 Thesolutionis (cid:195) (cid:33)â1(cid:195) (cid:33) (cid:88) n (cid:48) (cid:88) n (cid:48) (cid:179) (cid:48) (cid:180)â1(cid:179) (cid:48) (cid:180) Î² (cid:98)gmm = X i X i X i Y i = X X X Y =Î² (cid:98)ols , i=1 i=1 themultivariateleastsquaresestimator. ThustheunconstrainedGMMestimatorofthemultivariateregressionmodelisleastsquares. The estimatordoesnotdependontheweightmatrixsincethemodelisjust-identified. A important advantage of the GMM framework is the ability to incorporate cross-equation con- straints. ConsidertheclassofrestrictionsR (cid:48)Î²=c. MinimizationoftheGMMcriterionsubjecttothis restritionhassolutionsasdescribedin(13.15).TherestrictedGMMestimatoris Î² (cid:98)gmm =Î² (cid:98)ols â (cid:179) X (cid:48) XWX (cid:48) X (cid:180)â1 R (cid:181) R (cid:48) (cid:179) X (cid:48) XWX (cid:48) X (cid:180)â1 R (cid:182)â1 (cid:161) R (cid:48)Î² (cid:98)ols âc (cid:162) . Thisestimatordependsontheweightmatrixbecauseitisover-identified. (cid:48) AsimplechoiceforweightmatrixisW =X X.Thisleadstotheone-stepestimator Î² (cid:98)1 =Î² (cid:98)ols â (cid:179) X (cid:48) X (cid:180)â1 R (cid:181) R (cid:48) (cid:179) X (cid:48) X (cid:180)â1 R (cid:182)â1 (cid:161) R (cid:48)Î² (cid:98)ols âc (cid:162) . TheasymptoticallyefficientchoicesetsW =â¦ (cid:98) â1whereâ¦ (cid:98) =n â1(cid:80)n i=1 X (cid:48) i e (cid:98)i e (cid:98)i (cid:48) X i ande (cid:98)i =Y i âX i Î² (cid:98)1 . This leadstothetwo-stepestimator Î² (cid:98)2 =Î² (cid:98)ols â (cid:179) X (cid:48) Xâ¦ (cid:98) â1X (cid:48) X (cid:180)â1 R (cid:181) R (cid:48) (cid:179) X (cid:48) Xâ¦ (cid:98) â1X (cid:48) X (cid:180)â1 R (cid:182)â1 (cid:161) R (cid:48)Î² (cid:98)ols âc (cid:162) . Whentheregressors X arecommonacrossall equationsthemultivariateregressionmodel canbe writtenconvenientlyasin(11.3): Y =B (cid:48) X +e with(cid:69)(cid:163) Xe (cid:48)(cid:164)=0. Themomentrestrictionscanbewritten asthematrixsystem(cid:69)(cid:163) X (cid:161) Y (cid:48)âX (cid:48) B (cid:162)(cid:164)=0.Writtenasavectorsystemthisis(13.24)andleadstothesame restrictedGMMestimators. Thesearegeneralformulaforimposingrestrictions. Inspecificcases(suchasanexclusionrestric- tion)directmethodsmaybemoreconvenient. Inallcasesthesolutionisfoundbyminimizationofthe GMMcriterionJ(Î²)subjecttotherestriction. 13.19 DistanceTest In Section 13.14 we introduced Wald tests of the hypothesis (cid:72) :Î¸=Î¸ where Î¸=r (cid:161)Î²(cid:162) for a given 0 0 functionr (cid:161)Î²(cid:162) :(cid:82)k âÎâ(cid:82)q. Whenr (cid:161)Î²(cid:162) isnonlinearanalternativeistouseacriterion-basedstatistic. ThisissometimescalledtheGMMDistancestatisticandsometimescalledaLR-likestatistic(theLRis forlikelihood-ratio).TheideawasfirstputforwardbyNeweyandWest(1987a)",
    "page": 445,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". 13.19 DistanceTest In Section 13.14 we introduced Wald tests of the hypothesis (cid:72) :Î¸=Î¸ where Î¸=r (cid:161)Î²(cid:162) for a given 0 0 functionr (cid:161)Î²(cid:162) :(cid:82)k âÎâ(cid:82)q. Whenr (cid:161)Î²(cid:162) isnonlinearanalternativeistouseacriterion-basedstatistic. ThisissometimescalledtheGMMDistancestatisticandsometimescalledaLR-likestatistic(theLRis forlikelihood-ratio).TheideawasfirstputforwardbyNeweyandWest(1987a). Theideaistocomparetheunrestrictedandrestrictedestimatorsbycontrastingthecriterionfunc- tions.Theunrestrictedestimatortakestheform Î² (cid:98)gmm =argminJ(cid:98)(Î²) Î² where J(cid:98)(Î²)=n g (Î²) (cid:48)â¦ (cid:98) â1g (Î²) n n istheunrestrictedGMMcriterionwithanefficientweightmatrixestimateâ¦ (cid:98). Theminimizedvalueof thecriterionisJ(cid:98) =J(cid:98)(Î² (cid:98)gmm ).",
    "page": 445,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER13. GENERALIZEDMETHODOFMOMENTS 426 AsinSection13.15,theestimatorsubjecttor (cid:161)Î²(cid:162)=Î¸ is 0 Î² (cid:98)cgmm =argminJ(cid:101)(Î²) r(Î²)=Î¸ 0 where J(cid:101)(Î²)=ng (Î²) (cid:48)â¦ (cid:101) â1g (Î²) n n whichdependsonanefficientweightmatrixestimator,eitherâ¦ (cid:98) (thesameastheunrestrictedestimator) orâ¦ (cid:101) (theiteratedweightmatrixfromconstrainedestimation). Theminimizedvalueofthecriterionis J(cid:101) =J(cid:101) (cid:161)Î² (cid:98)cgmm (cid:162) . The GMM distance (or LR-like) statistic is the difference in the criterion functions: D = J(cid:101) âJ(cid:98). The distancetestsharestheusefulfeatureofLRtestsinthatitisanaturalby-productofthecomputationof alternativemodels. Thetesthasthefollowinglargesampledistribution. Theorem13.12 UnderAssumption12.2,Assumption7.3,and(cid:72) ,thenasnâ 0 â,DââÏ2.ForcsatisfyingÎ±=1âG (c),(cid:80)[D>c|(cid:72) ]ââÎ±.ThetestâReject q q 0 d (cid:72) ifD>câhasasymptoticsizeÎ±. 0 TheproofisgiveninSection13.28. Theorem 13.12 shows that the distance statistic has the same asymptotic distribution as Wald and likelihood ratio statistics and can be interpreted similarly. Small values of D mean that imposing the restrictiondoesnotresultinalargevalueofthemomentequations.Hencetherestrictionappearstobe compatiblewiththedata.Ontheotherhand,largevaluesofDmeanthatimposingtherestrictionresults inamuchlargervalueofthemomentequations,implyingthattherestrictionisnotcompatiblewiththe data. The finding that the asymptotic distribution is chi-squared allows the calculation of asymptotic criticalvaluesandp-values. Wenowdiscussthechoiceofweightmatrix. Asmentionedaboveonesimplechoiceistosetâ¦ (cid:101) =â¦ (cid:98). Inthiscasewehavethefollowingresult. Theorem13.13 If â¦ (cid:101) =â¦ (cid:98) then D â¥0. Furthermore, if r is linear in Î² then D equalstheWaldstatistic. Thestatementthatâ¦ (cid:101) =â¦ (cid:98) impliesDâ¥0followsfromthefactthatinthiscasethecriterionfunctions J(cid:98)(Î²)= J(cid:101)(Î²)areidenticalsotheconstrainedminimumcannotbesmallerthantheunconstrained. The statementthatlinearhypothesesandâ¦ (cid:101) =â¦ (cid:98) impliesD=W followsfromapplyingtheexpressionforthe constrainedGMMestimator(13.19)andusingthecovariancematrixformula(13.11). ThefactthatDâ¥0whenâ¦ (cid:101) =â¦ (cid:98) motivatedNeweyandWest(1987a)torecommendthischoice.How- ever, this is necessary. Setting â¦ (cid:101) to be the constrained efficient weight matrix is natural for efficient estimationofÎ² (cid:98)cgmm .IntheeventthatD<0thetestsimplyfailstoreject(cid:72) 0 atanysignificancelevel. AsdiscussedinSection9.17fortestsofnonlinearhypothesestheWaldstatisticcanworkquitepoorly. In particular, the Wald statistic is affected by how the hypothesis r (cid:161)Î²(cid:162) is formulated. In contrast, the distance statistic D is not affected by the algebraic formulation of the hypothesis. Current evidence",
    "page": 446,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER13. GENERALIZEDMETHODOFMOMENTS 427 suggests that the D statistic appears to have good sampling properties, and is a preferred test statistic relativetotheWaldstatisticfornonlinearhypotheses.(SeeB.E.Hansen(2006).) In Stata the command estat overid after ivregress gmm can be used to report the value of the GMMcriterion J. By estimatingthetwonestedGMMregressionsthevalues J(cid:98)and J(cid:101)canbeobtained andD computed. 13.20 Continuously-UpdatedGMM Analternativetothetwo-stepGMMestimatorcanbeconstructedbylettingtheweightmatrixbean explicitfunctionofÎ².Theseleadstothecriterionfunction (cid:195) (cid:33)â1 J(Î²)=ng (Î²) (cid:48) 1 (cid:88) n g (Î²)g (Î²) (cid:48) g (Î²). n n i i n i=1 The Î² (cid:98)which minimizes this function is called the continuously-updatedGMM(CU-GMM)estimator andwasintroducedbyL.Hansen,HeatonandYaron(1996). Acomplicationisthatthecontinuously-updatedcriterionJ(Î²)isnotquadraticinÎ².Thismeansthat minimization requires numerical methods. It may appear that the CU-GMM estimator is the same as theiteratedGMMestimatorbutthisisnotthecaseatall. Theysolvedistinctfirst-orderconditionsand canbequitedifferentinapplications. RelativetotraditionalGMMtheCU-GMMestimatorhaslowerbiasbutthickerdistributionaltails. Whileithasreceivedconsiderabletheoreticalattentionitisnotusedcommonlyinapplications. 13.21 OverIdentificationTest InSection12.31weintroducedtheSargan(1958)overidentificationtestforthe2SLSestimatorunder theassumptionofhomoskedasticity. L.Hansen(1982)generalizedthetesttocovertheGMMestimator allowingforgeneralheteroskedasticity. Recall,overidentifiedmodels((cid:96)>k)arespecialinthesensethattheremaynotbeaparametervalue Î²suchthatthemomentcondition(cid:72) :(cid:69)[Ze]=0holds.Thusthemodelâtheoveridentifyingrestrictions 0 âaretestable. For example, take the linear model Y =Î²(cid:48) X +Î²(cid:48) X +e with (cid:69)[X e]=0 and (cid:69)[X e]=0. It is pos- 1 1 2 2 1 2 sible that Î² = 0 so that the linear equation may be written as Y = Î²(cid:48) X +e. However, it is possible 2 1 1 that Î² (cid:54)=0. In this case it is impossible to find a value of Î² such that both (cid:69)(cid:163) X (cid:161) Y âX (cid:48)Î² (cid:162)(cid:164)=0 and 2 1 1 1 1 (cid:69)(cid:163) X (cid:161) Y âX (cid:48)Î² (cid:162)(cid:164)=0holdsimultaneously. Inthissenseanexclusionrestrictioncanbeseenasanoveri- 2 1 1 dentifyingrestriction. Notethatg ââ(cid:69)[Ze]andthusg canbeusedtoassessthehypothesis(cid:69)[Ze]=0. Assumingthat n n p an efficient weight matrix estimator is used the criterion function at the parameter estimator is J = J(Î² (cid:98)gmm )=ng (cid:48) n â¦ (cid:98) â1g n .Thisisaquadraticforming n andisthusanaturalteststatisticfor(cid:72) 0 :(cid:69)[Ze]=0. Notethatweassumethatthecriterionfunctionisconstructedwithanefficientweightmatrixestimator. Thisisimportantforthedistributiontheory. Theorem13.14 UnderAssumption12.2thenasnââ,J=J (cid:161)Î² (cid:98)gmm (cid:162)ââÏ2 (cid:96)âk",
    "page": 447,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Assumingthat n n p an efficient weight matrix estimator is used the criterion function at the parameter estimator is J = J(Î² (cid:98)gmm )=ng (cid:48) n â¦ (cid:98) â1g n .Thisisaquadraticforming n andisthusanaturalteststatisticfor(cid:72) 0 :(cid:69)[Ze]=0. Notethatweassumethatthecriterionfunctionisconstructedwithanefficientweightmatrixestimator. Thisisimportantforthedistributiontheory. Theorem13.14 UnderAssumption12.2thenasnââ,J=J (cid:161)Î² (cid:98)gmm (cid:162)ââÏ2 (cid:96)âk . d ForcsatisfyingÎ±=1âG(cid:96)âk (c),(cid:80)[J>c|(cid:72) 0 ]ââÎ±sothetestâReject(cid:72) 0 ifJ>câ hasasymptoticsizeÎ±.",
    "page": 447,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER13. GENERALIZEDMETHODOFMOMENTS 428 TheproofofthetheoremislefttoExercise13.13. Thedegreesoffreedomoftheasymptoticdistributionarethenumberofoveridentifyingrestrictions. IfthestatisticJ exceedsthechi-squarecriticalvaluewecanrejectthemodel.Basedonthisinformation aloneitisunclearwhatiswrongbutitistypicallycauseforconcern.TheGMMoveridentificationtestis ausefulby-productoftheGMMmethodologyanditisadvisabletoreportthestatisticJ wheneverGMM istheestimationmethod. Whenover-identifiedmodelsareestimatedbyGMMitiscustomarytoreport theJ statisticasageneraltestofmodeladequacy. InStatathecommandestat overidaferivregress gmmcanbeusedtoimplementtheoveriden- tificationtest.TheGMMcriterionJ anditsasymptoticp-valueusingtheÏ2 distributionarereported. (cid:96)âk 13.22 SubsetOverIdentificationTests InSection12.32weintroducedsubsetoveridentificationtestsforthe2SLSestimatorundertheas- sumptionofhomoskedasticity.InthissectionwedescribehowtoconstructanalogoustestsfortheGMM estimatorundergeneralheteroskedasticity. Recall, subset overidentification tests are used when it is desired to focus attention on a subset of instrumentswhosevalidityisquestioned.PartitionZ =(Z ,Z )withdimensions(cid:96) and(cid:96) ,respectively, a b a b where Z contains the instruments which are believed to be uncorrelated with e and Z contains the a b instrumentswhichmaybecorrelatedwithe.Itisnecessarytoselectthispartitionsothat(cid:96) >k,sothat a theinstrumentsZ aloneidentifytheparameters. a Given this partition the maintained hypothesis is (cid:69)[Z e]=0. The null and alternative hypotheses a are (cid:72) : (cid:69)[Z e] = 0 and (cid:72) : (cid:69)[Z e] (cid:54)= 0. The GMM test is constructed as follows. First, estimate the 0 b 1 b modelbyefficientGMMwithonlythesmallerset Z a ofinstruments. Let J(cid:101)denotetheresultingGMM criterion. Second, estimate the model by efficient GMM with the full set Z =(Z ,Z ) of instruments. a b Let J(cid:98)denotetheresultingGMMcriterion. Theteststatisticisthedifferenceinthecriterionfunctions: C = J(cid:98) âJ(cid:101). This is similar to the GMM distance statistic presented in Section 13.19. The difference is thatthedistancestatisticcomparesmodelswhichdifferbasedontheparameterrestrictionswhiletheC statisticcomparesmodelsbasedondifferentinstrumentsets. TypicallyC â¥0. However,thisisnotnecessaryandC <0canarise. Ifthisoccursitleadstoanon- rejectionof(cid:72) . 0 If the smaller instrument set Z a is just-identified so that (cid:96) a =k then J(cid:101) =0 soC = J(cid:98)is simply the standardoveridentificationtest.Thisiswhywehaverestrictedattentiontothecase(cid:96) >k. a Thetesthasthefollowinglargesampledistribution. Theorem13.15 UnderAssumption12.2and(cid:69)(cid:163) Z X (cid:48)(cid:164) hasfullrankk, thenas a nââ,C â d âÏ2 (cid:96) b . Forc satisfyingÎ±=1âG(cid:96) b (c),(cid:80)[C >c|(cid:72) 0 ]ââÎ±. Thetest âReject(cid:72) ifC >câhasasymptoticsizeÎ±. 0 TheproofofTheorem13.15ispresentedinSection13.28",
    "page": 448,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". a Thetesthasthefollowinglargesampledistribution. Theorem13.15 UnderAssumption12.2and(cid:69)(cid:163) Z X (cid:48)(cid:164) hasfullrankk, thenas a nââ,C â d âÏ2 (cid:96) b . Forc satisfyingÎ±=1âG(cid:96) b (c),(cid:80)[C >c|(cid:72) 0 ]ââÎ±. Thetest âReject(cid:72) ifC >câhasasymptoticsizeÎ±. 0 TheproofofTheorem13.15ispresentedinSection13.28. InStatathecommandestat overid zbaferivregress gmmcanbeusedtoimplementasubset overidentificationtestwherezbisthename(s)oftheinstruments(s)testedforvalidity. ThestatisticC anditsasymptoticp-valueusingtheÏ2 distributionarereported. (cid:96) 2",
    "page": 448,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER13. GENERALIZEDMETHODOFMOMENTS 429 13.23 EndogeneityTest InSection12.29weintroducedtestsforendogeneityinthecontextof2SLSestimation. Endogeneity testsaresimpletoimplementintheGMMframeworkasasubsetoveridentificationtest. Themodelis Y =Z (cid:48)Î² +Y (cid:48)Î² +ewherethemaintainedassumptionisthattheregressorsZ andexcludedinstruments 1 1 2 2 1 Z areexogenoussothat(cid:69)[Z e]=0and(cid:69)[Z e]=0. ThequestioniswhetherornotY isendogenous. 2 1 2 2 Thenullhypothesisis(cid:72) :(cid:69)[Y e]=0withthealternative(cid:72) :(cid:69)[Y e](cid:54)=0. 0 2 1 2 TheGMMtestisconstructedasfollows. First, estimatethemodelbyefficientGMMusing(Z ,Z ) 1 2 as instruments for (Z 1 ,Y 2 ). Let J(cid:101)denote the resulting GMM criterion. Second, estimate the model by efficientGMM2 using(Z 1 ,Z 2 ,Y 2 )asinstrumentsfor(Z 1 ,Y 2 ). Let J(cid:98)denotetheresultingGMMcriterion. Theteststatisticisthedifferenceinthecriterionfunctions:C =J(cid:98) âJ(cid:101). Thedistributiontheoryforthetestisaspecialcaseofoveridentificationtesting. Theorem13.16 UnderAssumption12.2and(cid:69)(cid:163) Z Y (cid:48)(cid:164) hasfullrankk ,thenas 2 2 2 n ââ,C ââÏ2 . Forc satisfyingÎ±=1âG (c), (cid:80)[C >c|(cid:72) ]âÎ±. Thetest d k2 k2 0 âReject(cid:72) ifC >câhasasymptoticsizeÎ±. 0 InStatathecommandestat endogenousaferivregress gmmcanbeusedtoimplementthetest forendogeneity.ThestatisticC anditsasymptoticp-valueusingtheÏ2 distributionarereported. k2 13.24 SubsetEndogeneityTest InSection12.30weintroducedsubsetendogeneitytestsfor2SLSestimation. GMMtestsaresimple toimplementassubsetoveridentificationtests. ThemodelisY =Z (cid:48)Î² +Y (cid:48)Î² +Y (cid:48)Î² +e with(cid:69)[Ze]=0 1 1 2 2 3 3 wheretheinstrumentvectoris Z =(Z ,Z ). Thek Ã1variablesY aretreatedasendogenousandthe 1 2 3 3 k Ã1variablesY aretreatedaspotentiallyendogenous. The hypothesistotestisthatY isexogenous, 2 2 2 or(cid:72) :(cid:69)[Y e]=0against(cid:72) :(cid:69)[Y e](cid:54)=0. Thetestrequiresthat(cid:96) â¥(k +k )sothatthemodelcanbe 0 2 1 2 2 2 3 estimatedunder(cid:72) . 1 TheGMMtestisconstructedasfollows. First, estimatethemodelbyefficientGMMusing(Z ,Z ) 1 2 asinstrumentsfor(Z 1 ,Y 2 ,Y 3 ).LetJ(cid:101)denotetheresultingGMMcriterion.Second,estimatethemodelby efficientGMMusing(Z 1 ,Z 2 ,Y 2 )asinstrumentsfor(Z 1 ,Y 2 ,Y 3 ).LetJ(cid:98)denotetheresultingGMMcriterion. Theteststatisticisthedifferenceinthecriterionfunctions:C =J(cid:98) âJ(cid:101). Thedistributiontheoryforthetestisaspecialcaseofthetheoryofoveridentificationtesting. Theorem13.17 UnderAssumption12.2and(cid:69)(cid:163) Z (cid:161) Y (cid:48) ,Y (cid:48)(cid:162)(cid:164) hasfullrankk +k , 2 2 3 2 3 thenasn ââ,C ââÏ2 . Forc satisfyingÎ±=1âG (c), (cid:80)[C >c|(cid:72) ]ââÎ±. d k2 k2 0 ThetestâReject(cid:72) ifC >câhasasymptoticsizeÎ±. 0 InStata,thecommandestat endogenous x2aferivregress gmmcanbeusedtoimplementthe testforendogeneitywherex2isthename(s)ofthevariable(s)testedforendogeneity.ThestatisticC and itsasymptoticp-valueusingtheÏ2 distributionarereported",
    "page": 449,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Theorem13.17 UnderAssumption12.2and(cid:69)(cid:163) Z (cid:161) Y (cid:48) ,Y (cid:48)(cid:162)(cid:164) hasfullrankk +k , 2 2 3 2 3 thenasn ââ,C ââÏ2 . Forc satisfyingÎ±=1âG (c), (cid:80)[C >c|(cid:72) ]ââÎ±. d k2 k2 0 ThetestâReject(cid:72) ifC >câhasasymptoticsizeÎ±. 0 InStata,thecommandestat endogenous x2aferivregress gmmcanbeusedtoimplementthe testforendogeneitywherex2isthename(s)ofthevariable(s)testedforendogeneity.ThestatisticC and itsasymptoticp-valueusingtheÏ2 distributionarereported. k2 2IfthehomoskedasticweightmatrixisusedthisGMMestimatorequalsleastsquares,butwhentheweightmatrixallowsfor heteroskedasticitytheefficientGMMestimatordoesnotequalleastsquaresasthemodelisoveridentified.",
    "page": 449,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER13. GENERALIZEDMETHODOFMOMENTS 430 13.25 NonlinearGMM GMMapplieswheneveraneconomicorstatisticalmodelimpliesthe(cid:96)Ã1momentcondition (cid:69)(cid:163) g (Î²) (cid:164)=0. i whereg (Î²)isapossiblynonlinearfunctionoftheparametersÎ². Often,thisisallthatisknown. Identi- i ficationrequires(cid:96)â¥k=dim(Î²).TheGMMestimatorminimizes J(Î²)=n g (Î²) (cid:48) W(cid:99)g (Î²) n n forsomeweightmatrixW(cid:99)where 1 (cid:88) n g (Î²)= g (Î²). n n i i=1 TheefficientGMMestimatorcanbeconstructedbysetting (cid:195) (cid:33)â1 W(cid:99) = n 1 (cid:88) n g (cid:98)i g (cid:98)i (cid:48)âg n g (cid:48) n , i=1 withg (cid:98)i =g i (Î² (cid:101))constructedusingapreliminaryconsistentestimatorÎ² (cid:101),perhapsobtainedwithW(cid:99) =I(cid:96). As in the case of the linear model the weight matrix can be iterated until convergence to obtain the iteratedGMMestimator. Proposition13.1 DistributionofNonlinearGMMEstimator (cid:112) Undergeneralregularityconditions, n (cid:161)Î² (cid:98)gmm âÎ²(cid:162)ââN (cid:161) 0,VÎ² (cid:162) where d VÎ² =(cid:161) Q (cid:48) WQ (cid:162)â1(cid:161) Q (cid:48) Wâ¦WQ (cid:162)(cid:161) Q (cid:48) WQ (cid:162)â1 withâ¦=(cid:69)(cid:163) g g (cid:48)(cid:164) and i i (cid:183) â (cid:184) Q=(cid:69) g (Î²) . âÎ²(cid:48) i IftheefficientweightmatrixisusedthenVÎ² =(cid:161) Q (cid:48)â¦â1Q (cid:162)â1 . Theproofofthisresultisomittedasitusesmoreadvancedtechniques. Theasymptoticcovariancematricescanbeestimatedbysamplecounterpartsofthepopulationma- trices.Forthecaseofageneralweightmatrix, (cid:179) (cid:48) (cid:180)â1(cid:179) (cid:48) (cid:180)(cid:179) (cid:48) (cid:180)â1 V(cid:98)Î² = Q(cid:98) W(cid:99)Q(cid:98) Q(cid:98) W(cid:99) â¦ (cid:98)W(cid:99)Q(cid:98) Q(cid:98) W(cid:99)Q(cid:98) where â¦ (cid:98) = 1 (cid:88) n (cid:161) g i (Î² (cid:98))âg (cid:162)(cid:161) g i (Î² (cid:98))âg (cid:162)(cid:48) n i=1 n g =n â1(cid:88) g i (Î² (cid:98)) i=1",
    "page": 450,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER13. GENERALIZEDMETHODOFMOMENTS 431 and 1 (cid:88) n â Q(cid:98) = n âÎ²(cid:48) g i (Î² (cid:98)). i=1 Forthecaseoftheiteratedefficientweightmatrix, V(cid:98)Î² = (cid:179) Q(cid:98) (cid:48) â¦ (cid:98) â1Q(cid:98) (cid:180)â1 . All of the methods discussed in this chapter â Wald tests, constrained estimation, distance tests, overidentificationtests,endogeneitytestsâapplysimilarlytothenonlinearGMMestimator. 13.26 BootstrapforGMM The bootstrap for 2SLS (Section 12.23) can be used for GMM estimation. The standard bootstrap â â â algorithm generates bootstrap samples by sampling the triplets (Y ,X ,Z ) independently and with i i i replacementfromtheoriginalsample.TheGMMestimatorisappliedtothebootstrapsampletoobtain thebootstrapestimatesÎ² (cid:98) â . ThisisrepeatedB timestocreateasampleofB bootstrapdraws. Given gmm thesedraws, bootstrapconfidenceintervals, includingpercentile, BCpercentile, BC andpercentile-t, a arecalculatedconventionally. Forvarianceandstandarderrorestimationthesamecautionsapplyasfor2SLS.Itisdifficulttoknow iftheGMMestimatorhasafinitevarianceinagivenapplication.Itisbesttoavoidusingthebootstrapto calculatestandarderrors.Instead,usethebootstrapforpercentileandpercentile-tconfidenceintervals. Whenthemodelisoveridentified,asdiscussedfor2SLS,bootstrapGMMinferencewillnotachieve anasymptoticrefinementunlessthebootstrapestimatorisrecenteredtosatisfytheorthogonalitycon- dition.WenowdescribetherecenteringrecommendedbyHallandHorowitz(1996). ForlinearGMMwthweightmatrixW therecenteredGMMbootstrapestimatoris Î² (cid:98) â gm â m =(cid:161) X â(cid:48) Z â W â Z â(cid:48) X â(cid:162)â1(cid:161) X â(cid:48) Z â W â(cid:161) Z â(cid:48) Y ââZ (cid:48) (cid:98) e (cid:162)(cid:162) whereW â isthebootstrapversionofW and (cid:98) e=Y âXÎ² (cid:98)gmm .ForefficientGMM, (cid:195) (cid:33)â1 W â= 1 (cid:88) n Z â Z â(cid:48)(cid:161) Y ââX â(cid:48)Î² (cid:101) â(cid:162)2 n i i i i i=1 forpreliminaryestimatorÎ² (cid:101) â . For nonlinear GMM (Section 13.25) the bootstrap criterion function is modified. The recentered bootstrapcriterionis J ââ (Î²)=n (cid:161) g â n (Î²)âg n (Î² (cid:98)gmm ) (cid:162)(cid:48) W â(cid:161) g â n (Î²)âg n (Î² (cid:98)gmm ) (cid:162) g â (Î²)= 1 (cid:88) n g â (Î²) n n i i=1 whereg n (Î² (cid:98)gmm )isfromthesamplenotfromthebootstrapdata.Thebootstrapestimatoris Î² (cid:98) ââ =argminJ ââ (Î²). gmm Thebootstrapcanbeusedtocalculatethep-valueoftheGMMoveridentificationtest.FortheGMM estimatorwithanefficientweightmatrixthestandardoveridentificationtestistheHansenJ statistic J=n g n (Î² (cid:98)gmm ) (cid:48)â¦ (cid:98) â1g n (Î² (cid:98)gmm ).",
    "page": 451,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER13. GENERALIZEDMETHODOFMOMENTS 432 Therecenteredbootstrapanalogis (cid:179) (cid:180)(cid:48) (cid:179) (cid:180) J ââ=n g â n (Î² (cid:98) â gm â m )âg n (Î² (cid:98)gmm ) â¦ (cid:98) ââ1 g â n (Î² (cid:98) â gm â m )âg n (Î² (cid:98)gmm ) . ââ OneachbootstrapsampleJ (b)iscalculatedandstored.Thebootstrapp-valueis p â= 1 (cid:88) B 1(cid:169) J ââ (b)>S (cid:170) . B b=1 ââ Thisbootstrapp-valueisasymptoticallyvalidsinceJ satisfiestheoveridentifiedmomentconditions. 13.27 ConditionalMomentEquationModels Inmanycontexts,aneconomicmodelimpliesconditionalmomentrestrictionoftheform (cid:69)(cid:163) e (Î²)|Z (cid:164)=0 i i wheree (Î²)issomesÃ1functionoftheobservationandtheparameters. Inmanycasess=1. Itturns i outthatthisconditionalmomentrestrictionismorepowerfulthantheunconditionalmomentequation modeldiscussedthroughoutthischapter. Forexample,thelinearmodelY =X (cid:48)Î²+ewithinstrumentsZ fallsintothisclassundertheassump- tion(cid:69)[e|Z]=0.Inthiscasee (Î²)=Y âX (cid:48)Î². i i i It is also helpful to realize that conventional regression models also fall into this class except that in this case X = Z. For example, in linear regression e (Î²)=Y âX (cid:48)Î², while in a nonlinear regression i i i model e (Î²)=Y âm(X ,Î²). In a joint model of the conditional mean (cid:69)[Y |X =x]=x (cid:48)Î² and variance i i i var[Y |X =x]=f (x) (cid:48)Î³,then ï£± Y âX (cid:48)Î² ï£´ ï£² i i e (cid:161)Î²,Î³(cid:162)= . i ï£´ ï£³ (cid:161) Y âX (cid:48)Î²(cid:162)2âf (X ) (cid:48)Î³ i i i Heres=2. Given a conditional moment restriction an unconditional moment restriction can always be con- structed.Thatisforany(cid:96)Ã1functionÏ(cid:161) Z,Î²(cid:162) wecansetg (Î²)=Ï(cid:161) Z ,Î²(cid:162) e (Î²)whichsatisfies(cid:69)(cid:163) g (Î²) (cid:164)= i i i i 0andhencedefinesanunconditionalmomentequationmodel.Theobviousproblemisthattheclassof functionsÏisinfinite.Whichshouldbeselected? Thisisequivalenttotheproblemofselectionofthebestinstruments. If Z â(cid:82)isavalidinstrument satisfying(cid:69)[e|Z]=0,thenZ,Z2,Z3,...,etc.,areallvalidinstruments.Whichshouldbeused? Onesolutionistoconstructaninfinitelistofpotentinstrumentsandthenusethefirstkinstruments. Howisk tobedetermined?Thisisanareaoftheorystillunderdevelopment.Onestudyofthisproblem isDonaldandNewey(2001). Anotherapproachistoconstructtheoptimalinstrumentwhichminimizestheasymptoticvariance. TheformwasuncoveredbyChamberlain(1987).Takethecases=1.Let (cid:183) â (cid:175) (cid:184) R i =(cid:69) âÎ² e i (Î²) (cid:175) (cid:175) (cid:175) Z i andÏ2 =(cid:69)(cid:163) e (Î²)2|Z (cid:164) . Thentheoptimalinstrumentis A =âÏâ2R . Theoptimalmomentis g (Î²)= i i i i i i i A e (Î²).Settingg (Î²)tobethischoice(whichiskÃ1,soisjust-identified)yieldstheGMMestimatorwith i i i lowestasymptoticvariance.InpracticeA isunknown,butitsformhelpsusthinkaboutconstructionof i goodinstruments.",
    "page": 452,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER13. GENERALIZEDMETHODOFMOMENTS 433 Inthelinearmodele (Î²)=Y âX (cid:48)Î²notethatR =â(cid:69)[X |Z ]andÏ2 =(cid:69)(cid:163) e2|Z (cid:164) . Thismeansthe i i i i i i i i i optimalinstrumentis A =Ïâ2(cid:69)[X |Z ]. Inthecaseoflinearregression X =Z so A =Ïâ2Z .Hence i i i i i i i i i efficientGMMisequivalenttoGLS! Inthecaseofendogenousvariablesnotethattheefficientinstrument A involvestheestimationof i the conditional mean of X given Z. In other words, to get the best instrument for X we need the best conditionalmeanmodelforX givenZ notjustanarbitrarylinearprojection.Theefficientinstrumentis alsoinverselyproportionaltotheconditionalvarianceofe.ThisisthesameastheGLSestimator;namely thatimproved efficiency canbe obtainedifthe observations are weighted inversely to theconditional varianceoftheerrors. 13.28 TechnicalProofs* ProofofTheorem13.12Sete (cid:101)i =Y i âX i (cid:48)Î² (cid:98)cgmm ande (cid:98)i =Y i âX i (cid:48)Î² (cid:98)gmm .Bystandardcovariancematrixanal- ysisâ¦ (cid:98) âââ¦andâ¦ (cid:101) âââ¦. Thuswecanreplaceâ¦ (cid:98) andâ¦ (cid:101) inthecriteriawithoutaffectingtheasymptotic p p distribution.Inparticular J(cid:101)(Î² (cid:98)cgmm )= 1 (cid:101) e (cid:48) Zâ¦ (cid:101) â1Z (cid:48) (cid:101) e n = 1 (cid:101) e (cid:48) Zâ¦ (cid:98) â1Z (cid:48) (cid:101) e+o p (1). (13.25) n Nowobservethat Z (cid:48) (cid:101) e=Z (cid:48) (cid:98) eâZ (cid:48) X (cid:161)Î² (cid:98)cgmm âÎ² (cid:98)gmm (cid:162) . Thus 1 (cid:101) e (cid:48) Zâ¦ (cid:98) â1Z (cid:48) (cid:101) e= 1 (cid:98) e (cid:48) Zâ¦ (cid:98) â1Z (cid:48) (cid:98) eâ 2(cid:161)Î² (cid:98)cgmm âÎ² (cid:98)gmm (cid:162)(cid:48) X (cid:48) Zâ¦ (cid:98) â1Z (cid:48) (cid:98) e n n n + 1(cid:161)Î² (cid:98)cgmm âÎ² (cid:98)gmm (cid:162)(cid:48) X (cid:48) Zâ¦ (cid:98) â1Z (cid:48) X (cid:161)Î² (cid:98)cgmm âÎ² (cid:98)gmm (cid:162) n =J(cid:98)(Î² (cid:98)gmm )+ 1(cid:161)Î² (cid:98)cgmm âÎ² (cid:98)gmm (cid:162)(cid:48) X (cid:48) Zâ¦ (cid:98) â1Z (cid:48) X (cid:161)Î² (cid:98)cgmm âÎ² (cid:98)gmm (cid:162) (13.26) n wherethesecondequalityholdssince X (cid:48) Zâ¦ (cid:98) â1Z (cid:48) (cid:98) e =0isthefirst-orderconditionforÎ² (cid:98)gmm . By(13.16) andTheorem13.4,under(cid:72) 0 (cid:112) n (cid:161)Î² (cid:98)cgmm âÎ² (cid:98)gmm (cid:162)=â(cid:161) X (cid:48) Zâ¦â1Z (cid:48) X (cid:162)â1 R (cid:179) R (cid:48)(cid:161) X (cid:48) Zâ¦â1Z (cid:48) X (cid:162)â1 R (cid:180)â1 R (cid:48) (cid:112) n (cid:161)Î² (cid:98)gmm âÎ²(cid:162)+o p (1) ââ(cid:161) Q (cid:48)â¦â1Q (cid:162)â1 RZ (13.27) d where Z â¼N(0,V ) (13.28) R V = (cid:179) RV (cid:48)(cid:161) Q (cid:48)â¦â1Q (cid:162)â1 R (cid:180)â1 . R Puttingtogether(13.25),(13.26),(13.27)and(13.28), D=J(cid:101)(Î² (cid:98)cgmm )âJ(cid:98)(Î² (cid:98)gmm ) (cid:112) (cid:112) = n (cid:161)Î² (cid:98)cgmm âÎ² (cid:98)gmm (cid:162)(cid:48) 1 X (cid:48) Zâ¦ (cid:98) â1 1 Z (cid:48) X n (cid:161)Î² (cid:98)cgmm âÎ² (cid:98)gmm (cid:162) n n ââZ (cid:48) V â1Z â¼Ï2 R q d",
    "page": 453,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER13. GENERALIZEDMETHODOFMOMENTS 434 sinceV >0andZisqÃ1. â  R ProofofTheorem13.15LetÎ² (cid:101)denotetheGMMestimatorobtainedwiththeinstrumentsetZ a andletÎ² (cid:98) denotetheGMMestimatorobtainedwiththeinstrumentsetZ.Sete (cid:101)i =Y i âX i (cid:48)Î² (cid:101),e (cid:98)i =Y i âX i (cid:48)Î² (cid:98), n â¦ (cid:101) =n â1(cid:88) Z ai Z a (cid:48) i e (cid:101)i 2 i=1 n â¦ (cid:98) =n â1(cid:88) Z i Z i (cid:48) e (cid:98)i 2. i=1 LetR bethe(cid:96)Ã(cid:96) selectormatrixsothatZ =R (cid:48) Z.Notethat a a n â¦ (cid:101) =R (cid:48) n â1(cid:88) Z i Z i (cid:48) e (cid:101)i 2R. i=1 By standard covariance matrix analysis, â¦ (cid:98) âââ¦ and â¦ (cid:101) ââR (cid:48)â¦R. Also, 1Z (cid:48) X ââQ, say. By the CLT, p p n p n â1/2Z (cid:48) eââZ whereZ â¼N(0,â¦).Then d n â1/2Z (cid:48) (cid:98) e= (cid:181) I(cid:96) â (cid:181) 1 Z (cid:48) X (cid:182)(cid:181) 1 X (cid:48) Zâ¦ (cid:98) â1 1 Z (cid:48) X (cid:182)â1(cid:181) 1 X (cid:48) Z (cid:182) â¦ (cid:98) â1 (cid:182) n â1/2Z (cid:48) e n n n n ââ (cid:179) I(cid:96) âQ (cid:161) Q (cid:48)â¦â1Q (cid:162)â1 Q (cid:48)â¦â1 (cid:180) Z d and n â1/2Z (cid:48) a(cid:101) e=R (cid:48) (cid:181) I(cid:96) â (cid:181) n 1 Z (cid:48) X (cid:182)(cid:181) n 1 X (cid:48) ZRâ¦ (cid:101) â1R (cid:48) n 1 Z (cid:48) X (cid:182)â1(cid:181) n 1 X (cid:48) Z (cid:182) Râ¦ (cid:101) â1R (cid:48) (cid:182) n â1/2Z (cid:48) e ââR (cid:48) (cid:181) I(cid:96) âQ (cid:179) Q (cid:48) R (cid:161) R (cid:48)â¦R (cid:162)â1 R (cid:48) Q (cid:180)â1 Q (cid:48) R (cid:161) R (cid:48)â¦R (cid:162)â1 R (cid:48) (cid:182) Z d jointly. BylinearrotationsofZ andRwecansetâ¦=I(cid:96)tosimplifythenotation.ThussettingP Q =Q (cid:161) Q (cid:48) Q (cid:162)â1 Q (cid:48) , P R =R (cid:161) R (cid:48) R (cid:162)â1 R (cid:48) andZ â¼N(0,I(cid:96))wehave J(cid:98) ââZ (cid:48)(cid:161) I(cid:96) âP Q (cid:162) Z d and J(cid:101) ââZ (cid:48) (cid:179) P R âP R Q (cid:161) Q (cid:48) P R Q (cid:162)â1 Q (cid:48) P R (cid:180) Z. d Itfollowsthat C =J(cid:98) âJ(cid:101) ââZ (cid:48) AZ d where A= (cid:179) I(cid:96) âP Q âP R +P R Q (cid:161) Q (cid:48) P R Q (cid:162)â1 Q (cid:48) P R (cid:180) . Thisisaquadraticforminastandardnormalvectorandthematrix Aisidempotent(thisisstraightfor- wardtocheck). Z (cid:48) AZ isthusdistributedÏ2 withdegreesoffreedomd equalto d rank(A)=tr (cid:179) I(cid:96) âP Q âP R +P R Q (cid:161) Q (cid:48) P R Q (cid:162)â1 Q (cid:48) P R (cid:180) =(cid:96)âkâ(cid:96) +k=(cid:96) . a b ThustheasymptoticdistributionofC isÏ2 asclaimed. â  (cid:96) b _____________________________________________________________________________________________",
    "page": 454,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER13. GENERALIZEDMETHODOFMOMENTS 435 13.29 Exercises Exercise13.1 Takethemodel Y =X (cid:48)Î²+e (cid:69)[Xe]=0 e2=Z (cid:48)Î³+Î· (cid:69)(cid:163) ZÎ·(cid:164)=0. Findthemethodofmomentsestimators (cid:161)Î² (cid:98),Î³ (cid:98) (cid:162) for (cid:161)Î²,Î³(cid:162) . Exercise13.2 TakethemodelY =X (cid:48)Î²+e with(cid:69)[e|Z]=0. LetÎ² (cid:98)gmm betheGMMestimatorusingthe weightmatrixW =(cid:161) Z (cid:48) Z (cid:162)â1 .Undertheassumption(cid:69)(cid:163) e2|Z (cid:164)=Ï2showthat n (cid:112) n (cid:161)Î² (cid:98) âÎ²(cid:162)ââN (cid:179) 0,Ï2(cid:161) Q (cid:48) M â1Q (cid:162)â1 (cid:180) d whereQ=(cid:69)(cid:163) ZX (cid:48)(cid:164) andM=(cid:69)(cid:163) ZZ (cid:48)(cid:164) . Exercise13.3 TakethemodelY =X (cid:48)Î²+e with(cid:69)[Ze]=0.Lete (cid:101)i =Y i âX i (cid:48)Î² (cid:101)whereÎ² (cid:101)isconsistentforÎ² (e.g.aGMMestimatorwithsomeweightmatrix).AnestimatoroftheoptimalGMMweightmatrixis (cid:195) (cid:33)â1 W(cid:99) = n 1 (cid:88) n Z i Z i (cid:48) e (cid:101)i 2 . i=1 ShowthatW(cid:99) âââ¦â1whereâ¦=(cid:69)(cid:163) ZZ (cid:48) e2(cid:164) . p Exercise13.4 InthelinearmodelestimatedbyGMMwithgeneralweightmatrixW theasymptoticvari- anceofÎ² (cid:98)gmm is V =(cid:161) Q (cid:48) WQ (cid:162)â1 Q (cid:48) Wâ¦WQ (cid:161) Q (cid:48) WQ (cid:162)â1 . (a) LetV bethismatrixwhenW =â¦â1.ShowthatV =(cid:161) Q (cid:48)â¦â1Q (cid:162)â1 . 0 0 (b) WewanttoshowthatforanyW,VâV ispositivesemi-definite(forthenV isthesmallerpossible 0 0 covariancematrixandW =â¦â1istheefficientweightmatrix). Todothisstartbyfindingmatrices AandB suchthatV =A (cid:48)â¦AandV =B (cid:48)â¦B. 0 (c) ShowthatB (cid:48)â¦A=B (cid:48)â¦B andthereforethatB (cid:48)â¦(AâB)=0. (d) UsetheexpressionsV =A (cid:48)â¦A, A=B+(AâB),andB (cid:48)â¦(AâB)=0toshowthatV â¥V . 0 Exercise13.5 ProveTheorem13.8. Exercise13.6 DerivetheconstrainedGMMestimator(13.16). Exercise13.7 ShowthattheconstrainedGMMestimator(13.16)withtheefficientweightmatrixis(13.19). Exercise13.8 ProveTheorem13.9. Exercise13.9 ProveTheorem13.10.",
    "page": 455,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER13. GENERALIZEDMETHODOFMOMENTS 436 Exercise13.10 TheequationofinterestisY =m(X,Î²)+ewith(cid:69)[Ze]=0wherem(x,Î²)isaknownfunc- tion,Î²iskÃ1andZ is(cid:96)Ã1.ShowhowtoconstructanefficientGMMestimatorforÎ². Exercise13.11 AsacontinuationofExercise12.7derivetheefficientGMMestimatorusingtheinstru- mentZ =(X X2) (cid:48) .Doesthisdifferfrom2SLSand/orOLS? Exercise13.12 InthelinearmodelY =X (cid:48)Î²+ewith(cid:69)[Xe]=0theGMMcriterionfunctionforÎ²is J(Î²)= 1(cid:161) Y âXÎ²(cid:162)(cid:48) Xâ¦ (cid:98) â1X (cid:48)(cid:161) Y âXÎ²(cid:162) (13.29) n whereâ¦ (cid:98) =n â1(cid:80)n i=1 X i X i (cid:48) e (cid:98)i 2, e (cid:98)i =Y i âX i (cid:48)Î² (cid:98)are the OLS residuals, and Î² (cid:98) =(cid:161) X (cid:48) X (cid:162)â1 X (cid:48) Y is least squares. TheGMMestimatorofÎ²subjecttotherestrictionr(Î²)=0is Î² (cid:101) =argminJ n (Î²). r(Î²)=0 TheGMMteststatistic(thedistancestatistic)ofthehypothesisr(Î²)=0is D=J(Î² (cid:101))= min J(Î²). (13.30) r(Î²)=0 (a) ShowthatyoucanrewriteJ(Î²)in(13.29)as J(Î²)=n (cid:161)Î²âÎ² (cid:98) (cid:162)(cid:48) V(cid:98) â Î² 1(cid:161)Î²âÎ² (cid:98) (cid:162) andthusÎ² (cid:101)isthesameastheminimumdistanceestimator. (b) ShowthatunderlinearhypothesesthedistancestatisticD in(13.30)equalstheWaldstatistic. Exercise13.13 TakethelinearmodelY =X (cid:48)Î²+e with(cid:69)[Ze]=0. ConsidertheGMMestimatorÎ² (cid:98)ofÎ². Let J =ng n (Î² (cid:98)) (cid:48)â¦ (cid:98) â1g n (Î² (cid:98))denotethetestofoveridentifyingrestrictions. Showthat J ââÏ2 (cid:96)âk asnââ d bydemonstratingeachofthefollowing. (a) Sinceâ¦>0,wecanwriteâ¦â1=CC (cid:48) andâ¦=C (cid:48)â1C â1forsomematrixC. (b) J=n (cid:161) C (cid:48) g (Î² (cid:98)) (cid:162)(cid:48)(cid:161) C (cid:48)â¦ (cid:98)C (cid:162)â1 C (cid:48) g (Î² (cid:98)). n n (c) C (cid:48) g n (Î² (cid:98))=D n C (cid:48) g n (Î²)whereg n (Î²)= n 1Z (cid:48) e and D n =I(cid:96) âC (cid:48) (cid:181) 1 Z (cid:48) X (cid:182)(cid:181)(cid:181) 1 X (cid:48) Z (cid:182) â¦ (cid:98) â1 (cid:181) 1 Z (cid:48) X (cid:182)(cid:182)â1(cid:181) 1 X (cid:48) Z (cid:182) â¦ (cid:98) â1C (cid:48)â1. n n n n (d) D n ââI(cid:96) âR (cid:161) R (cid:48) R (cid:162)â1 R (cid:48) whereR=C (cid:48)(cid:69)(cid:163) ZX (cid:48)(cid:164) . p (e) n1/2C (cid:48) g n (Î²)ââuâ¼N(0,I(cid:96)). d (f) Jââu (cid:48) (cid:179) I(cid:96) âR (cid:161) R (cid:48) R (cid:162)â1 R (cid:48) (cid:180) u. d (g) u (cid:48) (cid:179) I(cid:96) âR (cid:161) R (cid:48) R (cid:162)â1 R (cid:48) (cid:180) uâ¼Ï2 (cid:96)âk . Hint: I(cid:96) âR (cid:161) R (cid:48) R (cid:162)â1 R (cid:48) isaprojectionmatrix.",
    "page": 456,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER13. GENERALIZEDMETHODOFMOMENTS 437 Exercise13.14 TakethemodelY =X (cid:48)Î²+e with(cid:69)[Ze]=0, Y â(cid:82), X â(cid:82)k, Z â(cid:82)(cid:96) , (cid:96)â¥k. Considerthe statistic J(Î²)=nm (Î²) (cid:48) Wm (Î²) n n m (Î²)= 1 (cid:88) n Z (cid:161) Y âX (cid:48)Î²(cid:162) n n i i i i=1 forsomeweightmatrixW >0. (a) Takethehypothesis(cid:72) :Î²=Î² .DerivetheasymptoticdistributionofJ(Î² )under(cid:72) asnââ. 0 0 0 0 (b) WhatchoiceforW yieldsaknownasymptoticdistributioninpart(a)? (Bespecificaboutdegrees offreedom.) (c) WritedownanappropriateestimatorW(cid:99) forW whichtakesadvantageof(cid:72) 0 . (Youdonotneedto demonstrateconsistencyorunbiasedness.) (d) Describeanasymptotictestof(cid:72) against(cid:72) :Î²(cid:54)=Î² basedonthisstatistic. 0 1 0 (e) Usetheresultinpart(d)toconstructaconfidenceregionforÎ². Whatcanyousayabouttheform of this region? For example, does the confidence region take the form of an ellipse, similar to conventionalconfidenceregions? Exercise13.15 ConsiderthemodelY =X (cid:48)Î²+ewith(cid:69)[Ze]=0and R (cid:48)Î²=0 (13.31) with Y â(cid:82), X â(cid:82)k, Z â(cid:82)(cid:96) , (cid:96)>k. The matrix R is kÃq with 1â¤ q <k. You have a random sample (Y ,X ,Z :i =1,...,n). i i i Forsimplicity,assumetheefficientweightmatrixW =(cid:161)(cid:69)(cid:163) ZZ (cid:48) e2(cid:164)(cid:162)â1 isknown. (a) WriteouttheGMMestimatorÎ² (cid:98)ignoringconstraint(13.31). (b) WriteouttheGMMestimatorÎ² (cid:101)addingtheconstraint(13.31). (cid:112) (c) Findtheasymptoticdistributionof n (cid:161)Î² (cid:101) âÎ²(cid:162) asnââunderAssumption(13.31). Exercise13.16 Theobserveddatais{Y ,X ,Z }â(cid:82)Ã(cid:82)kÃ(cid:82)(cid:96) ,k>1and(cid:96)>k>1,i =1,...,n.Themodel i i i isY =X (cid:48)Î²+ewith(cid:69)[Ze]=0. (a) GivenaweightmatrixW >0writedowntheGMMestimatorÎ² (cid:98)forÎ². (b) Supposethemodelismisspecified.Specifically,assumethatforsomeÎ´(cid:54)=0, e=Î´n â1/2+u (13.32) (cid:69)[u|Z]=0 withÂµ =(cid:69)[Z](cid:54)=0.Showthat(13.32)impliesthat(cid:69)[Ze](cid:54)=0. Z (cid:112) (c) Express n (cid:161)Î² (cid:98) âÎ²(cid:162) asafunctionofW,n,Î´,andthevariables(X i ,Z i ,u i ). (cid:112) (d) Findtheasymptoticdistributionof n (cid:161)Î² (cid:98) âÎ²(cid:162) underAssumption(13.32).",
    "page": 457,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER13. GENERALIZEDMETHODOFMOMENTS 438 Exercise13.17 The model is Y = ZÎ²+XÎ³+e with (cid:69)[e|Z]=0, X â(cid:82) and Z â(cid:82). X is potentially en- dogenous and Z is exogenous. Someone suggests estimating (Î²,Î³) by GMM using the pair (Z,Z2) as instruments.Isthisfeasible?Underwhatconditionsisthisavalidestimator? Exercise13.18 Theobservationsarei.i.d., (Y ,X ,Q :i =1,...,n),where X iskÃ1andQ ismÃ1.The i i i modelisY =X (cid:48)Î²+ewith(cid:69)[Xe]=0and(cid:69)[Qe]=0.FindtheefficientGMMestimatorforÎ². Exercise13.19 YouwanttoestimateÂµ=(cid:69)[Y]undertheassumptionthat(cid:69)[X]=0,whereY and X are scalarandobservedfromarandomsample.FindanefficientGMMestimatorforÂµ. Exercise13.20 ConsiderthemodelY =X (cid:48)Î²+egiven(cid:69)[Ze]=0andR (cid:48)Î²=0.ThedimensionsareX âRk andZ âR (cid:96) with(cid:96)>k.ThematrixR iskÃq,1â¤q<k.DeriveanefficientGMMestimatorforÎ². Exercise13.21 TakethelinearequationY =X (cid:48)Î²+eandconsiderthefollowingestimatorsofÎ². 1. Î² (cid:98):2SLSusingtheinstrumentsZ 1 . 2. Î² (cid:101):2SLSusingtheinstrumentsZ 2 . 3. Î²:GMMusingtheinstrumentsZ =(Z ,Z )andtheweightmatrix 1 2 (cid:195) (cid:161) Z (cid:48) Z (cid:162)â1Î» 0 (cid:33) W = 1 0 1 (cid:161) Z (cid:48) Z (cid:162)â1 (1âÎ») 2 2 forÎ»â(0,1). FindanexpressionforÎ²whichshowsthatitisaspecificweightedaverageofÎ² (cid:98)andÎ² (cid:101). Exercise13.22 Consider the just-identified model Y = X (cid:48)Î² +X (cid:48)Î² +e with (cid:69)[Ze]=0 where X =(X (cid:48) 1 1 2 2 1 X (cid:48) ) (cid:48)â(cid:82)k andZ â(cid:82)k.Wewanttotest(cid:72) :Î² =0.Threeeconometriciansarecalledforadvice. 2 0 1 â¢ Econometrician1proposestesting(cid:72) byaWaldstatistic. 0 â¢ Econometrician2suggeststesting(cid:72) bytheGMMDistanceStatistic. 0 â¢ Econometrician3suggeststesting(cid:72) usingthetestofoveridentifyingrestrictions. 0 You are asked to settle this dispute. Explain the advantages and/or disadvantages of the different proceduresinthisspecificcontext. Exercise13.23 TakethemodelY =X (cid:48)Î²+e with(cid:69)[Xe]=0andÎ²=QÎ¸,whereÎ²iskÃ1,Q iskÃmwith m<k,Q isknown,andÎ¸ismÃ1.Theobservations(Y ,X )arei.i.d.acrossi =1,...,n. i i UndertheseassumptionswhatistheefficientestimatorofÎ¸? Exercise13.24 TakethemodelY =Î¸+ewith(cid:69)[Xe]=0,Y â(cid:82),X â(cid:82)k and(Y ,X )arandomsample. i i (a) FindtheefficientGMMestimatorofÎ¸. (b) Isthismodelover-identifiedorjust-identified? (c) FindtheGMMteststatisticforover-identification.",
    "page": 458,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER13. GENERALIZEDMETHODOFMOMENTS 439 Exercise13.25 TakethemodelY =X (cid:48)Î²+ewith(cid:69)[Xe]=0whereX containsaninterceptso(cid:69)[e]=0.An enterprisingeconometriciannoticesthatthisimpliesthenmomentconditions (cid:69)[e ]=0,i =1,...,n. i GivenannÃnweightmatrixW,thisimpliesaGMMcriterion J(Î²)=(cid:161) Y âXÎ²(cid:162)(cid:48) W (cid:161) Y âXÎ²(cid:162) . (a) Underi.i.d.sampling,showthattheefficientweightmatrixisW =Ïâ2I whereÏ2=(cid:69)(cid:163) e2(cid:164) . n (b) UsingtheweightmatrixW =Ïâ2I n findtheGMMestimatorÎ² (cid:98)thatminimizesJ(Î²). (c) FindasimpleexpressionfortheminimizedcriteriaJ(Î² (cid:98)). (d) Theorem13.14saysthatcriterionsuchas J(Î² (cid:98))areasymptoticallyÏ2 where(cid:96)isthenumberof (cid:96)âk moments. WhiletheassumptionsofTheorem13.14donotapplytothiscontext,whatis(cid:96)here? Thatis,whichÏ2distributionistheassertedasymptoticdistribution? (e) Doestheanswerin(d)makesense?Explainyourreasoning. Exercise13.26 TakethemodelY =X (cid:48)Î²+ewith(cid:69)[e|X]=0and(cid:69)(cid:163) e2|X (cid:164)=Ï2.Aneconometricianmore enterprisingthantheoneinpreviousquestionnoticesthatthisimpliesthenk momentconditions (cid:69)[X e ]=0, i =1,...,n. i i Wecanwritethemomentsusingmatrixnotationas(cid:69) (cid:104) X (cid:48)(cid:161) Y âXÎ²(cid:162) (cid:105) where ï£« X (cid:48) 0 Â·Â·Â· 0 ï£¶ 1 ï£¬ 0 X (cid:48) 0 ï£· X =ï£¬ ï£¬ . . 2 . ï£· ï£·. ï£¬ . . . . . . ï£· ï£­ ï£¸ 0 0 Â·Â·Â· X (cid:48) n GivenannkÃnk weightmatrixW thisimpliesaGMMcriterion J(Î²)=(cid:161) Y âXÎ²(cid:162)(cid:48) XWX (cid:48)(cid:161) Y âXÎ²(cid:162) . (a) Calculateâ¦=(cid:69) (cid:104) X (cid:48) ee (cid:48) X (cid:105) . (b) The econometrician decides to set W = â¦â , the Moore-Penrose generalized inverse of â¦. (See SectionA.6.)Note:Ausefulfactisthatforavectora, (cid:161) aa (cid:48)(cid:162)â =aa (cid:48)(cid:161) a (cid:48) a (cid:162)â2 . (c) FindtheGMMestimatorÎ² (cid:98)thatminimizesJ(Î²). (d) FindasimpleexpressionfortheminimizedcriterionJ(Î² (cid:98)). (e) CommentonwhethertheÏ2approximationfromTheorem13.14isappropriateforJ(Î² (cid:98)). Exercise13.27 ContinuationofExercise12.23,basedontheempiricalworkreportedinAcemoglu,John- sonandRobinson(2001). (a) Re-estimatethemodelestimatedinpart(j)byefficientGMM.Usethe2SLSestimatesasthefirst- stepfortheweightmatrixandthencalculatetheGMMestimatorusingthisweightmatrixwithout furtheriteration.Reporttheestimatesandstandarderrors.",
    "page": 459,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER13. GENERALIZEDMETHODOFMOMENTS 440 (b) CalculateandreporttheJ statisticforoveridentification. (c) ComparetheGMMand2SLSestimates.Discussyourfindings. Exercise13.28 ContinuationofExercise12.25,whichinvolvedestimationofawageequationby2SLS. (a) Re-estimatethemodelinpart(a)byefficientGMM.Dotheresultschangemeaningfully? (b) Re-estimatethemodelinpart(d)byefficientGMM.Dotheresultschangemeaningfully? (c) ReporttheJ statisticforoveridentification.",
    "page": 460,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Part IV Dependent and Panel Data 441",
    "page": 461,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Chapter 14 Time Series 14.1 Introduction AtimeseriesY â(cid:82)m isaprocesswhichissequentiallyorderedovertime. Inthistextbookwefocus t ondiscretetimeserieswheret isaninteger,thoughthereisalsoaconsiderableliteratureoncontinuous- timeprocesses.Todenotethetimeperioditistypicaltousethesubscriptt.Thetimeseriesisunivariate ifm=1andmultivariateifm>1. Thischapterisprimarilyfocusedonunivariatetimeseriesmodels, thoughwedescribetheconceptsforthemultivariatecasewhentheaddedgeneralitydoesnotaddextra complications. Most economic time series are recorded at discrete intervals such as annual, quarterly, monthly, weekly, ordaily. Thenumberofobservations s peryeariscalledthefrequency. Inmostcaseswewill denotetheobservedsamplebytheperiodst=1,...,n. Becauseofthesequentialnatureoftimeseriesweexpectthatobservationscloseincalendertime, e.g. Y t anditslaggedvalueY tâ1 ,willbedependent. Thistypeofdependencestructurerequiresadiffer- entdistributionaltheorythanforcross-sectionalandclusteredobservationssincewecannotdividethe sampleintoindependentgroups. Manyoftheissueswhichdistinguishtimeseriesfromcross-section econometricsconcernthemodelingofthesedependencerelationships. There are many excellent textbooks for time series analysis. The encyclopedic standard is Hamil- ton(1994). OthersincludeHarvey(1990),Tong(1990),BrockwellandDavis(1991),FanandYao(2003), LÃ¼tkepohl(2005),Enders(2014),andKilianandLÃ¼tkepohl(2017). Fortextbooksontherelatedsubject offorecastingseeGranger(1989),GrangerandNewbold(1986),andElliottandTimmermann(2016). 14.2 Examples Manyeconomictimeseriesaremacroeconomicvariables. AnexcellentresourceforU.S.macroeco- nomicdataaretheFRED-MDandFRED-QDdatabaseswhichcontainawidesetofmonthlyandquar- terlyvariables, assembledandmaintainedbytheSt. LouisFederalReserveBank. SeeMcCrackenand Ng (2015). The datasets FRED-MD and FRED-QD for 1959-2017 are posted on the textbook website. FRED-MDhas129variablesover708months.FRED-QDhas248variablesover236quarters. Whenworkingwithtimeseriesdataoneofthefirsttasksistoplottheseriesagainsttime. InFigures 14.1-14.4 we plot eight example time series from FRED-QD and FRED-MD. As is conventional in time seriesplotsthex-axisdisplayscalendardates(inthiscaseyears)andthey-axisdisplaysthelevelofthe series.Theseriesplottedare:(1a)RealU.S.GDP(gdpc1);(1b)U.S.-Canadaexchangerate(excausx);(2a) InterestrateonU.S.10-yearTreasurybond(gs10);(2b)Realcrudeoilprice(oilpricex);(3a)U.S.unem- ploymentrate(unrate);(3b)U.S.realnon-durablesconsumptiongrowthrate(growthrateofpcndx);(4a) 442",
    "page": 462,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 443 sralloD 2102 deniahC fo snoilliB 1960 1970 1980 1990 2000 2010 2020 00061 00031 00001 0007 0004 (a)U.S.RealGDP ralloD .S.U/ralloD naidanaC 1960 1970 1980 1990 2000 2010 2020 6.1 5.1 4.1 3.1 2.1 1.1 0.1 (b)U.S.-CanadaExchangeRate Figure14.1:U.S.GDPandExchangeRate egatnecreP 1960 1970 1980 1990 2000 2010 2020 41 21 01 8 6 4 2 (a)InterestRateon10-YearTreasury lerraB rep sralloD 2102 1960 1970 1980 1990 2000 2010 2020 021 001 08 06 04 02 0 (b)RealCrudeOilPrice Figure14.2:InterestRateandCrudeOilPrice U.S.CPIinflationrate(growthrateofcpiaucsl);(4b)S&P500return(growthrateofsp500).(1a)and(3b) arequarterlyseries,therestaremonthly. Many of the plots are smooth, meaning that the neighboring values (in calendar time) are similar tooneanotherandhenceareseriallycorrelated. Someoftheplotsarenon-smooth,meaningthatthe neighboring values are not similar and hence less correlated. At least one plot (real GDP) displays an upwardtrend.",
    "page": 463,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 444 egatnecreP 1960 1970 1980 1990 2000 2010 2020 11 01 9 8 7 6 5 4 (a)U.S.UnemploymentRate egatnecreP 1960 1970 1980 1990 2000 2010 2020 3 2 1 0 1â 2â (b)ConsumptionGrowthRate Figure14.3:UnemploymentRateandConsumptionGrowthRate egatnecreP 1960 1970 1980 1990 2000 2010 2020 5.1 0.1 5.0 0.0 5.0â 0.1â 5.1â (a)U.S.InflationRate egatnecreP 1960 1970 1980 1990 2000 2010 2020 01 5 0 5â 01â 51â 02â (b)S&P500Return Figure14.4:U.S.InflationRateandS&P500Return 14.3 DifferencesandGrowthRates Itiscommontotransformseriesbytakinglogarithms,differencesand/orgrowthrates. Threeofthe seriesinFigures14.3-14.4(consumptiongrowth,inflation[growthrateofCPIindex],andS&P500return) aredisplayedasgrowthrates. Thistransformationmaybedoneforanumberofdifferentreasons. The mostcredibleisthatthisisthesuitablevariableforthedesiredanalysis. ManyaggregateseriessuchasrealGDParetransformedbytakingnaturallogarithms. Thisflattens theapparentexponentialgrowthandmakesfluctuationsproportionate.",
    "page": 464,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 445 ThefirstdifferenceofaseriesY is t âY t =Y t âY tâ1 . Theseconddifferenceis â2Y t =âY t ââY tâ1 . Higher-orderdifferencescanbedefinedsimilarlybutarenotusedinpractice. Theannual,oryear-on-year,changeofaseriesY withfrequencysis t â s Y t =Y t âY tâs . Thereareseveralmethodstocalculategrowthrates. Theone-periodgrowthrateisthepercentage changefromperiodtâ1toperiodt: (cid:181)âY (cid:182) (cid:181) Y (cid:182) Q =100 t =100 t â1 . (14.1) t Y tâ1 Y tâ1 Themultiplicationby100isnotessentialbutscalesQ sothatitisapercentage. Thisisthetransforma- t tionusedfortheplotsinFigures14.3(b)-14.4(a)(b). Fornon-annualdatatheone-periodgrowthrate(14.1)maybeunappealingforinterpretation. Con- sequently, statisticalagenciescommonlyreportâannualizedâgrowthrateswhichistheannualgrowth which would occur if the one-period growth rate is compounded for a full year. For a series with fre- quencystheannualizedgrowthrateis (cid:181)(cid:181) Y (cid:182)s (cid:182) A =100 t â1 . (14.2) t Y tâ1 Noticethat A isanonlinearfunctionofQ . t t Year-on-yeargrowthratesare (cid:181)â Y (cid:182) (cid:181) Y (cid:182) G =100 s t =100 t â1 . t Y tâs Y tâs Thesedonotneedannualization. Growthratesarecloselyrelatedtologarithmictransformations. Forsmallgrowthrates,Q , A and t t G areapproximatelyfirstdifferencesinlogarithms: t Q (cid:39)100âlogY t t A (cid:39)400âlogY t t G (cid:39)100â logY . t s t ForanalysisusinggrowthratesIrecommendtheone-periodgrowthrates(14.1)ordifferencedlog- arithmsratherthantheannualizedgrowthrates(14.2). Whileannualizedgrowthratesarepreferredfor reporting, they are a highly nonlinear transformation which is unnatural for statistical analysis. Dif- ferenced logarithms are the most common choice and are recommended for models which combine log-levelsandgrowthratesforthenthemodelsarelinearinallvariables. 14.4 Stationarity Recallthatcross-sectionalobservationsareconventionallytreatedasrandomdrawsfromanunder- lyingpopulation. Thisisnotanappropriatemodelfortimeseriesprocessesduetoserialdependence.",
    "page": 465,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 446 Instead,wetreattheobservedsample{Y ,...,Y }asarealizationofadependentstochasticprocess. Itis 1 n oftenusefultoview{Y 1 ,...,Y n }asasubsetofanunderlyingdoubly-infinitesequence{...,Y tâ1 ,Y t ,Y t+1 ,...}. ArandomvectorY t canbecharacterizedbyitsdistribution. Asetsuchas(Y t ,Y t+1 ,...,Y t+(cid:96))canbe characterizedbytheirjointdistribution. Importantfeaturesofthesedistributionsaretheirmeans,vari- ances,andcovariances.Sincethereisonlyoneobservedtimeseriessample,inordertolearnaboutthese distributionsthereneedstobesomesortofconstancy. Thismayonlyholdafterasuitabletransforma- tionsuchasgrowthrates(asdiscussedintheprevioussection). Themostcommonlyassumedformofconstancyisstationarity. Therearetwodefinitions. Thefirst issufficientforconstructionoflinearmodels. Definition14.1 {Y }iscovarianceorweaklystationaryifthemeanÂµ=(cid:69)[Y ] t t andcovariancematrixÎ£=var[Y ]=(cid:69) (cid:104) (cid:161) Y âÂµ(cid:162)(cid:161) Y âÂµ(cid:162)(cid:48)(cid:105) arefiniteandarein- t t t dependentoft,andtheautocovariances Î(k)=cov(Y t ,Y tâk )=(cid:69) (cid:104) (cid:161) Y t âÂµ(cid:162)(cid:161) Y tâk âÂµ(cid:162)(cid:48)(cid:105) areindependentoft forallk. IntheunivariatecasewetypicallywritethevarianceasÏ2andautocovariancesasÎ³(k). ThemeanÂµandvarianceÎ£arefeaturesofthemarginaldistributionofY (thedistributionofY at t t aspecifictimeperiodt). Theirconstancyasstatedintheabovedefinitionmeansthatthesefeaturesof thedistributionarestableovertime. TheautocovariancesÎ(k)arefeaturesofthebivariatedistributionsof(Y t ,Y tâk ). Theirconstancyas statedinthedefinitionmeansthatthecorrelationpatternsbetweenadjacentY arestableovertimeand t onlydependonthenumberoftimeperiodsk separatingthevariables. BysymmetrywehaveÎ(âk)= Î(k) (cid:48) .IntheunivariatecasethissimplifiestoÎ³(âk)=Î³(k). TheautocovariancessummarizethelineardependencebetweenY anditslags.Ascale-freemeasure t oflineardependenceintheunivariatecasearetheautocorrelations Ï(k)=corr(Y t ,Y tâk )= (cid:112) v c a o r v [Y (Y t ] t v , a Y r tâ [Y k t ) â1 ] = Î³ Ï (k 2 ) = Î³ Î³ ( ( k 0) ) . NoticebysymmetrythatÏ(âk)=Ï(k). Theseconddefinitionofstationarityconcernstheentirejointdistribution. Definition14.2 {Y } is strictly stationary if the joint distribution of t (Y t ,...,Y t+(cid:96))isindependentoft forall(cid:96). Thisisthenaturalgeneralizationofthecross-sectiondefinitionofidenticaldistributions. Strictsta- tionarityimpliesthatthe(marginal)distributionofY doesnotvaryovertime. Italsoimpliesthatthe t bivariate distributions of (Y t ,Y t+1 ) and multivariate distributions of (Y t ,...,Y t+(cid:96)) are stable over time. Undertheassumptionofaboundedvarianceastrictlystationaryprocessiscovariancestationary1. 1Moregenerally,thetwoclassesarenon-nestedsincestrictlystationaryinfinitevarianceprocessesarenotcovariancesta- tionary.",
    "page": 466,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 447 Forformalstatisticaltheorywewillgenerallyrequirethestrongerassumptionofstrictstationarity. Thereforeifwelabelaprocessasâstationaryâyoushouldinterpretitasmeaningâstrictlystationaryâ. ThecoremeaningofbothweakandstrictstationarityisthesameâthatthedistributionofY isstable t overtime. TounderstandtheconceptitmaybeusefultoreviewtheplotsinFigures14.1-14.4. Arethese stationary processes? If so, we would expect that the mean and variance to be stable over time. This seems unlikely to apply to the series in Figures 14.1 and 14.2, as in each case it is difficult to describe whatistheâtypicalâvalueoftheseries.StationaritymaybeappropriatefortheseriesinFigures14.3and 14.4aseachoscillateswithafairlyregularpattern.Itisdifficult,however,toknowwhetherornotagiven timeseriesisstationarysimplybyexaminingatimeseriesplot. Astraightforwardbutessentialrelationshipisthatani.i.d.processisstrictlystationary. Theorem14.1 IfY isi.i.d.,thenitstrictlystationary. t Herearesomeexamplesofstrictlystationaryscalarprocesses.Ineach,e isi.i.d.and(cid:69)[e ]=0. t t Example14.1 Y t =e t +Î¸e tâ1 . Example14.2 Y =Z forsomerandomvariableZ. t Example14.3 Y =(â1)tZ forarandomvariableZ whichissymmetricallydistributedabout0. t Example14.4 Y =Zcos(Î¸t)forarandomvariableZ symmetricallydistributedabout0. t Herearesomeexamplesofprocesseswhicharenotstationary. Example14.5 Y =t. t Example14.6 Y =(â1)t. t Example14.7 Y =cos(Î¸t). t (cid:112) Example14.8 Y = te . t t Example14.9 Y t =e t +t â1/2e tâ1 . Example14.10 Y t =Y tâ1 +e t withY 0 =0. Fromtheexampleswecanseethatstationaritymeansthatthedistributionisconstantovertime. It doesnotmean,however,thattheprocesshassomesortoflimiteddependence,northatthereisanab- senceofperiodicpatterns. Theserestrictionsareassociatedwiththeconceptsofergodicityandmixing whichweshallintroduceinsubsequentsections.",
    "page": 467,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 448 14.5 TransformationsofStationaryProcesses Oneoftheimportantpropertiesofstrictstationarityisthatitispreservedbytransformation.Thatis, transformationsofstrictlystationaryprocessesarealsostrictlystationary.Thisincludestransformations whichincludethefullhistoryofY . t Theorem14.2 IfY t isstrictlystationaryand X t =Ï(Y t ,Y tâ1 ,Y tâ2 ,...)â(cid:82)q isa randomvectorthenX isstrictlystationary. t Theorem 14.2 is extremely useful both for the study of stochastic processes which are constructed fromunderlyingerrorsandforthestudyofsamplestatisticssuchaslinearregressionestimatorswhich arefunctionsofsampleaveragesofsquaresandcross-productsoftheoriginaldata. WegivetheproofofTheorem14.2inSection14.47. 14.6 ConvergentSeries Atransformationwhichincludesthefullpasthistoryisaninfinite-ordermovingaverage. Forscalar Y andcoefficientsa definethevectorprocess j â (cid:88) X t = a j Y tâj . (14.3) j=0 Manytime-seriesmodelsinvolverepresentationsandtransformationsoftheform(14.3). Theinfiniteseries(14.3)existsifitisconvergent,meaningthatthesequence (cid:80)N j=0 a j Y tâj hasafinite limitasN ââ.SincetheinputsY arerandomwedefinethisasaprobabilitylimit. t Definition14.3 The infinite series (14.3) converges almost surely if (cid:80)N j=0 a j Y tâj has a finite limit as N â â with probability one. In this case wedescribeX asconvergent. t Theorem14.3 If Y t is strictly stationary, (cid:69)|Y| < â, and (cid:80)â j=0 (cid:175) (cid:175)a j (cid:175) (cid:175) < â, then (14.3)convergesalmostsurely.Furthermore,X isstrictlystationary. t TheproofofTheorem14.3isprovidedinSection14.47.",
    "page": 468,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 449 14.7 Ergodicity Stationarityaloneisnotsufficientfortheweaklawoflargenumbersastherearestrictlystationary processeswithnotime seriesvariation. As wedescribedearlier, anexample ofastationary processis Y =Z forsomerandomvariableZ.Thisisrandombutconstantoveralltime.Animplicationisthatthe t samplemeanofY =Z willbeinconsistentforthepopulationmean. t What is a minimal assumption beyond stationarity so that the law of large numbers applies? This topic is called ergodicity. It is sufficiently important that it is treated as a separate area of study. We mention only a few highlights here. For a rigorous treatment see a standard textbook such as Walters (1982). A time series Y is ergodic if all invariant events are trivial, meaning that any event which is unaf- t fectedbytime-shiftshasprobabilityeitherzeroorone. Thisdefinitionisratherabstractanddifficultto graspbutfortunatelyitisnotneededbymosteconomists. AusefulintuitionisthatifY isergodicthenitssamplepathswillpassthroughallpartsofthesample t spacenevergettingâstuckâinasubregion. Wewillfirstdescribethepropertiesofergodicserieswhicharerelevantforourneedsandfollowwith themorerigoroustechnicaldefinitions.ForproofsoftheresultsseeSection14.47. First,manystandardtimeseriesprocessescanbeshowntobeergodic. Ausefulstartingpointisthe observationthatani.i.d.sequenceisergodic. Theorem14.4 IfY â(cid:82)m isi.i.d.thenitstrictlystationaryandergodic. t Second,ergodicity,likestationarity,ispreservedbytransformation. Theorem14.5 If Y â (cid:82)m is strictly stationary and ergodic and X = t t Ï(Y t ,Y tâ1 ,Y tâ2 ,...)isarandomvector,thenX t isstrictlystationaryandergodic. Asanexample,theinfinite-ordermovingaveragetransformation(14.3)isergodiciftheinputiser- godicandthecoefficientsareabsolutelyconvergent. Theorem14.6 IfY t isstrictlystationary,ergodic,(cid:69)|Y|<â,and (cid:80)â j=0 (cid:175) (cid:175)a j (cid:175) (cid:175) <â thenX t =(cid:80)â j=0 a j Y tâj isstrictlystationaryandergodic. Wenowpresentausefulproperty.ItisthattheCesÃ rosumoftheautocovarianceslimitstozero. Theorem14.7 IfY â(cid:82)isstrictlystationary,ergodic,and(cid:69)(cid:163) Y2(cid:164)<â,then t 1 (cid:88) n n l â im ân (cid:96)=1 cov(Y t ,Y t+(cid:96))=0. (14.4)",
    "page": 469,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 450 Theresult(14.4)canbeinterpretedasthatthecovariancesâonaverageâtendtozero. Someauthors havemis-statedergodicityasimplyingthatthecovariancestendtozerobutthisisnotcorrect,as(14.4) allows,forexample,thenon-convergentsequencecov(Y t ,Y t+(cid:96))=(â1) (cid:96) .Thereasonwhy(14.4)ispartic- ularlyusefulisbecauseitissufficientfortheWLLNaswediscoverlaterinTheorem14.9. We now give the formal definition of ergodicity for interested readers. As the concepts will not be usedagainmostreaderscansafelyskipthisdiscussion. As we stated above, by definition the series Y â(cid:82)m is ergodic if all invariant events are trivial. To t understand this we introduce some technical definitions. First, we can write an event as A =(cid:169) Y(cid:101)t âG (cid:170) where Y(cid:101)t =(...,Y tâ1 ,Y t ,Y t+1 ,...) is an infinite history andG â(cid:82)mâ . Second, the (cid:96)th time-shift of Y(cid:101)t is definedasY(cid:101)t+(cid:96) =(...,Y tâ1+(cid:96),Y t+(cid:96),Y t+1+(cid:96),...).ThusY(cid:101)t+(cid:96)replaceseachobservationinY(cid:101)t byits(cid:96)th shifted valueY t+(cid:96).Atime-shiftoftheevent A=(cid:169) Y(cid:101)t âG (cid:170) is A(cid:96) =(cid:169) Y(cid:101)t+(cid:96) âG (cid:170) .Third,anevent Aiscalledinvariant ifitisunaffectedbyatime-shift, sothat A(cid:96) = A. ThusreplacinganyhistoryY(cid:101)t withitsshiftedhistory Y(cid:101)t+(cid:96) doesnâtchangetheevent. Invarianteventsareratherspecial. Anexampleofaninvarianteventis A={maxââ<t<âY t â¤0}. Fourth,anevent Aiscalledtrivialifeither(cid:80)[A]=0or(cid:80)[A]=1. Youcanthink oftrivialeventsasessentiallynon-random. Recall,bydefinitionY isergodicifallinvarianteventsare t trivial.Thismeansthatanyeventwhichisunaffectedbyatimeshiftistrivialâisessentiallynon-random. Forexample,againconsidertheinvariantevent A={maxââ<t<âY t â¤0}.IfY t =Z â¼N(0,1)forallt then (cid:80)[A]=(cid:80)[Z â¤0]=0.5. Sincethisdoesnotequal0or1thenY =Z isnotergodic. However,ifY isi.i.d. t t N(0,1)then(cid:80)[maxââ<t<âY t â¤0]=0. Thisisatrivialevent. ForY t tobeergodic(itisinthiscase)all suchinvarianteventsmustbetrivial. Animportanttechnicalresultisthatergodicityisequivalenttothefollowingproperty. Theorem14.8 AstationaryseriesY â(cid:82)m isergodiciffforallevents AandB t 1 (cid:88) n lim (cid:80)[A(cid:96) â©B]=(cid:80)[A](cid:80)[B]. (14.5) nâân (cid:96)=1 Thisresultisratherdeepsowedonotproveithere. SeeWalters(1982),Corollary1.14.2,orDavid- son (2020), Theorem 14.7. The limit in (14.5) is the CesÃ ro sum of (cid:80)[A(cid:96) â©B]. The Theorem of CesÃ ro Means(TheoremA.4ofIntroductiontoEconometrics)showsthatasufficientconditionfor(14.5)isthat (cid:80)[A(cid:96) â©B] â (cid:80)[A](cid:80)[B] which is known as mixing. Thus mixing implies ergodicity. Mixing, roughly, meansthatseparatedeventsareasymptoticallyindependent. Ergodicityisweaker,onlyrequiringthat theeventsareasymptoticallyindependentâonaverageâ.WediscussmixinginSection14.12. 14.8 ErgodicTheorem Theergodictheoremisoneofthemostfamousresultsintimeseriestheory",
    "page": 470,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". The Theorem of CesÃ ro Means(TheoremA.4ofIntroductiontoEconometrics)showsthatasufficientconditionfor(14.5)isthat (cid:80)[A(cid:96) â©B] â (cid:80)[A](cid:80)[B] which is known as mixing. Thus mixing implies ergodicity. Mixing, roughly, meansthatseparatedeventsareasymptoticallyindependent. Ergodicityisweaker,onlyrequiringthat theeventsareasymptoticallyindependentâonaverageâ.WediscussmixinginSection14.12. 14.8 ErgodicTheorem Theergodictheoremisoneofthemostfamousresultsintimeseriestheory. Thereareactuallysev- eralformsofthetheorem,mostofwhichconcernalmostsureconvergence. Forsimplicitywestatethe theoremintermsofconvergenceinprobability.",
    "page": 470,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 451 Theorem14.9 ErgodicTheorem. IfY â(cid:82)m isstrictlystationary,ergodic,and(cid:69)(cid:107)Y(cid:107)<â,thenasnââ, t (cid:176) (cid:176) (cid:69)(cid:176)Y âÂµ(cid:176)ââ0 (14.6) (cid:176) (cid:176) and Y ââÂµ (14.7) p whereÂµ=(cid:69)[Y]. Theergodictheoremshowsthatergodicityissufficientforconsistentestimation. Themomentcon- dition(cid:69)(cid:107)Y(cid:107)<âisthesameasintheWLLNfori.i.d.observations. Wenowprovideaproofoftheergodictheoremforthescalarcaseundertheadditionalassumption thatvar[Y]=Ï2<â.AproofwhichrelaxesthisassumptionisprovidedinSection14.47. Bydirectcalculation var (cid:104) Y (cid:105) = 1 (cid:88) n (cid:88) n Î³(cid:161) tâj (cid:162) n2 t=1j=1 whereÎ³((cid:96))=cov(Y t ,Y t+(cid:96)). ThedoublesumisoverallelementsofannÃn matrixwhosetjth element isÎ³(cid:161) tâj (cid:162) . ThediagonalelementsareÎ³(0)=Ï2,thefirstoff-diagonalelementsareÎ³(1),thesecondoff- diagonalelementsareÎ³(2)andsoon.Thismeansthattherearepreciselyndiagonalelementsequalling Ï2,2(nâ1)equallingÎ³(1),etc.Thustheaboveequals var (cid:104) Y (cid:105) = 1 (cid:161) nÏ2+2(nâ1)Î³(1)+2(nâ2)Î³(2)+Â·Â·Â·+2Î³(nâ1) (cid:162) n2 Ï2 2 (cid:88) n (cid:181) (cid:96)(cid:182) = + 1â Î³((cid:96)). (14.8) n n n (cid:96)=1 Thisisaratherintruigingexpression.Itshowsthatthevarianceofthesamplemeanpreciselyequals Ï2/n (whichisthevarianceofthesamplemeanunderi.i.d. sampling)plusaweightedCesÃ romeanof theautocovariances. Thelatteriszerounderi.i.d. samplingbutisnon-zerootherwise. Theorem14.7 showsthattheCesÃ romeanoftheautocovariancesconvergestozero. Letw n(cid:96) =2((cid:96)/n2),whichsatisfy theconditionsoftheToeplitzLemma(TheoremA.5ofIntroductiontoEconometrics).Then 2 (cid:88) n (cid:181) (cid:96)(cid:182) 2 n (cid:88) â1 (cid:88) (cid:96) n (cid:88) â1 (cid:195) 1 (cid:88) (cid:96) (cid:33) n 1â n Î³((cid:96))= n2 Î³(j)= w n(cid:96) (cid:96) Î³(j) ââ0. (14.9) (cid:96)=1 (cid:96)=1j=1 (cid:96)=1 j=1 (cid:104) (cid:105) Together,wehaveshownthat(14.8)iso(1)underergodicity.Hencevar Y â0.Markovâsinequality establishesthatY ââÂµ. p 14.9 ConditioningonInformationSets Inthepastfewsectionswehaveintroducedtheconceptoftheinfinitehistories.Wenowconsider conditionalexpectationsgiveninfinitehistories. First,somebasics. Recallfromprobabilitytheorythatanoutcomeisanelementofasamplespace. An event is a set of outcomes. A probability law is a rule which assigns non-negative real numbers to",
    "page": 471,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 452 events. Whenoutcomesareinfinitehistoriestheneventsarecollectionsofsuchhistoriesandaproba- bilitylawisarulewhichassignsnumberstocollectionsofinfinitehistories. Nowwewishtodefineaconditionalexpectationgivenaninfinitepasthistory. Specifically,wewish todefine (cid:69) tâ1 [Y t ]=(cid:69)[Y t |Y tâ1 ,Y tâ2 ,...] (14.10) theexpectedvalueofY t giventhehistoryY(cid:101)tâ1 =(Y tâ1 ,Y tâ2 ,...)uptotimet. Intuitively,(cid:69) tâ1 [Y t ]isthe meanoftheconditionaldistribution,thelatterreflectingtheinformationinthehistory.Mathematically thiscannotbedefinedusing(2.6)asthelatterrequiresajointdensityfor(Y t ,Y tâ1 ,Y tâ2 ,...)whichdoes not make much sense. Instead, we can appeal to Theorem 2.13 which states that the conditional ex- pectation(14.10)existsif(cid:69)|Y t |<âandtheprobabilities(cid:80)(cid:163) Y(cid:101)tâ1 âA (cid:164) aredefined. Thelattereventsare discussedinthepreviousparagraph.Thustheconditionalexpectationiswelldefined. Inthistextbookwehaveavoidedmeasure-theoreticterminologytokeepthepresentationaccessible, andbecauseitismybeliefthatmeasuretheoryismoredistractingthanhelpful.However,itisstandardin thetimeseriesliteraturetofollowthemeasure-theoreticconventionofwriting(14.10)astheconditional expectationgivenaÏ-field. So attherisk ofbeingoverly-technicalwe will follow this conventionand writetheexpectation(14.10)as(cid:69)[Y t |F tâ1 ]whereF tâ1 =Ï(cid:161) Y(cid:101)tâ1 (cid:162) istheÏ-fieldgeneratedbythehistory Y(cid:101)tâ1 .AÏ-field(alsoknownasaÏ-algebra)isacollectionofsetssatisfyingcertainregularityconditions2. See Introduction to Econometrics, Section 1.14. The Ï-field generated by a random variable Y is the collection of measurable events involving Y. Similarly, the Ï-field generated by an infinite history is thecollectionofmeasurableeventsinvolvingthishistory. Intuitively,F tâ1 containsalltheinformation available in the history Y(cid:101)tâ1 . Consequently, economists typically call F tâ1 an information set rather thanaÏ-field. AsIsaid,inthistextbookweendeavortoavoidmeasuretheoreticcomplicationssowill followtheeconomistsâlabelratherthantheprobabilistsâ,butusethelatterâsnotationasisconventional. Tosummarize,wewillwriteF t =Ï(Y t ,Y tâ1 ,...)toindicatetheinformationsetgeneratedbyaninfinite history(Y t ,Y tâ1 ,...),andwillwrite(14.10)as(cid:69)[Y t |F tâ1 ]. WenowdescribesomepropertiesaboutinformationsetsF . t First,theyarenested: F tâ1 âF t . Thismeansthatinformationaccumulatesovertime. Information isnotlost. Second, it is important to be precise about which variables are contained in the information set. Someeconomistsaresloppyandrefertoâtheinformationsetattimetâwithoutspecifyingwhichvari- ables are in the information set. It is better to be specific. For example, the information sets F = 1t Ï(Y t ,Y tâ1 ,...)andF 2t =Ï(Y t ,X t ,Y tâ1 ,X tâ1 ...)aredistincteventhoughtheyarebothdatedattimet",
    "page": 472,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". t First,theyarenested: F tâ1 âF t . Thismeansthatinformationaccumulatesovertime. Information isnotlost. Second, it is important to be precise about which variables are contained in the information set. Someeconomistsaresloppyandrefertoâtheinformationsetattimetâwithoutspecifyingwhichvari- ables are in the information set. It is better to be specific. For example, the information sets F = 1t Ï(Y t ,Y tâ1 ,...)andF 2t =Ï(Y t ,X t ,Y tâ1 ,X tâ1 ...)aredistincteventhoughtheyarebothdatedattimet. Third,theconditionalexpectations(14.10)followthelawofiteratedexpectationsandthecondition- ingtheorem,thus (cid:69)[(cid:69)[Y t |F tâ1 ]|F tâ2 ]=(cid:69)[Y t |F tâ2 ] (cid:69)[(cid:69)[Y t |F tâ1 ]]=(cid:69)[Y t ], and (cid:69)[Y tâ1 Y t |F tâ1 ]=Y tâ1 (cid:69)[Y t |F tâ1 ]. 14.10 MartingaleDifferenceSequences Animportantconceptineconomicsisunforecastability,meaningthattheconditionalexpectationis theunconditionalexpectation. Thisissimilartothepropertiesofaregressionerror. Anunforecastable processiscalledamartingaledifferencesequence(MDS). 2AÏ-fieldcontainstheuniversalset,isclosedundercomplementation,andclosedundercountableunions.",
    "page": 472,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 453 A MDS e is defined with respect to a specific sequence of information sets F . Most commonly t t the latter are the natural filtration F t =Ï(e t ,e tâ1 ,...) (the past history of e t ) but it could be a larger informationset.Theonlyrequirementisthate isadaptedtoF ,meaningthat(cid:69)[e |F ]=e . t t t t t Definition14.4 The process (e ,F ) is a Martingale Difference Sequence t t (MDS)ife t isadaptedtoF t ,(cid:69)|e t |<â,and(cid:69)[e t |F tâ1 ]=0. In words, a MDS e is unforecastable in the mean. It is useful to notice that if we apply iterated t expectations(cid:69)[e t ]=(cid:69)[(cid:69)[e t |F tâ1 ]]=0.ThusaMDSismeanzero. The definition of a MDS requires the information sets F to contain the information in e , but is t t broaderinthesensethatitcancontainmoreinformation.Whennoexplicitdefinitionisgivenitisstan- dardtoassumethatF isthenaturalfiltration.However,itisbesttoexplicitlyspecifytheinformation t setssothereisnoconfusion. ThetermâmartingaledifferencesequenceâreferstothefactthatthesummedprocessS =(cid:80)t e t j=1 j is a martingale and e is its first-difference. A martingale S is a process which has a finite mean and t t (cid:69)[S t |F tâ1 ]=S tâ1 . Ife isi.i.d.andmeanzeroitisaMDSbutthereverseisnotthecase.Toseethis,firstsupposethate t t isi.i.d. andmeanzero.ItisthenindependentofF tâ1 =Ï(e tâ1 ,e tâ2 ,...)so(cid:69)[e t |F tâ1 ]=(cid:69)[e t ]=0. Thus ani.i.d.shockisaMDSasclaimed. Toshowthatthereverseisnottrueletu bei.i.d.N(0,1)andset t e t =u t u tâ1 . (14.11) Bytheconditioningtheorem (cid:69)[e t |F tâ1 ]=u tâ1 (cid:69)[u t |F tâ1 ]=0 so e is a MDS. The process (14.11) is not, however, i.i.d. One way to see this is to calculate the first t autocovarianceofe2,whichis t cov (cid:161) e2,e2 (cid:162)=(cid:69)(cid:163) e2e2 (cid:164)â(cid:69)(cid:163) e2(cid:164)(cid:69)(cid:163) e2 (cid:164) t tâ1 t tâ1 t tâ1 =(cid:69)(cid:163) u2(cid:164)(cid:69)(cid:163) u4 (cid:164)(cid:69)(cid:163) u2 (cid:164)â1 t tâ1 tâ2 =2(cid:54)=0. Sincethecovarianceisnon-zero,e isnotanindependentsequence.Thuse isaMDSbutnoti.i.d. t t AnimportantpropertyofasquareintegrableMDSisthatitisseriallyuncorrelated. Toseethis,ob- servethatbyiteratedexpectations,theconditioningtheorem,andthedefinitionofaMDS,fork>0, cov(e t ,e tâk )=(cid:69)[e t e tâk ] =(cid:69)[(cid:69)[e t e tâk |F tâ1 ]] =(cid:69)[(cid:69)[e t |F tâ1 ]e tâk ] =(cid:69)[0e tâk ] =0. Thustheautocovariancesandautocorrelationsarezero.",
    "page": 473,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 454 Aprocessthatisseriallyuncorrelated,however,isnotnecessarilyaMDS.Taketheprocesse =u + t t u tâ1 u tâ2 withu t i.i.d.N(0,1).Theprocesse t isnotaMDSsince(cid:69)[e t |F tâ1 ]=u tâ1 u tâ2 (cid:54)=0.However, cov(e t ,e tâ1 )=(cid:69)[e t e tâ1 ] =(cid:69)[(u t +u tâ1 u tâ2 )(u tâ1 +u tâ2 u tâ3 )] =(cid:69)(cid:163) u t u tâ1 +u t u tâ2 u tâ3 +u t 2 â1 u tâ2 +u tâ1 u t 2 â2 u tâ3 (cid:164) =(cid:69)[u t ](cid:69)[u tâ1 ]+(cid:69)[u t ](cid:69)[u tâ2 ](cid:69)[u tâ3 ] +(cid:69)(cid:163) u t 2 â1 (cid:164)(cid:69)[u tâ2 ]+(cid:69)[u tâ1 ](cid:69)(cid:163) u t 2 â2 (cid:164)(cid:69)[u tâ3 ] =0. Similarly,cov(e t ,e tâk )=0fork(cid:54)=0.Thuse t isseriallyuncorrelated.Wehaveprovedthefollowing. Theorem14.10 If(e ,F )isaMDSand(cid:69)(cid:163) e2(cid:164)<âthene isseriallyuncorre- t t t t lated. Anotherimportantspecialcaseisahomoskedasticmartingaledifferencesequence. Definition14.5 TheMDS(e ,F )isaHomoskedasticMartingaleDifference t t Sequenceif(cid:69)(cid:163) e t 2|F tâ1 (cid:164)=Ï2. AhomoskedasticMDSshouldmoreproperlybecalledaconditionallyhomoskedasticMDSbecause the property concerns the conditional distribution rather than the unconditional. That is, any strictly stationaryMDSsatisfiesaconstantvariance(cid:69)(cid:163) e2(cid:164) butonlyahomoskedasticMDShasaconstantcondi- t tionalvariance(cid:69)(cid:163) e t 2|F tâ1 (cid:164) . AhomoskedaticMDSisanalogoustoaconditionallyhomoskedasticregressionerror.Itisintermedi- atebetweenaMDSandani.i.d.sequence.Specifically,asquareintegrableandmeanzeroi.i.d.sequence isahomoskedasticMDSandthelatterisaMDS. Thereverseisnotthecase.First,aMDSisnotnecessarilyconditionallyhomoskedastic.Considerthe examplee t =u t u tâ1 givenpreviouslywhichweshowedisaMDS.Itisnotconditionallyhomoskedastic, however,since (cid:69)(cid:163) e t 2|F tâ1 (cid:164)=u t 2 â1 (cid:69)(cid:163) u t 2|F tâ1 (cid:164)=u t 2 â1 whichistime-varying.ThusthisMDSe isconditionallyheteroskedastic.Second,ahomoskedasticMDS t isnotnecessarilyi.i.d. Considerthefollowingexample. Sete t =(cid:112) 1â2/Î· tâ1 T t ,whereT t isdistributed asstudentt withdegreeoffreedomparameterÎ· tâ1 =2+e t 2 â1 . Thisisscaledsothat(cid:69)[e t |F tâ1 ]=0and (cid:69)(cid:163) e t 2|F tâ1 (cid:164)=1,andisthusahomoskedasticMDS.Theconditionaldistributionofe t dependsone tâ1 throughthedegreeoffreedomparameter.Hencee isnotanindependentsequence. t OnewaytothinkaboutthedifferencebetweenMDSandi.i.d. shocksisintermsofforecastability. Ani.i.d. processisfullyunforecastableinthatnofunctionofani.i.d. processisforecastable. AMDSis unforecastableinthemeanbutothermomentsmaybeforecastable.",
    "page": 474,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 455 14.11 CLTforMartingaleDifferences We are interested in an asymptotic approximation for the distribution of the normalized sample mean 1 (cid:88) n S = (cid:112) u (14.12) n t n t=1 whereu ismeanzerowithvariance(cid:69)(cid:163) u u (cid:48)(cid:164)=Î£<â.InthissectionwepresentaCLTforthecasewhere t t t u isamartingaledifferencesequence. t Theorem14.11 MDSCLTIfu isastrictlystationaryandergodicmartingale t differencesequenceand(cid:69)(cid:163) u u (cid:48)(cid:164)=Î£<â,thenasnââ, t t 1 (cid:88) n S = (cid:112) u ââN(0,Î£). n t n t=1 d TheconditionsforTheorem14.11aresimilartotheLindeberg-LÃ©vyCLT.Theonlydifferenceisthat thei.i.d.assumptionhasbeenreplacedbytheassumptionofastrictlystationarityandergodicMDS. TheproofofTheorem14.11istechnicallyadvancedsowedonotpresentthefulldetails,butinstead refer readers to Theorem 3.2 of Hall and Heyde (1980) or Theorem 25.3 of Davidson (2020) (which are more general than Theorem 14.11, not requiring strict stationarity). To illustrate the role of the MDS assumptionwegiveasketchoftheproofinSection14.47. 14.12 Mixing Formanyresults,includingaCLTforcorrelated(non-MDS)series,weneedastrongerrestrictionon thedependencebetweenobservationsthanergodicity. Recalling the property (14.5) of ergodic sequences we can measure the dependence between two events AandB bythediscrepancy Î±(A,B)=|(cid:80)[Aâ©B]â(cid:80)[A](cid:80)[B]|. (14.13) Thisequals0when A andB areindependentandispositiveotherwise. Ingeneral,Î±(A,B)canbeused tomeasurethedegreeofdependencebetweentheevents AandB. Nowconsiderthetwoinformationsets(Ï-fields) F â t â =Ï(...,Y tâ1 ,Y t ) F t â=Ï(Y t ,Y t+1 ,...). Thefirstisthehistoryoftheseriesupuntilperiodt andthesecondisthehistoryoftheseriesstarting in period t and going forward. We then separate the information sets by (cid:96) periods, that is, take F â tâ â (cid:96) andFâ . Wecanmeasurethedegreeofdependencebetweentheinformationsetsbytakingallevents t ineachandthentakingthelargestdiscrepancy(14.13).Thisis Î±((cid:96))= sup Î±(A,B). AâF â tâ â (cid:96),BâF t â",
    "page": 475,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 456 TheconstantsÎ±((cid:96))areknownasthemixingcoefficients.WesaythatY isstrongmixingifÎ±((cid:96))â0 t as(cid:96)ââ. Thismeansthatasthetimeseparationincreasesbetweentheinformationsets,thedegreeof dependencedecreases,eventuallyreachingindependence. FromtheTheoremofCesÃ roMeans(TheoremA.4ofIntroductiontoEconometrics), strongmixing implies(14.5)whichisequivalenttoergodicity.Thusamixingprocessisergodic. AnintuitionconcerningmixingcanbecolorfullyillustratedbythefollowingexampleduetoHalmos (1956).Amartiniisadrinkconsistingofalargeportionofginandasmallpartofvermouth.Supposethat youpouraservingofginintoamartiniglass,pourasmallamountofvermouthontop,andthenstirthe drinkwithaswizzlestick.Ifyourstirringprocessismixing,witheachturnofthestickthevermouthwill becomemoreevenlydistributedthroughoutthegin,andasymptotically(asthenumberofstirstendsto infinity)thevermouthandgindistributionswillbecomeindependent3.Ifso,thisisamixingprocess. Forapplicationsmixingisoftenusefulwhenwecancharacterizetherateatwhichthecoefficients Î±((cid:96))declinetozero. Therearetwotypesofconditionswhichareseeninasymptotictheory: ratesand summation. RateconditionstaketheformÎ±((cid:96))=O((cid:96)âr)orÎ±((cid:96))=o((cid:96)âr). Summationconditionstake theform (cid:80)â Î±((cid:96))r <âor (cid:80)â (cid:96)sÎ±((cid:96))r <â. (cid:96)=0 (cid:96)=0 Therearealternativemeasuresofdependencebeyond(14.13)andmanyhavebeenproposed.Strong mixingisoneoftheweakest(andthusembracesawidesetoftimeseriesprocesses)butisinsufficiently strongforsomeapplications. Anotherpopulardependencemeasureisknownasabsoluteregularityor Î²-mixing.TheÎ²-mixingcoefficientsare (cid:175) (cid:104) (cid:105) (cid:175) Î²((cid:96))= sup (cid:69)(cid:175) (cid:175) (cid:80) A|F â tâ â (cid:96) â(cid:80)[A](cid:175) (cid:175) . AâFâ t AbsoluteregularityisstrongerthanstrongmixinginthesensethatÎ²((cid:96))ââimpliesÎ±((cid:96))â0,andrate conditionsfortheÎ²-mixingcoefficientsimplythesameratesforthestrongmixingcoefficients. Onereasonwhymixingisusefulforapplicationsisthatitispreservedbytransformations. Theorem14.12 If Y has mixing coefficients Î± ((cid:96)) and X = t Y t Ï(Y t ,Y tâ1 ,Y tâ2 ,...,Y tâq ) then X t has mixing coefficients Î± X ((cid:96)) â¤ Î± Y ((cid:96)âq) (for (cid:96) â¥ q). The coefficients Î± (m) satisfy the same summation and rate X conditionsasÎ± ((cid:96)). Y Alimitationoftheaboveresultisthatitisconfinedtoafinitenumberoflagsunlikethetransforma- tionresultsforstationarityandergodicity. Mixingcanbeausefultoolbecauseofthefollowinginequalities. 3Ofcourse,ifyoureallymakeanasymptoticnumberofstirsyouwillneverfinishstirringandyouwonâtbeabletoenjoythe martini.Henceinpracticeitisadvisedtostopstirringbeforethenumberofstirsreachesinfinity.",
    "page": 476,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 457 Theorem14.13 LetF â t âandF t â beconstructedfromthepair(X t ,Z t ). 1. If|X |â¤C and|Z |â¤C then t 1 t 2 |cov(X tâ(cid:96),Z t )|â¤4C 1 C 2 Î±((cid:96)). 2. If(cid:69)|X |r <âand(cid:69)|Z |q<âfor1/r+1/q<1then t t |cov(X tâ(cid:96),Z t )|â¤8 (cid:161)(cid:69)|X t |r(cid:162)1/r(cid:161)(cid:69)|Z t |q(cid:162)1/qÎ±((cid:96))1â1/râ1/q. 3. If(cid:69)[Z ]=0and(cid:69)|Z |r <âforr â¥1then t t (cid:175) (cid:104) (cid:175) (cid:105)(cid:175) (cid:69)(cid:175) (cid:175) (cid:69) Z t (cid:175) (cid:175) F â tâ â (cid:96) (cid:175) (cid:175) â¤6 (cid:161)(cid:69)|Z t |r(cid:162)1/rÎ±((cid:96))1â1/r. TheproofisgiveninSection14.47.Ournextresultfollowsfairlydirectlyfromthedefinitionofmixing. Theorem14.14 IfY isi.i.d.thenitisstrongmixingandergodic. t 14.13 CLTforCorrelatedObservations InthissectionwedevelopaCLTforthenormalizedmeanS definedin(14.12)allowingthevariables n u tobeseriallycorrelated. t In(14.8)wefoundthatinthescalarcase n (cid:181) (cid:96)(cid:182) var[S ]=Ï2+2 (cid:88) 1â Î³((cid:96)) n n (cid:96)=1 whereÏ2=var[u t ]andÎ³((cid:96))=cov(u t ,u tâ(cid:96)).SinceÎ³(â(cid:96))=Î³((cid:96))thiscanbewrittenas n (cid:181) |(cid:96)|(cid:182) (cid:88) var[S ]= 1â Î³((cid:96)). (14.14) n n (cid:96)=ân InthevectorcasedefinethevarianceÎ£=(cid:69)(cid:163) u u (cid:48)(cid:164) andthematrixcovarianceÎ((cid:96))=(cid:69)(cid:163) u u (cid:48) (cid:164) which t t t tâ(cid:96) satisfiesÎ(â(cid:96))=Î((cid:96)) (cid:48) .Weobtainbyacalculationanalogousto(14.14) n (cid:181) (cid:96)(cid:182) n (cid:181) |(cid:96)|(cid:182) var[X ]=Î£+ (cid:88) 1â (cid:161)Î((cid:96))+Î((cid:96)) (cid:48)(cid:162)= (cid:88) 1â Î((cid:96)). (14.15) n n n (cid:96)=1 (cid:96)=ân A necessary condition for S to converge to a normal distributionisthat the variance (14.15) con- n vergestoalimit.Indeed, (cid:88) n (cid:181) (cid:96)(cid:182) 1n (cid:88) â1 (cid:88) (cid:96) (cid:88) â 1â Î((cid:96))= Î(j)â Î((cid:96)) (14.16) n n (cid:96)=1 (cid:96)=1j=1 (cid:96)=0",
    "page": 477,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 458 wheretheconvergenceholdsbytheTheoremofCesÃ roMeansifthelimitin(14.16)isconvergent.Anec- essaryconditionforthistoholdisthatthecovariancesÎ((cid:96))declinetozeroas(cid:96)ââ,whichisstronger than ergodicity. A sufficient condition is that the covariances are absolutely summable which can be verifiedusingamixinginequality.Usingthetriangleinequality(B.16)andTheorem14.13.2,forr >2 â â (cid:88)(cid:107)Î((cid:96))(cid:107)â¤8 (cid:161)(cid:69)(cid:107)u (cid:107)r(cid:162)2/r (cid:88) Î±((cid:96))1â2/r. t (cid:96)=0 (cid:96)=0 Thisimpliesthat(14.15)convergesif(cid:69)(cid:107)u (cid:107)r <âand (cid:80)â Î±((cid:96))1â2/r <â.Weconcludethatunderthese t (cid:96)=0 assumptions â var[S ]â (cid:88) Î((cid:96)) d=efâ¦. (14.17) n (cid:96)=ââ Thematrixâ¦playsaspecialroleintheinferencetheoryfortmeseries.Itisoftencalledthelong-run varianceofu asitisthevarianceofsamplemeansinlargesamples. t ItturnsoutthattheseconditionsaresufficientfortheCLT. Theorem14.15 Ifu isstrictlystationarywithmixingcoefficientsÎ±((cid:96)),(cid:69)[u ]= t t 0,forsomer >2,(cid:69)(cid:107)u (cid:107)r <âand (cid:80)â Î±((cid:96))1â2/r <â,then(14.17)isconver- t (cid:96)=1 gentandS =n â1/2(cid:80)n u âdâN(0,â¦). n t=1 t TheproofisinSection14.47. Thetheoremrequiresr >2finitemomentswhichisstrongerthantheMDSCLT.Thesummability conditiononthemixingcoefficientsinTheorem14.15isconsiderablystrongerthanergodicity. There isatrade-offinvolvingthechoiceofr. Alargerr meansmoremomentsarerequiredfinitebutaslower decayinthecoefficientsÎ±((cid:96))isallowed. Smallerr islessrestrictiveregardingmomentsbutrequiresa fasterdecayrateinthemixingcoefficients. 14.14 LinearProjection In Chapter 2 we extensively studied the properties of linear projection models. In the context of stationarytimeserieswecanusesimilartools. Animportantextensionistoallowforprojectionsonto infinitedimensionalrandomvectors.ForthisanalysisweassumethatY iscovariancestationary. t Recallthatwhen(Y,X)haveajointdistributionwithboundedvariancesthelinearprojectionofY (cid:104) (cid:105) ontoX (thebestlinearpredictor)istheminimizerofS (cid:161)Î²(cid:162)=(cid:69) (cid:161) Y âÎ²(cid:48) X (cid:162)2 andhasthesolution P [Y |X]=X (cid:48)(cid:161)(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:162)â1(cid:69)[XY]. Thisprojectionisuniqueandhasauniqueprojectionerrore=Y âP [Y |X]. ThisideaextendstoanyHilbertspaceincludingtheinfinitepasthistoryY(cid:101)tâ1 =(...,Y tâ2 ,Y tâ1 ). From theprojectiontheoremforHilbertspaces(seeTheorem2.3.1ofBrockwellandDavis(1991))theprojec- tionP tâ1 [Y t ]=P (cid:163) Y t |Y(cid:101)tâ1 (cid:164) ofY t ontoY(cid:101)tâ1 isuniqueandhasauniqueprojectionerror e t =Y t âP tâ1 [Y t ]. (14.18) Theprojectionerrorismeanzero, hasfinitevarianceÏ2=(cid:69)(cid:163) e2(cid:164)â¤(cid:69)(cid:163) Y2(cid:164)<â, andisseriallyuncorre- t t lated.ByTheorem14.2,ifY t isstrictlystationarythenP tâ1 [Y t ]ande t arestrictlystationary. Thustheprojectionerrorsareseriallyuncorrelated.Westatetheseresultsformally.",
    "page": 478,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 459 Theorem14.16 IfY â(cid:82)iscovariancestationaryithastheprojectionequation t Y t =P tâ1 [Y t ]+e t . Theprojectionerrore satisfies t (cid:69)[e ]=0 t (cid:69)(cid:163) e tâj e t (cid:164)=0 j â¥1 and Ï2=(cid:69)(cid:163) e2(cid:164)â¤(cid:69)(cid:163) Y2(cid:164)<â. (14.19) t t IfY isstrictlystationarythene isstrictlystationary. t t 14.15 WhiteNoise Theprojectionerrore ismeanzero,hasafinitevariance,andisseriallyuncorrelated.Thisdescribes t whatisknownasawhitenoiseprocess. Definition14.6 Theprocesse iswhitenoiseif(cid:69)[e ]=0,(cid:69)(cid:163) e2(cid:164)=Ï2<â,and t t t cov(e t ,e tâk )=0fork(cid:54)=0. A MDS is white noise (Theorem 14.10) but the reverse is not true as shown by the example e = t u t +u tâ1 u tâ2 giveninSection14.10,whichiswhitenoisebutnotaMDS.Therefore,thefollowingtypes ofshocksarenested:i.i.d.,MDS,andwhitenoise,withi.i.d.beingthemostnarrowclassandwhitenoise thebroadest. Itishelpfultoobservethatawhitenoiseprocesscanbeconditionallyheteroskedasticas theconditionalvarianceisunrestricted. 14.16 TheWoldDecomposition InSection14.14weshowedthatacovariancestationaryprocesshasawhitenoiseprojectionerror. Thisresultcanbeusedtoexpresstheseriesasaninfinitelinearfunctionoftheprojectionerrors.Thisis afamousresultknownastheWolddecomposition.",
    "page": 479,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 460 Theorem14.17 The Wold Decomposition If Y is covariance stationary and t Ï2>0whereÏ2 istheprojectionerrorvariance(14.19),thenY hasthelinear t representation â (cid:88) Y t =Âµ t + b j e tâj (14.20) j=0 wheree arethewhitenoiseprojectionerrors(14.18),b =1, t 0 â (cid:88) b2<â, (14.21) j j=1 and Âµ t = m li â m â P tâm [Y t ]. (14.22) TheWolddecompositionshowsthatY canbewrittenasalinearfunctionofthewhitenoiseprojec- t tionerrorsplusÂµ . Theinfinitesumin(14.20)isalsoknownasalinearprocess. TheWolddecomposi- t tionisafoundationalresultforlineartimeseriesanalysis. Sinceanycovariancestationaryprocesscan bewritteninthisformatthisjustifieslinearmodelsasapproximations. TheseriesÂµ istheprojectionofY onthehistoryfromtheinfinitepast. ItisthepartofY which t t t isperfectlypredictablefromitspastvaluesandiscalledthedeterministiccomponent. Inmostcases Âµ = Âµ, the unconditional mean of Y . However, it is possible for stationary processes to have more t t substantivedeterministiccomponents.Anexampleis (cid:189) (â1)t withprobability1/2 Âµ = t (â1)t+1 withprobability1/2. Thisseriesisstrictlystationary,meanzero,andvarianceone. However,itisperfectlypredictablegiven theprevioushistoryasitsimplyoscillatesbetweenâ1and1. In practical applied time series analysis, deterministic components are typically excluded by as- sumption. We call a stationary time series non-deterministic4 if Âµ =Âµ, a constant. In this case the t Wolddecompositionhasasimplerform. Theorem14.18 If Y is covariance stationary and non-deterministic then Y t t hasthelinearrepresentation â (cid:88) Y t =Âµ+ b j e tâj , j=0 whereb satisfy(14.21)ande arethewhitenoiseprojectionerrors(14.18). j t AlimitationoftheWolddecompositionistherestrictiontolinearity.Effectively,itsaysthatthereisa validlinearapproximationwithintheclassoflinearmodels. Itexcludesalternative(nonlinear)models byassumption. ForaproofofTheorem14.17seeSection14.47. 4Mostauthorsdefinepurelynon-deterministicasthecaseÂµ t =0.Weallowforanon-zeromeansotoaccomodatepractical timeseriesapplications.",
    "page": 480,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 461 14.17 LagOperator Analgebraicconstructwhichisusefulfortheanalysisoftimeseriesmodelsisthelagoperator. Definition14.7 ThelagoperatorLsatisfiesLY t =Y tâ1 . DefiningL2=LL,weseethatL2Y t =LY tâ1 =Y tâ2 .Ingeneral,LkY t =Y tâk . UsingthelagoperatortheWolddecompositioncanbewrittenintheformat Y =Âµ+b e +b Le +b L2e +Â·Â·Â· t 0 t 1 t 2 t =Âµ+(cid:161) b +b L+b L2+Â·Â·Â·(cid:162) e 0 1 2 t =Âµ+b(L)e t whereb(z)=b +b z+b z2+Â·Â·Â·isaninfinite-orderpolynomial.TheexpressionY =Âµ+b(L)e iscompact 0 1 2 t t waytowritetheWoldrepresentation. 14.18 AutoregressiveWoldRepresentation FromTheorem14.16, Y satisfiesaprojectionontoitsinfinitepast. Theorem14.18showsthatthis t projectionequalsalinearfunctionofthelaggedprojectionerrors.Analternativeistowritetheprojection asalinearfunctionofthelaggedY . Itturnsoutthattoobtainauniqueandconvergentrepresentation t weneedastrengtheningoftheconditions. Theorem14.19 If Y is covariance stationary, non-deterministic, with Wold t representationY =b(L)e ,suchthat|b(z)|â¥Î´>0forallcomplex|z|â¤1,and t t for some integer s â¥0 the Wold coefficients satisfy (cid:80)â j=0 (cid:161)(cid:80)â k=0 ksb j+k (cid:162)2 <â, thenY hastherepresentation t â (cid:88) Y t =Âµ+ a j Y tâj +e t (14.23) j=1 for some coefficients Âµ and a . The coefficients satisfy (cid:80)â ks|a | < â so j k=0 k (14.23)isconvergent. Equation(14.23)isknownasaninfinite-orderautoregressiverepresentationwithautoregressiveco- efficientsa . j A solution to the equation b(z)=0 is a root of the polynomial b(z). The assumption |b(z)|>0 for |z|â¤1meansthattherootsofb(z)lieoutsidetheunitcircle|z|=1(thecircleinthecomplexplanewith radiusone). Theorem14.19makesthestrongerrestrictionthat|b(z)|isboundedawayfrom0for z on or within the unit circle. The need for this strengthening is less intuitive but essentially excludes the possibilityofaninfinitenumberofrootsoutsidebutarbitrarilyclosetotheunitcircle.Thesummability assumptionontheWoldcoefficientsensuresconvergenceoftheautoregressivecoefficientsa . j",
    "page": 481,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 462 Tounderstandtherestrictionontherootsofb(z)considerthesimplecaseb(z)=1âb z. (Belowwe 1 callthisaMA(1)model.)Therequirement|b(z)|â¥Î´for|z|â¤1means|b |â¤1âÎ´.Thustheassumptionin 1 Theorem14.19boundsthecoefficientstrictlybelow1. Nowconsideraninfinitepolynomialcaseb(z)= â (cid:89)(cid:161) 1âb j z (cid:162) .TheassumptioninTheorem14.19requiressup j (cid:175) (cid:175)b j (cid:175) (cid:175) <1. j=1 Theorem14.19isattributedtoWienerandMasani(1958).ForarecenttreatmentandproofseeCorol- lary6.1.17ofPolitisandMcElroy(2020). Theseauthors(asiscommonintheliterature)statetheiras- sumptionsdifferentlythanwedoinTheorem14.19. First,insteadoftheconditiononb(z)theybound frombelowthespectraldensityfunction f(Î»)ofY . Wedonotdefinethespectraldensityinthistextso t we restate their condition in terms of the linear process polynomial b(z). Second, instead of the con- ditionontheWoldcoefficientstheyrequirethattheautocovariancessatisfy (cid:80)â ks (cid:175) (cid:175) Î³(k) (cid:175) (cid:175) <â. Thisis k=0 implied by out stated summability condition on the b (using the expression for Î³(k) in Section 14.21 j belowandsimplifying). 14.19 LinearModels Intheprevioustwosectionsweshowedthatanynon-deterministiccovariancestationarytimeseries hastheprojectionrepresentation â (cid:88) Y t =Âµ+ b j e tâj j=0 andunderinvertibilityconditionssatisfiestheautoregressiverepresentation â (cid:88) Y t =Âµ+ a j Y tâj +e t j=1 whereinbothequationstheerrorse arewhitenoiseprojectionerrors. Theserepresentationshelpus t understandthatlinearmodelscanbeusedasapproximationsforstationarytimeseries. Forthenextseveralsectionswereversetheanalysis.Wewillassumeaspecificlinearmodelandthen studythepropertiesoftheresultingtimeseries.Inparticularwewillbeseekingconditionsunderwhich thestatedprocessisstationary. Thishelpsusunderstandthepropertiesoflinearmodels. Throughout, weassumethattheerrore isastrictlystationaryandergodicwhitenoiseprocess.Thisallowsasaspecial t casethestrongerassumptionthate isi.i.d. butislessrestrictive. Inparticular,itallowsforconditional t heteroskedasticity. 14.20 MovingAverageProcesses Thefirst-ordermovingaverageprocess,denotedMA(1),is Y t =Âµ+e t +Î¸e tâ1 wheree isastrictlystationaryandergodicwhitenoiseprocess. Themodeliscalledaâmovingaverageâ t becauseY t isaweightedaverageoftheshockse t ande tâ1 .",
    "page": 482,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 463 ItisstraightforwardtocalculatethataMA(1)hasthefollowingmoments. (cid:69)[Y ]=Âµ t var[Y ]=(cid:161) 1+Î¸2(cid:162)Ï2 t Î³(1)=Î¸Ï2 Î¸ Ï(1)= 1+Î¸2 Î³(k)=Ï(k)=0, kâ¥2. ThustheMA(1)processhasanon-zerofirstautocorrelationwiththeremainderzero. AnMA(1)processwithÎ¸(cid:54)=0isseriallycorrelatedwitheachpairofadjacentobservations(Y tâ1 ,Y t ) correlated. IfÎ¸>0thepairarepositivelycorrelated, whileifÎ¸<0theyarenegativelycorrelated. The serialcorrelationislimitedinthatobservationsseparatedbymultipleperiodsaremutuallyindependent. Theqth-ordermovingaverageprocess,denotedMA(q),is Y t =Âµ+Î¸ 0 e t +Î¸ 1 e tâ1 +Î¸ 2 e tâ2 +Â·Â·Â·+Î¸ q e tâq whereÎ¸ =1.ItisstraightforwardtocalculatethataMA(q)hasthefollowingmoments. 0 (cid:69)[Y ]=Âµ t (cid:195) (cid:33) q var[Y ]= (cid:88) Î¸2 Ï2 t j j=0 (cid:195) qâk (cid:33) Î³(k)= (cid:88) Î¸ j+k Î¸ j Ï2, kâ¤q j=0 (cid:80)qâkÎ¸ Î¸ Ï(k)= j=0 j+k j (cid:80)q Î¸2 j=0 j Î³(k)=Ï(k)=0, k>q. Inparticular,aMA(q)hasq non-zeroautocorrelationswiththeremainderzero. AMA(q)processY isstrictlystationaryandergodic. t AMA(q)processwithmoderatelylargeqcanhaveconsiderablymorecomplicateddependencerela- tionsthananMA(1)process. OnespecificpatternwhichcanbeinducedbyaMAprocessissmoothing. SupposethatthecoefficientsÎ¸ allequal1.ThenY isasmoothedversionoftheshockse . j t t To illustrate, Figure 14.5(a) displays a plot of a simulated white noise (i.i.d. N(0,1)) process with n = 120 observations. Figure 14.5(b) displays a plot of an MA(8) process constructed with the same innovations,withÎ¸ =1, j =1,...,8. Youcanseethatthewhitenoisehasnopredictablebehaviorwhile j theMA(8)issmooth. 14.21 Infinite-OrderMovingAverageProcess Aninfinite-ordermovingaverageprocess,denotedMA(â),alsoknownasalinearprocess,is â (cid:88) Y t =Âµ+ Î¸ j e tâj (14.24) j=0",
    "page": 483,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 464 1985 1990 1995 2000 2005 2010 2015 2 1 0 1â 2â 1990 1995 2000 2005 2010 2015 (a)WhiteNoise 6 4 2 0 2â 4â 6â 8â (b)MA(8) Figure14.5:WhiteNoiseandMA(8) wheree t isastrictlystationaryandergodicwhitenoiseprocessand (cid:80)â j=0 (cid:175) (cid:175) Î¸ j (cid:175) (cid:175) <â. FromTheorem14.6, Y isstrictlystationaryandergodic.Alinearprocesshasthefollowingmoments: t (cid:69)[Y ]=Âµ t (cid:195)â (cid:33) var[Y ]= (cid:88) Î¸2 Ï2 t j j=0 (cid:195)â (cid:33) Î³(k)= (cid:88) Î¸ j+k Î¸ j Ï2 j=0 (cid:80)â Î¸ Î¸ Ï(k)= j=0 j+k j . (cid:80)q Î¸2 j=0 j 14.22 First-OrderAutoregressiveProcess Thefirst-orderautoregressiveprocess,denotedAR(1),is Y t =Î± 0 +Î± 1 Y tâ1 +e t (14.25) wheree isastrictlystationaryandergodicwhitenoiseprocess. TheAR(1)modelisprobablythesingle t mostimportantmodelineconometrictimeseriesanalysis. AsasimplemotivatingexampleletY beistheemploymentlevel(numberofjobs)inaneconomy. t Supposethatafixedfraction1âÎ± ofemployeeslosetheirjobandarandomnumberu ofnewemploy- 1 t eesarehiredeachperiod.SettingÎ± =(cid:69)[u ]ande =u âÎ± ,thisimpliesthelawofmotion(14.25). 0 t t t 0 ToillustratethebehavioroftheAR(1)process,Figure14.6plotstwosimulatedAR(1)processes.Each isgeneratedusingthewhitenoiseprocesse displayedinFigure14.5(a). TheplotinFigure14.6(a)sets t Î± =0.5andtheplotinFigure14.6(b)setsÎ± =0.95. Youcanseehowbotharemoresmooththanthe 1 1 whitenoiseprocessandthatthesmoothingincreaseswithÎ±.",
    "page": 484,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 465 1985 1990 1995 2000 2005 2010 2015 3 2 1 0 1â 2â 3â 1985 1990 1995 2000 2005 2010 2015 (a)AR(1)withÎ±[1]=0.5 4 2 0 2â 4â 6â (b)AR(1)withÎ±=0.95 Figure14.6:AR(1)Processes Ourfirstgoalistoobtainconditionsunderwhich(14.25)isstationary.Wecandosobyshowingthat Y canbewrittenasaconvergentlinearprocessandthenappealingtoTheorem14.5. Tofindalinear t processrepresentationforY wecanusebackwardrecursion. NoticethatY in(14.25)dependsonits t t previousvalueY tâ1 .Ifwetake(14.25)andlagitoneperiodwefindY tâ1 =Î± 0 +Î± 1 Y tâ2 +e tâ1 .Substituting thisinto(14.25)wefind Y t =Î± 0 +Î± 1 (Î± 0 +Î± 1 Y tâ2 +e tâ1 )+e t =Î± 0 +Î± 1 Î± 0 +Î±2 1 Y tâ2 +Î± 1 e tâ1 +e t . Similarlywecanlag(14.31)twicetofindY tâ2 =Î± 0 +Î± 1 Y tâ3 +e tâ2 andcanbeusedtosubstituteoutY tâ2 . Continuingrecursivelyt times,wefind Y =Î± (cid:161) 1+Î± +Î±2+Â·Â·Â·+Î±tâ1(cid:162)+Î±tY +Î±tâ1e +Î±tâ2e +Â·Â·Â·+e t 0 1 1 1 1 0 1 1 1 2 t tâ1 tâ1 =Î± 0 (cid:88) Î± 1 j+Î± 1 tY 0 + (cid:88) Î± 1 j e tâj . (14.26) j=0 j=0 ThusY t equalsaninterceptplusthescaledinitialconditionÎ± 1 tY 0 andthemovingaverage (cid:80)t j â = 1 0 Î± 1 j e tâj . Now suppose we continue this recursion into the infinite past. By Theorem 14.3 this converges if (cid:80)â |Î± |j <â.Thelimitisprovidedbythefollowingwell-knownresult. j=0 1 â Theorem14.20 (cid:88) Î²k= 1 isabsolutelyconvergentif (cid:175) (cid:175) Î²(cid:175) (cid:175) <1. 1âÎ² k=0 Theseriesconvergesbytheratiotest(seeTheoremA.3ofIntroductiontoEconometrics). Tofindthe limit, â â â A= (cid:88) Î²k=1+ (cid:88) Î²k=1+Î² (cid:88) Î²k=1+Î²A. k=0 k=1 k=0",
    "page": 485,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 466 Solving,wefind A=1/(1âÎ²). Thustheinterceptin(14.26)convergestoÎ± /(1âÎ± ).Wededucethefollowing: 0 1 Theorem14.21 If(cid:69)|e |<âand|Î± |<1thentheAR(1)process(14.25)hasthe t 1 convergentrepresentation â Y t =Âµ+ (cid:88) Î± 1 j e tâj (14.27) j=0 whereÂµ=Î± /(1âÎ± ).TheAR(1)processY isstrictlystationaryandergodic. 0 1 t WecancomputethemomentsofY from(14.27) t â (cid:69)[Y t ]=Âµ+ (cid:88) Î±k 1 (cid:69)[e tâk ]=Âµ k=0 â Ï2 var[Y t ]= k (cid:88) =0 Î±2 1 kvar[e tâk ]= 1âÎ±2 1 . Aninformalwaytocalculatethemomentsisasfollows.Applyexpectationstobothsidesof(14.25) (cid:69)[Y t ]=Î± 0 +Î± 1 (cid:69)[Y tâ1 ]+(cid:69)[e t ]=Î± 0 +Î± 1 (cid:69)[Y tâ1 ]. Stationarityimplies(cid:69)[Y tâ1 ]=(cid:69)[Y t ].Solvingwefind(cid:69)[Y t ]=Î± 0 /(1âÎ± 1 ).Similarly, var[Y t ]=var[Î±Y tâ1 +e t ]=Î±2 1 var[Y tâ1 ]+var[e t ]=Î±2 1 var[Y tâ1 ]+Ï2. Stationarityimpliesvar[Y tâ1 ]=var[Y t ]. Solvingwefindvar[Y t ]=Ï2/(1âÎ±2 1 ). Thismethodisusefulfor calculationofautocovariancesandautocorrelations.ForsimplicitysetÎ± =1.Wefind 0 Î³(1)=(cid:69)[Y tâ1 Y t ]=(cid:69)[Y tâ1 (Î± 1 Y tâ1 +e t )]=Î± 1 var[Y t ] so Ï(1)=Î³(1)/var[Y ]=Î± . t 1 Furthermore, Î³(k)=(cid:69)[Y tâk Y t ]=(cid:69)[Y tâk (Î± 1 Y tâ1 +e t )]=Î± 1 Î³(kâ1). Byrecursionweobtain Î³(k)=Î±kvar[Y ] 1 t Ï(k)=Î±k. 1 ThustheAR(1)processwithÎ± (cid:54)=0hasnon-zeroautocorrelationsofallorderswhichdecaytozerogeo- 1 metricallyask increases.ForÎ± >0theautocorrelationsareallpositive.ForÎ± <0theautocorrelations 1 1 alternateinsign. WecanalsoexpresstheAR(1)processusingthelagoperatornotation: (1âÎ± L)Y =Î± +e . (14.28) 1 t 0 t",
    "page": 486,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 467 We can write this as Î±(L)Y =Î± +e where Î±(L)=1âÎ± L. We call Î±(z)=1âÎ± z the autoregressive t 0 t 1 1 polynomialofY . t Thissuggestsanalternativewayofobtainingtherepresentation(14.27). Wecaninverttheoperator (1âÎ± L) to write Y as a function of lagged e . That is, suppose that the inverse operator (1âÎ± L) â1 1 t t 1 exists.Thenwecanusethisoperatoron(14.28)tofind Y =(1âÎ± L) â1(1âÎ± L)Y =(1âÎ± L) â1(Î± +e ). (14.29) t 1 1 t 1 0 t Whatistheoperator(1âÎ± L) â1?RecallfromTheorem14.20thatfor|x|<1, 1 â (cid:88) xj = 1 =(1âx) â1. 1âx j=0 Evaluatethisexpressionatx=Î± z.Wefind 1 â (1âÎ± z) â1= (cid:88) Î±j zj. (14.30) 1 1 j=0 Settingz=Lthisis â (1âÎ± L) â1= (cid:88) Î±j Lj. 1 1 j=0 Substitutedinto(14.29)weobtain Y =(1âÎ± L) â1(Î± +e ) t 1 0 t (cid:195)â (cid:33) = (cid:88) Î±jLj (Î± +e ) 0 t j=0 â = (cid:88) Î±j Lj(Î± +e ) 1 0 t j=0 â = (cid:88) Î± 1 j(cid:161)Î± 0 +e tâj (cid:162) j=0 Î± â = 1â 0 Î± + (cid:88) Î± 1 j e tâj 1 j=0 whichis(14.27).Thisisvalidfor|Î± |<1. 1 Thisillustratesanotherimportantconcept.WesaythatapolynomialÎ±(z)isinvertibleif â Î±(z) â1= (cid:88) a zj j j=0 isabsolutelyconvergent.Inparticular,theAR(1)autoregressivepolynomialÎ±(z)=1âÎ± zisinvertibleif 1 |Î± |<1.ThisisthesameconditionasforstationarityoftheAR(1)process.Invertibilityturnsouttobea 1 usefulproperty. 14.23 UnitRootandExplosiveAR(1)Processes TheAR(1)process(14.25)isstationaryif|Î±|<1.Whathappensotherwise? IfÎ± =0andÎ± =1themodelisknownasarandomwalk. 0 1 Y t =Y tâ1 +e t .",
    "page": 487,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 468 Thisisalsocalledaunitrootprocess,amartingale,oranintegratedprocess.Byback-substitution t (cid:88) Y =Y + e . t 0 j j=1 Thustheinitialconditiondoesnotdisappearforlarget. Consequentlytheseriesisnon-stationary. The autoregressivepolynomialÎ±(z)=1âz isnotinvertible,meaningthatY cannotbewrittenasaconver- t gentfunctionoftheinfinitepasthistoryofe . t ThestochasticbehaviorofarandomwalkisnoticablydifferentfromastationaryAR(1)process. It wandersupanddownwithequallikelihoodandisnotmean-reverting. Whileithasnotendencytore- turntoitspreviousvaluesthewanderingnatureofarandomwalkcangivetheillusionofmeanreversion. Thedifferenceisthatarandomwalkwilltakeaverylargenumberoftimeperiodstoârevertâ. 1985 1990 1995 2000 2005 2010 2015 6 4 2 0 2â 4â 6â 8â 1985 1990 1995 2000 2005 2010 2015 (a)RandomWalk 0 5â 01â 51â 02â 52â 03â (b)RandomWalk Figure14.7:RandomWalkProcesses Toillustrate, Figure14.7plotstwoindependentrandomwalkprocesses. Theplotinpanel(a)uses theinnovationsfromFigure14.5(a). Theplotinpanel(b)usesanindependentsetofi.i.d. N(0,1)errors. You can see that the plot in panel (a) appears similar to the MA(8) and AR(1) plots in the sense that the series is smooth with long swings, but the difference is that the series does not return to a long- term mean. It appears to have drifted down over time. The plot in panel (b) appears to have quite differentbehavior,fallingdramaticallyovera5-yearperiod,andthenappearingtostabilize. Theseare bothcommonbehaviorsofrandomwalkprocesses. IfÎ± >1theprocessisexplosive.Themodel(14.25)withÎ± >1exhibitsexponentialgrowthandhigh 1 1 sensitivitytoinitialconditions. Explosiveautoregressiveprocessesdonotseemtobegooddescriptions formosteconomictimeseries. WhileaggregatetimeseriessuchastheGDPprocessdisplayedinFigure 14.1(a)exhibitasimilarexponentialgrowthpattern,theexponentialgrowthcantypicallyberemovedby takinglogarithms. ThecaseÎ± <â1inducesexplosiveoscillatinggrowthanddoesnotappeartobeempiricallyrelevant 1 foreconomicapplications.",
    "page": 488,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 469 14.24 Second-OrderAutoregressiveProcess Thesecond-orderautoregressiveprocess,denotedAR(2),is Y t =Î± 0 +Î± 1 Y tâ1 +Î± 2 Y tâ2 +e t (14.31) wheree isastrictlystationaryandergodicwhitenoiseprocess. ThedynamicpatternsofanAR(2)pro- t cessaremorecomplicatedthananAR(1)process. As a motivating example consider the multiplier-accelerator model of Samuelson (1939). It might beabitdatedasamodelbutitissimplesohopefullymakesthepoint. Aggregateoutput(inanecon- omywithnotrade)isdefinedasY =Consumption +Investment +Gov .Supposethatindividualsmake t t t t theirconsumptiondecisionsonthepreviousperiodâsincomeConsumption t =bY tâ1 ,firmsmaketheir investmentdecisionsonthechangeinconsumptionInvestment =dâC , andgovernmentspendingis t t randomGov =a+e .Thenaggregateoutputfollows t t Y t =a+b(1+d)Y tâ1 âbdY tâ2 +e t (14.32) whichisanAR(2)process. Usingthelagoperatorwecanwrite(14.31)as Y âÎ± LY âÎ± L2Y =Î± +e , t 1 t 2 t 0 t orÎ±(L)Y =Î± +e whereÎ±(L)=1âÎ± LâÎ± L2.WecallÎ±(z)theautoregressivepolynomialofY . t 0 t 1 2 t We would like to find the conditions for the stationarity of Y . It turns out that it is convenient to t transformtheprocess(14.31)intoaVAR(1)process(tobestudiedinthenextchapter).SetY(cid:101)t =(Y t ,Y tâ1 ) (cid:48) , whichisstationaryifandonlyifY t isstationary.Equation(14.31)impliesthatY(cid:101)t satisfies (cid:181) Y t (cid:182) = (cid:181) Î± 1 Î± 2 (cid:182)(cid:181) Y tâ1 (cid:182) + (cid:181) a 0 +e t (cid:182) Y tâ1 1 0 Y tâ2 0 or Y(cid:101)t =AY(cid:101)tâ1 +e (cid:101)t (14.33) (cid:181) Î± Î± (cid:182) where A= 1 2 ande =(a +e ,0) (cid:48) . Equation(14.33)fallsintheclassofVAR(1)modelsstudied (cid:101)t 0 t 1 0 inSection15.6.Theorem15.6showsthattheVAR(1)processisstrictlystationaryandergodiciftheinno- vationssatisfy(cid:69)(cid:107)e (cid:107)<âandalleigenvaluesÎ»of Aarelessthanoneinabsolutevalue.Theeigenvalues (cid:101)t satisfydet(AâI Î»)=0,where 2 (cid:181) Î± âÎ» Î± (cid:182) det(AâI Î»)=det 1 2 =Î»2âÎ»Î± âÎ± =Î»2Î±(1/Î») 2 1 âÎ» 1 2 and Î±(z) = 1âÎ± zâÎ± z2 is the autoregressive polynomial. Thus the eigenvalues satisfy Î±(1/Î») = 0. 1 2 FactoringtheautoregressivepolynomialasÎ±(z)=(1âÎ» z)(1âÎ» z)thesolutionsÎ±(1/Î»)=0mustequal 1 2 Î» andÎ» .Thequadraticformulashowsthattheseequal 1 2 (cid:113) Î± Â± Î±2+4Î± Î» = 1 1 2 . (14.34) j 2 TheseeigenvaluesarerealifÎ±2+4Î± â¥0andarecomplexconjugatesotherwise. TheAR(2)processis 1 2 stationaryifthesolutions(14.34)satisfy (cid:175) (cid:175) Î» j (cid:175) (cid:175) <1.",
    "page": 489,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 470 â2 â1 0 1 2 0.1 5.0 0.0 5.0â 0.1â 5.1â a 1 a 2 a -a =1 a +a =1 2 1 1 2 Real Factors Complex Factors a =- 1 2 Figure14.8:StationarityRegionforAR(2) Using (14.34) to solve for the AR coefficients in terms of the eigenvalues we find Î± =Î» +Î» and 1 1 2 Î± =âÎ» Î» . Withsomealgebra(thedetailsaredeferredtoSection14.47)wecanshowthat|Î» |<1and 2 1 2 1 |Î» |<1iffthefollowingrestrictionsholdontheautoregressivecoefficients: 2 Î± +Î± <1 (14.35) 1 2 Î± âÎ± <1 (14.36) 2 1 Î± >â1. (14.37) 2 Theserestrictionsdescribeatrianglein(Î± ,Î± )spacewhichisshowninFigure14.8.Coefficientswithin 1 2 thistrianglecorrespondtoastationaryAR(2)process. Furthermore,thetriangleisdividedintotworegionsasmarkedinFigure14.8: theregionabovethe parabolaÎ±2+4Î± =0producingrealeigenvaluesÎ» ,andtheregionbelowtheparabolaproducingcom- 1 2 j plexeigenvaluesÎ» . Thisisinterestingbecausewhentheeigenvaluesarecomplextheautocorrelations j ofY displaydampedoscillations. ForthisreasonthedynamicpatternsofanAR(2)canbemuchmore t complicatedthanthoseofanAR(1). TaketheSamuelsonmultiplier-acceleratormodel(14.32).Youcancalculatethatthemodelhascom- plexeigenvalues(andthusoscillations)forcertainvaluesofbandd,includingbâ¤0.8anddâ¥0.4. Theorem14.22 If(cid:69)|e t |<âand (cid:175) (cid:175) Î» j (cid:175) (cid:175) <1forÎ» j definedin(14.34), orequiva- lentlyiftheinequalities(14.35)-(14.37)hold,thentheAR(2)process(14.31)is absolutelyconvergent,strictlystationary,andergodic. TheproofispresentedinSection14.47. Toillustrate,Figure14.9displaystwosimulatedAR(2)processes.Theplotinpanel(a)setsÎ± =Î± = 1 2 0.4. ThesecoefficientsproducerealfactorssotheprocessdisplaysbehaviorsimilartothatoftheAR(1) processes. Theplotinpanel(b)setsÎ± =1.3andÎ± =â0.8. Thesecoefficientsproducecomplexfactors 1 2 sotheprocessdisplaysoscillations.",
    "page": 490,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 471 1985 1990 1995 2000 2005 2010 2015 3 2 1 0 1â 2â 3â 4â 1985 1990 1995 2000 2005 2010 2015 (a)AR(2) 4 2 0 2â 4â (b)AR(2)withComplexRoots Figure14.9:AR(2)Processes 14.25 AR(p)Processes Thepth-orderautoregressiveprocess,denotedAR(p),is Y t =Î± 0 +Î± 1 Y tâ1 +Î± 2 Y tâ2 +Â·Â·Â·+Î± p Y tâp +e t (14.38) wheree isastrictlystationaryandergodicwhitenoiseprocess. t Usingthelagoperator, Y âÎ± LY âÎ± L2Y âÂ·Â·Â·âÎ± L p Y =Î± +e , t 1 t 2 t p t 0 t orÎ±(L)Y =Î± +e where t 0 t Î±(L)=1âÎ± LâÎ± L2âÂ·Â·Â·âÎ± Lp. (14.39) 1 2 p WecallÎ±(z)theautoregressivepolynomialofY . t WefindconditionsforthestationarityofY byatechniquesimilartothatusedfortheAR(2)process. t SetY(cid:101)t =(Y t ,Y tâ1 ,...,Y tâp+1 ) (cid:48) ande (cid:101)t =(a 0 +e t ,0,...,0) (cid:48) .Equation(14.38)impliesthatY(cid:101)t satisfiestheVAR(1) equation(14.33)with ï£« Î± Î± Â·Â·Â· Î± Î± ï£¶ 1 2 pâ1 p ï£¬ 1 0 Â·Â·Â· 0 0 ï£· ï£¬ ï£· A=ï£¬ ï£¬ 0 1 Â·Â·Â· 0 0 ï£· ï£·. (14.40) ï£¬ ï£¬ . . . . . . ... . . . . . . ï£· ï£· ï£­ ï£¸ 0 0 Â·Â·Â· 1 0 AsshownintheproofofTheorem14.23below,theeigenvaluesÎ» ofAarethereciprocalsoftherootsr j j oftheautoregressivepolynomial(14.39).Therootsr arethesolutionstoÎ±(r )=0.Theorem15.6shows j j thatstationarityofY(cid:101)t holdsiftheeigenvaluesÎ» j arelessthanoneinabsolutevalue,orequivalentlywhen therootsr aregreaterthanoneinabsolutevalue.Forcomplexnumberstheequation|z|=1definesthe j unitcircle(thecirclewithradiusofunity).Wethereforesaythatâzliesoutsidetheunitcircleâif|z|>1.",
    "page": 491,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 472 Theorem14.23 If(cid:69)|e |<âandallrootsofÎ±(z)lieoutsidetheunitcirclethen t theAR(p)process(14.38)isabsolutelyconvergent, strictlystationary, ander- godic. WhentherootsofÎ±(z)lieoutsidetheunitcirclethenthepolynomialÎ±(z)isinvertible.Invertingthe autoregressiverepresentationÎ±(L)Y =Î± +e weobtainaninfinite-ordermovingaveragerepresentation t 0 t Y =Âµ+b(L)e t t where â b(z)=Î±(z) â1= (cid:88) b zj (14.41) j j=0 andÂµ=Î±(1) â1a . 0 Wehavethefollowingcharacterizationofthemovingaveragecoefficients. Theorem14.24 If all roots r of the autoregressive polynomial Î±(z) satisfy j (cid:175) (cid:175)r j (cid:175) (cid:175) > 1 then (14.41) holds with (cid:175) (cid:175)b j (cid:175) (cid:175) â¤ (cid:161) j+1 (cid:162)pÎ»j and (cid:80)â j=0 (cid:175) (cid:175)b j (cid:175) (cid:175) < â where (cid:175) (cid:175) Î»=max (cid:175)r â1(cid:175)<1. j(cid:175) j (cid:175) TheproofispresentedinSection14.47. 14.26 ImpulseResponseFunction Thecoefficientsofthemovingaveragerepresentation Y =b(L)e t t â (cid:88) = b j e tâj j=0 =b 0 e t +b 1 e tâ1 +b 2 e tâ2 +Â·Â·Â· are known among economists as the impulseresponsefunction(IRF). Often the IRF is scaled by the standarddeviationofe . Wediscussthisscalingattheendofthesection. Inlinearmodelstheimpulse t responsefunctionisdefinedasthechangeinY t+j duetoashockattimet.Thisis â âe Y t+j =b j . t Thismeansthatthecoefficientb canbeinterpretedasthemagnitudeoftheimpactofatimet shock j on the time t+j variable. Plots of b can be used to assess the time-propagation of shocks. This is a j standardmethodofanalysisformultivariatetimeseries. It is desirable to have a convenient method to calculate the impulse responses b from the coeffi- j cientsofanautoregressivemodel(14.38).Therearetwomethodswhichwenowdescribe.",
    "page": 492,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 473 Thefirstusesasimplerecursion. InthelinearAR(p)model,wecanseethatthecoefficientb isthe j simplederivative â â b j = âe Y t+j = âe Y j t 0 Wecancalculateb bygeneratingahistoryandperturbingtheshocke . Sincethiscalculationisunaf- j 0 fectedbyallothershockswecansimplysete =0fort(cid:54)=0andsete =1.Thisimpliestherecursion t 0 b =1 0 b =Î± b 1 1 0 b =Î± b +Î± b 2 1 1 2 0 . . . b j =Î± 1 b jâ1 +Î± 2 b jâ2 +Â·Â·Â·+Î± p b jâp . Thisrecursionisconvenientlycalculatedbythefollowingsimulation. SetY =0fort â¤0.Sete =1and t 0 e t =0fort>1.GenerateY t fortâ¥0byY t =Î± 1 Y tâ1 +Î± 2 Y tâ2 +Â·Â·Â·+Î± p Y tâp +e t .ThenY j =b j . Asecondmethodusesthevectorrepresentation(14.33)oftheAR(p)modelwithcoefficientmatrix (14.40).Byrecursion â Y(cid:101)t = (cid:88) Aje (cid:101)tâj . j=0 Here, Aj =AÂ·Â·Â·Ameansthe jth matrixproductof Awithitself.SettingS=(1,0,...0) (cid:48) wefind â Y t = (cid:88) S (cid:48) AjSe tâj . j=0 Bylinearity â b j = âe Y t+j =S (cid:48) AjS. (14.42) t Thusthecoefficientb canbecalculatedbyformingthematrixA,its j-foldproductAj,andthentaking j theupper-leftelement. As mentioned at the beginning of the section it is often desirable to scale the IRF so that it is the responsetoaone-deviationshock. LetÏ2=var[e ]anddefineÎµ =e /Ïwhichhasunitvariance. Then t t t theIRFatlag j is â IRF j = âÎµ Y t+j =Ïb j . t 14.27 ARMAandARIMAProcesses Theautoregressive-moving-averageprocess,denotedARMA(p,q),is Y t =Î± 0 +Î± 1 Y tâ1 +Î± 2 Y tâ2 +Â·Â·Â·+Î± p Y tâp +Î¸ 0 e t +Î¸ 1 e tâ1 +Î¸ 2 e tâ2 +Â·Â·Â·+Î¸ q e tâq where e is a strictly stationary and erogodicwhite noise process. Itcanbe writtenusing lag operator t notationasÎ±(L)Y =Î± +Î¸(L)e . t 0 t",
    "page": 493,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 474 Theorem14.25 The ARMA(p,q) process (14.38) is strictly stationary and er- godicifallrootsofÎ±(z)lieoutsidetheunitcircle.Inthiscasewecanwrite Y =Âµ+b(L)e t t whereb j =O (cid:161) jpÎ²j(cid:162) and (cid:80)â j=0 (cid:175) (cid:175)b j (cid:175) (cid:175) <â. TheprocessY followsanautoregressive-integratedmoving-averageprocess,denotedARIMA(p,d,q), t ifâdY isARMA(p,q).ItcanbewrittenusinglagoperatornotationasÎ±(L)(1âL)dY =Î± +Î¸(L)e . t t 0 t 14.28 MixingPropertiesofLinearProcesses Thereisaconsiderableprobabilityliteratureinvestigatingthemixingpropertiesoftimeseriespro- cesses. One challenge is that since autoregressive processes depend on the infinite past sequence of innovationse itisnotimmediatelyobviousiftheysatisfythemixingconditions. t In fact, a simple AR(1) is not necessarily mixing. A counter-example was developed by Andrews (1984).Heshowedthatiftheerrore hasatwo-pointdiscretedistributionthenanAR(1)Y isnotstrong t t mixing. Thereasonisthatadiscreteinnovationcombinedwiththeautoregressivestructuremeansthat byobservingY youcandeducewithnearcertaintythepasthistoryoftheshockse .Theexampleseems t t ratherspecialbutshowstheneedtobecarefulwiththetheory. TheintuitionstemmingfromAndrewsâ findingisthatforanautoregressiveprocesstobemixingitisnecessaryfortheerrorse tobecontinuous. t AusefulcharacterizationwasprovidedbyPhamandTran(1985). Theorem14.26 SupposethatY t =Âµ+(cid:80)â j=0 Î¸ j e tâj satisfiesthefollowingcon- ditions: 1. e isi.i.d.with(cid:69)|e |r <âforsomer >0anddensity f(x)whichsatisfies t t (cid:90) â (cid:175) (cid:175)f (xâu)âf(x) (cid:175) (cid:175)dxâ¤C|u| (14.43) ââ forsomeC <â. 2. AllrootsofÎ¸(z)=0lieoutsidetheunitcircleand (cid:80)â j=0 (cid:175) (cid:175) Î¸ j (cid:175) (cid:175) <â. 3. (cid:80)â k=1 (cid:179) (cid:80)â j=k (cid:175) (cid:175) Î¸ j (cid:175) (cid:175) (cid:180)r/(1+r) <â. ThenforsomeB<â â (cid:195) â (cid:33)r/(1+r) Î±((cid:96))â¤4Î²((cid:96))â¤B (cid:88) (cid:88)(cid:175) (cid:175) Î¸ j (cid:175) (cid:175) k=(cid:96) j=k andY isabsolutelyregularandstrongmixing. t Thecondition(14.43)isratherunusual,butspecifiesthate hasasmoothdensity.Thisrulesoutthe t counter-examplediscoveredbyAndrews(1984).",
    "page": 494,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 475 Thesummabilityconditiononthecoefficientsinpart3involvesatrade-offwiththenumberofmo- ments r. If e has all moments finite (e.g. normal errors) then we can set r = â and this condition t simplifiesto (cid:80)â k|Î¸ |<â.Foranyr thesummabilityconditionholdsifÎ¸ hasgeometricdecay. k=1 k j It is instructive to deduce how the decay in the coefficients Î¸ affects the rate for the mixing co- j efficients Î±((cid:96)). If (cid:175) (cid:175) Î¸ j (cid:175) (cid:175) â¤O (cid:161) j âÎ·(cid:162) then (cid:80)â j=k (cid:175) (cid:175) Î¸ j (cid:175) (cid:175) â¤O (cid:179) k â(Î·â1) (cid:180) so the rate is Î±((cid:96)) â¤ 4Î²((cid:96)) â¤O((cid:96)âs) for s=(cid:161)Î·â1 (cid:162) r/(1+r)â1. Mixingrequiress>0,whichholdsforsufficientlylargeÎ·. Forexample,ifr =4it holdsforÎ·>9/4. Theprimarymessagefromthissectionisthatlinearprocesses,includingautoregressiveandARMA processes,aremixingiftheinnovationssatisfysuitableconditions.Themixingcoefficientsdecayatrates relatedtothedecayratesofthemovingaveragecoefficients. 14.29 Identification Theparametersofamodelareidentifiediftheparametersareuniquelydeterminedbytheproba- bilitydistributionoftheobservations. Inthecaseoflineartimeseriesanalysiswetypicallyfocusonthe second moments of the observations (means, variances, covariances). We therefore say that the coef- ficients of a stationary MA, AR, or ARMA model are identified if they are uniquely determined by the autocorrelationfunction.Thatis,giventheautocorrelationfunctionÏ(k),arethecoefficientsunique? ItturnsoutthattheansweristhatMAandARMAmodelsaregenerallynotidentified.Identificationis achievedbyrestrictingtheclassofpolynomialoperators.Incontrast,ARmodelsaregenerallyidentified. LetusstartwiththeMA(1)model Y t =e t +Î¸e tâ1 . Ithasfirst-orderautocorrelation Î¸ Ï(1)= . 1+Î¸2 SetÏ=1/Î¸.Then Ï 1/Ï Î¸ = = =Ï(1). 1+Ï2 1+(1/Ï)2 1+Î¸2 ThustheMA(1)modelwithcoefficientÏ=1/Î¸producesthesameautocorrelationsastheMA(1)model withcoefficientÎ¸. Forexample, Î¸=1/2andÏ=2eachyieldÏ(1)=2/5. Thereisnoempiricalwayto distinguishbetweenthemodelsY t =e t +Î¸e tâ1 andY t =e t +Ïe tâ1 .ThusthecoefficientÎ¸isnotidentified. Thestandardsolutionistoselecttheparameterwhichproducesaninvertiblemovingaveragepoly- nomial. Since there is only one such choice this yields a unique solution. This may be sensible when there is reason to believe that shocks have their primary impact in the contemporaneous period and secondary(lesser)impactinthesecondperiod. NowconsidertheMA(2)model Y t =e t +Î¸ 1 e tâ1 +Î¸ 2 e tâ2 . Themovingaveragepolynomialcanbefactoredas Î¸(z)=(cid:161) 1âÎ² z (cid:162)(cid:161) 1âÎ² z (cid:162) 1 2 sothatÎ² Î² =Î¸ andÎ² +Î² =âÎ¸ .Theprocesshasfirst-andsecond-orderautocorrelations 1 2 2 1 2 1 Î¸ +Î¸ Î¸ âÎ² âÎ² âÎ²2Î² âÎ² Î²2 Ï(1)= 1 1 2 = 1 2 1 2 1 2 1+Î¸2+Î¸2 1+Î²2+Î²2+2Î² Î² +Î²2Î²2 1 2 1 2 1 2 1 2 Î¸ Î² Î² Ï(2)= 2 = 1 2 . 1+Î¸2+Î¸2 1+Î²2+Î²2+2Î² Î² +Î²2Î²2 1 2 1 2 1 2 1 2",
    "page": 495,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 476 IfwereplaceÎ² withÏ =1/Î² weobtain 1 1 1 â1/Î² âÎ² âÎ² /Î²2âÎ²2/Î² âÎ² âÎ² Î²2âÎ² âÎ²2Î² Ï(1)= 1 2 2 1 2 1 = 1 2 1 2 2 1 1+1/Î²2+Î²2+2Î² /Î² +Î²2/Î²2 Î²2+1+Î²2Î²2+2Î² Î² +Î²2 1 2 2 1 2 1 1 2 1 2 1 2 Î² /Î² Î² Î² Ï(2)= 2 1 = 1 2 1+1/Î²2+Î²2+2Î² /Î² +Î²2/Î²2 Î²2+1+Î²2Î²2+2Î² Î² +Î²2 1 2 2 1 2 1 1 1 2 1 2 2 whichisunchanged. SimilarlyifwereplaceÎ² withÏ =1/Î² weobtainunchangedfirst-andsecond- 2 2 2 orderautocorrelations. ItfollowsthatintheMA(2)modelthefactorsÎ² andÎ² northecoefficientsÎ¸ 1 2 1 andÎ¸ areidentified.ConsequentlytherearefourdistinctMA(2)modelswhichareidentifiablyindistin- 2 guishable. ThisanalysisextendstotheMA(q)model.ThefactorsoftheMApolynomialcanbereplacedbytheir inversesandconsequentlythecoefficientsarenotidentified. The standard solution is to confine attention to MA(q) models with invertible roots. This techni- callysolvestheidentificationdilemma.ThissolutioncorrespondstotheWolddecomposition,sinceitis definedintermsoftheprojectionerrorswhichcorrespondtotheinvertiblerepresentation. AdeeperidentificationfailureoccursinARMAmodels.ConsideranARMA(1,1)model Y t =Î±Y tâ1 +e t +Î¸e tâ1 . Writteninlagoperatornotation (1âÎ±L)Y =(1+Î¸L)e . t t TheidentificationfailureisthatwhenÎ±=âÎ¸thenthemodelsimplifiestoY =e . Thismeansthatthe t t continuumofmodelswithÎ±=âÎ¸areallidenticalandthecoefficientsarenotidentified. ThisextendstohigherorderARMAmodels. TaketheARMA(2,2)modelwritteninfactoredlagoper- atornotation (1âÎ± L)(1âÎ± L)Y =(1+Î¸ L)(1+Î¸ L)e . 1 2 t 1 2 t ThemodelswithÎ± =âÎ¸ ,Î± =âÎ¸ ,Î± =âÎ¸ ,orÎ± =âÎ¸ allsimplifytoanARMA(1,1). Thusallthese 1 1 1 2 2 1 2 2 modelsareidenticalandhencethecoefficientsarenotidentified. Theproblemiscalledâcancellingrootsâduetothefactthatitariseswhentherearetwoidenticallag polynomialfactorsintheARandMApolynomials. The standard solution in the ARMA literature is to assume that there are no cancelling roots. The troublewiththissolutionisthatthisisanassumptionaboutthetrueprocesswhichisunknown.Thusit isnotreallyasolutiontotheidentificationproblem. Onerecommendationistobecarefulwhenusing ARMAmodelsandbeawarethathighlyparameterizedmodelsmaynothaveuniquecoefficients. NowconsidertheAR(p)model(14.38).Itcanbewrittenas Y =X (cid:48)Î±+e (14.44) t t t where Î±=(Î± 0 ,Î± 1 ,...Î± p ) (cid:48) and X t =(1,Y tâ1 ,...,Y tâp ) (cid:48) . The MDS assumption implies that (cid:69)[e t ]=0 and (cid:69)[X e ]=0.ThismeansthatthecoefficientÎ±satisfies t t Î±=(cid:161)(cid:69)(cid:163) X X (cid:48)(cid:164)(cid:162)â1 ((cid:69)[X Y ]). (14.45) t t t t ThisequationisuniqueifQ=(cid:69)(cid:163) X X (cid:48)(cid:164) ispositivedefinite.ItturnsoutthatthisisgenericallytruesoÎ±is t t uniqueandidentified.",
    "page": 496,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 477 Theorem14.27 IntheAR(p)model(14.38),if0<Ï2<âthenQ >0andÎ±is uniqueandidentified. TheassumptionÏ2>0meansthatY isnotpurelydeterministic. t We can extend this result to approximating AR(p) models. That is, consider the equation (14.44) withouttheassumptionthatY isnecessarilyatrueAR(p)withaMDSerror. Instead, supposethatY t t isanon-deterministicstationaryprocess. (Recall,non-deterministicmeansthatÏ2>0whereÏ2 isthe projectionerrorvariance(14.19).) WethendefinethecoefficientÎ±asthebestlinearpredictor,whichis (14.45).Theerrore isdefinedbytheequation(14.44).Thisisalinearprojectionmodel. t Asinthecaseofanylinearprojection,theerrore satisfies(cid:69)[X e ]=0.Thismeansthat(cid:69)[e ]=0and t t t t (cid:69)(cid:163) Y tâj e t (cid:164)=0for j =1,...,p.However,theerrore t isnotnecessarilyaMDSnorwhitenoise. ThecoefficientÎ±isidentifiedifQ>0.TheproofofTheorem14.27(presentedinSection14.47)does notmakeuseoftheassumptionthatY isanAR(p)withaMDSerror.Rather,itonlyusestheassumption t that Ï2 >0. This holds in the approximate AR(p) model as well under the assumption that Y is non- t deterministic.WeconcludethatanyapproximatingAR(p)isidentified. Theorem14.28 If Y is strictly stationary, not purely deterministic, and t (cid:69)(cid:163) Y2(cid:164)<â, then for any p, Q =(cid:69)(cid:163) X X (cid:48)(cid:164)>0 and thus the coefficient vector t t t (14.45)isidentified. 14.30 EstimationofAutoregressiveModels We consider estimation of an AR(p) model for stationary, ergodic, and non-deterministic Y . The t modelis(14.44)where X t =(1,Y tâ1 ,...,Y tâp ) (cid:48) . ThecoefficientÎ±isdefinedbyprojectionin(14.45). The errorisdefinedby(14.44)andhasvarianceÏ2=(cid:69)(cid:163) e2(cid:164) . ThisallowsY tofollowatrueAR(p)processbut t t itisnotnecessary. Theleastsquaresestimatoris (cid:181) n (cid:182)â1(cid:181) n (cid:182) Î±= (cid:88) X X (cid:48) (cid:88) X Y . (cid:98) t t t t t=1 t=1 This notation presumes that there are n+p total observations on Y from which the first p are used t as initial conditions so that X 1 =(1,Y 0 ,Yâ1 ,...,Yâp+1 ) is defined. Effectively, this redefines the sample period. (Analternativenotationalchoiceistodefinetheperiodssothesumsrangefromobservations p+1ton.) Theleastsquaresresidualsaree =Y âX (cid:48)Î±.Theerrorvariancecanbeestimatedby Ï2=n â1(cid:80)n e2 (cid:98)t t t(cid:98) (cid:98) t=1(cid:98)t ors2=(cid:161) nâpâ1 (cid:162)â1(cid:80)n e2. t=1(cid:98)t IfY isstrictlystationaryandergodicthensoareX X (cid:48) andX Y .Theyhavefinitemeansif(cid:69)(cid:163) Y2(cid:164)<â. t t t t t t UndertheseassumptionstheErgodicTheoremimpliesthat 1 (cid:88) n X Y ââ(cid:69)[X Y ] (14.46) t t t t n t=1 p",
    "page": 497,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 478 and 1 (cid:88) T X X (cid:48)ââ(cid:69)(cid:163) X X (cid:48)(cid:164)=Q. T t=1 t t p t t Theorem14.28showsthatQ>0.Combinedwiththecontinuousmappingtheoremweseethat (cid:195) (cid:33)â1(cid:195) (cid:33) Î±= 1 (cid:88) T X X (cid:48) 1 (cid:88) T X Y ââ(cid:161)(cid:69)(cid:163) X X (cid:48)(cid:164)(cid:162)â1(cid:69)[X Y ]=Î±. (cid:98) T t=1 t t T t=1 t t p t t t t ItisstraightforwardtoshowthatÏ2isconsistentaswell. (cid:98) Theorem14.29 If Y is strictly stationary, ergodic, not purely deterministic, t and(cid:69)(cid:163) Y2(cid:164)<â,thenforanyp,Î±ââÎ±andÏ2ââÏ2asnââ. t (cid:98) p (cid:98) p ThisshowsthatunderverymildconditionsthecoefficientsofanAR(p)modelcanbeconsistentlyes- timatedbyleastsquares.Onceagain,thisdoesnotrequirethattheseriesY isactuallyanAR(p)process. t Itholdsforanystationaryprocesswiththecoefficientdefinedbyprojection. 14.31 AsymptoticDistributionofLeastSquaresEstimator TheasymptoticdistributionoftheleastsquaresestimatorÎ±dependsonthestochasticassumptions. (cid:98) Inthissectionwederivetheasymptoticdistributionundertheassumptionofcorrectspecification. Specifically,weassumethattheerrore isaMDS.AnimportantimplicationoftheMDSassumption t isthatsinceX t =(1,Y tâ1 ,...,Y tâp ) (cid:48) ispartoftheinformationsetF tâ1 ,bytheconditioningtheorem, (cid:69)[X t e t |F tâ1 ]=X t (cid:69)[e t |F tâ1 ]=0. Thus X e is a MDS. It has a finite variance if e has a finite fourth moment. To see this, by Theorem t t t 14.24,Y t =Âµ+(cid:80)â j=0 b j e tâj with (cid:80)â j=0 (cid:175) (cid:175)b j (cid:175) (cid:175) <â.UsingMinkowskiâsInequality, â (cid:161)(cid:69)|Y t |4(cid:162)1/4â¤ (cid:88)(cid:175) (cid:175)b j (cid:175) (cid:175) (cid:179) (cid:69)(cid:175) (cid:175)e tâj (cid:175) (cid:175) 4 (cid:180)1/4 <â. j=0 Thus(cid:69)(cid:163) Y4(cid:164)<â.TheCauchy-Schwarzinequalitythenshowsthat(cid:69)(cid:107)X e (cid:107)2<â.Wecanthenapplythe t t t martingaledifferenceCLT(Theorem14.11)toseethat 1 (cid:88) n (cid:112) X e ââN(0,Î£) t t n t=1 d whereÎ£=(cid:69)(cid:163) X X (cid:48) e2(cid:164) . t t t Theorem14.30 IfY followstheAR(p)model(14.38),allrootsofa(z)lieout- t s (cid:112) idetheunitcircle,(cid:69)[e t |F tâ1 ]=0,(cid:69)(cid:163) e t 4(cid:164)<â,and(cid:69)(cid:163) e t 2(cid:164)>0,thenasnââ, n(Î±âÎ±)ââN(0,V)whereV =Q â1Î£Q â1. (cid:98) d Thisisidenticalinformtotheasymptoticdistributionofleastsquaresincross-sectionregression. Theimplicationisthatasymptoticinferenceisthesame.Inparticular,theasymptoticcovariancematrix isestimatedjustasinthecross-sectioncase.",
    "page": 498,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 479 14.32 DistributionUnderHomoskedasticity Incross-sectionregressionwefoundthatthecovariancematrixsimplifiesundertheassumptionof conditional homoskedasticity. The same occurs in the time series context. Assume that the error is a homoskedasticMDS: (cid:69)[e t |F tâ1 ]=0 (cid:69)(cid:163) e t 2|F tâ1 (cid:164)=Ï2. Inthiscase Î£=(cid:69)(cid:163) X t X t (cid:48)(cid:69)(cid:163) e t 2|F tâ1 (cid:164)(cid:164)=QÏ2 andtheasymptoticdistributionsimplifies. Theorem14.31 Under the assumptions of Theorem 14.30, if in addition (cid:112) (cid:69)(cid:163) e t 2|F tâ1 (cid:164)=Ï2,thenasnââ, n(Î± (cid:98) âÎ±)ââN (cid:161) 0,V0(cid:162) whereV0=Ï2Q â1. d Theseresultsshowthatundercorrectspecification(aMDSerror)theformatoftheasymptoticdis- tributionoftheleastsquaresestimatorexactlyparallelsthecross-sectioncase.Ingeneralthecovariance matrix takes a sandwich form with components exactly equal to the cross-section case. Under condi- tionalhomoskedasticitythecovariancematrixsimpliesexactlyasinthecross-sectioncase. AparticularlyusefulinsightwhichcanbederivedfromTheorem14.31istofocusonthesimpleAR(1) withnointercept.InthiscaseQ=(cid:69)(cid:163) Y2(cid:164)=Ï2/(1âÎ±2)sotheasymptoticdistributionsimplifiesto t 1 (cid:112) n(Î± âÎ± )ââN (cid:161) 0,1âÎ±2(cid:162) . (cid:98)1 1 1 d ThustheasymptoticvariancedependsonlyonÎ± andisdecreasingwithÎ±2.AnintuitionisthatlargerÎ±2 1 1 1 meansgreatersignalandhencegreaterestimationprecision. Thisresultalsoshowsthattheasymptotic distributionisnon-similar: thevarianceisafunctionoftheparameterofinterest. Thismeansthatwe canexpect(fromadvancedstatisticaltheory)asymptoticinferencetobelessaccuratethanindicatedby nominallevels. Inthecontextofcross-sectiondatawearguedthatthehomoskedasticityassumptionwasdubiousex- ceptforoccassionaltheoreticalinsight.Forpracticalapplicationsitisrecommendedtouseheteroskedasticity- robusttheoryandmethodswhenpossible. Thesameargumentappliestothetimeseriescase. While thedistributiontheorysimplifiesunderconditionalhomoskedasticitythereisnoreasontoexpectho- moskedasticity to hold in practice. Therefore in applications it is better to use the heteroskedasticity- robustdistributionaltheorywhenpossible. Unfortunately,manyexistingtimeseriestextbooksreportthedistributiontheoryfrom(14.31). This hasinfluencedcomputersoftwarepackagesmanyofwhichalsobydefault(orexclusively)usetheho- moskedasticdistributiontheory.Thisisunfortunate. 14.33 AsymptoticDistributionUnderGeneralDependence IftheAR(p)model(14.38)holdswithwhitenoiseerrorsoriftheAR(p)isanapproximationwithÎ± defined as the best linear predictor then the MDS central limit theory does not apply. Instead, if Y is t strongmixingwecanusethecentrallimittheoryformixingprocesses(Theorem14.15).",
    "page": 499,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 480 Theorem14.32 AssumethatY isstrictlystationary,ergodic,andforsomer > t 4,(cid:69)|Y |r <âandthemixingcoefficientssatisfy (cid:80)â Î±((cid:96))1â4/r <â. LetÎ±be t (cid:96)=1 definedasthebestlinearprojectioncoefficients(14.45)fromanAR(p)model withprojectionerrorse .LetÎ±betheleastsquaresestimatorofÎ±.Then t (cid:98) â â¦= (cid:88) (cid:69)(cid:163) X tâ(cid:96)X t (cid:48) e t e tâ(cid:96) (cid:164) (cid:96)=ââ (cid:112) isconvergentand n(Î±âÎ±)ââN(0,V)asnââ,whereV =Q â1â¦Q â1. (cid:98) d Thisresultissubstantiallydifferentfromthecross-sectioncase.Itshowsthatmodelmisspecification (includingmisspecifyingtheorderoftheautoregression)rendersinvalidtheconventionalâheteroskedasticity- robustâcovariancematrixformula. Misspecifiedmodelsdonothaveunforecastable(martingalediffer- ence) errors so the regression scores X e are potentially serially correlated. The asymptotic variance t t takesasandwichformwiththecentralcomponentâ¦thelong-runvariance(recallSection14.13)ofthe regressionscoresX e . t t 14.34 CovarianceMatrixEstimation Undertheassumptionofcorrectspecificationcovariancematrixestimationisidenticaltothecross- sectioncase.Theasymptoticcovariancematrixestimatorunderhomoskedasticityis V(cid:98) 0=Ï (cid:98) 2Q(cid:98) â1 Q(cid:98) = n 1 t (cid:88) = n 1 X t X t (cid:48) Theestimators2maybeusedinsteadofÏ2. (cid:98) Theheteroskedasticity-robustasymptoticcovariancematrixestimatoris V(cid:98) =Q(cid:98) â1Î£ (cid:98)Q(cid:98) â1 (14.47) where Î£ (cid:98) = n 1 t (cid:88) = n 1 X t X t (cid:48) e (cid:98)t 2. Degree-of-freedomadjustmentsmaybemadeasinthecross-sectioncasethoughatheoreticaljustifica- tionhasnotbeendevelopedinthetimeseriescase. Standarderrorss (cid:161)Î± (cid:162) forindividualcoefficientestimatescanbeformedbytakingthescaleddiagonal (cid:98)j elementsofV(cid:98). Theorem14.33 UndertheassumptionsofTheorem14.32,asnââ,V(cid:98) ââV p and (cid:161)Î± âÎ± (cid:162) /s(Î± )ââN(0,1). (cid:98)j j (cid:98)j d Theorem14.33showsthatstandardcovariancematrixestimationisconsistentandtheresultingt- ratiosareasymptoticallynormal. Thismeansthatforstationaryautoregressions,inferencecanproceed usingconventionalregressionmethods.",
    "page": 500,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 481 14.35 CovarianceMatrixEstimationUnderGeneralDependence UndertheassumptionsofTheorem14.32theconventionalcovariancematrixestimatorsareincon- sistentastheydonotcapturetheserialdependenceintheregressionscores X e . Toconsistentlyesti- t t matethecovariancematrixweneedanestimatorofthelong-runvarianceâ¦. Theappropriateclassof estimatorsarecalledHeteroskedasticityandAutocorrelationConsistent(HAC)orHeteroskedasticity andAutocorrelationRobust(HAR)covariancematrixestimators. To understand the methods it is helpful to define the vector series u = X e and autocovariance t t t matricesÎ((cid:96))=(cid:69)(cid:163) u tâ(cid:96)u t (cid:48)(cid:164) sothat â (cid:88) â¦= Î((cid:96)). (cid:96)=ââ Sincethissumisconvergenttheautocovariancematricesconvergetozeroas(cid:96)ââ. Thereforeâ¦can beapproximatedbytakingafinitesumofautocovariancessuchas M (cid:88) â¦ = Î((cid:96)). M (cid:96)=âM ThenumberM issometimescalledthelagtruncationnumber.Otherauthorscallitthebandwidth.An estimatorofÎ((cid:96))is Î (cid:98)((cid:96))= n 1 (cid:88) u (cid:98)tâ(cid:96)u (cid:98)t (cid:48) 1â¤tâ(cid:96)â¤n whereu (cid:98)t =X t e (cid:98)t .Bytheergodictheoremwecanshowthatforany(cid:96),Î (cid:98)((cid:96))ââÎ((cid:96)).ThusforanyfixedM, p theestimator M (cid:88) â¦ (cid:98)M = Î (cid:98)((cid:96)) (14.48) (cid:96)=âM isconsistentforâ¦ . M If the serial correlation in X e is known to be zero after M lags, then â¦ =â¦ and the estimator t t M (14.48)isconsistentforâ¦. ThisestimatorwasproposedbyL.HansenandHodrick(1980)inthecontext ofmultiperiodforecastsandbyL.Hansen(1982)forthegeneralizedmethodofmoments. InthegeneralcasewecanselectM toincreasewithsamplesizen.IftherateatwhichM increasesis sufficientlyslowthenâ¦ (cid:98)M willbeconsistentforâ¦asfirstshownbyWhiteandDomowitz(1984). Onceweviewthelagtruncationnumber M asachoicetheestimator(14.48)hastwopotentialde- ficiencies. Oneisthatâ¦ (cid:98)M canchangenon-smoothlywith M whichmakesestimationresultssensitive tothechoiceofM. Theotheristhatâ¦ (cid:98)M maynotbepositivesemi-definiteandisthereforenotavalid covariance matrix estimator. We can see this in the simple case of scalar u and M =1. In this case t â¦ (cid:98)1 =Î³ (cid:98) (0) (cid:161) 1+2Ï (cid:98) (1) (cid:162) whichisnegativewhenÏ (cid:98) (1)<â1/2.Thusifthedataarestronglynegativelyautocor- relatedthevarianceestimatorcanbenegative.Anegativevarianceestimatormeansthatstandarderrors areill-defined(anaÃ¯vecomputationwillproduceacomplexstandarderrorwhichmakesnosense5). Thesetwodeficienciescanberesolvedifweamend(14.48)byaweightedsumofautocovariances. NeweyandWest(1987b)proposed M (cid:181) |(cid:96)| (cid:182) (cid:88) â¦ (cid:98)nw = 1â M+1 Î (cid:98)((cid:96)). (14.49) (cid:96)=âM 5Acommoncomputationalmishapisacomplexstandarderror.Thisoccurswhenacovariancematrixestimatorhasnega- tiveelementsonthediagonal.",
    "page": 501,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 482 Thisisaweightedsumoftheautocovariances. Otherweightfunctionscanbeused; theonein(14.49) isknownastheBartlettkernel6. NeweyandWest(1987b)showedthatthisestimatorhasthealgebraic propertythatâ¦ (cid:98)nw â¥0(itispositivesemi-definite),solvingthenegativevarianceproblem,anditisalsoa smoothfunctionofM.Thusthisestimatorsolvesthetwoproblemsdescribedabove. Forâ¦ (cid:98)nw tobeconsistentforâ¦thelagtrunctionnumberMmustincreasetoinfinitywithn.Sufficient conditionswereestablishedbyB.E.Hansen(1992). Theorem14.34 Under the assumptions of Theorem 14.32 plus (cid:80)â (cid:96)=1 Î±((cid:96))1/2â4/r <â,ifMââyetM3/n=O(1),thenasnââ,â¦ (cid:98)nw â p ââ¦. TheassumptionM3/n=O(1)technicallymeansthatM growsnofasterthann1/3 butthisdoesnot haveapracticalcounterpartotherthantheimplicationthatâM shouldbemuchsmallerthannâ. AimportantpracticalissueishowtoselectM. OnewaytothinkaboutitisthatM impactsthepre- cisionoftheestimatorâ¦ (cid:98)nw throughitsbiasandvariance. SinceÎ (cid:98)((cid:96))isasampleaverageitsvarianceis O(1/n)soweexpectthevarianceofâ¦ (cid:98)M tobeoforderO(M/n). Thebiasofâ¦ (cid:98)nw forâ¦ishardertocal- culatebutdependsontherateatwhichthecovariancesÎ((cid:96))decaytozero. Andrews(1991b)foundthat theM whichminimizesthemeansquarederrorofâ¦ (cid:98)nw satisfiestherateM=Cn1/3wheretheconstantC dependsontheautocovariances. Practicalrulestoestimateandimplementthisoptimallagtruncation parameterhavebeenproposedbyAndrews(1991b)andNeweyandWest(1994). Andrewsâruleforthe Newey-Westestimator(14.49)canbewrittenas (cid:195) (cid:33)1/3 Ï2 M= 6 n1/3 (14.50) (cid:161) 1âÏ2 (cid:162)2 whereÏisaserialcorrelationparameter. Whenu isscalarÏisthefirstautocorrelationofu . Andrews t t suggestedusinganestimatorofÏ toplugintothisformulatofindM. Analternativeistouseadefault valueofÏ.Forexample,ifwesetÏ=0.25orÏ=0.5thentheAndrewsruleisM=0.75n1/3orM=1.4n1/3. 14.36 TestingtheHypothesisofNoSerialCorrelation In some cases it may be of interest to test the hypothesis that the series Y is serially uncorrelated t againstthealternativethatitisseriallycorrelated.Therehavebeenmanyproposedtestsofthishypoth- esis.ThemostappropriateisbasedontheleastsquaresregressionofanAR(p)model.Takethemodel Y t =Î± 0 +Î± 1 Y tâ1 +Î± 2 Y tâ2 +Â·Â·Â·+Î± p Y tâp +e t with e a MDS. In this model the series Y is serially uncorrelated if the slope coefficients are all zero. t t Thusthehypothesisofinterestis (cid:72) :Î± =Â·Â·Â·=Î± =0 0 1 p (cid:72) :Î± (cid:54)=0forsome j â¥1. 1 j 6SeeAndrews(1991b)foradescriptionofpopularoptions.Inpractice,thechoiceofweightfunctionismuchlessimportant thanthechoiceoflagtruncationnumberM.",
    "page": 502,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 483 ThetestcanbeimplementedbyaWaldorFtest. EstimatetheAR(p)modelbyleastsquares. Form theWaldorFstatisticusingthevarianceestimator(14.47). (TheNewey-Westestimatorshouldnotbe usedasthereisnoserialcorrelationunderthenullhypothesis.)Acceptthehypothesisiftheteststatistic issmallerthanaconventionalcriticalvalue(orifthep-valueexceedsthesignificancelevel)andreject thehypothesisotherwise. Implementationofthistestrequiresachoiceofautoregressiveorderp.Thischoiceaffectsthepower of the test. A sufficient number of lags should be included so to pick up potential serial correlation patternsbutnotsomanythatthepowerofthetestisdiluted.Areasonablechoiceinmanyapplications istosetptoequalss,theseasonalperiodicity.Thusincludefourlagsforquarterlydataortwelvelagsfor monthlydata. 14.37 TestingforOmittedSerialCorrelation WhenusinganAR(p)modelitmaybeofinteresttoknowifthereisanyremainingserialcorrelation. Thiscanbeexpressedasatestforserialcorrelationintheerrororequivalentlyasatestforahigher-order autogressivemodel. TaketheAR(p)model Y t =Î± 0 +Î± 1 Y tâ1 +Î± 2 Y tâ2 +Â·Â·Â·+Î± p Y tâp +u t . (14.51) Thenullhypothesisisthatu isseriallyuncorrelatedandthealternativehypothesisisthatitisserially t correlated.Wecanmodelthelatterasamean-zeroautoregressiveprocess u t =Î¸ 1 u tâ1 +Â·Â·Â·+Î¸ q u tâq +e t . (14.52) Thehypothesisis (cid:72) :Î¸ =Â·Â·Â·=Î¸ =0 0 1 q (cid:72) :Î¸ (cid:54)=0forsome j â¥1. 1 j Aseeminglynaturaltestfor(cid:72) usesatwo-stepmethod. Firstestimate(14.51)byleastsquaresand 0 obtaintheresidualsu . Second, estimate(14.52)byleastsquaresbyregressingu onitslaggedvalues (cid:98)t (cid:98)t and obtain the Wald or F test for (cid:72) . This seems like a natural approach but it is muddled by the fact 0 thatthedistributionoftheWaldstatisticisdistortedbythetwo-stepprocedure.TheWaldstatisticisnot asymptotically chi-square so it is inappropriate to make a decision based on the conventional critical values. Oneapproachtoobtainthecorrectasymptoticdistributionistousethegeneralizedmethodof momentstreating(14.51)-(14.52)asatwo-equationjust-identifiedsystem. Aneasiersolutionistore-write(14.51)-(14.52)asahigher-orderautoregressionsothatwecanusea standardteststatistic. Toillustratehowthisworkstakethecaseq=1. Take(14.51)andlagtheequation once: Y tâ1 =Î± 0 +Î± 1 Y tâ2 +Î± 2 Y tâ3 +Â·Â·Â·+Î± p Y tâpâ1 +u tâ1 . MultiplythisbyÎ¸ andsubtractfrom(14.51)tofind 1 Y t âÎ¸ 1 Y tâ1 =Î± 0 +Î± 1 Y tâ1 +Î± 2 Y tâ2 +Â·Â·Â·+Î± p Y tâp +u t âÎ¸ 1 Î± 0 âÎ¸ 1 Î± 1 Y tâ2 âÎ¸ 1 Î± 2 Y tâ3 âÂ·Â·Â·âÎ¸ 1 Î± p Y tâpâ1 âÎ¸ 1 u tâ1 or Y t =Î± 0 (1âÎ¸ 1 )+(Î± 1 +Î¸ 1 )Y tâ1 +(Î± 2 âÎ¸ 1 Î± 1 )Y tâ2 +Â·Â·Â·âÎ¸ 1 Î± p Y tâpâ1 +e t .",
    "page": 503,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 484 ThisisanAR(p+1).ItsimplifiestoanAR(p)whenÎ¸ =0.Thus(cid:72) isequivalenttotherestrictionthatthe 1 0 coefficientonY tâpâ1 iszero. ThustestingthenullhypothesisofanAR(p)(14.51)againstthealternativethattheerrorisanAR(1) isequivalenttotestinganAR(p)againstanAR(p+1). ThelattertestisimplementedasaWald(orF)test onthecoefficientonY tâpâ1 . Moregenerally,testingthenullhypothesisofanAR(p)(14.51)againstthealternativethattheerroris anAR(q)isequivalenttotestingthatY isanAR(p)againstthealternativethatY isanAR(p+q).Thelatter t t testisimplementedasaWald(orF)testonthecoefficientsonY tâpâ1 ,...,Y tâpâ1 .Ifthestatisticissmaller thanthecriticalvalues(orthep-valueislargerthanthesignificancelevel)thenwerejectthehypothesis that the AR(p) is correctly specified in favor of the alternative that there is omitted serial correlation. OtherwiseweacceptthehypothesisthattheAR(p)modeliscorrectlyspecified. Anotherwayofderivingthetestisasfollows. Write(14.51)and(14.52)usinglagoperatornotation Î±(L)Y =Î± +u withÎ¸(L)u =e .ApplyingtheoperatorÎ¸(L)tothefirstequationweobtainÎ¸(L)Î±(L)Y = t 0 t t t t Î±â+e whereÎ±â=Î¸(1)Î± .TheproductÎ¸(L)Î±(L)isapolynomialoforderp+q soY isanAR(p+q). 0 t 0 0 t While this discussion is all good fun, it is unclear if there is good reason to use the test described inthissection. Economictheorydoesnottypicallyproducehypothesesconcerningtheautoregressive order. Consequentlythereisrarelyacasewherethereisscientificinterestintesting,say,thehypothesis that a series is an AR(4) or any other specific autoregressive order. Instead, practitioners tend to use hypothesistestsforanotherpurposeâmodelselection. Thatis, inpracticeuserswanttoknowâWhat autoregressivemodelshouldbeusedâinaspecificapplicationandresorttohypothesisteststoaidinthis decision. Thisisaninappropriateuseofhypothesistestsbecausetestsaredesignedtoprovideanswers toscientificquestionsratherthanbeingdesignedtoselectmodelswithgoodapproximationproperties. Instead, model selection should be based on model selection tools. One is described in the following section. 14.38 ModelSelection Whatisanappropriatechoiceofautoregressiveorderp?Thisistheproblemofmodelselection. AgoodchoiceistominimizetheAkaikeinformationcriterion(AIC) AIC(p)=nlogÏ2(p)+2p (cid:98) where Ï2(p) is the estimated residual variance from an AR(p). The AIC is a penalized version of the (cid:98) Gaussianlog-likelihoodfunctionfortheestimatedregressionmodel.Itisanestimatorofthedivergence betweenthefittedmodelandthetrueconditionaldensity(seeSection28.4).Byselectingthemodelwith thesmallestvalueoftheAICyouselectthemodelwiththesmallestestimateddivergenceâthehighest estimatedfitbetweentheestimatedandtruedensities. The AIC is also a monotonic transformation of an estimator of the one-step-ahead forecast mean squarederror. ThusselectingthemodelwiththesmallestvalueoftheAICyouareselectingthemodel withthesmallestestimatedforecasterror",
    "page": 504,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". The AIC is also a monotonic transformation of an estimator of the one-step-ahead forecast mean squarederror. ThusselectingthemodelwiththesmallestvalueoftheAICyouareselectingthemodel withthesmallestestimatedforecasterror. OnepossiblehiccupincomputingtheAICcriterionformultiplemodelsisthatthesamplesizeavail- able for estimation changes as p changes. (If you increase p, you need more initial conditions.) This rendersAICcomparisonsinappropriate. Thesamesampleâthesamenumberofobservationsâshould beusedforestimationofallmodels. Theappropriateremedyistofixauppervaluep,andthenreserve thefirstpasinitialconditions.ThenestimatethemodelsAR(1),AR(2),...,AR(p)onthis(unified)sample. TheAICofanestimatedregressionmodelcanbedisplayedinStatabyusingtheestimates stats command.",
    "page": 504,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 485 14.39 Illustrations WeillustrateautoregressiveestimationwiththreeempiricalexamplesusingU.S.quarterlytimeseries fromtheFRED-QDdatafile. Table14.1:U.S.GDPARModels AR(0) AR(1) AR(2) AR(3) AR(4) Î± 0.65 0.40 0.34 0.34 0.34 0 (0.06) (0.08) (0.10) (0.10) (0.11) [0.09] [0.08] [0.09] [0.09] [0.09] Î± 0.39 0.34 0.33 0.34 1 (0.09) (0.10) (0.10) (0.10) [0.10] [0.10] [0.10] [0.10] Î± 0.14 0.13 0.13 2 (0.11) (0.13) (0.14) [0.10] [0.10] [0.11] Î± 0.02 0.03 3 (0.11) (0.12) [0.07] [0.09] Î± â0.02 4 (0.12) [0.13] AIC 329 306 305 307 309 1. Standarderrorsrobusttoheteroskedasticityinparenthesis. 2. Newey-Weststandarderrorsinsquarebrackets,withM=5. ThefirstexampleisrealGDPgrowthrates(growthrateofgdpc1).Weestimateautoregressivemodels oforder0through4usingthesamplefrom1980-20177. Thisisacommonlyestimatedmodelinapplied macroeconomic practice and is the empirical version of the Samuelson multiplier-accelerator model discussedinSection14.24.Thecoefficientestimates,conventional(heteroskedasticity-robust)standard errors,Newey-West(withM =5)standarderrors,andAIC,aredisplayedinTable14.1. Thissamplehas 152observations.ThemodelselectedbytheAICcriterionistheAR(2).Theestimatedmodelhaspositive andsmallvaluesforthefirsttwoautoregressivecoefficients. Thismeansthatquarterlyoutputgrowth ratesarepositivelycorrelatedfromquartertoquarter,butonlymildlyso,andmostofthecorrelationis captured by the first lag. The coefficients of this model are inthe real section ofFigure 14.8, meaning thatthedynamicsoftheestimatedmodeldonotdisplayoscillations. Thecoefficientsoftheestimated AR(4)modelarenearlyidenticaltotheAR(2)model.TheconventionalandNewey-Weststandarderrors aresomewhatdifferentfromoneanotherfortheAR(0)andAR(4)models,butarenearlyidenticaltoone anotherfortheAR(1)andAR(2)models Oursecondexampleisrealnon-durablesconsumptiongrowthrates(growthrateofpcndx). Thisis motivatedbyaninfluentialpaperbyRobertHall(1978)whoarguedthatthepermanentincomehypoth- esisimpliesthatchangesinconsumptionshouldbeunpredictable(martingaledifferences). Totestthis 7Thissub-samplewasusedforestimationasithasbeenarguedthatthegrowthrateofU.S.GDPslowedaroundthisperiod. Thegoalwastoestimatethemodeloveraperiodoftimewhentheseriesisplausiblystationary.",
    "page": 505,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 486 modelHall(1978)estimatedanAR(4)model. Ourestimatedregressionusingthefullsample(n=231) andheteroskedasticity-robuststandarderrorsisreportedinthefollowingequation. LetC denotethe t consumptiongrowthrate. C(cid:99)t = 0.15 C tâ1 + 0.11 C tâ2 + 0.13 C tâ3 + 0.02 C tâ4 + 0.35 . (0.07) (0.07) (0.07) (0.08) (0.09) Hallâshypothesisisthatallautoregressivecoefficientsshouldbezero. Wetestthisjointhypothesiswith anF statisticandfindF=3.32withap-valueof p =0.012. Thisissignificantatthe5%levelandclose to the 1% level. The first three autoregressive coefficients appear to be positive, but small, indicat- ingpositiveserialcorrelation. Thisevidenceis(mildly)inconsistentwithHallâshypothesis. Wereport heteroskedasticity-robust standard errors (not Newey-West standard errors) since the purpose was to testthehypothesisofnoserialcorrelation. Table14.2:U.S.InflationARModels AR(1) AR(2) AR(3) AR(4) AR(5) Î± 0.004 0.003 0.003 0.003 0.003 0 (0.034) (0.032) (0.032) (0.032) (0.032) [0.023] [0.028] [0.029] [0.031] [0.032] Î± â0.26 â0.36 â0.36 â0.36 â0.37 1 (0.08) (0.07) (0.07) (0.07) (0.07) [0.05] [0.07] [0.07] [0.07] [0.07] Î± â0.36 â0.37 â0.42 â0.43 2 (0.07) (0.06) (0.06) (0.06) [0.06] [0.05] [0.07] [0.07] Î± â0.00 â0.06 â0.08 3 (0.09) (0.10) (0.11) [0.09] [0.12] [0.13] Î± â0.16 â0.18 4 (0.08) (0.08) [0.09] [0.09] Î± â0.04 5 (0.07) [0.06] AIC 342 312 314 310 312 1. Standarderrorsrobusttoheteroskedasticityinparenthesis. 2. Newey-Weststandarderrorsinsquarebrackets,withM=5. ThethirdexampleisthefirstdifferenceofCPIinflation(firstdifferenceofgrowthrateofcpiaucsl). ThisismotivatedbyStockandWatson(2007)whoexaminedforecastingmodelsforinflationrates. We estimateautoregressivemodelsoforder1through8usingthefullsample(n =226); wereportmodels 1through5inTable14.2. ThemodelwiththelowestAICistheAR(4). Allfourestimatedautoregressive coefficientsarenegative,mostparticularlythefirsttwo.Thetwosetsofstandarderrorsarequitesimilar fortheAR(4)model.TherearemeaningfuldifferencesonlyforthelowerorderARmodels.",
    "page": 506,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 487 14.40 TimeSeriesRegressionModels Leastsquaresregressionmethodscanbeusedbroadlywithstationarytimeseries.Interpretationand usefulnesscandepend,however,onconstructivedynamicspecifications.Furthermore,itisnecessaryto beawareoftheserialcorrelationpropertiesoftheseriesinvolved,andtousetheappropriatecovariance matrixestimatorwhenthedynamicshavenotbeenexplicitlymodeled. Let(Y ,X )bepairedobservationswithY thedependentvariableand X avectorofregressorsin- t t t t cludinganintercept.TheregressorscancontainlaggedY sothisframeworkincludestheautoregressive t modelasaspecialcase.Alinearregressionmodeltakestheform Y =X (cid:48)Î²+e . (14.53) t t t Thecoefficientvectorisdefinedbyprojectionandthereforeequals Î²=(cid:161)(cid:69)(cid:163) X X (cid:48)(cid:164)(cid:162)â1(cid:69)[X Y ]. (14.54) t t t t Theerrore isdefinedby(14.53)andthusitspropertiesaredeterminedbythatrelationship. Implicitly t themodelassumesthatthevariableshavefinitesecondmomentsand(cid:69)(cid:163) X X (cid:48)(cid:164)>0,otherwisethemodel t t isnotuniquelydefinedandaregressorcouldbeeliminated. Bythepropertyofprojectiontheerroris uncorrelatedwiththeregressors(cid:69)[X e ]=0. t t TheleastsquaresestimatorofÎ²is (cid:195) (cid:33)â1(cid:195) (cid:33) T T Î² (cid:98) = (cid:88) X t X t (cid:48) (cid:88) X t Y t . t=1 t=1 Undertheassumptionthatthejointseries(Y ,X )isstrictlystationaryandergodictheestimatoriscon- t t sistent. Under the mixing and moment conditions of Theorem 14.32 the estimator is asymptotically normalwithageneralcovariancematrix However, underthestrongerassumptionthattheerrorisaMDStheasymptoticcovariancematrix simplifies.Itisworthwhileinvestigatingthisconditionfurther.Thenecessaryconditionis(cid:69)[e t |F tâ1 ]= 0whereF tâ1 isaninformationsettowhich(e tâ1 ,X t )isadapted. Thisnotationmayappearsomewhat oddbutrecallintheautoregessivecontextthatX t =(1,Y tâ1 ,...,Y tâp )containsvariablesdatedtimetâ1 andpreviously,thusX t inthiscontextisaâtimetâ1âvariable. Thereasonwhyweneed(e tâ1 ,X t )tobe adaptedtoF tâ1 isthatfortheregressionfunctionX t (cid:48)Î²tobetheconditionalmeanofY t givenF tâ1 , X t mustbepartoftheinformationsetF tâ1 .Underthisassumption (cid:69)[X t e t |F tâ1 ]=X t (cid:69)[e t |F tâ1 ]=0 so(X e ,F )isaMDS.ThismeanswecanapplytheMDSCLTtoobtaintheasymptoticdistribution. t t t Wesummarizethisdiscussionwiththefollowingformalstatement.",
    "page": 507,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 488 Theorem14.35 If(Y ,X )isstrictlystationary,ergodic,withfinitesecondmo- t t ments,andQ=(cid:69)(cid:163) X X (cid:48)(cid:164)>0,thenÎ²in(14.54)isuniquelydefinedandtheleast t t squaresestimatorisconsistent,Î² (cid:98) ââÎ². p If in addition, (cid:69)[e t |F tâ1 ] = 0, where F tâ1 is an information set to which (e tâ1 ,X t )isadapted,(cid:69)|Y t |4<â,and(cid:69)(cid:107)X t (cid:107)4<â,then (cid:112) n (cid:161)Î² (cid:98) âÎ²(cid:162)ââN (cid:161) 0,Q â1â¦Q â1(cid:162) (14.55) d asnââ,whereâ¦=(cid:69)(cid:163) X X (cid:48) e2(cid:164) . t t t Alternatively,ifforsomer >4,(cid:69)|Y |r <â,(cid:69)(cid:107)X (cid:107)r <â,andthemixingcoeffi- t t cientsfor(Y ,X )satisfy (cid:80)â Î±((cid:96))1â4/r <â,then(14.55)holdswith t t (cid:96)=1 â â¦= (cid:88) (cid:69)(cid:163) X tâ(cid:96)X t (cid:48) e t e tâ(cid:96) (cid:164) . (cid:96)=ââ 14.41 Static,DistributedLag,andAutoregressiveDistributedLagModels Inthissectionwedescribestandardlineartimeseriesregressionmodels. Let(Y ,Z )bepairedobservationswithY thedependentvariableandZ anobservedregressorvec- t t t t torwhichdoesnotincludelaggedY . t Thesimplestregressionmodelisthestaticequation Y =Î±+Z (cid:48)Î²+e . t t t Thisis(14.53)bysetting X =(1,Z (cid:48) ) (cid:48) . StaticmodelsaremotivatedtodescribehowY and Z co-move. t t t t Theiradvantageistheirsimplicity.Thedisadvantageisthattheyaredifficulttointerpret.Thecoefficient is the best linear predictor (14.54) but almost certainly is dynamically misspecified. The regression of Y on contemporeneous Z is difficult to interpret without a causal framework since the two may be t t simultaneous. Ifthisregressionisestimateditisimportantthatthestandarderrorsbecalculatedusing theNewey-Westmethodtoaccountforserialcorrelationintheerror. A model which allows the regressor to have impact over several periods is called a distributedlag (DL)model.Ittakestheform Y t =Î±+Z t (cid:48) â1 Î² 1 +Z t (cid:48) â2 Î² 2 +Â·Â·Â·+Z t (cid:48) âq Î² q +e t . Itisalsopossibletoincludethecontemporenousregressor Z . InthismodeltheleadingcoefficientÎ² t 1 representstheinitialimpactofZ onY ,Î² representstheimpactinthesecondperiod,andsoon. The t t 2 cumulativeimpactisthesumofthecoefficientsÎ² +Â·Â·Â·+Î² whichiscalledthelong-runmultiplier. 1 q The distributed lag model falls in the class (14.53) by setting X =(1,Z (cid:48) ,Z (cid:48) ,...,Z (cid:48) ) (cid:48) . While it t tâ1 tâ2 tâq allows for a lagged impact of Z on Y , the model does not incorporate serial correlation so the error t t e shouldbeexpectedtobeseriallycorrelated. Thusthemodelis(typically)dynamicallymisspecified t which can make interpretation difficult. It is also necessary to use Newey-West standard errors to ac- countfortheserialcorrelation. Amorecompletemodelcombinesautoregressiveanddistributedlags.Ittakestheform Y t =Î± 0 +Î± 1 Y tâ1 +Â·Â·Â·+Î± p Y tâp +Z t (cid:48) â1 Î² 1 +Â·Â·Â·+Z t (cid:48) âq Î² q +e t .",
    "page": 508,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 489 This is called an autoregressive distributed lag (AR-DL) model. It nests both the autoregressive and distributed lag models thereby combining serial correlation and dynamic impact. The AR-DL model fallsintheclass(14.53)bysettingX t =(1,Y tâ1 ,...,Y tâp ,Z t (cid:48) â1 ,...,Z t (cid:48) âq ) (cid:48) . Ifthelagordersp andq areselectedsufficientlylargetheAR-DLmodelwillhaveanerrorwhichis approximatelywhitenoiseinwhichcasethemodelcanbeinterpretedasdynamicallywell-specifiedand conventionalstandarderrormethodscanbeused. InanAR-DLspecificationthelong-runmultiplieris Î² +Â·Â·Â·+Î² 1 q 1âÎ± âÂ·Â·Â·âÎ± 1 p whichisanonlinearfunctionofthecoefficients. 14.42 TimeTrends Manyeconomictimeserieshavemeanswhichchangeovertime. Ausefulwaytothinkaboutthisis thecomponentsmodel Y =T +u t t t whereT isthetrendcomponentandu isthestochasticcomponent. Thelattercanbemodeledbya t t linearprocessorautoregression Î±(L)u =e . t t Thetrendcomponentisoftenmodeledasalinearfunctioninthetimeindex T =Î² +Î² t t 0 1 oraquadraticfunctionintime T =Î² +Î² t+Î² t2. t 0 1 2 Thesemodelsaretypicallynotthoughtofasbeingliterallytruebutratherasusefulapproximations. Whenwewritedowntimeseriesmodelswewritetheindexas t =1,...,n. Butinpracticalapplica- tions the time index corresponds to a date, e.g. t =1960,1961,...,2017. Furtheremore, if the data is at a higher frequency than annual then it is incremented in fractional units. This is not of fundamental importance;itmerelychangesthemeaningoftheinterceptÎ² andslopeÎ² .Consequentlytheseshould 0 1 notbeinterpretedoutsideofhowthetimeindexisdefined. One traditional way of dealing with time trends is to âdetrendâ the data. This means using an es- timationmethodtoestimatethetrendandsubtractitoff. Thesimplestmethodisleastsquareslinear detrending.Giventhelinearmodel Y =Î² +Î² t+u (14.56) t 0 1 t thecoefficientsareestimatedbyleastsquares. Thedetrendedseriesistheresidualu . Moreintricate (cid:98)t methodscanbeusedbuttheyhaveasimilarflavor. Tounderstandthepropertiesofthedetrendingmethodwecanapplyanasymptoticapproximation. Atimetrendisnotastationaryprocesssoweshouldbethoughtfulbeforeapplyingstandardtheory.We willstudyasymptoticsfornon-stationaryprocessesinmoredetailinChapter16soourtreatmenthere willbebrief. Itturnsoutthatmostofourconventionalproceduresworkjustfinewithtimetrends(and quadraticsintime)asregressors. Theratesofconvergencechangebutthisdoesnotaffectanythingof practicalimportance.",
    "page": 509,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 490 Letusdemonstratethattheleastsquaresestimatorofthecoefficientsin(14.56)isconsistent.Wecan writetheestimatoras (cid:181) Î² (cid:98)0 âÎ² 0 (cid:182) = (cid:181) n (cid:80)n t=1 t (cid:182)â1(cid:181) (cid:80)n t=1 u t (cid:182) . Î² (cid:98)1 âÎ² 1 (cid:80)n t=1 t (cid:80)n t=1 t2 (cid:80)n t=1 tu t Weneedtostudythebehaviorofthesumsinthedesignmatrix. Forthisthefollowingresultisuseful, whichfollowsbytakingthelimitoftheRiemannsumfortheintegral (cid:82)1 xrdx=1/(1+r). 0 Theorem14.36 Foranyr >0,asnââ,n â1âr(cid:80)n tr ââ1/(1+r). t=1 Theorem14.36impliesthat 1 (cid:88) n 1 tâ n2 t=1 2 and 1 (cid:88) n t2â 1 . n3 t=1 3 Whatisinterestingabouttheseresultsisthatthesumsrequirenormalizationsotherthann â1! Tohandlethisinmultipleregressionitisconvenienttodefineascalingmatrixwhichnormalizeseach (cid:183) (cid:184) 1 0 elementintheregressionbyitsconvergencerate. DefinethematrixD = . Thefirstdiagonal n 0 n elementistheinterceptandsecondforthetimetrend.Then D (cid:181) Î² (cid:98)0 âÎ² 0 (cid:182) =D (cid:181) n (cid:80)n t=1 t (cid:182)â1 D D â1 (cid:181) (cid:80)n t=1 u t (cid:182) n Î² (cid:98)1 âÎ² 1 n (cid:80)n t=1 t (cid:80)n t=1 t2 n n (cid:80)n t=1 tu t = (cid:181) D â1 (cid:181) n (cid:80)n t=1 t (cid:182) D â1 (cid:182)â1(cid:181) (cid:80)n t=1 u t (cid:182) n (cid:80)n t (cid:80)n t2 n 1(cid:80)n tu t=1 t=1 n t=1 t (cid:181) n 1(cid:80)n t (cid:182)â1(cid:181) (cid:80)n u (cid:182) = 1(cid:80)n t 1 n (cid:80)n t=1 t2 1(cid:80) i n =1 tu t . n t=1 n2 t=1 n i=1 t Multiplyingbyn1/2weobtain (cid:181) n n 3 1 / / 2 2 (cid:161) (cid:161) Î² Î² (cid:98) (cid:98) 1 0 â â Î² Î² 1 0 (cid:162) (cid:162) (cid:182) = (cid:181) n 1 2 (cid:80) 1 n t=1 t n 1 n 1 3 2 (cid:80) (cid:80) n t n t = = 1 1 t t 2 (cid:182)â1(cid:181) n n 1 3 1 1 / / 2 2 (cid:80) (cid:80) n t n t = = 1 1 t u u t t (cid:182) . Thedenominatormatrixsatisfies (cid:181) 1 1 (cid:80)n t (cid:182) (cid:181) 1 1 (cid:182) n2 t=1 â 2 1 (cid:80)n t 1 (cid:80)n t2 1 1 n2 t=1 n3 t=1 2 3 whichisinvertible.SettingX =(t/n,1),thenumeratorvectorcanbewrittenasn â1/2(cid:80)n X u .Ithas nt t=1 nt t variance (cid:176) (cid:176) (cid:176) (cid:176) (cid:176) (cid:176) var (cid:183) n 1 1/2 t (cid:88) = n 1 X nt u t (cid:184)(cid:176) (cid:176) (cid:176) (cid:176) = (cid:176) (cid:176) (cid:176) (cid:176)n 1 t (cid:88) = n 1j (cid:88) = n 1 X nt X n (cid:48) j (cid:69)(cid:163) u t u j (cid:164) (cid:176) (cid:176) (cid:176) (cid:176) (cid:112) â â¤ 2 (cid:88) (cid:176) (cid:176) (cid:69)(cid:163) u t u j (cid:164)(cid:176) (cid:176) <â (cid:96)=ââ",
    "page": 510,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 491 byTheorem14.15ifu satisfiesthemixingandmomentconditionsforthecentrallimittheorem. This t meansthatthenumeratorvectorisO (1).(Itisalsoasymptoticallynormalbutwedeferthisdemonstra- p tionfornow.)Weconcludethat (cid:181) n n 1 3 / / 2 2 (cid:161) (cid:161) Î² Î² (cid:98) (cid:98) 0 1 â â Î² Î² 0 1 (cid:162) (cid:162) (cid:182) =O p (1). Thisshowsthatbothcoefficientsareconsistent,Î² (cid:98)0 convergesatthestandardn1/2rate,andÎ² (cid:98)1 converges atthefastern3/2 rate. Theconsistencyofthecoefficientestimatorsmeansthatthedetrendingmethod isconsistent. Analternativeistoincludeatimetrendintheestimatedregression. Ifwehaveanautoregression,a distributedlag,oranAL-DLmodel,weaddatimeindextoobtainamodeloftheform Y t =Î± 0 +Î± 1 Y tâ1 +Â·Â·Â·+Î± p Y tâp +Z t (cid:48) â1 Î² 1 +Â·Â·Â·+Z t (cid:48) âq Î² q +Î³t+e t . Estimation by least squares is equivalent to estimation after linear detrending by the FWL theorem. Inclusion of a linear (and possibly quadratic) time trend in a regression model is typically the easiest methodtoincorporatetimetrends. 14.43 Illustration WeillustratethemodelsdescribedintheprevioussectionusingaclassicalPhillipscurveforinfla- tionprediction.A.W.Phillips(1958)famouslyobservedthattheunemploymentrateandthewageinfla- tionratearenegativelycorrelatedovertime. Equationsrelatingtheinflationrate,orthechangeinthe inflation rate, to macroeconomic indicators suchas the unemployment rate are typically described as âPhillipscurvesâ.AsimplePhillipscurvetakestheform âÏ =Î±+Î²U +e (14.57) t t t where Ï is price inflation andU is the unemployment rate. This specification relates the change in t t inflationinagivenperiodtotheleveloftheunemploymentrateinthepreviousperiod. The least squares estimate of (14.57) using U.S. quarterly series from FRED-QD is reported in the firstcolumnofTable14.3.Bothheteroskedasticity-robustandNewey-Weststandarderrorsarereported. TheNewey-Weststandarderrorsaretheappropriatechoicesincetheestimatedequationisstaticâno modelingoftheserialcorrelation. Inthisexamplethemeasuredimpactoftheunemploymentrateon inflationappearsminimal. Theestimateisconsistentwithasmalleffectoftheunemploymentrateon theinflationratebutitisnotpreciselyestimated. Adistributedlag(DL)modeltakestheform âÏ t =Î±+Î² 1 U tâ1 +Î² 2 U tâ2 +Â·Â·Â·+Î² q U tâq +e t . (14.58) Theleastsquaresestimateof(14.58)isreportedinthesecondcolumnofTable14.3. Theestimatesare quitedifferentfromthestaticmodel. Weseelargenegativeimpactsinthefirstandthirdperiods,coun- teredbyalargepositiveimpactinthesecondperiod. Themodelsuggeststhattheunemploymentrate hasastrongimpactontheinflationratebutthelong-runimpactismitigated. Thelong-runmultiplier isreportedatthebottomofthecolumn. Thepointestimateofâ0.022isquitesmallandsimilartothe staticestimate. Itimpliesthatanincreaseintheunemploymentrateby5percentagepoints(atypical recession)decreasesthelong-runannualinflationratebyaboutahalfofapercentagepoint. AnAR-DLtakestheform âÏ t =Î± 0 +Î± 1 âÏ tâ1 +Â·Â·Â·+Î± p âÏ tâp +Î² 1 U tâ1 +Â·Â·Â·+Î² q U tâq +e t . (14.59)",
    "page": 511,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 492 Theleastsquaresestimateof(14.59)isreportedinthethirdcolumnofTable14.3. Thecoefficientesti- matesaresimilartothosefromthedistributedlagmodel. Thepointestimateofthelong-runmultiplier isalsonearlyidenticalbutwithasmallerstandarderror. Table14.3:PhillipsCurveRegressions StaticModel DLModel AR-DLModel U â0.023 t (0.025) [0.017] U tâ1 â0.59 â0.62 (0.20) (0.16) [0.16] [0.12] U tâ2 1.14 0.88 (0.29) (0.25) [0.28] [0.21] U tâ3 â0.68 â0.36 (0.22) (0.25) [0.25] [0.24] U tâ4 0.12 0.05 (0.11) (0.12) [0.11] [0.12] Ï tâ1 â0.43 (0.08) [0.08] Ï tâ2 â0.47 (0.10) [0.09] Ï tâ3 â0.14 (0.10) [0.11] Ï tâ4 â0.19 (0.08) [0.09] Multiplier â0.023 â0.022 â0.021 [0.017] [0.012] [0.008] 1. Standarderrorsrobusttoheteroskedasticityinparenthesis. 2. Newey-WeststandarderrorsinsquarebracketswithM=5. 14.44 GrangerCausality In the AR-DL model (14.59) the unemployment rate has no predictive impact on the inflation rate underthecoefficientrestrictionÎ² =Â·Â·Â·=Î² =0.ThisrestrictioniscalledGrangernon-causality.When 1 q thecoefficientsarenon-zerowesaythattheunemploymentrateâGrangercausesâtheinflationrate.This",
    "page": 512,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 493 definitionofcausalitywasdevelopedbyGranger(1969)andSims(1972). ThereasonwhywecallthisâGrangercausalityâratherthanâcausalityâisbecausethisisnotastruc- turaldefinition.Analternativelabelisâpredictivecausalityâ. Tobeprecise,assumethatwehavetwoseries(Y ,Z ). ConsidertheprojectionofY ontothelagged t t t historyofbothseries Y t =P tâ1 (Y t )+e t â â (cid:88) (cid:88) =Î± 0 + Î± j Y tâj + Î² j Z tâj +e t . j=1 j=1 We say that Z does not Granger-cause Y if Î² =0 for all j. If Î² (cid:54)=0 for some j then we say that Z t t j j t Granger-causesY . t ItisimportantthatthedefinitionincludestheprojectiononthepasthistoryofY .Grangercausality t meansthatZ helpstopredictY evenafterthepasthistoryofY hasbeenaccountedfor. t t t Thedefinitioncanalternativelybewrittenintermsofconditionalexpectationsratherthanprojec- tions.WecansaythatZ doesnotGranger-causeY if t t (cid:69)[Y t |Y tâ1 ,Y tâ2 ...;Z tâ1 ,Z tâ2 ,...]=(cid:69)[Y t |Y tâ1 ,Y tâ2 ,...]. GrangercausalitycanbetestedinAR-DLmodelsusingastandardWaldorFtest. Inthecontextof model(14.59)wereporttheFstatisticforÎ² =Â·Â·Â·=Î² =0.Thetestrejectsthehypothesis(andthusfinds 1 q evidenceofGrangercausality)ifthestatisticislargerthanthecriticalvalue(ifthep-valueissmall)and failstorejectthehypothesis(andthusfindsnoevidenceofcausality)ifthestatisticissmallerthanthe criticalvalue. Forexample,intheresultspresentedinTable14.3theFstatisticforthehypothesisÎ² =Â·Â·Â·=Î² =0 1 4 usingtheNewey-WestcovariancematrixisF=6.98withap-valueof0.000. Thisisstatisticallysignifi- cantatanyconventionallevelsowecanconcludethattheunemploymentratehasapredictivelycausal impactoninflation. Grangercausalityshouldnotbeinterpretedstructurallyoutsidethecontextofaneconomicmodel. ForexampleconsidertheregressionofGDPgrowthratesY onstockpricegrowthratesR . Weusethe t t quarterlyseriesfromFRED-QD,estimatinganAR-DLspecificationwithtwolags Y t = 0.22 Y tâ1 + 0.14 Y tâ2 + 0.03 R tâ1 + 0.01 R tâ2 . (0.09) (0.10) (0.01) (0.01) Thecoefficientsonthelaggedstockpricegrowthratesaresmallinmagnitudebutthefirstlagappears statistically significant. The F statistic for exclusion of (R tâ1 ,R tâ2 ) is F =9.3 with a p-value of 0.0002, whichishighlysignificant. WecanthereforerejectthehypothesisofnoGrangercausalityanddeduce thatstockpricesGranger-causeGDPgrowth. Weshouldbewaryofconcludingthatthisisstructurally causalâthatstockmarketmovementscauseoutputfluctuations. Amorereasonableexplanationfrom economic theory is that stock prices are forward-looking measures of expected future profits. When corporateprofitsareforecastedtorisethevalueofcorporatestockrises,biddingupstockprices. Thus stockpricesmoveinadvanceofactualeconomicactivitybutarenotnecessarilystructurallycausal.",
    "page": 513,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 494 CliveW.J.Granger Clive Granger (1934-2009) of England was one of the leading figures in time- series econometrics, and co-winner of the 2003 Nobel Memorial Prize in Eco- nomicSciences. Inadditiontoformalizingthedefinitionofcausalityknownas Granger causality, he invented the concept of cointegration, introduced spec- tral methods into econometrics, and formalized methods for the combination offorecasts. 14.45 TestingforSerialCorrelationinRegressionModels ConsidertheproblemoftestingforomittedserialcorrelationinanAR-DLmodelsuchas Y t =Î± 0 +Î± 1 Y tâ1 +Â·Â·Â·+Î± p Y tâp +Î² 1 Z tâ1 +Â·Â·Â·+Î² q Z tâq +u t . (14.60) Thenullhypothesisisthatu isseriallyuncorrelatedandthealternativehypothesisisthatitisserially t correlated.Wecanmodelthelatterasamean-zeroautoregressiveprocess u t =Î¸ 1 u tâ1 +Â·Â·Â·+Î¸ r u târ +e t . (14.61) Thehypothesisis (cid:72) :Î¸ =Â·Â·Â·=Î¸ =0 0 1 r (cid:72) :Î¸ (cid:54)=0forsome j â¥1. 1 j Therearetwowaystoimplementatestof(cid:72) against(cid:72) . Thefirstistoestimateequations(14.60)- 0 1 (14.61) sequentially by least squares and construct a test for (cid:72) on the second equation. This test is 0 complicatedbythetwo-stepestimation.Thereforethisapproachisnotrecommended. Thesecondapproachistocombineequations(14.60)-(14.61)intoasinglemodelandexecutethetest asarestrictionwithinthismodel. Onewaytomakethiscombinationisbyusinglagoperatornotation. Write(14.60)-(14.61)as Î±(L)Y t =Î± 0 +Î²(L)Z tâ1 +u t Î¸(L)u =e t t ApplyingtheoperatorÎ¸(L)tothefirstequationweobtain Î¸(L)Î±(L)Y t =Î¸(L)Î± 0 +Î¸(L)Î²(L)Z tâ1 +Î¸(L)u t or Î±â (L)Y t =Î±â 0 +Î²â (L)Z tâ1 +e t whereÎ±â (L)isap+r orderpolynomialandÎ²â (L)isaq+r orderpolynomial. Therestriction(cid:72) isthat 0 theseare p and q orderpolynomials. Thuswecanimplementatestof(cid:72) against(cid:72) byestimatingan 0 1 AR-DLmodelwithp+r andq+r lags,andtestingtheexclusionofthefinalr lagsofY andZ .Thistest t t hasaconventionalasymptoticdistributionsoissimpletoimplement. Thebasicmessageisthattestingforomittedserialcorrelationcanbeimplementinregressionmod- elsbyestimatingandcontrastingdifferentdynamicspecifications.",
    "page": 514,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 495 14.46 BootstrapforTimeSeries Recallthatthebootstrapapproximatesthesamplingdistributionofestimatorsandteststatisticsby theempiricaldistributionoftheobservations. Thetraditionalnon-parametricbootstrapisappropriate forindependentobservations.Fordependentobservationsalternativemethodsshouldbeused. Bootstrappingfortimeseriesisconsiderablymorecomplicatedthanthecrosssectioncase. Many methodshavebeenproposed.Oneofthechallengesisthattheoreticaljustificationsaremoredifficultto establishthanintheindependentobservationcase. Inthissectionwedescribethemostpopularmethodstoimplementbootstrapresamplingfortime seriesdata. RecursiveBootstrap 1. EstimateacompletemodelsuchasanAR(p)producingcoefficientestimatesÎ±andresidualse . (cid:98) (cid:98)t 2. Fixtheinitialcondition(Yâp+1 ,Yâp+2 ,...,Y 0 ). â 3. Simulatei.i.d.drawse fromtheempiricaldistributionoftheresiduals{e ,...,e }. t (cid:98)1 (cid:98)n â 4. CreatethebootstrapseriesY bytherecursiveformula t Y t â=Î± (cid:98)0 +Î± (cid:98)1 Y t â â1 +Î± (cid:98)2 Y t â â2 +Â·Â·Â·+Î± (cid:98)p Y t â âp +e t â . â ThisconstructioncreatesbootstrapsamplesY withthestochasticpropertiesoftheestimatedAR(p) t modelincludingtheauxiliaryassumptionthattheerrorsarei.i.d. Thismethodcanworkwellifthetrue â processisanAR(p).Oneflawisthatitimposeshomoskedasticityontheerrorse whichmaybedifferent t thanthepropertiesoftheactuale .AnotherlimitationisthatitisinappropriateforAR-DLmodelsunless t theconditioningvariablesarestrictlyexogenous. There are alternative versions of this basicmethod. First, instead of fixing the initial conditions at thesamplevaluesarandomblockcanbedrawnfromthesample. Thedifferenceisthatthisproduces anunconditionaldistributionratherthanaconditionalone.Second,insteadofdrawingtheerrorsfrom theresidualsaparametric(typicallynormal)distributioncanbeused.Thiscanimproveprecisionwhen samplesizesaresmallbutotherwiseisnotrecommended. PairwiseBootstrap 1. Writethesampleas{Y t ,X t }whereX t =(Y tâ1 ,...,Y tâp ) (cid:48) containsthelaggedvaluesusedinestima- tion. â â 2. Apply the traditional nonparametric bootstrap which samples pairs (Y ,X ) i.i.d. from {Y ,X } t t t t withreplacementtocreatethebootstrapsample. â â 3. Createthebootstrapestimatesonthisbootstrapsample,e.g.regressY onX . t t This construction is essentially the traditional nonparametric bootstrap but applied to the paired sample {Y ,X }. It does not mimic the time series correlations across observations. However, it does t t producebootstrapstatisticswiththecorrectfirst-orderasymptoticdistributionunderMDSerrors. This methodmaybeusefulwhenweareinterestedinthedistributionofnonlinearfunctionsofthecoefficient estimatesandthereforedesireanimprovementontheDeltaMethodapproximation. FixedDesignResidualBootstrap",
    "page": 515,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 496 1. Writethesampleas{Y t ,X t ,e (cid:98)t }where X t =(Y tâ1 ,...,Y tâp ) (cid:48) containsthelaggedvaluesusedinesti- mationande aretheresiduals. (cid:98)t 2. FixtheregressorsX attheirsamplevalues. t â 3. Simulatei.i.d.drawse fromtheempiricaldistributionoftheresiduals{e ,...,e }. t (cid:98)1 (cid:98)n 4. SetY â=X (cid:48)Î² (cid:98) +e â . t t t Thisconstructionissimilartothepairwisebootstrapbutimposesani.i.d. error. Itisthereforeonly validwhentheerrorsarei.i.d.(andthusexcludesheteroskedasticity). FixedDesignWildBootstrap 1. Writethesampleas{Y t ,X t ,e (cid:98)t }where X t =(Y tâ1 ,...,Y tâp ) (cid:48) containsthelaggedvaluesusedinesti- mationande aretheresiduals. (cid:98)t 2. FixtheregressorsX andresidualse attheirsamplevalues. t (cid:98)t 3. Simulatei.i.d. auxiliaryrandomvariablesÎ¾â withmeanzeroandvarianceone. SeeSection10.29 t foradiscussionofchoices. 4. Sete t â=Î¾â t e (cid:98)t andY t â=X t (cid:48)Î² (cid:98) +e t â . Thisconstructionissimilartothepairwiseandfixeddesignbootstrapcombinedwiththewildboot- strap.Thisimposestheconditionalmeanassumptionontheerrorbutallowsheteroskedasticity. BlockBootstrap 1. Writethesampleas{Y t ,X t }whereX t =(Y tâ1 ,...,Y tâp ) (cid:48) containsthelaggedvaluesusedinestima- tion. 2. Dividethesampleofpairedobservations{Y ,X }inton/mblocksoflengthm. t t 3. Resamplecompleteblocks.Foreachsimulatedsampledrawn/mblocks. â â 4. Pastetheblockstogethertocreatethebootstraptimeseries{Y ,X }. t t This construction allows for arbitrary stationary serial correlation, heteroskedasticity, and model- misspecification. Onechallengeisthattheblockbootstrapissensitivetotheblocklengthandtheway thatthedataarepartitionedintoblocks. Themethodmayalsoworklesswellinsmallsamples. Notice that the block bootstrap with m =1 is equal to the pairwise bootstrap and the latter is the traditional nonparametric bootstrap. Thus the block bootstrap is a natural generalization of the nonparametric bootstrap. 14.47 TechnicalProofs* Proof of Theorem 14.2 Define Y(cid:101)t =(Y t ,Y tâ1 ,Y tâ2 ,...)â(cid:82)mÃâ as the history of Y t up to time t. Write X t =Ï(Y(cid:101)t ).LetB bethepre-imageof {X t â¤x}(thevectorsY(cid:101) â(cid:82)mÃâ suchthatÏ(Y(cid:101))â¤x).Then (cid:80)[X t â¤x]=(cid:80)(cid:163)Ï(Y(cid:101)t )â¤x (cid:164)=(cid:80)(cid:163) Y(cid:101)t âB (cid:164) .",
    "page": 516,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 497 Since Y t is strictly stationary, (cid:80)(cid:163) Y(cid:101)t âB (cid:164) is independent8 of t. This means that the distribution of X t is independent of t. This argument can be extended to show that the distribution of (X t ,...,X t+(cid:96)) is independentoft.ThismeansthatX isstrictlystationaryasclaimed. â  t Proof of Theorem 14.3 By the Cauchy criterion for convergence (see Theorem A.2 of Introduction to Econometrics),S N =(cid:80)N j=0 a j Y tâj convergesalmostsurelyifforall(cid:178)>0, infsup (cid:175) (cid:175)S N+j âS N (cid:175) (cid:175) â¤(cid:178). N j>N Let A(cid:178)bethisevent.Itscomplementis Ac (cid:178) = (cid:92) â (cid:40) sup (cid:175) (cid:175) (cid:175) (cid:175) N (cid:88) +j a i Y tâi (cid:175) (cid:175) (cid:175) (cid:175) >(cid:178) (cid:41) . N=1 j>N(cid:175)i=N+1 (cid:175) Thishasprobability (cid:80)(cid:163) Ac (cid:178) (cid:164)â¤ N li â m â (cid:80) (cid:34) s j> u N p (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)i= N (cid:88) N + + j 1 a i Y tâi (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) >(cid:178) (cid:35) â¤ N li â m â 1 (cid:178) (cid:69) (cid:34) s j> u N p (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)i= N (cid:88) N + + j 1 a i Y tâi (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:35) â¤ 1 (cid:178) N li â m â i= (cid:88) â N+1 |a i |(cid:69)|Y tâi |=0. ThesecondequalityisMarkovâsinequality(B.36)andthefollowingisthetriangleinequality(B.1). The limitiszerosince (cid:80)â i=0 |a i |<âand(cid:69)|Y t |<â. Henceforall(cid:178)>0,(cid:80)(cid:163) Ac (cid:178) (cid:164)=0and(cid:80)[A(cid:178)]=1. Thismeans thatS convergeswithprobabilityone,asclaimed. N SinceY isstrictlystationarythenX isaswellbyTheorem14.2. â  t t ProofofTheorem14.4SeeTheorem14.14. â  ProofofTheorem14.5StrictstationarityfollowsfromTheorem14.2.LetY(cid:101)t andX(cid:101)t bethehistoriesofY t andX t .WriteX t =Ï(cid:161) Y(cid:101)t (cid:162) .LetAbeaninvarianteventforX t .Wewanttoshow(cid:80)[A]=0or1.TheeventA isacollectionofX(cid:101)t histories,andoccursifandandonlyifanassociatedcollectionofY(cid:101)t historiesoccur. Thatis,forsomesetsG andH, A=(cid:169) X(cid:101)t âG (cid:170)=(cid:169)Ï(cid:161) Y(cid:101)t (cid:162)âG (cid:170)=(cid:169) Y(cid:101)t âH (cid:170) . Theassumptionthat Aisinvariantmeansitisunaffectedbythetimeshift,thuscanbewrittenas A=(cid:169) X(cid:101)t+(cid:96) âG (cid:170)=(cid:169) Y(cid:101)t+(cid:96) âH (cid:170) . Thismeanstheevent (cid:169) Y(cid:101)t+(cid:96) âH (cid:170) isinvariant. SinceY t isergodictheeventhasprobability0or1. Hence (cid:80)[A]=0or1,asdesired. â  ProofofTheorem14.7SupposeY isdiscretewithsupporton(Ï ,...,Ï )andwithoutlossofgenerality t 1 N 8Anastutereadermaynoticethattheindependenceof(cid:80)(cid:163) Y(cid:101)t âB (cid:164) fromtdoesnotfollowdirectlyfromthedefinitionofstrict stationarity. Indeed,afullderivationrequiresameasure-theoretictreatment. SeeSection1.2.BofPetersen(1983)orSection 3.5ofStout(1974).",
    "page": 517,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 498 assume(cid:69)[Y ]=0.ThenbyTheorem14.8 t 1 (cid:88) n 1 (cid:88) n n l â im ân (cid:96)=1 cov(Y t ,Y t+(cid:96))= n l â im ân (cid:96)=1 (cid:69)[Y t Y t+(cid:96)] = n l â im ân 1 (cid:96) (cid:88) = n 1j (cid:88) N =1k (cid:88) N =1 Ï j Ï k (cid:80)(cid:163) Y t =Ï j ,Y t+(cid:96) =Ï k (cid:164) = j (cid:88) N =1k (cid:88) N =1 Ï j Ï k n l â im ân 1 (cid:96) (cid:88) = n 1 (cid:80)(cid:163) Y t =Ï j ,Y t+(cid:96) =Ï k (cid:164) N N = (cid:88) (cid:88) Ï j Ï k (cid:80)(cid:163) y t =Ï j (cid:164)(cid:80)[Y t+(cid:96) =Ï k ] j=1k=1 =(cid:69)[Y t ](cid:69)[Y t+(cid:96)] =0. whichis(14.4). Thiscanbeextendedtothecaseofcontinuousdistributionsusingthemonotonecon- vergencetheorem.SeeCorollary14.8ofDavidson(2020). â  ProofofTheorem14.9Weshow(14.6).(14.7)followsbyMarkovâsinequality(B.36). Withoutlossofgeneralitywefocusonthescalarcaseandassume(cid:69)[Y ]=0. Fix(cid:178)>0. PickB large t enoughsuchthat (cid:178) (cid:69)|Y 1 {|Y |>B}|â¤ (14.62) t t 4 whichisfeasiblesince(cid:69)|Y |<â.Define t W =Y 1 {|Y |â¤B}â(cid:69)[Y 1 {|Y |â¤B}] t t t t t Z =Y 1 {|Y |>B}â(cid:69)[Y 1 {|Y |>B}]. t t t t t NoticethatW isaboundedtransformationoftheergodicseriesY .Thusby(14.4)and(14.9)thereisan t t nsufficientlylargesothat var[W t ] + 2 (cid:88) n (cid:179) 1â m(cid:180) cov (cid:161) W ,W (cid:162)â¤ (cid:178)2 (14.63) t j n n m=1 n 4 Bythetriangleinequality(B.1) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:69)(cid:175)Y(cid:175)=(cid:69)(cid:175)W +Z(cid:175)â¤(cid:69)(cid:175)W(cid:175)+(cid:69)(cid:175)Z(cid:175). (14.64) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) Byanotherapplicationofthetriangleinequalityand(14.62) (cid:175) (cid:175) (cid:178) (cid:69)(cid:175)Z(cid:175)â¤(cid:69)|Z |â¤2(cid:69)|Y 1 (|Y |>B)|â¤ . (14.65) (cid:175) (cid:175) t t t 2 ByJensenâsinequality(B.27),directcalculation,and(14.63) (cid:179) (cid:175) (cid:175)(cid:180)2 (cid:183)(cid:175) (cid:175)2 (cid:184) (cid:69)(cid:175)W(cid:175) â¤(cid:69) (cid:175)W(cid:175) (cid:175) (cid:175) (cid:175) (cid:175) = 1 (cid:88) n (cid:88) n (cid:69)(cid:163) W W (cid:164) n2 t=1j=1 t j = var[W t ] + 2 (cid:88) n (cid:179) 1â m(cid:180) cov (cid:161) W ,W (cid:162) t j n n m=1 n (cid:178)2 â¤ . 4",
    "page": 518,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 499 Thus (cid:175) (cid:175) (cid:178) (cid:69)(cid:175)W(cid:175)â¤ . (14.66) (cid:175) (cid:175) 2 (cid:175) (cid:175) Together, (14.64), (14.65) and (14.66) show that (cid:69)(cid:175)Y(cid:175)â¤(cid:178). Since Îµ is arbitrary, this establishes (14.6) as (cid:175) (cid:175) claimed. â  ProofofTheorem14.11(sketch)BytheCramÃ©r-Wolddevice(Theorem8.4fromIntroductiontoEcono- metrics)itissufficienttoestablishtheresultforscalaru . LetÏ2=(cid:69)(cid:163) u2(cid:164) . ByaTaylorseriesexpansion, t t forxsmalllog(1+x)(cid:39)xâx2/2.Takingexponentialsandrearrangingweobtaintheapproximation (cid:181) x2(cid:182) exp(x)(cid:39)(1+x)exp . (14.67) 2 FixÎ».Define j (cid:181) Î» (cid:182) (cid:89) T = 1+(cid:112) u j t i=1 n V = 1 (cid:88) n u2. n n t=1 t Sinceu isstrictlystationaryandergodic,V âpâÏ2bytheErgodicTheorem(Theorem14.9).Sinceu is t n t aMDS (cid:69)[T ]=1. (14.68) n (cid:179) (cid:180) Toseethis,defineF t =Ï(...,u tâ1 ,u t ).NoteT j =T jâ1 1+(cid:112) Î» u j .Byiteratedexpectations n (cid:69)[T n ]=(cid:69)[(cid:69)[T n |F nâ1 ]] (cid:183) (cid:183) Î» (cid:175) (cid:184)(cid:184) =(cid:69) T nâ1 (cid:69) 1+(cid:112) u n (cid:175) (cid:175) F nâ1 n (cid:175) =(cid:69)[T nâ1 ]=Â·Â·Â·=(cid:69)[T 1 ] =1. Thisis(14.68). ThemomentgeneratingfunctionofS is n (cid:34) (cid:35) (cid:183) (cid:181) Î» n (cid:182)(cid:184) n (cid:181) Î» (cid:182) (cid:88) (cid:89) (cid:69) exp (cid:112) u =(cid:69) exp (cid:112) u t t n t=1 i=1 n (cid:34) (cid:35) n (cid:183) Î» (cid:184) (cid:181)Î»2 (cid:182) (cid:39)(cid:69) (cid:89) 1+(cid:112) u exp u2 (14.69) i=1 n t 2n t (cid:183) (cid:181)Î»2V (cid:182)(cid:184) =(cid:69) T exp n n 2 (cid:183) (cid:181)Î»2Ï2(cid:182)(cid:184) (cid:39)(cid:69) T exp (14.70) n 2 (cid:181)Î»2Ï2(cid:182) =exp . 2 Theapproximationin(14.69)is(14.67).Theapproximation(14.70)isV ââÏ2.(Arigorousjustification n p whichallowsthissubstitutionintheexpectationistechnical.) Thefinalequalityis(14.68). Thisshows thatthemomentgeneratingfunctionofS isapproximatelythatofN (cid:161) 0,Ï2(cid:162) ,asclaimed. n",
    "page": 519,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 500 Theassumptionthatu isaMDSiscriticalfor(14.68). T isanonlinearfunctionoftheerrorsu so t n t awhitenoiseassumptioncannotbeusedinstead. TheMDSassumptionisexactlytheminimalcondi- tionneededtoobtain(14.68). ThisiswhytheMDSassumptioncannotbeeasilyreplacedbyamilder assumptionsuchaswhitenoise. â  Proof of Theorem 14.13.1 Without loss of generality suppose (cid:69)[X t ] = 0 and (cid:69)[Z t ] = 0. Set Î· tâm = sgn (cid:161)(cid:69)(cid:163) Z t |F â tâ â m(cid:164)(cid:162) . By iterated expectations, |X t |â¤C 1 , (cid:175) (cid:175) (cid:69)(cid:163) Z t |F â tâ â m(cid:164)(cid:175) (cid:175) =Î· tâm (cid:69)(cid:163) Z t |F â tâ â m(cid:164) , and again usingiteratedexpectations |cov(X tâm ,Z t )|=(cid:175) (cid:175) (cid:69)(cid:163)(cid:69)(cid:163) X tâm Z t |F â tâ â m(cid:164)(cid:164)(cid:175) (cid:175) =(cid:175) (cid:175) (cid:69)(cid:161) X tâm (cid:69)(cid:163) Z t |F â tâ â m(cid:164)(cid:162)(cid:175) (cid:175) â¤C 1 (cid:69)(cid:175) (cid:175) (cid:69)(cid:163) Z t |F â tâ â m(cid:164)(cid:175) (cid:175) =C 1 (cid:69)(cid:163)Î· tâm (cid:69)(cid:163) Z t |F â tâ â m(cid:164)(cid:164) =C 1 (cid:69)(cid:163)(cid:69)(cid:163)Î· tâm Z t |F â tâ â m(cid:164)(cid:164) =C 1 (cid:69)(cid:163)Î· tâm Z t (cid:164) =C cov (cid:161)Î· ,Z (cid:162) . (14.71) 1 tâm t Setting Î¾ t =sgn (cid:161)(cid:69)(cid:163) X tâm |F t â(cid:164)(cid:162) , by a similar argument (14.71) is bounded byC 1 C 2 cov (cid:161)Î· tâm ,Î¾ t (cid:162) . Set A 1 =1(cid:169)Î· tâm =1 (cid:170) , A 2 =1(cid:169)Î· tâm =â1 (cid:170) ,B 1 =1 {Î¾ t =1},B 2 =1 {Î¾ t =â1}.Wecalculate (cid:175) (cid:175)cov (cid:161)Î· tâm ,Î¾ t (cid:162)(cid:175) (cid:175) =|(cid:80)[A 1 â©B 1 ]+(cid:80)[A 2 â©B 2 ]â(cid:80)[A 2 â©B 1 ]â(cid:80)[A 1 â©B 2 ] â(cid:80)[A ](cid:80)[B ]â(cid:80)[A ](cid:80)[B ]+(cid:80)[A ](cid:80)[B ]+(cid:80)[A ](cid:80)[B ]| 1 1 2 2 2 1 1 2 â¤4Î±(m). Together,|cov(X tâm ,z t )|â¤4C 1 C 2 Î±(m)asclaimed. â  ProofofTheorem14.13.2Assume(cid:69)[X ]=0and(cid:69)[Z ]=0.Wefirstshowthatif|X |â¤C then t t t |cov(X tâ(cid:96),Z t )|â¤6C (cid:161)(cid:69)|Z t |r(cid:162)1/rÎ±((cid:96))1â1/r. (14.72) Indeed,ifÎ±((cid:96))=0theresultisimmediatesoassumeÎ±((cid:96))>0.SetD=Î±((cid:96)) â1/r((cid:69)|Z |r)1/r,V =Z 1 {|Z |â¤D} t t t t andW =Z 1 {|Z |>D}.Usingthetriangleinequality(B.1)andthenpart1,since|X |â¤C and|V |â¤D, t t t t t |cov(X tâ(cid:96),Z t )|â¤|cov(X tâ(cid:96),V t )|+|cov(X tâ(cid:96),W t )|â¤4CDÎ±((cid:96))+2C(cid:69)|w t |. Also, (cid:69)|W t |=(cid:69)|Z t 1 {|Z t |>D}|=(cid:69) (cid:175) (cid:175) (cid:175) (cid:175)|Z |Z | t r | â r 1 1 {|Z t |>D} (cid:175) (cid:175) (cid:175) (cid:175) â¤ (cid:69) D |Z râ t | 1 r =Î±((cid:96))(râ1)/r(cid:161)(cid:69)|Z t |r(cid:162)1/r t usingthedefinitionofD.Togetherwehave |cov(X tâ(cid:96),Z t )|â¤6C (cid:161)(cid:69)|X t |r(cid:162)1/rÎ±((cid:96))1â1/r. whichis(14.72)asclaimed. NowsetC =Î±((cid:96)) â1/r((cid:69)|X |r)1/r,V =X 1 {|X |â¤C}andW =X 1 {|X |>C}",
    "page": 520,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Also, (cid:69)|W t |=(cid:69)|Z t 1 {|Z t |>D}|=(cid:69) (cid:175) (cid:175) (cid:175) (cid:175)|Z |Z | t r | â r 1 1 {|Z t |>D} (cid:175) (cid:175) (cid:175) (cid:175) â¤ (cid:69) D |Z râ t | 1 r =Î±((cid:96))(râ1)/r(cid:161)(cid:69)|Z t |r(cid:162)1/r t usingthedefinitionofD.Togetherwehave |cov(X tâ(cid:96),Z t )|â¤6C (cid:161)(cid:69)|X t |r(cid:162)1/rÎ±((cid:96))1â1/r. whichis(14.72)asclaimed. NowsetC =Î±((cid:96)) â1/r((cid:69)|X |r)1/r,V =X 1 {|X |â¤C}andW =X 1 {|X |>C}. Usingthetrianglein- t t t t t t t equalityand(14.72) |cov(X tâ(cid:96),Z t )|â¤|cov(V tâ(cid:96),Z t )|+|cov(W tâ(cid:96),Z t )|. Since|V |â¤C,using(14.72)andthedefinitionofC t |cov(V tâ(cid:96),Z t )|â¤6C (cid:161)(cid:69)|Z t |q(cid:162)1/qÎ±((cid:96))1â1/q=6 (cid:161)(cid:69)|X t |r(cid:162)1/r(cid:161)(cid:69)|Z t |q(cid:162)1/qÎ±((cid:96))1â1/qâ1/r.",
    "page": 520,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 501 UsingHÃ¶lderâsinequality(B.31)andthedefinitionofC |cov(W tâ(cid:96),Z t )|â¤2 (cid:161)(cid:69)|W t |q/(qâ1)(cid:162)(qâ1)/q(cid:161)(cid:69)|Z t |q(cid:162)1/q =2 (cid:161)(cid:69)(cid:163)|X |q/(qâ1)1 {|X |>C} (cid:164)(cid:162)(qâ1)/q(cid:161)(cid:69)|Z |q(cid:162)1/q t t t =2 (cid:181) (cid:69) (cid:183) |X t |r 1 {|X |>C} (cid:184)(cid:182)(qâ1)/q (cid:161)(cid:69)|Z |q(cid:162)1/q |X |râq/(qâ1) t t t â¤ 2 (cid:161)(cid:69)|X |r(cid:162)(qâ1)/q(cid:161)(cid:69)|Z |q(cid:162)1/q Cr(qâ1)/qâ1 t t =2 (cid:161)(cid:69)|X |r(cid:162)1/r(cid:161)(cid:69)|Z |q(cid:162)1/qÎ±((cid:96))1â1/qâ1/r. t t Togetherwehave |cov(X tâ(cid:96),Z t )|â¤8 (cid:161)(cid:69)|X t |r(cid:162)1/r(cid:161)(cid:69)|Z t |q(cid:162)1/qÎ±((cid:96))1â1/râ1/q asclaimed. â  ProofofTheorem14.13.3SetÎ· tâ(cid:96) =sgn (cid:161)(cid:69)(cid:163) Z t (cid:175) (cid:175) F â tâ â (cid:96)(cid:164)(cid:162) whichsatisfies (cid:175) (cid:175) Î· tâ(cid:96) (cid:175) (cid:175) â¤1. SinceÎ· tâ(cid:96) isF â tâ â (cid:96) - measurable, iterated expectations, using (14.72) withC =1, the conditional Jensenâs inequality (B.28), anditeratedexpectations, (cid:175) (cid:104) (cid:175) (cid:105)(cid:175) (cid:104) (cid:104) (cid:175) (cid:105)(cid:105) (cid:69)(cid:175) (cid:175) (cid:69) Z t (cid:175) (cid:175) F â tâ â (cid:96) (cid:175) (cid:175) =(cid:69) Î· tâ(cid:96) (cid:69) Z t (cid:175) (cid:175) F â tâ â (cid:96) (cid:104) (cid:104) (cid:175) (cid:105)(cid:105) =(cid:69) (cid:69) Î· tâ(cid:96)Z t (cid:175) (cid:175) F â tâ â (cid:96) =(cid:69)(cid:163)Î· tâ(cid:96)Z t (cid:164) â¤6 (cid:179) (cid:69) (cid:175) (cid:175) (cid:175) (cid:69) (cid:104) Z t (cid:175) (cid:175) (cid:175) F â tâ â (cid:96) (cid:105)(cid:175) (cid:175) (cid:175) r(cid:180)1/r Î±((cid:96))1â1/r â¤6 (cid:179) (cid:69) (cid:179) (cid:69) (cid:104) |Z t |r (cid:175) (cid:175) (cid:175) F â tâ â (cid:96) (cid:105)(cid:180)(cid:180)1/r Î±((cid:96))1â1/r =6 (cid:161)(cid:69)|Z |r|(cid:162)1/rÎ±((cid:96))1â1/r t asclaimed. â  ProofofTheorem14.15BytheCramÃ©r-Wolddevice(Theorem8.4ofIntroductiontoEconometrics)itis sufficienttoprovetheresultforthescalarcase. OurproofmethodisbasedonaMDSapproximation. Thetrickistoestablishtherelationship u t =e t +Z t âZ t+1 (14.73) wheree isastrictlystationaryandergodicMDSwith(cid:69)(cid:163) e2(cid:164)=â¦and(cid:69)|Z |<â.DefiningSe = (cid:112)1 (cid:80)n e , t t t n n t=1 t wehave S n = (cid:112) 1 (cid:88) n (e t +Z t âZ t+1 )=S n e +(cid:112) Z 1 â Z (cid:112)n+1 . (14.74) n t=1 n n ThefirstcomponentontherightsideisasymptoticallyN(0,â¦)bytheMDSCLT(Theorem14.11). The secondandthirdtermsareo (1)byMarkovâsinequality(B.36). p Thedesiredrelationship(14.73)holdsasfollows.SetF t =Ï(...,u tâ1 ,u t ), â (cid:88) e t = ((cid:69)[u t+(cid:96) |F t ]â(cid:69)[u t+(cid:96) |F tâ1 ]) (14.75) (cid:96)=0",
    "page": 521,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 502 and â (cid:88) Z t = (cid:69)[u t+(cid:96) |F tâ1 ]. (cid:96)=0 Youcanverifythatthesedefinitionssatisfy(14.73)given(cid:69)[u |F ]=u .ThevariableZ hasafinitemean t t t t sincebythetriangleinequality(B.1),Theorem14.13.3,andtheassumptions (cid:175) (cid:175) (cid:69)|Z t |=(cid:69) (cid:175) (cid:175) (cid:175) (cid:88) â (cid:69)[u t+(cid:96) |F tâ1 ] (cid:175) (cid:175) (cid:175) â¤6 (cid:161)(cid:69)|u t |r(cid:162)1/r (cid:88) â Î±((cid:96))1â1/r <â, (cid:175)(cid:96)=0 (cid:175) (cid:96)=0 thefinalinequalitysince (cid:80)â Î±((cid:96))1â2/r <âimplies (cid:80)â Î±((cid:96))1â1/r <â. (cid:96)=0 (cid:96)=0 The series e in (14.75) has a finite mean by the same calculation as for Z . It is a MDS since by t t iteratedexpectations (cid:34) â (cid:35) (cid:88) (cid:69)[e t |F tâ1 ]=(cid:69) ((cid:69)[u t+(cid:96) |F t ]â(cid:69)[u t+(cid:96) |F tâ1 ])|F tâ1 (cid:96)=0 â (cid:88) = ((cid:69)[(cid:69)[u t+(cid:96) |F t ]|F tâ1 ]â(cid:69)[(cid:69)[u t+(cid:96) |F tâ1 ]|F tâ1 ]) (cid:96)=0 â (cid:88) = ((cid:69)[u t+(cid:96) |F tâ1 ]â(cid:69)[u t+(cid:96) |F tâ1 ]) (cid:96)=0 =0. ItisstrictlystationaryandergodicbyTheorem14.2sinceitisafunctionofthehistory(...,u tâ1 ,u t ). Theproofiscompletedbyshowingthate hasafinitevariancewhichequalsâ¦. Thetrickieststepis t toshowthatvar[e t ]<â.Since (cid:112) (cid:112) (cid:69)|S |â¤ var[S ]â â¦ n n (cid:112) (asshownin(14.17))itfollowsthat(cid:69)|S |â¤2 â¦fornsufficientlylarge. Using(14.74)and(cid:69)|Z |<â,for n t nsufficientlylarge, (cid:69)(cid:175) (cid:175)S n e(cid:175) (cid:175) â¤(cid:69)|S n |+ (cid:69) (cid:112) |Z 1 | + (cid:69)| (cid:112) Z n+1 | â¤3 (cid:112) â¦. (14.76) n n Now define e Bt = e t 1 {|e t |â¤B}â(cid:69)[e t 1 {|e t |â¤B}|F tâ1 ] which is a bounded MDS. By Theorem 14.11, (cid:112)1 (cid:80)n e âdâN (cid:161) 0,Ï2(cid:162) whereÏ2 =(cid:69)(cid:163) e2 (cid:164) .Sincethesequenceisuniformlyintegrablethisimplies n t=1 Bt B B Bt (cid:69) (cid:175) (cid:175) (cid:175) (cid:175) (cid:112) 1 n t (cid:88) = n 1 e Bt (cid:175) (cid:175) (cid:175) (cid:175) ââ(cid:69)(cid:175) (cid:175)N (cid:161) 0,Ï2 B (cid:162)(cid:175) (cid:175) = (cid:114) Ï 2 Ï B (14.77) using(cid:69)|N(0,1)|=2/Ï.Wewanttoshowthatvar[e ]<â.Supposenot.ThenÏ ââasBââ,sothere t B willbesomeB sufficientlylargesuchthattheright-sideof(14.77)exceedstheright-sideof(14.76). This isacontradiction.Wededucethatvar[e ]<â. t Examining(14.74),weseethatsincevar[S n ]ââ¦<âandvar (cid:163) S n e(cid:164)=var[e t ]<âthenvar[Z 1 âZ n+1 ]/n< â.SinceZ t isstationary,wededucethatvar[Z 1 âZ n+1 ]<â.Equation(14.74)impliesvar[e t ]=var (cid:163) S n e(cid:164)= var[S ]+o(1)ââ¦.Wededucethatvar[e ]=â¦asclaimed. â  n t ProofofTheorem14.17 (Sketch) Consider the projection of Y t onto (...,e tâ1 ,e t )",
    "page": 522,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". This isacontradiction.Wededucethatvar[e ]<â. t Examining(14.74),weseethatsincevar[S n ]ââ¦<âandvar (cid:163) S n e(cid:164)=var[e t ]<âthenvar[Z 1 âZ n+1 ]/n< â.SinceZ t isstationary,wededucethatvar[Z 1 âZ n+1 ]<â.Equation(14.74)impliesvar[e t ]=var (cid:163) S n e(cid:164)= var[S ]+o(1)ââ¦.Wededucethatvar[e ]=â¦asclaimed. â  n t ProofofTheorem14.17 (Sketch) Consider the projection of Y t onto (...,e tâ1 ,e t ). Since the projection errorse areuncorrelated,thecoefficientsofthisprojectionarethebivariateprojectioncoefficientsb = t j (cid:104) (cid:105) (cid:69)(cid:163) Y t e tâj (cid:164) /(cid:69) e t 2 âj .Theleadingcoefficientis b = (cid:69)[Y t e t ] = (cid:80)â j=1 Î± j (cid:69)(cid:163) Y tâj e t (cid:164)+(cid:69)(cid:163) e t 2(cid:164) =1 0 Ï2 Ï2",
    "page": 522,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 503 usingTheorem14.16.ByBesselâsInequality(BrockwellandDavis,1991,Corollary2.4.1), â â (cid:88) b2=Ïâ4(cid:88) ((cid:69)[Y e ])2â¤Ïâ4(cid:161)(cid:69)(cid:163) Y2(cid:164)(cid:162)2<â j t t t j=1 j=1 since(cid:69)(cid:163) Y2(cid:164)<âbytheassumptionofcovariancestationarity. t TheerrorfromtheprojectionofY t onto(...,e tâ1 ,e t )isÂµ t =Y t â(cid:80)â j=0 b j e tâj .Thefactthatthiscanbe writtenas(14.22)istechnical.SeeTheorem5.7.1ofBrockwellandDavis(1991). â  ProofofTheorem14.22Inthetextweshowedthat (cid:175) (cid:175) Î» j (cid:175) (cid:175) <1issufficientforY t tobestrictlystationaryand ergodic. Wenowverifythat (cid:175) (cid:175) Î» j (cid:175) (cid:175) <1isequivalentto(14.35)-(14.37). TherootsÎ» j aredefinedin(14.34). Considerseparatelythecasesofrealrootsandcomplexroots. Supposethattherootsarereal,whichoccurswhenÎ±2 1 +4Î± 2 â¥0.Then (cid:175) (cid:175) Î» j (cid:175) (cid:175) <1iff|Î± 1 |<2and (cid:113) (cid:113) Î± + Î±2+4Î± Î± â Î±2+4Î± 1 1 2 <1 and â1< 1 1 2 . 2 2 Equivalently,thisholdsiff Î±2+4Î± <(2âÎ± )2=4â4Î± +Î±2 and Î±2+4Î± <(2+Î± )2=4+4Î± +Î±2 1 2 1 1 1 1 2 1 1 1 orequivalentlyiff Î± <1âÎ± and Î± <1+Î± 2 1 2 1 whichare(14.35)and(14.36).Î±2+4Î± â¥0and|Î± |<2implyÎ± â¥âÎ±2/4â¥â1,whichis(14.37). 1 2 1 2 1 Nowsupposetherootsarecomplex,whichoccurswhenÎ±2+4Î± <0. Thesquaredmodulusofthe 1 2 (cid:179) (cid:113) (cid:180) rootsÎ» = Î± Â± Î±2+4Î± /2are j 1 1 2 ï£«(cid:113) ï£¶2 (cid:175) (cid:175) Î» j (cid:175) (cid:175) 2= (cid:179)Î± 1 (cid:180)2 âï£¬ ï£­ Î±2 1 +4Î± 2 ï£· ï£¸ =âÎ± 2 . 2 2 Thustherequirement (cid:175) (cid:175) Î» j (cid:175) (cid:175) <1issatisfiediffÎ± 2 >â1,whichis(14.37).Î±2 1 +4Î± 2 <0andÎ± 2 >â1implyÎ±2 1 < â4Î± <4,so|Î± |<2.Î±2+4Î± <0and|Î± |<2implyÎ± +Î± <Î± âÎ±2/4<1andÎ± âÎ± <âÎ±2/4âÎ± <1 2 1 1 2 1 1 2 1 1 2 1 1 1 whichare(14.35)and(14.36). â  ProofofTheorem14.23TocompletetheproofweneedtoestablishthattheeigenvaluesÎ» ofAdefined j in(14.40)equalthereciprocalsoftherootsr oftheautoregressivepolynomialÎ±(z)of(14.39). Ourgoal j isthereforetoshowthatifÎ»satisfiesdet (cid:161) AâI Î»(cid:162)=0thenitsatisfiesÎ±(1/Î»)=0. p Noticethat (cid:181) âÎ»+Î± Î±(cid:48) (cid:182) AâI Î»= 1 (cid:101) p a B whereÎ±(cid:48) =(Î± ,...,Î± ), a (cid:48) =(1,0,...,0), andB isalower-diagonalmatrixwithâÎ»onthediagonaland1 (cid:101) 2 p immediatelybelowthediagonal.Noticethatdet(B)=(âÎ»)pâ1andbydirectcalculation ï£« Î»â1 0 Â·Â·Â· 0 0 ï£¶ ï£¬ Î»â2 Î»â1 Â·Â·Â· 0 0 ï£· ï£¬ ï£· B â1=âï£¬ ï£¬ Î»â3 Î»â2 Â·Â·Â· 0 0 ï£· ï£·. ï£¬ ï£¬ . . . . . . ... . . . . . . ï£· ï£· ï£­ ï£¸ Î»âp+1 Î»âp+2 Â·Â·Â· Î»â2 Î»â1",
    "page": 523,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 504 Usingthepropertiesofthedeterminant(TheoremA.1.5) (cid:181) âÎ»+Î± Î±(cid:48) (cid:182) det (cid:161) AâI Î»(cid:162)=det 1 (cid:101) p a B =det(B) (cid:161)âÎ»+Î± âÎ±(cid:48) B â1a (cid:162) 1 (cid:101) =(âÎ»)p(cid:161) 1âÎ± Î»â1âÎ± Î»â2âÎ± Î»â3âÂ·Â·Â·âÎ± Î»âp(cid:162) 1 2 3 p =(âÎ»)pÎ±(1/Î»). ThusifÎ»satisfiesdet (cid:161) AâI Î»(cid:162)=0thenÎ±(1/Î»)=0asrequired. â  p ProofofTheorem14.24BytheFundamentalTheoremofAlgebrawecanfactortheautoregressivepoly- nomial as Î±(z)=(cid:81) (cid:96) p =1 (1âÎ» (cid:96)z) where Î» (cid:96) =r (cid:96) â1. By assumption |Î» (cid:96) |<1. Inverting the autoregressive polynomialweobtain p Î±(z) â1= (cid:89) (1âÎ» (cid:96)z) â1 (cid:96)=1 p (cid:195)â (cid:33) = (cid:89) (cid:88) Î»j zj (cid:96) (cid:96)=1 j=0 â (cid:195) (cid:33) = (cid:88) (cid:88) Î»i1Â·Â·Â·Î»ip zj 1 p j=0 i1 +Â·Â·Â·+ip =j â = (cid:88) b zj j j=0 withb j =(cid:80) i1 +Â·Â·Â·+ip =j Î»i 1 1Â·Â·Â·Î»i p p. Usingthetriangleinequalityandthestarsandbarstheorem(Theorem1.10ofIntroductiontoEcono- metrics) (cid:175) (cid:175)b j (cid:175) (cid:175) â¤ (cid:88) |Î» 1 |i1Â·Â·Â·(cid:175) (cid:175) Î» p (cid:175) (cid:175) ip i1 +Â·Â·Â·+ip =j â¤ (cid:88) Î»j i1 +Â·Â·Â·+ip =j (cid:195) (cid:33) p+jâ1 â¤ Î»j j (cid:161) p+jâ1 (cid:162) ! = Î»j (cid:161) pâ1 (cid:162) !j! â¤(cid:161) j+1 (cid:162)pÎ»j asclaimed.Wenextverifytheconvergenceof (cid:80)â j=0 (cid:175) (cid:175)b j (cid:175) (cid:175) â¤(cid:80)â j=0 (cid:161) j+1 (cid:162)pÎ»j.Notethat (cid:161) j+1 (cid:162)pÎ»j lim =Î»<1. jââ (cid:161) j (cid:162)pÎ»jâ1 Bytheratiotest(TheoremA.3.2ofIntroductiontoEconometrics) (cid:80)â (cid:161) j+1 (cid:162)pÎ»j isconvergent. â  j=0 ProofofTheorem14.27IfQ issingularthenthereissomeÎ³suchthatÎ³(cid:48) QÎ³=0. WecannormalizeÎ³ to haveaunitcoefficientonY tâ1 (orthefirstnon-zerocoefficientotherthantheintercept). Wethenhave",
    "page": 524,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 505 (cid:183) (cid:184) (cid:183) (cid:184) that(cid:69) (cid:179) Y tâ1 â(cid:161) 1,Y tâ2 ,...,Y tâp) (cid:162)(cid:48) Ï (cid:180)2 =0forsomeÏ,orequivalently(cid:69) (cid:179) Y t â(cid:161) 1,Y tâ1 ,...,Y tâp+1) (cid:162)(cid:48) Ï (cid:180)2 = (cid:104) (cid:105) 0. SettingÎ²=(Ï(cid:48) ,0) (cid:48) thisimplies(cid:69) (cid:161) Y âÎ²(cid:48) X (cid:162)2 =0. SinceÎ±isthebestlinearpredictorwemusthave t t (cid:104) (cid:105) Î²=Î±. ThisimpliesÏ2=(cid:69) (cid:161) Y âÎ±(cid:48) X (cid:162)2 =0. ThiscontradictstheassumptionÏ2>0. Weconcludethat t t Q isnotsingular. â  _____________________________________________________________________________________________ 14.48 Exercises Exercise14.1 ForascalartimeseriesY definethesampleautocovarianceandautocorrelation t Î³ (cid:98) (k)=n â1 (cid:88) n (cid:179) Y t âY (cid:180)(cid:179) Y tâk âY (cid:180) t=k+1 (cid:179) (cid:180)(cid:179) (cid:180) Ï(k)= Î³ (cid:98) (k) = (cid:80)n t=k+1 Y t âY Y tâk âY . (cid:98) Î³ (cid:98) (0) (cid:80)n (cid:179) Y âY (cid:180)2 t=1 t Assumetheseriesisstrictlystationary,ergodic,strictlystationary,and(cid:69)(cid:163) Y2(cid:164)<â. t ShowthatÎ³(k)ââÎ³(k)andÏ(k)ââÎ³(k)asnââ.(UsetheErgodicTheorem.) (cid:98) (cid:98) p p Exercise14.2 Showthatif(e t ,F t )isaMDSandX t isF t -measurablethenu t =X tâ1 e t isaMDS. Exercise14.3 LetÏ2 t =(cid:69)(cid:163) e t 2|F tâ1 (cid:164) .Showthatu t =e2âÏ2 t isaMDS. Exercise14.4 Continuingthepreviousexercise,showthatif(cid:69)(cid:163) e4(cid:164)<âthen t n n â1/2(cid:88)(cid:161) e2âÏ2(cid:162)ââN (cid:161) 0,v2(cid:162) . t t t=1 d Expressv2intermsofthemomentsofe . t Exercise14.5 Astochasticvolatilitymodelis Y =Ï e t t t logÏ2=Ï+Î²logÏ2 +u t tâ1 t wheree andu areindependenti.i.d.N(0,1)shocks. t t (a) WritedownaninformationsetforwhichY isaMDS. t (b) Showthatif (cid:175) (cid:175) Î²(cid:175) (cid:175) <1thenY t isstrictlystationaryandergodic. Exercise14.6 Verifytheformula Ï(1)=Î¸/ (cid:161) 1+Î¸2(cid:162) foraMA(1)process. (cid:179) (cid:180) (cid:179) (cid:180) Exercise14.7 VerifytheformulaÏ(k)= (cid:80)â j=0 Î¸ j+k Î¸ j / (cid:80)q j=0 Î¸2 j foraMA(â)process. Exercise14.8 SupposeY t =Y tâ1 +e t withe t i.i.d.(0,1)andY 0 =0.Findvar[Y t ].IsY t stationary? Exercise14.9 TaketheAR(1)modelwithnointerceptY t =Î± 1 Y tâ1 +e t .",
    "page": 525,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 506 (a) Findtheimpulseresponsefunctionb j = â â et Y t+j . (b) LetÎ± betheleastsquaresestimatorofÎ± .Findanestimatorofb . (cid:98)1 1 j (c) Let s(Î± )beastandarderrorforÎ± . Usethedeltamethodtofinda95%asymptoticconfidence (cid:98)1 (cid:98)1 intervalforb . j Exercise14.10 TaketheAR(2)modelY t =Î± 1 Y tâ1 +Î± 2 Y tâ1 +e t . (a) Findexpressionsfortheimpulseresponsesb ,b ,b andb . 1 2 3 4 (b) Let(Î± ,Î± )betheleastsquaresestimator.Findanestimatorofb . (cid:98)1 (cid:98)2 2 (c) LetV(cid:98) betheestimatedcovariancematrixforthecoefficients. Usethedeltamethodtofinda95% asymptoticconfidenceintervalforb . 2 Exercise14.11 Showthatthemodels Î±(L)Y =Î± +e t 0 t and Î±(L)Y =Âµ+u t t Î±(L)u =e t t areidentical.FindanexpressionforÂµintermsofÎ± andÎ±(L). 0 Exercise14.12 Takethemodel Î±(L)Y =u t t Î²(L)u =e t t whereÎ±(L)andÎ²(L)arep andq orderlagpolynomials.Showthattheseequationsimplythat Î³(L)Y =e t t forsomelagpolynomialÎ³(L).WhatistheorderofÎ³(L)? Exercise14.13 SupposethatY t =e t +u t +Î¸u tâ1 whereu t ande t aremutuallyindependenti.i.d. (0,1) processes. (a) ShowthatY t isaMA(1)processY t =Î· t +ÏÎ· tâ1 forawhitenoiseerrorÎ· t . Hint:CalculatetheautocorrelationfunctionofY . t (b) FindanexpressionforÏintermsofÎ¸. (c) SupposeÎ¸=1.FindÏ. Exercise14.14 Supposethat Y =X +e t t t X t =Î±X tâ1 +u t where the errors e and u are mutually independent i.i.d. processes. Show that Y is an ARMA(1,1) t t t process.",
    "page": 526,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 507 Exercise14.15 AGaussianARmodelisanautoregressionwithi.i.d. N(0,Ï2)errors. ConsidertheGaus- sianAR(1)model Y t =Î± 0 +Î± 1 Y tâ1 +e t e â¼N (cid:161) 0,Ï2(cid:162) t with|Î± |<1.ShowthatthemarginaldistributionofY isalsonormal: 1 t (cid:195) (cid:33) Î± Ï2 Y â¼N 0 , . t 1âÎ± 1 1âÎ±2 1 Hint:UsetheMArepresentationofY . t Exercise14.16 AssumethatY isaGaussianAR(1)asinthepreviousexercise.Calculatethemoments t Âµ=(cid:69)[Y ] t (cid:104) (cid:105) Ï2 =(cid:69) (cid:161) Y âÂµ(cid:162)2 Y t (cid:104) (cid:105) Îº=(cid:69) (cid:161) Y âÂµ(cid:162)4 t A colleague suggests estimating the parameters (Î± ,Î± ,Ï2) of the Gaussian AR(1) model by GMM ap- 0 1 plied to the corresponding sample moments. He points out that there are three moments and three parameters,soitshouldbeidentified.Canyoufindaflawinhisapproach? Hint:Thisissubtle. Exercise14.17 Takethenonlinearprocess Y =Y Î± u1âÎ± t tâ1 t whereu isi.i.d.withstrictlypositivesupport. t (a) FindtheconditionunderwhichY isstrictlystationaryandergodic. t (b) FindanexplicitexpressionforY t asafunctionof(u t ,u tâ1 ,...). Exercise14.18 Takethequarterlyseriespnfix(nonresidentialrealprivatefixedinvestment)fromFRED-QD. (a) Transformtheseriesintoquarterlygrowthrates. (b) EstimateanAR(4)model.Reportusingheteroskedastic-consistentstandarderrors. (c) RepeatusingtheNewey-Weststandarderrors,usingM=5. (d) Commentonthemagnitudeandinterpretationofthecoefficients. (e) Calculate(numerically)theimpulseresponsesfor j =1,...,10. Exercise14.19 Takethequarterlyseriesoilpricex(realpriceofcrudeoil)fromFRED-QD. (a) Transformtheseriesbytakingfirstdifferences. (b) EstimateanAR(4)model.Reportusingheteroskedastic-consistentstandarderrors.",
    "page": 527,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER14. TIMESERIES 508 (c) TestthehypothesisthattherealoilpricesisarandomwalkbytestingthatthefourARcoefficients jointlyequalzero. (d) Interpretthecoefficientestimatesandtestresult. Exercise14.20 Takethemonthlyseriesunrate(unemploymentrate)fromFRED-MD. (a) EstimateAR(1)throughAR(8)models,usingthesamplestartingin1960m1sothatallmodelsuse thesameobservations. (b) ComputetheAICforeachARmodelandreport. (c) WhichARmodelhasthelowestAIC? (d) Reportthecoefficientestimatesandstandarderrorsfortheselectedmodel. Exercise14.21 Takethequarterlyseriesunrate(unemploymentrate)andclaimsx(initialclaims)from FRED-QD.âInitialclaimsâarethenumberofindividualswhofileforunemploymentinsurance. (a) Estimateadistributedlagregressionoftheunemploymentrateoninitialclaims.Uselags1through 4.Whichstandarderrormethodisappropriate? (b) Estimateanautoregressivedistributedlagregressionoftheunemploymentrateoninitialclaims. Uselags1through4forbothvariables. (c) TestthehypothesisthatinitialclaimsdoesnotGrangercausetheunemploymentrate. (d) Interpretyourresults. Exercise14.22 Take the quarterly series gdpc1 (real GDP) and houst (housing starts) from FRED-QD. âHousingstartsâarethenumberofnewhousesonwhichconstructionisstarted. (a) TransformtherealGDPseriesintoitsonequartergrowthrate. (b) EstimateadistributedlagregressionofGDPgrowthonhousingstarts.Uselags1through4.Which standarderrormethodisappropriate? (c) EstimateanautoregressivedistributedlagregressionofGDPgrowthonhousingstarts. Uselags1 through2forGDPgrowthand1through4forhousingstarts. (d) TestthehypothesisthathousingstartsdoesnotGrangercauseGDPgrowth. (e) Interpretyourresults.",
    "page": 528,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Chapter 15 Multivariate Time Series 15.1 Introduction A multivariate time series Y =(Y ,...,Y ) (cid:48) is an mÃ1 vector process observed in sequence over t 1t mt time,t=1,...,n.Multivariatetimeseriesmodelsprimarilyfocusonthejointmodelingofthevectorseries Y . Themostcommonmultivariatetimeseriesmodelsusedbyeconomistsarevectorautoregressions t (VARs).VARswereintroducedtoeconometricsbySims(1980). Some excellent textbooks and review articles on multivariate time series include Hamilton (1994), Watson(1994),Canova(1995),LÃ¼tkepohl(2005),Ramey(2016),StockandWatson(2016),andKilianand LÃ¼tkepohl(2017). 15.2 MultipleEquationTimeSeriesModels Tomotivatevectorautoregressionsletusstartbyreviewingtheautoregressivedistributedlagmodel ofSection14.41forthecaseoftwoseriesY =(Y ,Y ) (cid:48) withasinglelag.AnAR-DLmodelforY is t 1t 2t 1t Y 1t =Î± 0 +Î± 1 Y 1tâ1 +Î² 1 Y 2tâ1 +e 1t . Similarly,anAR-DLmodelforY is 2t Y 2t =Î³ 0 +Î³ 1 Y 2tâ1 +Î´ 1 Y 1tâ1 +e 2t . Thesetwoequationsspecifythateachvariableisalinearfunctionofitsownlagandthelagofthe othervariable.InsodoingwefindthatthevariablesontherighthandsideofeachequationareY tâ1 . Wecansimplifytheequationsbycombiningtheregressorsstackingthetwoequationstogetherand writingthevectorerrorase =(e ,e ) (cid:48) tofind t 1t 2t Y t =a 0 +A 1 Y tâ1 +e t wherea is2Ã1and A is2Ã2. ThisisabivariatevectorautoregressivemodelforY . Itspecifiesthat 0 1 t themultivariateprocessY t isalinearfunctionofitsownlagY tâ1 pluse t . Itisthecombinationoftwo equationseachofwhichisanautoregressivedistributedlagmodel. Thusamultivariateautoregression issimplyasetofautoregressivedistributedlagmodels. Theabovederivationassumedasinglelag.Iftheequationsincludeplagsofeachvariableweobtain thepth ordervectorautoregressive(VAR)model Y t =a 0 +A 1 Y tâ1 +A 2 Y tâ2 +Â·Â·Â·+A p Y tâp +e t . (15.1) 509",
    "page": 529,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 510 ThisisabivariatevectorautoregressivemodelforY . t Furthermore,thereisnothingspecialaboutthetwovariablecase.Thenotationin(15.1)allowsY to t beavectorofdimensionm inwhichcasethematrices A(cid:96) aremÃm andtheerrore t ismÃ1. Wewill denotetheelementsof A(cid:96)usingthenotation ï£® ï£¹ a 11,(cid:96) a 12,(cid:96) Â·Â·Â· a 1m,(cid:96) A(cid:96) = ï£¯ ï£¯ ï£¯ a 2 . 1,(cid:96) a 2 . 2,(cid:96) Â·Â·Â· a 2m . ,(cid:96) ï£º ï£º ï£º. ï£¯ . . . . . . ï£º ï£° ï£» a m1,(cid:96) a m2,(cid:96) Â·Â·Â· a mm,(cid:96) Theerrore =(e ,...,e ) (cid:48) isthecomponentofY =(Y ,...,Y ) (cid:48) whichisunforecastableattimetâ t 1t mt t 1t mt 1. However,thecomponentsofY arecontemporaneouslycorrelated. Thereforethecontemporaneous t covariancematrixÎ£=(cid:69)(cid:163) ee (cid:48)(cid:164) isnon-diagonal. TheVARmodelfallsintheclassofmultivariateregressionmodelsstudiedinChapter11. In the following several sections we take a step back and provide a rigorous foundation for vector autoregressionsforstationarytimeseries. 15.3 LinearProjection InSection14.14wederivedthelinearprojectionoftheunivariateseriesY onitsinfinitepasthis- t tory. We now extend this to the multivariate case. Define the multivariate infinite past history Y(cid:101)tâ1 = (...,Y tâ2 ,Y tâ1 ). The projection of Y t onto Y(cid:101)tâ1 , written P tâ1 [Y t ] = P (cid:163) Y t |Y(cid:101)tâ1 (cid:164) , is unique and has a uniqueprojectionerror e t =Y t âP tâ1 [Y t ]. (15.2) Wewillcalltheprojectionerrorse theâinnnovationsâ. t Theinnovationse aremeanzeroandseriallyuncorrelated.Westatethisformally. t Theorem15.1 IfY iscovariancestationaryithastheprojectionequation t Y t =P tâ1 [Y t ]+e t . Theinnovationse t satisfy(cid:69)[e t ]=0,(cid:69)(cid:163) e tâ(cid:96)e t (cid:48)(cid:164)=0for(cid:96)â¥1,andÎ£=(cid:69)(cid:163) ee (cid:48)(cid:164)<â. IfY isstrictlystationarythene isstrictlystationary. t t Theuncorrelatednessoftheprojectionerrorsisapropertyofamultivariatewhitenoiseprocess. Definition15.1 Thevectorprocesse ismultivariatewhitenoiseif(cid:69)[e ]=0, t t (cid:69)(cid:163) e e (cid:48)(cid:164)=Î£<â,and(cid:69)(cid:163) e e (cid:48) (cid:164)=0for(cid:96)(cid:54)=0. t t t tâ(cid:96)",
    "page": 530,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 511 15.4 MultivariateWoldDecomposition By projecting Y onto the past history of the white noise innovations e we obtain a multivariate t t versionoftheWolddecomposition. Theorem15.2 IfY iscovariancestationaryandnon-deterministicthenithas t thelinearrepresentation â (cid:88) Y t =Âµ+ Î (cid:96)e tâ(cid:96) (15.3) (cid:96)=0 where e are the white noise projection errors and Î = I . The coefficient t 0 m matricesÎ (cid:96)aremÃm. Wecanwritethemovingaveragerepresentationusingthelagoperatornotationas Y =Âµ+Î(L)e t t where â Î(z)= (cid:88) Î (cid:96)z (cid:96) . (cid:96)=0 AmultivariateversionofTheorem14.19canalsobeestablished. Theorem15.3 If Y is covariance stationary, non-deterministic, with Wold t representation Y = Î(L)e , such that Î» (Îâ (z)Î(z)) â¥ Î´ > 0 for all t t min complex |z| â¤ 1, and for some integer s â¥ 0 the Wold coefficients satisfy (cid:80)â j=0 (cid:176) (cid:176) (cid:80)â k=0 ksÎ j+k (cid:176) (cid:176) 2<â,thenY t hasaninfinite-orderautoregressiverepre- sentation A(L)Y =a +e (15.4) t 0 t where â A(z)=I m â (cid:88) A(cid:96)z (cid:96) (cid:96)=1 andthecoefficientssatisfy (cid:80)â ks(cid:107)A (cid:107)<â.Theseriesin(15.4)isconvergent. k=1 k ForaproofseeSection2ofMeyerandKreiss(2015). WecanalsoprovideananalogofTheorem14.6. Theorem15.4 If e â (cid:82)m is strictly stationary, ergodic, (cid:69)(cid:107)e (cid:107) < â, and t t (cid:80)â (cid:96)=0 (cid:107)Î (cid:96) (cid:107)<â,thenY t =(cid:80)â (cid:96)=0 Î (cid:96)e tâ(cid:96)isstrictlystationaryandergodic.",
    "page": 531,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 512 TheproofofTheorem15.4isastraightforwardextensionofTheorem14.6soisomitted. ThemovingaverageandautoregressivelagpolynomialssatisfytherelationshipÎ(z)=A(z) â1. Forsomepurposes(suchasimpulseresponsecalculations)weneedtocalculatethemovingaverage coefficientmatricesÎ (cid:96)fromtheautoregressivecoefficientmatricesA(cid:96).Whilethereisnotaclosed-form solutionthereisasimplerecursionbywhichthecoefficientsmaybecalculated. Theorem15.5 For j â¥1,Î j =(cid:80) (cid:96) j =1 A(cid:96) Î jâ(cid:96). Toseethis,supposeforsimplicitya =0andthattheinnovationssatisfye =0fort(cid:54)=0.ThenY =0 0 t t fort<0.Usingtheregressionequation(15.4)fortâ¥0wesolveforeachY .Fort=0 t Y =e =Î e 0 0 0 0 whereÎ =I .Fort=1 0 m Y =A Y =A Î e =Î e 1 1 0 1 0 0 1 0 whereÎ =A Î .Fort=2 1 1 0 Y =A Y +A Y =A Î e +A Î e =Î e 2 1 1 2 0 1 1 0 2 0 0 2 0 whereÎ =A Î +A Î .Fort=3 2 1 1 2 0 Y =A Y +A Y +A Y =A Î e +A Î e +A Î e =Î e 3 1 2 2 1 3 0 1 2 0 2 1 0 3 0 0 3 0 whereÎ =A Î +A Î +A Î .Thecoefficientssatisfythestatedrecursionasclaimed. 3 1 2 2 2 2 0 15.5 ImpulseResponse One of the most important concepts in applied multivariate time series is the impulse response function(IRF)whichisdefinedasthechangeinY duetoachangeinaninnovationorshock. Inthis t sectionwedefinethebaselineIRFâtheunnormalizednon-orthogonalizedimpulseresponsefunction âwhichisthechangeinY duetoachangeinaninnovatione . Specifically,wedefinetheimpulsere- t t sponseofvariablei withrespecttoinnovation j asthechangeinthetimet projectionoftheith variable Y it+h duetothe jth innovatione jt â IRF ij (h)= âe P t [Y it+h ]. jt Therearem2suchresponsesforeachhorizonh.WecanwritethemasanmÃmmatrix â IRF(h)= âe (cid:48) P t [Y t+h ]. t RecallthemultivariateWoldrepresentation â (cid:88) Y t =Âµ+ Î (cid:96)e tâ(cid:96). (cid:96)=0",
    "page": 532,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 513 Wecancalculatethattheprojectionontothehistoryattimet is â â (cid:88) (cid:88) P t [Y t+h ]=Âµ+ Î (cid:96)e t+hâ(cid:96) =Âµ+ Î h+(cid:96)e tâ(cid:96). (cid:96)=h (cid:96)=0 We deduce that the impulse response is IRF(h)=Î , the hth moving average coefficient matrix. The h invididualimpulseresponseisIRF (h)=Î ,theijth elementofÎ . ij h,ij h Herewehavedefinedtheimpulseresponseintermsofthelinearprojectionoperator.Analternative is to define the impulse response in terms of the conditional expectation operator. The two coincide whentheinnovationse areamartingaledifferencesequence(andthuswhenthetrueprocessislinear) t butotherwisewillnotcoincide. Typicallyweviewimpulseresponsesasafunctionofthehorizonh andplotthemasafunctionof h foreachpair(i,j). TheimpulseresponsefunctionIRF (h)isinterpretedashowtheith variablere- ij spondsovertimetothe jth innovation. Inalinearvectorautoregressiontheimpulseresponsefunctionissymmetricinnegativeandpositive innovations. Thatis,theimpactonY it+h ofapositiveinnovatione jt =1isIRF ij (h)andtheimpactof anegativeinnovatione =â1isâIRF (h). Furthermore,themagnitudeoftheimpactislinearinthe jt ij magnitudeoftheinnovation. Thustheimpactoftheinnovatione =2is2ÃIRF (h)andtheimpact jt ij oftheinnovatione =â2isâ2ÃIRF (h). Thismeansthattheshapeoftheimpulseresponsefunction jt ij isunaffectedbythemagnitudeoftheinnovation. (Theseareconsequencesofthelinearityofthevector autoregressivemodelnotnecessarilyfeaturesofthetrueworld.) Theimpulseresponsefunctionscanbescaledasdesired.Onestandardchoiceistoscalesothatthe innovationscorrespondtooneunitoftheimpulsevariable. Thusiftheimpulsevariableismeasuredin dollarstheimpulseresponsecanbescaledtocorrespondtoachangein$1orsomemultiplesuchasa milliondollars. Iftheimpulsevariableismeasuredinpercentagepoints(e.g. aninterestrate)thenthe impulseresponsecanbescaledtocorrespondtoachangeofonepercentagepoint(e.g. from3%to4%) ortocorrespondtoachangeofonebasispoint(e.g.from3.05%to3.06%).Anotherstandardchoiceisto scaletheimpulseresponsestocorrespondtoaâonestandarddeviationâinnovation. Thisoccurswhen theinnovationshavebeenscaledtohaveunitvariances. Inthislattercaseimpulseresponsefunctions canbeinterpretedasresponsesduetoaâtypicalâsized(onestandarddeviation)innovation. CloselyrelatedtotheIRFisthecumulativeimpulseresponsefunction(CIRF)definedas h â h (cid:88) (cid:88) CIRF(h)= âe (cid:48) P t [Y t+(cid:96)]= Î (cid:96). (cid:96)=1 t (cid:96)=1 Thecumulativeimpulseresponseistheaccumulated(summed)responsesonY fromtime t to t+h. t Thelimitofthecumulativeimpulseresponseashââisthelong-runimpulseresponsematrix â C = lim CIRF(h)= (cid:88) Î (cid:96) =Î(1)=A(1) â1. hââ (cid:96)=1 Thisisthefull(summed)effectoftheinnovationoveralltime",
    "page": 533,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". CloselyrelatedtotheIRFisthecumulativeimpulseresponsefunction(CIRF)definedas h â h (cid:88) (cid:88) CIRF(h)= âe (cid:48) P t [Y t+(cid:96)]= Î (cid:96). (cid:96)=1 t (cid:96)=1 Thecumulativeimpulseresponseistheaccumulated(summed)responsesonY fromtime t to t+h. t Thelimitofthecumulativeimpulseresponseashââisthelong-runimpulseresponsematrix â C = lim CIRF(h)= (cid:88) Î (cid:96) =Î(1)=A(1) â1. hââ (cid:96)=1 Thisisthefull(summed)effectoftheinnovationoveralltime. ItisusefultoobservethatwhenaVARisestimatedondifferencedobservationsâY thencumulative t impulseresponseis (cid:34) (cid:35) â h â (cid:88) CIRF(h)= âe (cid:48) P t âY t+(cid:96) = âe (cid:48) P t [Y t+h ] t (cid:96)=1 t whichistheimpulseresponseforthevariableY inlevels.Moregenerally,whenaVARisestimatedwith t somevariablesinlevelsandsomeindifferencesthenthecumulativeimpulseresponseforthesecond groupwillcoincidewiththeimpulseresponsesforthesamevariablesmeasuredinlevels.",
    "page": 533,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 514 ItistypicaltoreportcumulativeimpulseresponsefunctionsforvariableswhichenteraVARindiffer- ences. Infact,inthiscontextmanyauthorswilllabelthecumulativeimpulseresponseasâtheimpulse responseâ. 15.6 VAR(1)Model Thefirst-ordervectorautoregressiveprocess,denotedVAR(1),is Y t =a 0 +A 1 Y tâ1 +e t wheree isastrictlystationaryandergodicwhitenoiseprocess. t We are interested in conditions under which Y is a stationary process. Let Î» (A) denote the ith t i eigenvalueof A. Theorem15.6 Ife isstrictlystationary, ergodic, (cid:69)(cid:107)e (cid:107)<â, and|Î» (A )|<1 t t i 1 fori =1,...,m,thentheVAR(1)processY isstrictlystationaryandergodic. t TheproofisgiveninSection15.31. 15.7 VAR(p)Model Thepth-ordervectorautoregressiveprocess,denotedVAR(p),is Y t =a 0 +A 1 Y tâ1 +Â·Â·Â·+A p Y tâp +e t wheree isastrictlystationaryandergodicwhitenoiseprocess. t Wecanwritethemodelusingthelagoperatornotationas A(L)Y =a +e t 0 t where A(z)=I âA zâÂ·Â·Â·âA zp. m 1 p The condition for stationarity of the system can be expressed as a restriction on the roots of the determinantalequationoftheautoregressivepolynomial. Recall, arootr ofdet(A(z))isasolutionto det(A(r))=0. Theorem15.7 Ifallrootsr ofdet(A(z))satisfy|r|>1thentheVAR(p)process Y isstrictlystationaryandergodic. t TheproofisstructurallyidenticaltothatofTheorem14.23soisomitted.",
    "page": 534,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 515 15.8 RegressionNotation Definethe (cid:161) mp+1 (cid:162)Ã1vector ï£« ï£¶ 1 ï£¬ ï£¬ Y tâ1 ï£· ï£· X t =ï£¬ ï£¬ Y tâ2 ï£· ï£· ï£¬ . ï£· ï£¬ . ï£· . ï£­ ï£¸ Y tâp andthemÃ(mp+1)matrix A (cid:48)=(cid:161) a A A Â·Â·Â· A (cid:162) . ThentheVARsystemofequationscanbe 0 1 2 p writtenas Y =A (cid:48) X +e . (15.5) t t t Thisisamultivariateregressionmodel.Theerrorhascovariancematrix Î£=(cid:69)(cid:163) e e (cid:48)(cid:164) . (15.6) t t Wecanalsowritethecoefficientmatrixas A=(cid:161) a a Â·Â·Â· a (cid:162) wherea isthevectorofcoeffi- 1 2 m j cientsforthe jth equation.ThusY =a (cid:48) X +e . jt j t jt Ingeneral,ifY isstrictlystationarywecandefinethecoefficientmatrix Abylinearprojection. t A=(cid:161)(cid:69)(cid:163) X X (cid:48)(cid:164)(cid:162)â1(cid:69)(cid:163) X Y (cid:48)(cid:164) . t t t t ThisholdswhetherornotY isactuallyaVAR(p)process.Bythepropertiesofprojectionerrors t (cid:69)(cid:163) X e (cid:48)(cid:164)=0. (15.7) t t Theprojectioncoefficientmatrix Aisidentifiedif(cid:69)(cid:163) X X (cid:48)(cid:164) isinvertible. t t Theorem15.8 IfY isstrictlystationaryand0<Î£<âforÎ£definedin(15.6), t thenQ=(cid:69)(cid:163) X X (cid:48)(cid:164)>0andthecoefficientvector(14.45)isidentified. t t TheproofisgiveninSection15.31. 15.9 Estimation FromChapter11thesystemsestimatorofamultivariateregressionisleastsquares. Theestimator canbewrittenas (cid:181) n (cid:182)â1(cid:181) n (cid:182) A(cid:98) = (cid:88) X t X t (cid:48) (cid:88) X t Y t (cid:48) . t=1 t=1 Alternatively,thecoefficientestimatorforthe jth equationis (cid:181) n (cid:182)â1(cid:181) n (cid:182) a = (cid:88) X X (cid:48) (cid:88) X Y . (cid:98)j t t t jt t=1 t=1",
    "page": 535,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 516 (cid:48) Theleastsquaresresidualvectorise (cid:98)t =Y t âA(cid:98) X t .Theestimatorofthecovariancematrixis Î£ (cid:98) = n 1 t (cid:88) = n 1 e (cid:98)t e (cid:98)t (cid:48) . (15.8) (Thismaybeadjustedfordegrees-of-freedomifdesired,butthereisnoestablishedfinite-samplejusti- ficationforaspecificadjustment.) IfY isstrictlystationaryandergodicwithfinitevariancesthenwecanapplytheErgodicTheorem t (Theorem14.9)todeducethat 1 (cid:88) n X Y (cid:48)ââ(cid:69)(cid:163) X Y (cid:48)(cid:164) n t=1 t t p t t and n (cid:88) X X (cid:48)ââ(cid:69)(cid:163) X X (cid:48)(cid:164) . t t t t t=1 p Since the latter is positive definite by Theorem 15.8 we conclude that A(cid:98) is consistent for A. Standard manipulationsshowthatÎ£ (cid:98)isconsistentaswell. Theorem15.9 IfY t isstrictlystationary,ergodic,and0<Î£<âthen A(cid:98) ââ A p andÎ£ (cid:98) ââÎ£asnââ. p VARmodelscanbeestimatedinStatausingthevarcommand. 15.10 AsymptoticDistribution Set ï£« ï£¶ ï£« ï£¶ a a 1 (cid:98)1 a=vec(A)=ï£¬ ï£­ . . . ï£· ï£¸ , a (cid:98) =vec (cid:161) A(cid:98) (cid:162)=ï£¬ ï£­ . . . ï£· ï£¸ . a a m (cid:98)m BythesameanalysisasinTheorem14.30combinedwithTheorem11.1weobtainthefollowing. Theorem15.10 Suppose that Y follows the VAR(p) model, all roots r of t (cid:112) det(A(z))satisfy|r|>1,(cid:69)[e t |F tâ1 ]=0,(cid:69)(cid:107)e t (cid:107)4<â,andÎ£>0,thenasnââ, n(aâa)ââN(0,V)where (cid:98) d V =Q â1â¦Q â1 Q=I âQ m Q=(cid:69)(cid:163) X X (cid:48)(cid:164) t t â¦=(cid:69)(cid:163) e e (cid:48)âX X (cid:48)(cid:164) . t t t t",
    "page": 536,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 517 Notice that the theorem uses the strong assumption that the innovation is a martingale difference sequence(cid:69)[e t |F tâ1 ]=0. ThismeansthattheVAR(p)modelisthecorrectconditionalmeanforeach variable.Inwords,thesearethecorrectlagsandthereisnoomittednonlinearity. IfwefurtherstrengthentheMDSassumptiontoconditionalhomoskedasticity (cid:69)(cid:163) e t e t (cid:48) |F tâ1 (cid:164)=Î£ thentheasymptoticvariancesimplifiesas â¦=Î£âQ V =Î£âQ â1. Incontrast,iftheVAR(p)isanapproximationthentheMDSassumptionisnotappropriate. Inthis casetheasymptoticdistributioncanbederivedundermixingconditions. Theorem15.11 AssumethatY isstrictlystationary,ergodic,andforsomer > t 4,(cid:69)(cid:107)Y (cid:107)r <âandthemixingcoefficientssatisfy (cid:80)â Î±((cid:96))1â4/r <â. Leta be t (cid:96)=1 theprojectioncoefficientvectorande theprojectionerror. Thenasn ââ, (cid:112) t n(aâa)ââN(0,V)where (cid:98) d V =(cid:161) I âQ â1(cid:162)â¦(cid:161) I âQ â1(cid:162) m m Q=(cid:69)(cid:163) X X (cid:48)(cid:164) t t â â¦= (cid:88) (cid:69)(cid:163) e tâ(cid:96)e t (cid:48)âX tâ(cid:96)X t (cid:48)(cid:164) . (cid:96)=ââ ThistheoremdoesnotrequirethatthetrueprocessisaVAR.Instead,thecoefficientsaredefinedas thosewhichproducethebest(meansquare)approximation,andtheonlyrequirementsonthetruepro- cessaregeneraldependenceconditions. Thetheoremshowsthatthecoefficientestimatorsareasymp- toticallynormalwithacovariancematrixwhichtakesaâlong-runâsandwichform. 15.11 CovarianceMatrixEstimation Theclassichomoskedasticestimatorofthecovariancematrixforaequals (cid:98) V(cid:98) 0 a(cid:98) =Î£ (cid:98) â(cid:161) X (cid:48) X (cid:162)â1 . (15.9) Estimatorsadjustedfordegree-of-freedomcanalsobeusedthoughthereisnoestablishedfinite-sample justification. Thisvarianceestimatorisappropriateundertheassumptionthattheconditionalmeanis correctlyspecifiedasaVAR(p)andtheinnovationsareconditionallyhomoskedastic. Theheteroskedasticity-robustestimatorequals V(cid:98)a(cid:98) = (cid:179) I n â(cid:161) X (cid:48) X (cid:162)â1 (cid:180) (cid:181) (cid:88) n (cid:161) e (cid:98)t e (cid:98)t (cid:48)âX t X t (cid:48)(cid:162) (cid:182) (cid:179) I n â(cid:161) X (cid:48) X (cid:162)â1 (cid:180) . (15.10) t=1 Thisvarianceestimatorisappropriateundertheassumptionthattheconditionalmeaniscorrectlyspec- ifiedasaVAR(p)butdoesnotrequirethattheinnovationsareconditionallyhomoskedastic.",
    "page": 537,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 518 TheNewey-Westestimatorequals V(cid:98)a(cid:98) = (cid:179) I n â(cid:161) X (cid:48) X (cid:162)â1 (cid:180) â¦ (cid:98)M (cid:179) I n â(cid:161) X (cid:48) X (cid:162)â1 (cid:180) (15.11) M â¦ (cid:98)M = (cid:88) w(cid:96) (cid:88) (e (cid:98)tâ(cid:96) âX tâ(cid:96)) (cid:161) e (cid:98)t (cid:48)âX t (cid:48)(cid:162) (cid:96)=âM 1â¤tâ(cid:96)â¤n |(cid:96)| w(cid:96) =1â . M+1 ThenumberM iscalledthelagtruncationnumber.Anunweightedversionsetsw(cid:96) =1.TheNewey-West estimatordoesnotrequirethattheVAR(p)iscorrectlyspecified. Traditionaltextbookshaveonlyusedthehomoskedasticvarianceestimationformula(15.9)andcon- sequentlyexistingsoftwarefollowsthesameconvention. Forexample,thevarcommandinStatadis- playsonlyhomoskedasticstandarderrors.Someresearchersusetheheteroskedasticity-robustestimator (15.10).TheNewey-Westestimator(15.11)isnotcommonlyusedforVARmodels. Asymptotic approximations tend to be much less accurate under time series dependence than for independent observations. Therefore bootstrap methods are popular. In Section 14.46 we described severalbootstrapmethodsfortimeseriesobservations. WhileSection14.46focusedonunivariatetime series,theextensiontomultivariateobservationsisstraightforward. 15.12 SelectionofLagLengthinanVAR Foradata-dependentruletopickthelaglength p itisrecommendedtominimizeaninformation criterion.TheformulafortheAICis AIC(p)=nlogdetÎ£ (cid:98)(p)+2K(p) Î£ (cid:98)(p)= 1 (cid:88) n e (cid:98)t (p)e (cid:98)t (p) (cid:48) n t=1 K(p)=m(pm+1) whereK(p)isthenumberofparametersande (p)istheOLSresidualvectorfromthemodelwithplags. (cid:98)t Thelogdeterminantisthecriterionfromthemultivariatenormallikelihood. InStatatheAICforasetofestimatedVARmodelscanbecomparedusingthevarsoccommand. It shouldbenoted,however,thattheStataroutineactuallydisplaysAIC(p)/n=logdetÎ£ (cid:98)(p)+2K(p)/n.This doesnotaffecttherankingofthemodelsbutmakesthedifferencesbetweenmodelsappearmisleadingly small. 15.13 Illustration We estimate a three-variable system which is a simplified version of a model often used to study theimpactofmonetarypolicy. ThethreevariablesarequarterlyfromFRED-QD:realGDPgrowthrate (100âlog(GDP )),GDPinflationrate(100âlog(P )),andtheFederalfundsinterestrate.VARsfromlags1 t t through8wereestimatedbyleastsquares.ThemodelwiththesmallestAICistheVAR(6).Thecoefficient estimatesand(homoskedastic)standarderrorsfortheVAR(6)arereportedinTable15.1. Examining the coefficients in the table we can see that GDP displays a moderate degree of serial correlationandshowsalargeresponsetothefederalfundsrate,especiallyatlags2and3. Inflationalso displays serial correlation, shows minimal response to GDP, and also has meaningful response to the federal funds rate. The federal funds rate has the strongest serial correlation. Overall, it is difficult to",
    "page": 538,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 519 readtoomuchmeaningintothecoefficientestimatesduetothecomplexityoftheinteractions.Because ofthisdifficultyitistypicaltofocusonotherrepresentationsofthecoefficientestimatessuchasimpulse responseswhichwediscussintheupcomingsections. 15.14 PredictiveRegressions Insomecontexts(includingprediction)itisusefultoconsidermodelswherethedependentvariable isdatedmultipleperiodsaheadoftheright-hand-sidevariables.Theseequationscanbesingleequation ormultivariate;wecanconsiderbothasspecialcasesofaVAR(asasingleequationmodelcanbewritten asoneequationtakenfromaVARsystem).Anh-steppredictiveVAR(p)takestheform Y t+h =b 0 +B 1 Y t +Â·Â·Â·+B p Y tâp+1 +u t . (15.12) The integer h â¥ 1 is the horizon. A one-step predictive VAR equals a standard VAR. The coefficients shouldbeviewedasthebestlinearpredictorsofY t+h given(Y t ,...,Y tâp+1 ). ThereisaninterestingrelationshipbetweenaVARmodelandthecorrespondingh-steppredictive VARmodel. Theorem15.12 IfY isaVAR(p)processthenitsh-steppredictiveregression t isapredictiveVAR(p)withu aMA(h-1)processandB =Î =IRF(h). t 1 h TheproofofTheorem15.12ispresentedinSection15.31. Thereareseveralimplicationsofthistheorem.First,ifY isaVAR(p)processthenthecorrectnumber t oflagsforanh-steppredictiveregressionisalsop lags. Second,theerrorinapredictiveregressionisa MAprocessandisthusseriallycorrelated. Thelineardependence,however,iscappedbythehorizon. Third,theleadingcoefficientmatrixcorrespondstothehthmovingaveragecoefficientmatrixwhichalso equalsthehth impulseresponsematrix. Thepredictiveregression(15.12)canbeestimatedbyleastsquares.Wecanwritetheestimatesas Y t+h =b(cid:98)0 +B(cid:98)1 Y t +Â·Â·Â·+B(cid:98)p Y tâp+1 +u (cid:98)t . (15.13) ForadistributiontheoryweneedtoapplyTheorem15.11sincetheinnovationsu areamovingaverage t and thus violate the MDS assumption. It follows as well that the covariance matrix for the estimators should be estimated by the Newey-West (15.11) estimator. There is a difference, however. Since u is t knowntobeaMA(h-1)areasonablechoiceistosetM=hâ1andusethesimpleweightsw(cid:96) =1.Indeed, thiswastheoriginalsuggestionbyL.HansenandHodrick(1980). ForadistributionaltheorywecanapplyTheorem15.11. Letbbethevectorofcoefficientsin(15.12) andb(cid:98)thecorrespondingleastsquaresestimator.LetX t bethevectorofregressorsin(15.12).",
    "page": 539,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 520 Table15.1:VectorAutoregression GDP INF FF GDP tâ1 0.25 0.01 0.08 (0.07) (0.02) (0.02) GDP tâ2 0.23 â0.02 0.04 (0.07) (0.02) (0.02) GDP tâ3 0.00 0.03 0.01 (0.07) (0.02) (0.02) GDP tâ4 0.14 0.04 â0.02 (0.07) (0.02) (0.02) GDP tâ5 â0.02 â0.03 0.04 (0.07) (0.02) (0.02) GDP tâ6 0.05 â0.00 â0.01 (0.06) (0.02) (0.02) INF tâ1 0.11 0.57 0.01 (0.20) (0.07) (0.05) INF tâ2 â0.17 0.10 0.17 (0.23) (0.08) (0.06) INF tâ3 0.01 0.09 â0.05 (0.23) (0.08) (0.06) INF tâ4 0.16 0.14 â0.05 (0.23) (0.08) (0.06) INF tâ5 0.12 â0.05 â0.05 (0.24) (0.08) (0.06) INF tâ6 â0.14 0.10 0.09 (0.21) (0.07) (0.05) FF tâ1 0.13 0.28 1.14 (0.26) (0.08) (0.07) FF tâ2 â1.50 â0.27 â0.53 (0.38) (0.12) (0.10) FF tâ3 1.40 0.12 0.53 (0.40) (0.13) (0.10) FF tâ4 â0.57 â0.13 â0.28 (0.41) (0.13) (0.11) FF tâ5 0.01 0.25 0.28 (0.40) (0.13) (0.10) FF tâ6 0.47 â0.27 â0.24 (0.26) (0.08) (0.07) Intercept 1.15 0.22 â0.33 (0.54) (0.18) (0.14)",
    "page": 540,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 521 Theorem15.13 IfY isstrictlystationary, ergodic, Î£>0, andfor somer >4, t (cid:69)(cid:107)Y (cid:107)r <âandthemixingcoefficientssatisfy (cid:80)â Î±((cid:96))1â4/r <â,thenasnâ (cid:112)t (cid:96)=1 â, n (cid:161) b(cid:98) âb (cid:162)ââN(0,V)where d V =(cid:161) I âQ â1(cid:162)â¦(cid:161) I âQ â1(cid:162) m m Q=(cid:69)(cid:163) X X (cid:48)(cid:164) t t â â¦= (cid:88) (cid:69)(cid:163) (u (cid:98)tâ(cid:96) âX tâ(cid:96)) (cid:161) u (cid:98)t (cid:48)âX t (cid:48)(cid:162)(cid:164) . (cid:96)=ââ 15.15 ImpulseResponseEstimation Reportingofimpulseresponseestimatesisoneofthemostcommonapplicationsofvectorautore- gressivemodeling.Thereareseveralmethodstoestimatetheimpulseresponsefunction.Inthissection wereviewthemostcommonestimatorbasedontheestimatedVARparameters. WithinaVAR(p)modeltheimpulseresponsesaredeterminedbytheVARcoefficients. Wecanwrite thismappingasÎ h =g h (A). Theplug-inapproachsuggeststheestimatorÎ (cid:98)h =g h (A(cid:98))giventheVAR(p) coefficient estimator A(cid:98). These are the impulse responses implied by the estimated VAR coefficients. Whileitispossibletoexplicitlywritethefunction g (A), acomputationallysimpleapproachistouse h Theorem15.5whichshowsthattheimpulseresponsematricescanbewrittenasasimplerecursionin theVARcoefficients.Thustheimpulseresponseestimatorsatisfiestherecursion min[h,p] (cid:88) Î (cid:98)h = A(cid:98)(cid:96) Î (cid:98)hâ(cid:96). (cid:96)=1 WethensetI(cid:100)RF(h)=Î (cid:98)h . Thisisthethemostcommonlyusedmethodforimpulseresponseestimationanditisthemethod implementedinstandardpackages. Since A(cid:98) israndomsoisI(cid:100)RF(h)asitisanonlinearfunctionof A(cid:98). Usingthedeltamethod,wededuce thattheelementsofI(cid:100)RF(h)(theimpulseresponses)areasymptoticallynormallydistributed.Withsome messyalgebraexplicitexpressionsfortheasymptoticvariancescanbeobtained.Sampleversionscanbe usedtocalculateasymptoticstandarderrors.Thesecanbeusedtoformasymptoticconfidenceintervals fortheimpulseresponses. Theasymptoticapproximations, however, canbepoor. Aswediscussedearliertheasymptoticap- proximationsforthedistributionofthecoefficients A(cid:98) canbepoorduetotheserialdependenceinthe observations.TheasymptoticapproximationsforI(cid:100)RF(h)canbesignificantlyworsebecausetheimpulse responses are highly nonlinear functions of the coefficients. For example, in the simple AR(1) model withcoefficientestimateÎ±thehth impulseresponseisÎ±h whichishighlynonlinearforevenmoderate (cid:98) (cid:98) horizonsh. Consequently, asymptotic approximations are less popular than bootstrap approximations. The most popular bootstrap approximation uses the recursive bootstrap (see Section 14.46) using the fit- ted VAR model and then calculates confidence intervals for the impulse responses with the percentile method. Anunfortunatefeatureofthischoiceisthatthepercentilebootstrapconfidenceintervalisbi- asedsincethenonlinearimpulseresponseestimatesarebiasedandthepercentilebootstrapaccentuates bias.",
    "page": 541,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 522 Someadvantagesoftheestimationmethodasdescribedisthatitproducesimpulseresponseesti- mateswhicharedirectlyrelatedtotheestimatedVAR(p)modelandareinternallyconsistentwithone another. The method is also numerically stable. It is efficient when the true process is a true VAR(p) with conditionally homoskedastic MDS innovations. When the true process is not a VAR(p) it can be thoughtofasanon-parametricestimatoroftheimpulseresponseifp islarge(orselectedappropriately inadata-dependentfashion,suchasbytheAIC). AdisadvantageofthisestimatoristhatitisahighlynonlinearfunctionoftheVARcoefficientestima- tors.Thereforethedistributionoftheimpulseresponseestimatorisunlikelytobewellapproximatedby thenormaldistribution. WhentheVAR(p)isnotthetrueprocessthenitispossiblethatthenonlinear transformationaccentuatesthemisspecificationbias. ImpulseresponsefunctionscanbecalculatedanddisplayedinStatausingtheirfcommand. The commandirf createisusedto calculateimpulse response functions andconfidenceintervals. The defaultconfidenceintervalsareasymptotic(deltamethod). Bootstrap(recursivemethod)standarder- rors can be substituted using the bs option. The command irf graph irf produces graphs of the impulseresponsefunctionalongwith95%asymptoticconfidenceintervals. Thecommandirf graph cirf produces the cumulative impulse response function. It may be useful to know that the impulse responseestimatesareunscaledsorepresenttheresponseduetoaone-unitchangeintheimpulsevari- able. A limitation of the Stata irf command is that there are limited options for standard error and confidenceintervalconstruction.Theasymptoticstandarderrorsarecalculatedusingthehomoskedas- ticformulanotthecorrectheteroskedasticformula. Thebootstrapconfidenceintervalsarecalculated usingthenormalapproximationbootstrapconfidenceinterval,theleastreliablebootstrapconfidence intervalmethod. Betteroptionssuchasthebias-correctedpercentileconfidenceintervalarenotpro- videdasoptions. 15.16 LocalProjectionEstimator JordÃ (2005)observedthattheimpulseresponsecanbeestimatedbyaleastsquarespredictivere- gression.ThekeyisTheorem15.12whichestablishedthatÎ =B ,theleadingcoefficientmatrixinthe h 1 h-steppredictiveregression. Themethodisasfollows. Foreachhorizonh estimateapredictiveregression(15.12)toobtainthe leadingcoefficientmatrixestimatorB(cid:98)1 . TheestimatorisI(cid:100)RF(h)=B(cid:98)1 andisknownasthelocalprojec- tionestimator. Theorem15.13showsthatthelocalprojectionimpulseresponseestimatorisasymptoticallynormal. Newey-West methods must be used for calculation of asymptotic standard errors since the regression errorsareseriallycorrelated. JordÃ (2005)speculatesthatthelocalprojectionestimatorwillbelesssensitivetomisspecification sinceitisastraightforwardlinearestimator. Thisisintuitivebutunclear",
    "page": 542,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". TheestimatorisI(cid:100)RF(h)=B(cid:98)1 andisknownasthelocalprojec- tionestimator. Theorem15.13showsthatthelocalprojectionimpulseresponseestimatorisasymptoticallynormal. Newey-West methods must be used for calculation of asymptotic standard errors since the regression errorsareseriallycorrelated. JordÃ (2005)speculatesthatthelocalprojectionestimatorwillbelesssensitivetomisspecification sinceitisastraightforwardlinearestimator. Thisisintuitivebutunclear. Theorem15.12reliesonthe assumptionthatY isaVAR(p)process,andfailsotherwise.ThusifthetrueprocessisnotaVAR(p)then t thecoefficientmatrixB in(15.12)doesnotcorrespondtothedesiredimpulseresponsematrixÎ and 1 h hencewillbemisspecified.Theaccuracy(inthesenseoflowbias)ofboththeconventionalandthelocal projectionestimatorreliesonp beingsufficientlylargethattheVAR(p)modelisagoodapproximation tothetrueinfinite-orderregression(15.4).Withoutaformaltheoryitisdifficulttoknowwhichestimator ismorerobustthantheother. Oneimplementationchallengeisthechoiceofp.Whilethemethodallowsforp tovaryacrosshori- zonhthereisnowell-establishedmethodforselectionoftheVARorderforpredictiveregressions.(Stan- dardselectioncriteriasuchasAICareinappropriateunderseriallycorrelatederrorsjustasconventional standarderrorsareinappropriate.) Thereforetheseeminglynaturalchoiceistousethesame p forall",
    "page": 542,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 523 horizonsandbasethischoiceontheone-stepVARmodelwhereAICcanbeusedformodelselection. Anadvantageofthelocalprojectionmethodisthatitisadirectestimatoroftheimpulseresponse andthuspossiblymorerobustthantheconventionalmethod. Itisalinearestimatorandthuslikelyto haveabetter-behavedasymptoticdistribution. Adisadvantageisthatthemethodreliesonaregression(15.12)thathasseriallycorrelatederrors.The latterarehighlycorrelatedatlonghorizonsandthisrenderstheestimatorimprecise. Localprojection estimatorstendtobelesssmoothandmoreerraticthanthoseproducedbytheconventionalestimator reflectingapossiblelackofprecision. 15.17 RegressiononResiduals If the innovations e were observed it would be natural to directly estimate the coefficients of the t multivariateWolddecomposition.Wewouldpickamaximumhorizonhandthenestimatetheequation Y t =Âµ+Î 1 e tâ1 +Î 2 e tâ2 +Â·Â·Â·+Î h e tâh +u t where â (cid:88) u t =e t + Î (cid:96)e tâ(cid:96). (cid:96)=h+1 Thevariables(e tâ1 ,...,e tâh )areuncorrelatedwithu t sotheleastsquaresestimatorofthecoefficientsis consistentandasymptoticallynormal. Sinceu isseriallycorrelatedtheNewey-Westmethodshouldbe t usedtocalculatestandarderrors. Inpracticetheinnovationse arenotobserved. Iftheyarereplacedbytheresidualse fromanesti- t (cid:98)t matedVAR(p)thenwecanestimatethecoefficientsbyleastsquaresappliedtotheequation Y t =Âµ+Î 1 e (cid:98)tâ1 +Î 2 e (cid:98)tâ2 +Â·Â·Â·+Î h e (cid:98)tâh +u (cid:98)t . ThisideaoriginatedwithDurbin(1960). This is a two-step estimator with generated regressors. (See Section 12.26.) The impulse response estimatorsareconsistentandasymptoticallynormalbutwithanon-standardcovariancematrixdueto thetwo-stepestimation. Conventional,robust,andNewey-Weststandarderrorsdonotaccountforthis withoutmodification. ChangandSakata(2007)proposedasimplifiedversionoftheDurbinregression.Noticethatforany horizonhwecanrewritetheWolddecompositionas Y t+h =Âµ+Î h e t +v t+h where hâ1 â (cid:88) (cid:88) v t = Î (cid:96)e tâ(cid:96) + Î (cid:96)e tâ(cid:96). (cid:96)=0 (cid:96)=h+1 The regressor e t is uncorrelated with v t+h . Thus Î h can be estimated by a regression of Y t+h on e t . Inpracticewecanreplacee bytheleastsquaresresiduale fromanestimatedVAR(p)toestimatethe t (cid:98)t regression Y t+h =Âµ+Î h e (cid:98)t +v (cid:98)t+h . (15.14) SimilartotheDurbinregressiontheChang-Sakataestimatorisatwo-stepestimatorwithagenerated regressor. However,asittakestheformstudiedinSection12.27itcanbeshownthattheChang-Sakata two-stepestimatorhasthesameasymptoticdistributionastheidealizedone-stepestimatorasife were t",
    "page": 543,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 524 observed. Thusthestandarderrorsdonotneedtobeadjustedforgeneratedregressorswhichisanad- vantage. TheerrorsareseriallycorrelatedsoNewey-Weststandarderrorsshouldbeused. Thevariance oftheerrorv t+h islargerthanthevarianceoftheerroru t intheDurbinregressionsotheChang-Sakata estimatormaybelessprecisethantheDurbinestimator. Chang and Sakata (2007) also point out the following implication of the FWL theorem. The least squares slope estimator in (15.14) is algebraically identical1 to the slope estimator B(cid:98)1 in a predictive regressionwithpâ1lags.ThustheChang-Sakataestimatorissimilartoalocalprojectionestimator. 15.18 OrthogonalizedShocks Wecanusetheimpulseresponsefunctiontoexaminehowtheinnnovationsimpactthetime-paths ofthevariables. Adifficultyininterpretation,however,isthattheelementsoftheinnovationvectore t arecontemporeneouslycorrelated.Thuse ande are(ingeneral)notindependent,soconsequentlyit jt it doesnotmakesensetotreate ande asfundamentalâshocksâ.Anotherwayofdescribingtheproblem jt it isthatitdoesnotmakesense,forexample,todescribetheimpactofe whileâholdingâe constant. jt it Thenaturalsolutionistoorthogonalizetheinnovationssothattheyareuncorrelatedandthenview theorthogonalizederrorsasthefundamentalâshocksâ.Recallthate ismeanzerowithcovariancematrix t Î£. WecanfactorÎ£intotheproductofanmÃm matrixB withitstransposeÎ£=BB (cid:48) . ThematrixB is called a âsquare rootâ of Î£. (See Section A.13.) Define Îµ = B â1e . The random vector Îµ has mean t t t zeroandcovariancematrixB â1Î£B â1(cid:48)=B â1BB (cid:48) B â1(cid:48)=I . TheelementsÎµ =(Îµ ,...,Îµ )aremutually m t 1t mt uncorrelated.Wecanwritetheinnovationsasafunctionoftheorthogonalizederrorsas e =BÎµ . (15.15) t t To distinguish Îµ from e we will typically call Îµ the âorthogonalized shocksâ or more simply as the t t t âshocksâandcontinuetocalle theâinnovationsâ. t Whenm>1thereisnotauniquesquarerootmatrixB sothereisnotauniqueorthogonalization. Themostcommonchoice(andwasoriginallyadvocatedbySims(1980))istheCholeskydecomposition (seeSectionA.16).ThissetsB tobelowertriangular,meaningthatittakestheform ï£® ï£¹ b 0 0 11 B= ï£° b 21 b 22 0 ï£» b b b 31 32 33 withnon-negativediagonalelements. WecanwritetheCholeskydecompositionofamatrix A asC = chol(A)whichmeansthat A=CC (cid:48) withC lowertriangular.Wethusset B=chol(Î£). (15.16) Equivalently,theinnovationsarerelatedtotheorthogonalizedshocksbytheequations e =b Îµ 1t 11 1t e =b Îµ +b Îµ 2t 21 1t 22 2t e =b Îµ +b Îµ +b Îµ . 3t 31 1t 31 2t 33 3t Thisstructureisrecursive. Theinnovatione isafunctiononlyofthesingleshockÎµ . Theinno- 1t 1t vatione isafunctionoftheshocksÎµ andÎµ ,andtheinnovatione isafunctionofallthreeshocks. 2t 1t 2t 3t 1Technically,ifthesamplelengthsareadjusted.",
    "page": 544,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 525 AnotherwayoflookingatthestructureisthatthefirstshockÎµ affectsallthreeinnovation,thesecond 1t shockÎµ affectse ande ,andthethirdshockÎµ onlyaffectse . 2t 2t 3t 3t 3t Arecursivestructureisanexclusionrestriction.TherecursivestructureexcludesÎµ andÎµ contem- 2t 3t poreneouslyaffectinge ,andexcludesÎµ contemporeneouslyaffectinge . 1t 3t 2t WhenusingtheCholeskydecompositiontherecursivestructureisdeterminedbytheorderingofthe variablesinthesystem. Theordermattersandisthekeyidentifyingassumption. Wewillreturntothis issuelater. Finally,wementionthatthesystem(15.15)isequivalenttothesystem Ae =Îµ (15.17) t t where A=B â1islowertriangularwhenB islowertriangular. Therepresentation(15.15)ismoreconve- nient,however,formostofourpurposes. 15.19 OrthogonalizedImpulseResponseFunction Wehavedefinedtheimpulseresponsefunctionasthechangeinthetimetprojectionofthevariables Y t+h duetotheinnovatione t .Aswediscussedintheprevioussectionsincetheinnovationsarecontem- poreneouslycorrelateditmakesbettersensetofocusonchangesduetotheorthogonalizedshocksÎµ . t Consequentlywedefinetheorthgonalizedimpulseresponsefunction(OIRF)as â OIRF(h)= âÎµ(cid:48) P t [Y t+h ]. t WecanwritethemultivariateWoldrepresentationas â â (cid:88) (cid:88) Y t =Âµ+ Î (cid:96)e tâ(cid:96) =Âµ+ Î (cid:96)BÎµ tâ(cid:96) (cid:96)=0 (cid:96)=0 whereB isfrom(15.16).Wededucethat OIRF(h)=Î B=IRF(h)B. h Thisisthenon-orthogonalizedimpulseresponsematrixmultipliedbythematrixsquarerootB. WritetherowsofthematrixÎ as h ï£® Î¸(cid:48) ï£¹ 1h Î = h ï£° ï£» Î¸(cid:48) mh andthecolumnsofthematrixB asB=[b ,...,b ].Wecanseethat 1 m OIRF (h)=[Î B] =Î¸(cid:48) b . ij h ij ih j Therearem2suchresponsesforeachhorizonh. Thecumulativeorthogonalizedimpulseresponsefunction(COIRF)is h h (cid:88) (cid:88) COIRF(h)= OIRF((cid:96))= Î (cid:96)B. (cid:96)=1 (cid:96)=1",
    "page": 545,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 526 15.20 OrthogonalizedImpulseResponseEstimation WehavediscussedestimationofthemovingaveragematricesÎ (cid:96).WeneedanestimatorofB. WefirstestimatetheVAR(p)modelbyleastsquares. Thisgivesusthecoefficientmatrices A(cid:98) andthe error covariance matrix Î£ (cid:98). From the latter we apply the Cholesky decomposition B(cid:98) =chol (cid:161)Î£ (cid:98) (cid:162) so that (cid:48) Î£ (cid:98) =B(cid:98)B(cid:98) .(SeeSectionA.16forthealgorithm.)Theorthogonalizedimpulseresponseestimatorsare O(cid:129)IRF(h)=Î (cid:98)h B(cid:98) =Î¸ (cid:98) i (cid:48) h b(cid:98)j . TheestimatorO(cid:129)IRF(h)isanonlinearfunctionof A(cid:98) andÎ£ (cid:98). Itisasymptoticallynormallydistributed by the delta method. This allows for explicit calculation of asymptotic standard errors. These can be usedtoformasymptoticconfidenceintervalsfortheimpulseresponses. Asdiscussedearlier,theasymptoticapproximationscanbequitepoor. Consequentlybootstrapap- proximationsaremorewidelyusedthanasymptoticmethods. OrthogonalizedimpulseresponsefunctionscanbedisplayedinStatausingtheirfcommand. The command irf graph oirf produces graphs of the orthogonalized impulse response function along with95%asymptoticconfidenceintervals. Thecommandirf graph coirfproducesthecumulative orthogonalizedimpulseresponsefunction. ItmayalsobeusefultoknowthattheOIRFarescaledfora one-standarddeviationshocksotheimpulseresponserepresentstheresponseduetoaone-standard- deviationchangeintheimpulsevariable. Asdiscussedearlier,theStatairfcommandhaslimitedop- tionsforstandarderrorandconfidenceintervalconstruction. Theasymptoticstandarderrorsarecal- culatedusingthehomoskedasticformulanotthecorrectheteroskedasticformula. Thebootstrapconfi- denceintervalsarecalculatedusingthenormalapproximationbootstrapconfidenceinterval. 15.21 Illustration Toillustrateweusethethree-variablesystemfromSection15.13. Weusetheordering(1)realGDP growthrate, (2)inflationrate, (3)Federalfundsinterestrate. Wediscussthechoicelaterwhenwedis- cuss identification. We use the estimated VAR(6) and calculate the orthogonalized impulse response functionsusingthestandardVARestimator. InFigure15.1wedisplaytheestimatedorthogonalizedimpulseresponseoftheGDPgrowthratein responsetoaonestandarddeviationincreaseinthefederalfundsrate. Theleftplotshowstheimpulse responsefunctionandthemiddleplotthecumulativeimpulseresponsefunction. Aswediscussedear- liertheinterpretationoftheimpulseresponseandthecumulativeimpulseresponsedependsonwhether thevariableenterstheVARindifferencesorinlevels. Inthiscase,GDPgrowthisthefirstdifferenceof thenaturallogarithm.Thustheleftplot(theimpulseresponsefunction)showstheeffectofinterestrates onthegrowthrateofGDP.Themiddleplot(thecumulativeimpulseresponse)showstheeffectonthe log-levelofGDP.TheleftplotshowsthattheGDPgrowthrateisnegativelyaffectedinthesecondquarter afteraninterestrateincrease(adropofabout0.2%,non-annualized),andthenegativeeffectscontinue for several quarters following",
    "page": 546,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Inthiscase,GDPgrowthisthefirstdifferenceof thenaturallogarithm.Thustheleftplot(theimpulseresponsefunction)showstheeffectofinterestrates onthegrowthrateofGDP.Themiddleplot(thecumulativeimpulseresponse)showstheeffectonthe log-levelofGDP.TheleftplotshowsthattheGDPgrowthrateisnegativelyaffectedinthesecondquarter afteraninterestrateincrease(adropofabout0.2%,non-annualized),andthenegativeeffectscontinue for several quarters following. The middle plot shows the effect on the level of GDP measured as per- centagechanges.ItshowsthataninterestrateincreasecausesGDPtofallforabout8quarters,reducing GDPbyabout0.6%. 15.22 ForecastErrorDecomposition An alternative tool to investigate an estimated VAR is the forecasterrordecomposition which de- composesmulti-stepforecasterrorvariancesbythecomponentshocks. Theforecasterrordecomposi- tionindicateswhichshockscontributetowardsthefluctuationsofeachvariableinthesystem.",
    "page": 546,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 527 .1 .3 0 0 -.2 .2 -.4 -.1 -.6 .1 -.2 -.8 -.3 -1 0 0 4 8 12 16 20 0 4 8 12 16 20 0 4 8 12 16 20 Quarters Quarters Quarters (a)ImpulseResponseFunction (b)CumulativeIRF (c)ErrorDecomposition Figure15.1:ResponseofGDPGrowthtoOrthogonalizedFedFundsShock Itisdefinedasfollows.Takethemovingaveragerepresentationoftheith variableY i,t+h writtenasa functionoftheorthogonalizedshocks â Y i,t+h =Âµ i + (cid:88) Î¸ i ((cid:96)) (cid:48) BÎµ t+hâ(cid:96). (cid:96)=0 ThebestlinearforecastofY t+h attimet is â Y i,t+h|t =Âµ i + (cid:88) Î¸ i ((cid:96)) (cid:48) BÎµ t+hâ(cid:96). (cid:96)=h Theh-stepforecasterroristhedifference hâ1 Y i,t+h âY i,t+h|t = (cid:88) Î¸ i ((cid:96)) (cid:48) BÎµ t+hâ(cid:96). (cid:96)=0 Thevarianceofthisforecasterroris hâ1 hâ1 var (cid:163) Y i,t+h âY i,t+h|t (cid:164)= (cid:88) var (cid:163)Î¸ i ((cid:96)) (cid:48) BÎµ t+hâ(cid:96) (cid:164)= (cid:88) Î¸ i ((cid:96)) (cid:48) BB (cid:48)Î¸ i ((cid:96)). (15.18) (cid:96)=0 (cid:96)=0 Toisolatethecontributionofthe jth shock,noticethat e =BÎµ =b Îµ +Â·Â·Â·+b Îµ . t t 1 1t m mt Thusthecontributionofthe jth shockisb Îµ . NowimaginereplacingBÎµ inthevariancecalculation j jt t bythe jth contributionb Îµ .Thisis j jt hâ1 hâ1 var (cid:163) Y it+h âY i,t+h|t (cid:164)= (cid:88) var (cid:163)Î¸ i ((cid:96)) (cid:48) b j Îµ jt+hâ(cid:96) (cid:164)= (cid:88)(cid:161)Î¸ i ((cid:96)) (cid:48) b j (cid:162)2 . (15.19) (cid:96)=0 (cid:96)=0 Examining(15.18)andusingB=[b ,...,b ]wecanwrite(15.18)as 1 m m hâ1 var (cid:163) Y i,t+h âY i,t+h|t (cid:164)= (cid:88) (cid:88)(cid:161)Î¸ i ((cid:96)) (cid:48) b j (cid:162)2 . (15.20) j=1(cid:96)=0",
    "page": 547,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 528 Theforecasterrordecompositionisdefinedastheratioofthe jth contributiontothetotalwhichis theratioof(15.19)to(15.20): (cid:80)hâ1(cid:161)Î¸ ((cid:96)) (cid:48) b (cid:162)2 FE (h)= (cid:96)=0 i j . ij (cid:80)m (cid:80)hâ1(cid:161)Î¸ ((cid:96)) (cid:48) b (cid:162)2 j=1 (cid:96)=0 i j The FE (h) lies in [0,1] and varies across h. Small values indicate that Îµ contributes only a small ij jt amount to the variance of Y . Large values indicate that Îµ contributes a major amount of the vari- it jt anceofÎµ . it Aforecasterrordecompositionrequiresorthogonalizedinnovations.Thereisnonon-orthogonalized version. TheforecasterrordecompositioncanbecalculatedanddisplayedinStatausingtheirfcommand. The command irf graph fevd produces graphs of the forecast error decomposition along with 95% asymptoticconfidenceintervals. Toillustrate,inFigure15.1(rightplot)wedisplaytheestimatedforecasterrordecompositionofthe GDPgrowthrateduetothefederalfundsrate. Thisshowsthecontributionofmovementsinthefederal fundsratetowardsfluctuationsinGDPgrowth.Theestimatedeffectisabout15%atlonghorizons.This isasmallbutimportantshareofthevarianceofGDPgrowth.Combinedwiththeimpulseresponsefunc- tionswelearntwolessons: (1)Monetarypolicy(movementsinthefederalfundsrate)canmeaningfully affectGDPgrowth;(2)MonetarypolicyonlyaccountsforasmallcomponentoffluctuationsinU.S.GDP. 15.23 IdentificationofRecursiveVARs As we have discussed a common method to orthogonalize the VAR errors is the lower triangular Cholesky decomposition which implies a recursive structure. The ordering of the variables is critical thisrecursivestructure. Unlesstheerrorsareuncorrelateddifferentorderingswillleadtodifferentim- pulseresponsefunctionsandforecasterrordecompositions.Theorderingmustbeselectedbytheuser; thereisnodata-dependentchoice. Inorderforimpulseresponsesandforecasterrordecompositionstobeinterpretedcausallytheor- thogonalizationmustbeidentifiedbytheuserbasedonastructuraleconomicargument. Thechoiceis similartotheexclusionrestrictionsnecessaryforspecificationofaninstrumentalvariablesregression. Byorderingthevariablesrecursivelyweareeffectivelyimposingexclusionrestrictions.Recallthatinour empirical example we usedthe ordering: (1) real GDP growthrate, (2)inflationrate, (3) Federal funds interestrate. ThismeansthatintheequationforGDPweexcludedthecontemporeneousinflationrate andinterestrate,andintheequationforinflationweexcludedthecontemporenousinterestrate.These areexclusionrestrictions.Aretheyjustified? Oneapproachistoorderfirstthevariableswhicharebelievedtobecontemporaneouslyaffectedby thefewestnumberofshocks.Onewayofthinkingaboutitisthattheyarethevariableswhichareâmost stickyâwithinaperiod. Thevariableslistedlastarethosewhicharebelievedtobecontemporanously affectedbythegreatestnumberofshocks",
    "page": 548,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Thevariableslistedlastarethosewhicharebelievedtobecontemporanously affectedbythegreatestnumberofshocks. Thesearetheoneswhichareabletorespondwithinasingle periodtotheshocksoraremostflexible.Inourexamplewelistedoutputfirst,pricessecondandinterest rateslast.Thisisconsistentwiththeviewthatoutputiseffectivelypre-determined(withinaperiod)and doesnot(withinaperiod)respondtopriceandinterestratemovements. Pricesareallowedtorespond withinaperiodinresponsetooutputchangesbutnotinresponsetointerestratechanges. Thelatter couldbejustifiedifinterestratechangesaffectinvestmentdecisionsbutthelattertakeatleastoneperiod toimplement. Bylistingthefederalfundsratelastthemodelallowsmonetarypolicytorespondwithin aperiodtocontemporeneousinformationaboutoutputandprices. In general, this line of reasoning suggests that production measures should be listed first, goods",
    "page": 548,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 529 pricessecond,andfinancialpriceslast.Thisreasoningismorecrediblewhenthetimeperiodsareshort, andlesscredibleforlongertimeperiods. Furtherjustificationsforpossiblerecursiveorderingscaninclude:(1)informationdelays;(2)imple- mentationdelays; (3) institutions; (4)marketstructure; (5)homogeneity; (6) imposingestimatesfrom othersources. Inmostcasessuchargumentscanbemadebutwillbeviewedasdebatableandrestric- tive.Inanysituationitisbesttobeexplicitaboutyourchoiceandreasoning. Returningtotheempiricalillustrationitisfairlyconventionaltoorderthefedfundsratelast. This allows the fed funds rate to respond to contemporeneous information about output and price growth and identifies the fed funds policy shock by the assumption that it does not have a contemporenous impactontheothervariables.Itisnotclear,however,howtoordertheothertwovariables.Forsimplicity consideratraditionalaggregatesupply/aggregatedemandmodelofthedeterminationofoutputandthe pricelevel. Iftheaggregatesupplycurveisperfectlyinelasticintheshortrun(onequarter)thenoutput is effectively fixed (sticky) so changes in aggregate demand affect prices but not output. Changes in aggregate supply affect both output and prices. Thus we would want to order GDP first and inflation second. ThischoicewouldidentifytheGDPerrorastheaggregatesupplyshock. Thisistheordering usedinourexample. Incontrast,supposethattheaggregatesupplycurveisperfectlyelasticintheshortrun.Thenprices arefixedandoutputisflexible. Changesinaggregatesupplyaffectbothpriceandoutputbutchanges in aggregate demand only affect output. In this case we would want to order inflation first and GDP second. Thischoiceidentifiestheinflationerrorastheaggregatesupplyshock,theoppositecasefrom thepreviousassumption! If the choice between perfectly elastic and perfectly inelastic aggregate supply is not credible then the supply and demand shocks cannot be separately identified based on ordering alone. In this case thefullsetofimpulseresponsesanderrordecompositionsarenotidentified.However,asubsetmaybe identified. Ingeneral,iftheshockscanbeorderedingroupsthenwecanidentifyanyshockforwhicha grouphasasinglevariable.Inourexample,considertheordering(1)GDPandinflation;(2)federalfunds rate. ThismeansthatthemodelassumesthatGDPandinflationdonotcontemporeneouslyrespondto interestratemovementsbutnootherrestrictionsareimposed. Inthiscasethefedfundspolicyshockis identified. Thismeansthatimpulseresponsesofallthreevariableswithrespecttothepolicyshockare identifiedandsimilarlytheforecasterrorcompositionoftheeffectofthefedfundsshockoneachvari- ableisidentified.ThesecanbeestimatedbyaVARusingtheordering(GDP,inflation,federalfundsrate) asdoneinourexampleorusingtheordering(inflation,GDP,federalfundsrate). Bothchoiceswilllead tothesameestimatedimpulseresponsesasdescribed. Theremainingimpulseresponses(responsesto GDPandinflationshocks),however,willdifferacrossthesetwoorderings",
    "page": 549,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Bothchoiceswilllead tothesameestimatedimpulseresponsesasdescribed. Theremainingimpulseresponses(responsesto GDPandinflationshocks),however,willdifferacrossthesetwoorderings. 15.24 OilPriceShocks Tofurtherillustratetheidentificationofimpulseresponsefunctionsbyrecursivestructuralassump- tions we repeat here some of the analysis from Kilian (2009). His paper concerns the identification of thefactorsaffectingcrudeoilprices,inparticularseparatingsupplyanddemandshocks. Thegoalisto determinehowoilpricesrespondtoeconomicshocksandhowtheresponsesdifferbythetypeofshock. To answer this question Kilian uses a three-variable VAR with monthly measures of global oil pro- duction,globaleconomicactivity,andtheglobalpriceofcrudeoilfor1973m2-2007m12.Heusesglobal variables since the price of crude oil is globally determined. One innovation in the paper is that Kil- ian develops a new index of global economic activity based on ocean freight rates. His motivation is thatshippingratesaredirectlyrelatedtotheglobaldemandforindustrialcommodities. Thisdatasetis postedonthetextbookwebpageasKilian2009.",
    "page": 549,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 530 Kilianarguesthatthesethreevariablesaredeterminedbythreeeconomicshocks: oilsupply,aggre- gatedemand,andoildemand. Hesuggeststhatoilsupplyshocksshouldbethoughtofasdisruptions in production, processing, or shipping. Aggregate demand is global economic activity. Kilian also ar- guesthatoildemandshocksareprimarilyduetotheprecautionarydemandforoildrivenbyuncertainty aboutfutureoilsupplyshortfalls. ToidentifytheshocksKilianmakesthefollowingexclusionrestrictions. First, heassumesthatthe short-run(onemonth)supplyofcrudeoilisinelasticwithrespecttoprice. Equivalently,oilproduction takesatleastonemonthtorespondtopricechanges.Thisrestrictionisbelievedtobeplausiblebecause oftechnologicalfactorsincrudeoilproduction.Itiscostlytoopennewoilfields;anditisnearlyimpossi- bletocapanoilwelloncetapped. Second,Kilianassumesthatintheshort-run(onemonth)globalreal economic activity does not respond to changes in oil prices (due to shocks specific to the oil market), while economic activity is allowed to respond to oil production shocks. This assumption is viewed by Kilianasplausibleduetothesluggishnessintheresponseofeconomicactivitytopricechanges. Crude oilprices,however,areallowedtorespondsimultaneouslytoallthreeshocks. Kilianâsidentificationstrategyissimilartothatdescribedintheprevioussectionforthesimpleag- gregatedemand/aggregatesupplymodel. Theseparationofsupplyanddemandshocksisachievedby exclusionrestrictionswhichimplyshort-runinelasticities.Theplausibilityoftheseassumptionsrestsin partonthemonthlyfrequencyofthedata.Whileitisplausiblethatoilproductionandeconomicactivity maynotrespondwithinonemonthtopriceshocks, itismuchlessplausiblethatthereisnoresponse forafullquarter.Kilianâsleastconvincingidentifyingassumption(inmyopinion)istheassumptionthat economicactivitydoesnotrespondsimultaneouslytooilpricechanges. Whilemucheconomicactivity ispre-plannedandhencesluggishtorespond,someeconomicactivity(recreationaldriving,forexam- ple)mayimmediatelyrespondtopricechanges. Kilianestimatesthethree-variableVARusing24lagsandcalculatestheorthogonalizedimpulsere- sponse functions using the ordering implied by these assumptions. He does not discuss the choice of 24 lags but presumably this is intended to allow for flexible dynamic responses. If the AIC is used for modelselection,threelagswouldbeselected.FortheanalysisreportedhereIused4lags.Theresultsare qualitativelysimilartothoseobtainedusing24lags.Foreaseofinterpretationoilsupplyisenteredneg- atively(multipliedbyâ1)sothatallthreeshocksarescaledtoincreaseoilprices. Theimpulseresponse functionsforthepriceofcrudeoilaredisplayedinFigure15.2for1-24months.Panel(a)displaysthere- sponseofcrudeoilpricesduetoanoilsupplyshock,panel(b)displaystheresponseduetoanaggregate demand shock, and panel (c) displays the response due to an oil-demand shock. Notice that all three figureshavebeendisplayedusingthesamey-axisscalingssothatthefiguresarecomparable",
    "page": 550,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Theimpulseresponse functionsforthepriceofcrudeoilaredisplayedinFigure15.2for1-24months.Panel(a)displaysthere- sponseofcrudeoilpricesduetoanoilsupplyshock,panel(b)displaystheresponseduetoanaggregate demand shock, and panel (c) displays the response due to an oil-demand shock. Notice that all three figureshavebeendisplayedusingthesamey-axisscalingssothatthefiguresarecomparable. Whatisnoticeableaboutthefiguresishowdifferentlycrudeoilpricesrespondtothethreetypesof shocks. Panel(a)showsthatoilpricesareonlyminimallyaffectedbyoilproductionshocks. Thereisan estimatedsmallshorttermincreaseinoilprices,butitisnotstatisticallysignificantanditreverseswithin oneyear. Panel(b)showsthatoilpricesaresignificantlyaffectedbyaggregatedemandshocksandthe effect cumulatively increases over two years. This is not surprising. Economic activity relies on crude oilandeconomicactivityisseriallycorrelated. Panel(c)showsthatoilpricesarestronglyimmediately affected by oil demand shocks but the effect attenuates over time. This is a reverse pattern than that foundforaggregatedemandshocks. TheKilian(2009)paperisanexcellentexampleofhowrecursiveorderingscanbeusedtoidentifyan orthogonalizedVARthroughacarefuldiscussionofthecausalsystemandtheuseofmonthlyobserva- tions.",
    "page": 550,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 531 10 10 10 8 8 8 6 6 6 4 4 4 2 2 2 0 0 0 -2 -2 -2 0 4 8 12 16 20 24 0 4 8 12 16 20 24 0 4 8 12 16 20 24 Months Months Months (a)SupplyShock (b)AggregateDemandSchock (c)OilDemandSchock Figure15.2:ResponseofOilPricestoOrthogonalizedShocks 15.25 StructuralVARs Recursive models do not allow for simultaneity between the elements of e and thus the variables t Y cannotbecontemporeneouslyendogenous. Thisishighlyrestrictiveandmaynotcrediblydescribe t manyeconomicsystems.Thereisageneralpreferenceintheeconomicscommunityforstructuralvec- tor autoregressive models (SVARs) which use alternative identification restrictions which do not rely exclusivelyonrecursiveness.TwopopularcategoriesofstructuralVARmodelsarethosebasedonshort- run(contemporeneous)restrictionsandthosebasedonlong-run(cumulative)restrictions. Inthissec- tionwereviewSVARsbasedonshort-runrestrictions. WhenweintroducedmethodstoorthogonalizetheVARerrorswepointedoutthatwecanrepresent therelationshipbetweentheerrorsandshocksusingeithertheequatione =BÎµ (15.15)ortheequation t t Ae =Îµ (15.17). Equation(15.15)writestheerrorsasafunctionoftheshocks. Equation(15.17)writes t t theerrorsasasimultaneoussystem.Abroaderclassofmodelscanbecapturedbytheequationsystem Ae =BÎµ (15.21) t t where(inthe3Ã3case) ï£® ï£¹ ï£® ï£¹ 1 a a b b b 12 13 11 12 13 A= ï£° a 21 1 a 23 ï£», B= ï£° b 21 b 22 b 23 ï£». (15.22) a a 1 b b b 31 32 31 32 33 (Note:ThismatrixAhasnothingtodowiththeregressioncoefficientmatrixA.Iapologizeforthedouble useof A,butIusethenotation(15.21)tobeconsistentwiththenotationelsewhereintheliterature.) Writtenout, e =âa e âa e +b Îµ +b Îµ +b Îµ 1t 12 2t 13 3t 11 1t 12 2t 13 3t e =âa e âa e +b Îµ +b Îµ +b Îµ 2t 21 1t 23 3t 21 1t 22 2t 23 3t e =âa e âa e +b Îµ +b Îµ +b Îµ . 3t 31 1t 32 2t 31 1t 32 2t 33 3t The diagonal elements of the matrix A are set to 1 as normalizations. This normalization allows the shocksÎµ tohaveunitvariancewhichisconvenientforimpulseresponsecalculations. it The system as written is under-identified. In this three-equation example, the matrix Î£ provides onlysixmoments,buttheabovesystemhas15freeparameters! Toachieveidentificationweneednine restrictions.",
    "page": 551,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 532 Inmostapplications,itiscommontostartwiththerestrictionthatforeachcommonnon-diagonal elementof AandB atmostonecanbenon-zero.Thatis,foranypairi (cid:54)=j,eitherb =0ora =0. ji ji WewillillustratebyusingasimplifiedversionofthemodelemployedbyBlanchardandPerotti(2002) whowereinterestedindecomposingtheeffectsofgovernmentspendingandtaxesonGDP.Theypro- posedathree-variablesystemconsistingofrealgovernmentspending(netoftransfers),realtaxrevenues (includingtransferpaymentsasnegativetaxes),andrealGDP.Allvariablesaremeasuredinlogs. They startwiththerestrictionsa =a =b =b =b =b =0,or 21 12 31 32 13 23 ï£® ï£¹ ï£® ï£¹ 1 0 a b b 0 13 11 12 A= ï£° 0 1 a 23 ï£», B= ï£° b 21 b 22 0 ï£». a a 1 0 0 b 31 32 33 ThisisdonesothatthattherelationshipbetweentheshocksÎµ andÎµ istreatedasreduced-formbut 1t 2t the coefficients in the A matrix can be interpreted as contemporeneous elasticities between the vari- ables. For example, a is the within-quarter elasticity of tax revenue with respect to GDP, a is the 23 31 within-quarterelasticityofGDPwithrespecttogovernmentspending,etc. We just described six restrictions while nine are required for identification. Blanchard and Perotti (2002)madeastrongcasefortwoadditionalrestrictions. First,thewithin-quarterelasticityofgovern- mentspendingwithrespecttoGDPiszero, a =0. Thisisbecausegovernmentfiscalpolicydoesnot 13 (andcannot)respondtonewsaboutGDPwithinthesamequarter. Sincetheauthorsdefinedgovern- ment spending as net of transfer payments there is no âautomatic stabilizerâ component of spending. Second, the within-quarter elasticity of tax revenue with respect to GDP can be estimated from exist- ingmicroeconometricstudies. Theauthorssurveytheavailableliteratureandseta =â2.08. Tofully 23 identify the model we need one final restriction. The authors argue that there is no clear case for any specificrestriction,andsoimposearecursiveB matrix(settingb =0)andexperimentwiththealter- 12 native b =0, finding that the two specifications are near-equivalent since the two shocks are nearly 21 uncorrelated.Insummarytheestimatedmodeltakestheform ï£® ï£¹ ï£® ï£¹ 1 0 0 b 0 0 11 A= ï£° 0 1 â2.08 ï£», B= ï£° b 21 b 22 0 ï£». a a 1 0 0 b 31 32 33 BlanchardandPerotti(2002)makeuseofbothmatrices A andB. Otherauthorsuseeitherthesim- plerstructureAe =Îµ ore =BÎµ .Ingeneral,eitherofthetwosimplerstructuresaresimplertocompute t t t t andinterpret. Takingthevarianceofthevariablesoneachsideof(15.21)wefind AÎ£A (cid:48)=BB (cid:48) . (15.23) Thisisasystemofquadraticequationsinthefreeparameters. Ifthemodel isjustidentifieditcanbe solved numerically to find the coefficients of A and B given Î£. Similarly, given the least squares error covariancematrixÎ£ (cid:98)wecannumericallysolveforthecoefficientsof A(cid:98) andB(cid:98). Whilemostapplicationsusejust-identifiedmodels,ifthemodelisover-identified(iftherearefewer freeparametersthanestimatedcomponentsofÎ£)thenthecoefficientsof A(cid:98) andB(cid:98) canbefoundusing minimum distance",
    "page": 552,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Ifthemodel isjustidentifieditcanbe solved numerically to find the coefficients of A and B given Î£. Similarly, given the least squares error covariancematrixÎ£ (cid:98)wecannumericallysolveforthecoefficientsof A(cid:98) andB(cid:98). Whilemostapplicationsusejust-identifiedmodels,ifthemodelisover-identified(iftherearefewer freeparametersthanestimatedcomponentsofÎ£)thenthecoefficientsof A(cid:98) andB(cid:98) canbefoundusing minimum distance. The implementation in Stata uses MLE (which simultaneously estimates the VAR coefficients). Thelatterisappropriatewhenthemodeliscorrectlyspecified(includingnormality)but otherwiseanunclearchoice. Giventheparameterestimatesthestructuralimpulseresponsefunctionis S(cid:129)IRF(h)=Î (cid:98)(h)A(cid:98) â1 B(cid:98).",
    "page": 552,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 533 Thestructuralforecasterrordecompositionsarecalculatedasbeforewithb replacedbythe jthcolumn j â1 of A(cid:98) B(cid:98). ThestructuralimpulseresponsesarenonlinearfunctionsoftheVARcoefficientandcovariancema- trixestimatorssobythedeltamethodareasymptoticallynormal. Thusasymptoticstandarderrorscan becalculated(usingnumericalderivativesifconvenient). Asfororthogonalizedimpulseresponsesthe asymptoticnormalapproximationisunlikelytobeagoodapproximationsobootstrapmethodsarean attractivealternative. StructuralVARsshouldbeinterpretedsimilarlytoinstrumentalvariableestimators. Theirinterpre- tationreliesonvalidexclusionrestrictionswhichcanonlybejustifiedbyexternalinformation. WereplicateasimplifiedversionofBlanchard-Perotti(2002).Weuse2quarterlyvariablesfromFRED- QDfor1959-2017: realGDP(gdpc1),realtaxrevenue(fgrecptx),andrealgovernmentspending(gcec1), allinnaturallogarithms. UsingtheAICforlaglengthselectionweestimateVARsfromonetoeightlags andselectaVAR(5). Themodelalsoincludesalinearandquadraticfunctionoftime3. Theestimated structuralimpulseresponsesofthethreevariableswithrespecttothegovernmentspendingshockare displayedinFigure15.3,andtheimpulseresponseswithrespecttothetaxrevenueshockaredisplayed inFigure15.4.TheestimatedimpulseresponsesaresimilartothosereportedbyBlanchard-Perotti. InFigure15.3weseethattheeffectofagovernmentspendingshockispersistent,increasinggovern- mentspendingabout1%forthefour-yearhorizon. Theeffectontaxrevenueisminimal. Theeffecton GDPispositive,small(around0.25%),butpersistent. 2 2 2 1.5 1.5 1.5 1 1 1 .5 .5 .5 0 0 0 -.5 -.5 -.5 -1 -1 -1 0 2 4 6 8 10 12 14 16 0 2 4 6 8 10 12 14 16 0 2 4 6 8 10 12 14 16 Quarters Quarters Quarters (a)Spending (b)Taxes (c)GDP Figure15.3:ResponsetoaGovernmentSpendingShock In Figure 15.4 we see that the effect of a tax revenue shock is quite different. The initial effect on tax revenue is high but diminishes to zero by about two years. The effect on government spending is mildlynegative4. TheeffectonGDPisnegativeandpersistent,andmoresubstantialthantheeffectof aspendingshock,reachingaboutâ0.5%atsixquarters. Together,theimpulseresponseestimatesshow thatchangesingovernmentspendingandtaxrevenuehavemeaningfuleconomicimpacts. Increased spendinghasapositiveeffectonGDPwhileincreasedtaxeshasanegativeeffect. TheBlanchard-Perotti(2002)paperisanexcellentexampleofhowcredibleexclusionrestrictionscan beusedtoidentifyanon-recursivestructuralsystemtohelpansweranimportanteconomicquestion. 2Thesearesimilarto,butnotthesameas,thevariablesusedbyBlanchardandPerotti. 3Theauthorsdetrendtheirdatausingaquadraticfunctionoftime. BytheFWLTheoremthisisequivalenttoincludinga quadraticintimeintheregression. 4Theestimatednegativeeffectisdifficulttoexplain,andwasnotdiscussedinBlanchard-Perotti.",
    "page": 553,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 534 2 2 2 1.5 1.5 1.5 1 1 1 .5 .5 .5 0 0 0 -.5 -.5 -.5 -1 -1 -1 0 2 4 6 8 10 12 14 16 0 2 4 6 8 10 12 14 16 0 2 4 6 8 10 12 14 16 Quarters Quarters Quarters (a)Spending (b)Taxes (c)GDP Figure15.4:ResponsetoaTaxRevenueShock Thewithin-quarterexogeneityofgovernmentspendingiscompellingandtheuseofexternalinforma- tiontofixtheelasticityoftaxrevenuewithrespecttoGDPisclever. Structural vector autoregressions can be estimated in Stata using the svar command. Short-run restrictions ofthe form (15.21) can be imposed using the aeq and beq options. Structural impulse re- sponsescanbedisplayedusingirf graph sirfandstructuralforecasterrordecompositionsusingirf graph sfevd. Unfortunately,Statadoesnotprovideaconvenientwaytodisplaycumulativestructural impulseresponsefunctions. Thesamelimitationsforstandarderrorandconfidenceintervalconstruc- tioninStataholdforstructuralimpulseresponsesasfornon-structuralimpulseresponses. 15.26 IdentificationofStructuralVARs ThecoefficientmatricesAandB in(15.21)areidentifiediftheycanbeuniquelysolvedfrom(15.23). Thisisasetofm(m+1)/2uniqueequationssothetotalnumberoffreecoefficientsin A andB cannot belargerthanm(m+1)/2,e.g.,6whenm=3. Thisistheorderconditionforidentification. Itisneces- sary,butnotsufficient. Itiseasytowritedownrestrictionswhichsatisfytheorderconditionbutdonot produceanidentifiedsystem. Itisdifficulttoseeifthesystemisidentifiedsimplybylookingattherestrictions(exceptintherecur- sivecase,whichisrelativelystraightforwardtoidentify). Anintuitivewayofverifyingidentificationisto useourknowledgeofinstrumentalvariables. Wecanidentifytheequationssequentially,oneatatime, orinblocks,usingthemetaphorofinstrumentalvariables. The general technique is as follows. Start by writing out the system imposing all restrictions and absorbingthediagonalelementsofB intotheshocks(sothattheyarestilluncorrelatedbuthavenon- unitvariances).FortheBlanchard-Perotti(2002)example,thisis e =Îµ 1t 1t e =2.08e +b Îµ +Îµ 2t 3t 21 1t 2t e =âa e âa e +Îµ . 3t 31 1t 32 2t 3t Take the equations one at a time and ask if they can be estimated by instrumental variables using the excluded variables as instruments. Once an equation has been verified as identified then its shock is identifiedandcanbeusedasaninstrumentsinceitisuncorrelatedwiththeshocksintheotherequa- tions.",
    "page": 554,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 535 Inthisexampletaketheequationsasordered. Thefirstequationisidentifiedastherearenocoef- ficientstoestimate. ThusÎµ isidentified. Forthesecondequationthereisonefreeparameterwhich 1t canbeestimatedbyleastsquaresofe â2.08e onÎµ ,whichisvalidsinceÎµ andÎµ areuncorrelated. 2t 3t 1t 1t 2t ThisidentifiesthesecondequationandtheshockÎµ . Thethirdequationhastwofreeparametersand 2t twoendogenousregressorssoweneedtwoinstruments. WecanusetheshocksÎµ andÎµ astheyare 1t 2t uncorrelatedwithÎµ andarecorrelatedwiththevariablese ande . Thusthisequationisidentified. 3t 1t 2t Wededucethatthesystemisidentified. ConsideranotherexamplebasedonKeating(1992).Heestimatedafour-variablesystemwithprices, thefedfundsrate,M2,andGDP.Hismodelfortheerrorstakestheform Ae =Îµ .Writtenoutexplicitly: t t e =Îµ P AS e =a e +Îµ FF 23 M MS e =a (e +e )+a e +Îµ M 31 P GDP 32 FF MD e =a e +a e +a e +Îµ GDP 41 P 42 FF 43 M IS wherethefourshocksareâaggregatesupplyâ,âmoneysupplyâ,âmoneydemandâ,andâI-Sâ. Thisstruc- turecanbebasedonthefollowingassumptions: Anelasticshort-runaggregatesupplycurve(pricesdo notrespondwithinaquarter);asimplemonetarysupplypolicy(thefedfundsrateonlyrespondswithin quartertothemoneysupply);moneydemandonlyrespondstonominaloutput(logpricepluslogreal output)andfedfundsratewithinquarter;andunrestrictedI-Scurve. Toanalyzeconditionsforidentificationwestartbycheckingtheordercondition.Thereare10coeffi- cientsinthesystem(includingthefourvariances),whichequalsm(m+1)/2sincem=4.Thustheorder conditionisexactlysatisfied. Wechecktheequationsforidentification.Westartwiththefirstequation.Ithasnocoefficientssois identifiedandthussoisÎµ .Thesecondequationhasonecoefficient.WecanuseÎµ asaninstrument AS AS becauseitisuncorrelatedwithÎµ .TherelevanceconditionwillholdifÎµ iscorrelatedwithe .From MS AS M thethirdequationweseethatthiswillholdifa (cid:54)=0. Giventhisassumptiona andÎµ areidentified. 31 23 MS Thethirdequationhastwocoefficientssowecanuse(Îµ ,Îµ )asinstrumentssincetheyareuncorre- AS MS latedwithÎµ . Îµ iscorrelatedwithe andÎµ iscorrelatedwithe . Thustherelevancecondition MD MS FF AS P issatisfied. Thefinalequationhasthreecoefficientssoweuse(Îµ ,Îµ ,Îµ )asinstruments. Theyare AS MS MD uncorrelatedwithÎµ andcorrelatedwiththevariables(e ,e ,e )sothisequationisidentified. IS P FF M Wefindthatthesystemisidentifiedifa (cid:54)=0. Thisrequiresthatmoneydemandrespondstonom- 31 inalGDPwhichisapredictionfromstandardmonetaryeconomics. Thisconditionseemsreasonable. Regardless,thepointofthisexerciseistodeterminespecificconditionsforidentificationandarticulate theminyouranalysis. 15.27 Long-RunRestrictions Toreview,thealgebraicidentificationproblemforimpulseresponseestimationisthatwerequirea squarerootmatrixB =Î£1/2 yetthelatterisnotuniqueandtheresultsaresensitivetothechoice. The non-uniquenessarisesbecauseB hasm2elementswhileÎ£hasm(m+1)/2freeelements.Therecursive solutionistosetB toequaltheCholeskydecompositionofÎ£,orequivalentlytospecifyB aslowertrian- gular",
    "page": 555,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". 15.27 Long-RunRestrictions Toreview,thealgebraicidentificationproblemforimpulseresponseestimationisthatwerequirea squarerootmatrixB =Î£1/2 yetthelatterisnotuniqueandtheresultsaresensitivetothechoice. The non-uniquenessarisesbecauseB hasm2elementswhileÎ£hasm(m+1)/2freeelements.Therecursive solutionistosetB toequaltheCholeskydecompositionofÎ£,orequivalentlytospecifyB aslowertrian- gular. StructuralVARsbasedonshort-run(contemporeneous)restrictionsgeneralizethisideabyallow- inggeneralrestrictionsonB basedoneconomicassumptionsaboutcontemporeneouscausalrelations and prior knowledge about B. Identification requires m(mâ1)/2 restrictions. Even more generally, a structuralVARcanbeconstructedbyimposingm(mâ1)/2restrictionsduetoanyknownstructureor featuresoftheimpulseresponsefunctions.",
    "page": 555,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 536 One important class of such structural VARs are those based on long-run restrictions. Some eco- nomic hypotheses imply restrictions on long-run impulse responses. These can provide a compelling caseforidentification. An influential example of a structural VAR based on a long-run restriction is Blanchard and Quah (1989). Theywereinterestedindecomposingtheeffectsofdemandandsupplyshocksonoutput. Their hypothesisisthatdemandshocksarelong-runneutralmeaningthatthelong-runimpactofademand shockonoutputiszero. Thisimpliesthatthelong-runimpulseresponseofoutputwithrespecttode- mandiszero.Thiscanbeusedasanidentifyingrestriction. Thelong-runstructuralimpulseresponseisthecumulativesumofallimpulseresponses â C = (cid:88) Î (cid:96)B=Î(1)B=A(1) â1B. (cid:96)=1 Along-runrestrictionisarestrictionplacedonthematrixC. Sincethesum A(1)isidentifiedthispro- videsidentifyinginformationonthematrixB. Blanchard and Quah (1989) suggest a bivariate VAR for the first-differenced logarithm of real GDP and the unemployment rate. Blanchard-Quah assume that the structural shocks are aggregate supply andaggregatedemand. Theyadoptthehypothesisthataggregatedemandhasnolong-runimpacton GDP.Thismeansthatthelong-runimpulseresponsematrixsatisfies (cid:183) (cid:184) c 0 C = 11 . (15.24) c c 21 22 AnotherwayofthinkingaboutthisisthatBlanchard-Quahlabelâaggregatesupplyâasthelong-runcom- ponentofGDPandlabelâaggregatedemandâasthetransitorycomponentofGDP. TherelationsC =A(1) â1B andBB (cid:48)=Î£imply CC (cid:48)=A(1) â1BB (cid:48) A(1) â1(cid:48)=A(1) â1Î£A(1) â1(cid:48) . (15.25) Thisisasetofm2 equationsbutbecausethematricesarepositivesemi-definitetherearem(m+1)/2 independentequations.IfthematrixChasm(m+1)/2freecoefficientsthenthesystemisidentified.This requiresm(mâ1)/2restrictions. IntheBlanchard-Quahexample,m=2soonerestrictionissufficient foridentification. Inmanyapplications,includingBlanchard-Quah,thematrixC islowertriangularwhichpermitsthe followingelegantsolution. Examining(15.25)weseethatC isamatrixsquarerootof A(1) â1Î£A(1) â1(cid:48) , andsinceC islowertriangularitistheCholeskydecomposition.WededuceC =chol (cid:161) A(1) â1Î£A(1) â1(cid:162) . Theplug-inestimatorforC isC(cid:98) =chol (cid:161) A(cid:98)(1) â1Î£ (cid:98)A(cid:98)(1) â1(cid:162) whereA(cid:98)(1)=I m âA(cid:98)1 âÂ·Â·Â·âA(cid:98)p .Byconstruc- tionthesolutionC(cid:98) willbelowertriangularandsatisfythedesiredrestriction. MoregenerallyiftherestrictionsonC donottakealowertriangularformthentheestimatorcanbe foundbynumericallysolvingthesystemofquadraticequations C(cid:98)C(cid:98) (cid:48) =A(cid:98)(1) â1Î£ (cid:98)A(cid:98)(1) â1(cid:48) . IneithercasetheestimatorisB(cid:98) =A(cid:98)(1)C(cid:98) andtheestimatorofthestructuralimpulseresponseis S(cid:129)IRF(h)=Î (cid:98)h B(cid:98) =Î (cid:98)h A(cid:98)(1)C(cid:98)",
    "page": 556,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". MoregenerallyiftherestrictionsonC donottakealowertriangularformthentheestimatorcanbe foundbynumericallysolvingthesystemofquadraticequations C(cid:98)C(cid:98) (cid:48) =A(cid:98)(1) â1Î£ (cid:98)A(cid:98)(1) â1(cid:48) . IneithercasetheestimatorisB(cid:98) =A(cid:98)(1)C(cid:98) andtheestimatorofthestructuralimpulseresponseis S(cid:129)IRF(h)=Î (cid:98)h B(cid:98) =Î (cid:98)h A(cid:98)(1)C(cid:98). Noticethatbyconstructionthelong-runimpulseresponseis â â (cid:88) S(cid:129)IRF(h)= (cid:88) Î (cid:98)h A(cid:98)(1)C(cid:98) =A(cid:98)(1) â1A(cid:98)(1)C(cid:98) =C(cid:98) (cid:96)=1 (cid:96)=1",
    "page": 556,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 537 soindeedC(cid:98) istheestimatedlong-runimpulseresponseandsatisfiesthedesiredrestriction. Long-runstructuralvectorautoregressionscanbeestimatedinStatausingthesvarcommandusing thelreqoption. Structuralimpulseresponsescanbedisplayedusingirf graph sirfandstructural forecasterrordecompositionsusingirf graph sfevd. ThisStataoptiondoesnotproduceasymptotic standard errors when imposing long-run restrictions so for confidence intervals bootstrapping is rec- ommended. The same limitations for such intervals constructed in Stata hold for structural impulse responsefunctionsastheothercasesdiscussed. Unfortunately,alimitationoftheStatasvarcommandisthatitdoesnotdisplaycumulativestruc- turalimpulseresponsefunctions.Inordertodisplaytheseoneneedstocumulatetheimpulseresponse estimates. Thiscanbedonebutthenstandarderrorsandconfidenceintervalsarenotavailable. This meansthatforseriousappliedworkprogrammingneedstobedoneoutsideofStata. 15.28 BlanchardandQuah(1989)Illustration Aswedescribedintheprevioussection,BlanchardandQuah(1989)estimatedabivariateVARinGDP growth and the unemployment rate assuming that the the structural shocks are aggregate supply and aggregatedemandimposingthatthatthelong-runresponseofGDPwithrespecttoaggregatedemand iszero. TheiroriginalapplicationusedU.S.datafor1950-1987. WerevisitusingFRED-QD(1959-2017). While Blanchard and Quah used a VAR(8) model the AIC selects a VAR(3). We use a VAR(4). To ease theinterpretationoftheimpulseresponsestheunemploymentrateisenterednegatively(multipliedby â1)sothatbothseriesarepro-cyclicalandpositiveshocksincreaseoutput.BlanchardandQuahuseda carefuldetrendingmethod;insteadweincludingalineartimetrendintheestimatedVAR. Thefittedreducedformmodelcoefficientssatisfy (cid:88) 4 (cid:181) 0.42 0.05 (cid:182) A(cid:98)(1)=I m â A(cid:98)j = â0.15 0.04 j=1 andtheresidualcovariancematrixis (cid:181) (cid:182) 0.531 0.095 Î£ (cid:98) = . 0.095 0.053 Wecalculate (cid:181) (cid:182) C(cid:98) =chol (cid:161) A(cid:98)(1) â1Î£ (cid:98)A(cid:98)(1) â1(cid:48)(cid:162)= 1.00 0 4.75 5.42 (cid:181) (cid:182) 0.67 0.28 B(cid:98) =A(cid:98)(1)C(cid:98) = . 0.05 0.23 ExaminingB(cid:98),theunemploymentrateiscontemporeneouslymostlyaffectedbytheaggregatedemand shock,whileGDPgrowthisaffectedbybothshocks. UsingthissquarerootofÎ£ (cid:98) weconstructthestructuralimpulseresponsefunctionsforGDPandthe unemploymentrateasafunctionofthetwoshocks(aggregatesupplyandaggregatedemand). Thecal- culationsweredoneinStata.UnfortunatelytheStatasvarcommandishighlylimitedanddoesnotpro- duce cumulative structural impulse responses which are needed for GDP (as it is estimated in growth rates). We calculated the impulse responses for GDP by cumulating the impulse responses for GDP growth. Thiscanbedoneforthepointestimatesbutdoesnotproducestandarderrors. Forconfidence intervalsexplicitprogrammingoftheestimationwouldberequired. InFigures15.5and15.6wedisplaytheestimatedstructuralimpulseresponsefunctions",
    "page": 557,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". We calculated the impulse responses for GDP by cumulating the impulse responses for GDP growth. Thiscanbedoneforthepointestimatesbutdoesnotproducestandarderrors. Forconfidence intervalsexplicitprogrammingoftheestimationwouldberequired. InFigures15.5and15.6wedisplaytheestimatedstructuralimpulseresponsefunctions. Figure15.5 displaystheimpulseresponsesofGDPandFigure15.6displaystheimpulseresponsesofthe(negative)",
    "page": 557,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 538 1.6 1.6 1.4 1.4 1.2 1.2 1 1 .8 .8 .6 .6 .4 .4 .2 .2 0 0 -.2 -.2 -.4 -.4 0 4 8 12 16 20 24 0 4 8 12 16 20 24 Quarters Quarters (a)SupplyShock (b)DemandShock Figure15.5:ResponseofGDP unemploymentrate. Theleftpanelsdisplaytheimpulseresponseswithrespecttotheaggregatesupply shockandtherightpanelstheimpulseresponseswithrespecttotheaggregatedemandshock. Figure 15.6displays95%normalapproximationbootstrapintervals,calculatedfrom10,000bootstrapreplica- tions.Thefourestimatedimpulseresponseshavesimilarhumpshapeswithapeakaroundfourquarters. TheestimatedfunctionsaresimilartothosefoundbyBlanchardandQuah(1989). .6 .6 .5 .5 .4 .4 .3 .3 .2 .2 .1 .1 0 0 -.1 -.1 -.2 -.2 0 4 8 12 16 20 24 0 4 8 12 16 20 24 Quarters Quarters (a)SupplyShock (b)DemandShock Figure15.6:ResponseofUnemploymentRate Letâsexamineandcontrastpanels(a)and(b)ofFigure15.5. ThesearetheresponsesofGDPtoag- gregatesupplyanddemandshocks,respectively. Wecanseeinpanel(a)thattheimpulseresponsedue toasupplyshockisimmediate,strong,andpersistent. Theeffectpeaksaroundfourquartersandthen flattenswithaneffectat24quarterssimilartotheimmediateeffect. Incontrastwecanseeinpanel(b) thattheeffectofademandshockismoremodest,peakssooner,anddecays,withtheeffectnearzeroby 24quarters. Thedecayreflectsthelong-runneutralityofdemandshocks. Whiletheestimatedeffectis transitorythedurationoftheeffectisstillmeaningfulouttothreeyears. Figure15.6displaystheresponsesoftheunemploymentrate. Itsresponsetoasupplyshock(panel (a))takesseveralquarterstotakeeffect,peaksaround5quarters,andthendecays. Theresponseofthe unemploymentratetoademandshock(panel(b))ismoreimmediate,peaksaround4quarters,andthen decays. Botharenearzeroby6years. Theconfidenceintervalsforthesupplyshockimpulseresponses",
    "page": 558,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 539 arewiderthanthoseforthedemandshocksindicatingthattheestimatesoftheimpulseresponsesdue tosupplyshocksarenotpreciselyestimated. 1 1 .8 .8 .6 .6 .4 .4 .2 .2 0 0 0 4 8 12 16 20 24 0 4 8 12 16 20 24 Quarters Quarters (a)GDP (b)UnemploymentRate Figure15.7:ForecastErrorDecomposition,%duetoSupplyShock Figure15.7displaystheestimatedstructuralforecasterrordecompositions.Sincethereareonlytwo errorsweonlydisplaythepercentagesquarederrorduetothesupplyshock. Inpanel(a)wedisplaythe forecasterrordecompositionforGDPandinpanel(b)theforecasterrordecompositionfortheunem- ploymentrate. Wecanseethatabout80%ofthefluctuationsinGDPareattributedtothesupplyshock. Fortheunemploymentratetheshort-termfluctuationsaremostlyattributedtothedemandshockbut thelong-runimpactisabout40%duetothesupplyshock.Theconfidenceintervalsareverywide,how- ever,indicatingthattheseestimatesarenotprecise. It is fascinating that the structural impulse response estimates shown here are nearly identical to thosefoundbyBlanchardandQuah(1989)despitethefactthatwehaveusedaconsiderablydifferent sampleperiod. 15.29 ExternalInstruments Structural VARs can also be identified and estimated using externalinstrumentalvariables. This methodisalsocalledProxySVARs.Considerthethree-variablesystemfortheinnovations e +a e +a e =Îµ (15.26) 1t 12 2t 13 3t 1t a e +e = Îµ +b Îµ =u (15.27) 21 1t 2t 2t 23 3t 2t a e + e = b Îµ +Îµ =u . (15.28) 31 1t 3t 32 2t 3t 3t Inthissystemwehaveusedthenormalizationb =b =b =1ratherthannormalizingthevariances 11 22 33 oftheshocks. SupposewehaveanexternalinstrumentalvariableZ whichsatisfiestheproperties t (cid:69)[Z Îµ ](cid:54)=0 (15.29) t 1t (cid:69)[Z Îµ ]=0 (15.30) t 2t (cid:69)[Z Îµ ]=0. (15.31) t 3t Equation(15.29)istherelevanceconditionâthattheinstrumentandtheshockÎµ arecorrelated.Equa- 1t tions(15.30)-(15.31)aretheexogeneityconditionâthattheinstrumentisuncorrelatedwiththeshocks Îµ andÎµ .Identificationrestsonthevalidityoftheseassumptions. 2t 3t",
    "page": 559,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 540 Supposee ,e ande wereobserved. Thenthecoefficient a in(15.27)canbeestimatedbyin- 1t 2t 3t 21 strumentalvariablesregressionofe one usingtheinstrumentalvariableZ .ThisisvalidbecauseZ 2t 1t t t is uncorrelated with u =Îµ +b Îµ under the assumptions (15.30)-(15.31) yet is correlated with e 2t 2t 23 3t 1t under (15.29). Given this estimator we obtain a residual u . Similarly we can estimate a in (15.27) (cid:98)2t 31 byinstrumentalvariablesregressionofe one usingtheinstrumentalvariable Z ,obtainingaresid- 3t 1t t ualu . Wecanthenestimate a and a inin(15.26)byinstrumentalvariablesregressionofe on (cid:98)3t 12 13 1t (e ,e )usingtheinstrumentalvariables(u ,u ). Thelatterarevalidinstrumentssince(cid:69)[u Îµ ]=0 2t 3t (cid:98)2t (cid:98)3t 2t 1t and(cid:69)[u Îµ ]=0sincethestructuralerrorsareuncorrelated, andbecause(u ,u )iscorrelatedwith 3t 1t 2t 3t (e ,e )byconstruction. ThisregressionalsoproducesaresidualÎµ whichisanappropriateestimator 2t 3t (cid:98)1t fortheshockÎµ . 1t Thisestimationmethodisnotspecialforathree-variablesystem; itcanbeappliedforanym. The identifiedcoefficientsarethoseinthefirstequation(15.26),thestructuralshockÎµ ,andtheimpacts(a 1t 21 andÎ± )ofthisshockontheothervariables. TheothershocksÎµ andÎµ arenotseparatelyidentified, 31 2t 3t andtheircorrelationstructure(b andb )isnotidentified. Anexceptionariseswhenm=2,inwhich 23 32 caseallcoefficientsandshocksareidentified. Whilee , e ande arenotobservedwecanreplacetheirvaluesbytheresidualse , e ande 1t 2t 3t (cid:98)1t (cid:98)2t (cid:98)3t fromtheestimatedVAR(p)model.Allofthecoefficientestimatesarethentwo-stepestimatorswithgen- eratedregressors. Thisaffectstheasymptoticdistributionsoconventionalasymptoticstandarderrors shouldnotbeused.Bootstrapconfidenceintervalsareappropriate. The structure (15.26)-(15.28) is convenient as four coefficients can be identified. Other structures canalsobeused.Considerthestructure e =Îµ +b Îµ +b Îµ 1t 1t 12 2t 23 3t e =b Îµ +Îµ +b Îµ 2t 21 1t 2t 23 3t e =b Îµ +b Îµ +Îµ 3t 31 1t 32 2t 3t If the same procedure is applied we canidentify the coefficients b and b and the shock Îµ but no 21 31 1t othercoefficientsorshocks.Inthisstructurethecoefficientsb andb cannotbeseparatelyidentified 12 23 becausetheshocksÎµ andÎµ arenotseparatelyidentified. 2t 3t FormoredetailsseeStockandWatson(2012)andMertensandRavn(2013). 15.30 DynamicFactorModels Dynamicfactormodelsareincreasinglypopularinappliedtimeseries,inparticularforforecasting. ForarecentdetailedreviewofthemethodsseeStockandWatson(2016)andthereferencestherein.For someofthefoundationaltheoryseeBai(2003)andBaiandNg(2002,2006). InSections11.13-11.16weintroducedthestandardmulti-factormodel(11.23): X =ÎF +u (15.32) t t t whereX andu arekÃ1,ÎiskÃr withr <k,andF isrÃ1.TheelementsofF arecalledthecommon t t t t factorsastheyaffectallelementsofX .ThecolumnsofÎarecalledthefactorloadings.Thevariablesu t t arecalledtheidiosyncraticerrors",
    "page": 560,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". ForarecentdetailedreviewofthemethodsseeStockandWatson(2016)andthereferencestherein.For someofthefoundationaltheoryseeBai(2003)andBaiandNg(2002,2006). InSections11.13-11.16weintroducedthestandardmulti-factormodel(11.23): X =ÎF +u (15.32) t t t whereX andu arekÃ1,ÎiskÃr withr <k,andF isrÃ1.TheelementsofF arecalledthecommon t t t t factorsastheyaffectallelementsofX .ThecolumnsofÎarecalledthefactorloadings.Thevariablesu t t arecalledtheidiosyncraticerrors. ItisoftenassumedthattheelementsofX havebeentransformedto t bemeanzeroandhavecommonvariances. In the time-series case it is natural to augment the model to allow for dynamic relationships. In particularwewouldliketoallowF andu tobeseriallycorrelated. Itisconvenienttoconsidervector t t",
    "page": 560,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 541 autoregressivemodelswhichcanbewrittenusinglagoperatornotationas A(L)F =v (15.33) t t B(L)u =e (15.34) t t where A(L) and B(L) are lag polynomials with p and q lags, respectively. Equations (15.32)-(15.33)- (15.34)togethermakethestandarddynamicfactormodel.Tosimplifythemodelandaididentification, furtherrestrictionsareoftenimposed,inparticularthatthelagpolynomialB(L)isdiagonal. Furthermorewemaywishtogeneralize(15.32)toallowF toimpactX viaadistributedlagrelation- t t ship.Thisgeneralizationcanbewrittenas X =Î(L)F +u (15.35) t t t whereÎ(L)isan(cid:96)th orderdistributedlagofdimensionkÃr. Equation(15.35),however,isnotfunda- mentallydifferentfrom(15.32).Thatis,ifwedefinethestackedfactorvectorF =(cid:161) F (cid:48) ,F (cid:48) ,...,F (cid:48) (cid:162)(cid:48) then t t tâ1 tâ(cid:96) (15.35)canbewrittenintheform(15.32)withF t replacingF t andthematrixÎreplacedby(Î 1 ,Î 2 ,...,Î (cid:96)). Hencewewillfocusonthestandardmodel(15.32)-(15.33)-(15.34). DefinetheinverselagoperatorsD(L)=A(L) â1andC(L)=B(L) â1. ThenbyapplyingC(L)to(15.32) andD(L)to(15.33)weobtain C(L)X =C(L)ÎF +C(L)u t t t =C(L)ÎD(L)v +e t t =Î(L)v +e t t where Î(L)=C(L)ÎD(L). For simplicity treat this lag polynomial as if it has (cid:96) lags. Using the same stackingtrickfromthepreviousparagraphanddefiningV =(cid:161) v (cid:48) ,v (cid:48) ,...,v (cid:48) (cid:162)(cid:48) wefindthatthismodel t t tâ1 tâ(cid:96) canbewrittenas C(L)X =HV +e (15.36) t t t forsomekÃr(cid:96)matrixH.Thisisknownasthestaticformofthedynamicfactormodel.ItshowsthatX t canbewrittenasafunctionofitsownlagsplusalinearfunctionoftheseriallyuncorrelatedfactorsV t andaseriallyuncorrelatederrore . t Thestaticform(15.36)isconvenientasfactorregressioncanbeusedforestimation. Themodelis identical to factor regression with additional regressors as described in Section 11.15. (The additional regressorsarethelaggedvaluesofX .)Inthatsectionitisdescribedhowtoestimatethecoefficientsand t factorsbyiteratingbetweenmultivariateleastsquaresandfactorregression. Toestimatetheexplicitdynamicmodel(15.32)-(15.33)-(15.34)state-spacemethodsareconvenient. FordetailsandreferencesseeStockandWatson(2016). Thedynamicfactormodel(15.32)-(15.33)-(15.34)canbeestimatedinStatausingdfactor. 15.31 TechnicalProofs* ProofofTheorem15.6Withoutlossofgeneralityassumea =0. 0 By the Jordan matrix decomposition (see Section A.13), A =PJP â1 where J =diag{J ,...,J } is in 1 r Jordan normal form. The dimension of each Jordan block J is determined by the multiplicity of the i eigenvaluesÎ» of A. ForuniqueeigenvaluesÎ» , J =Î» . ForeigenvaluesÎ» withdoublemultiplicitythe i i i i i Jordanblockstaketheform (cid:183) Î» 1 (cid:184) J = i . i 0 Î» i",
    "page": 561,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 542 Foreigenvalueswithmultiplicitys>2theJordanblocksaresÃsupperdiagonalwiththeeigenvalueon thediagonaland1immediatelyabovethediagonal(seeA.7). Define X t =P â1Y t andu t =P â1e t ,whichsatisfy X t = JX tâ1 +u t . Partition X t andu t conformably with J. Theith setsatisfy X it = J i X i,tâ1 +u it . Wenowshowthat X it isstrictlystationaryandergodic, fromwhichwededucethatY =PX isstrictlystationaryandergodic. t t Forsingledimensionblocks J i =Î» i ,soX it =Î» i X i,tâ1 +u it whichisanAR(1)modelwithcoefficient Î» and innovation u . The assumptions imply |Î» |<1 and (cid:69)|u |<â so the conditions of Theorem i it i it 14.21aresatisfied,implyingthatX isstrictlystationaryandergodic. it Forblockswithdimensiontwo,byback-substitutionwefind X it =(cid:80)â (cid:96)=0 J (cid:96) i u i,tâ(cid:96). Bydirectcalcula- tionwefindthat (cid:183) Î»(cid:96) (cid:96)Î»(cid:96)â1 (cid:184) J (cid:96) i = 0 i Î» i (cid:96) . i PartitioningX =(X ,X )andu =(u ,u )thismeansthat it 1it 2it it 1it 2it â â X 1it = (cid:88) Î»(cid:96) i u 1i,tâ(cid:96) + (cid:88) (cid:96)Î»(cid:96) i u 2i,tâ(cid:96) (cid:96)=0 (cid:96)=0 â X 2it = (cid:88) Î»(cid:96) i u 2i,tâ(cid:96). (cid:96)=0 Theseries (cid:80)â Î»(cid:96) and (cid:80)â (cid:96)Î»(cid:96) areconvergentbytheratiotest(TheoremA.3ofIntroductiontoEcono- (cid:96)=0 i (cid:96)=0 i metrics)since|Î» |<1.ThustheabovesumssatisfytheconditionsofTheorem14.6soarestrictlystation- i aryandergodicasrequired. Blockswithmultiplicitys>2arehandledbysimilarbutmoretediouscalculations. â  Proof of Theorem 15.8 The assumption that Î£ > 0 means that if we regress Y on Y ,...,Y and 1t 2t pt Y tâ1 ,...,Y tâp that the error will have positive variance. IfQ is singular then there is some Î³ such that Î³(cid:48) QÎ³=0.AsintheproofofTheorem14.28thismeansthattheregressionofY 1t onY 2t ,...,Y pt ,Y tâ1 ,...,Y tâp+1 hasazerovariance.Thisisacontradiction.WeconcludethatQ isnotsingular. â  ProofofTheorem15.12Thefirstpartofthetheoremisestablishedbyback-substitution. SinceY isa t VAR(p)process, Y t+h =a 0 +A 1 Y t+hâ1 +A 2 Y t+hâ2 +Â·Â·Â·+A p Y t+hâp +e t . Wethensubstituteoutthefirstlag.Wefind Y t+h =a 0 +A 1 (cid:161) a 0 +A 1 Y t+hâ2 +A 2 Y t+hâ3 +Â·Â·Â·+A p Y t+hâpâ1 +e tâ1 (cid:162)+A 2 Y t+hâ2 +Â·Â·Â·+A p Y t+hâp +e t =a 0 +A 1 a 0 +(A 1 A 1 +A 2 )Y t+hâ2 +(A 1 A 2 +A 3 )Y t+hâ3 +Â·Â·Â·+A p A p Y t+hâpâ1 +A 1 e tâ1 +e t . Wecontinuemakingsubstitutions. WitheachsubstitutiontheerrorincreasesitsMAorder. Afterhâ1 substitutionstheequationtakestheform(15.12)withu anMA(h-1)process. t TorecognizethatB =Î , noticethatthedeductionthatu isanMA(h-1)processmeansthatwe 1 h t canequivalentlywrite(15.12)as â (cid:88) Y t+h =b 0 + B j Y t+1âj +u t j=1 withB =0for j >p.Thatis,theequation(15.12)includesallrelevantlags.Bytheprojectionproperties j ofregressioncoefficientsthismeansthatthecoefficientB isinvarianttoreplacingtheregressorY by 1 t",
    "page": 562,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 543 theinnovationfromitsregressionontheotherlags.ThisistheVAR(p)modelitselfwhichhasinnovation e .WehavededucedthatthecoefficientB isequivalenttothatintheregression t 1 â (cid:88) Y t+h =b 0 +B 1 e t + B j Y t+1âj +u t . j=2 Noticethate t isuncorrelatedwiththeotherregressors. ThusB 1 = â â e(cid:48) P t [Y t+h ]=Î h asclaimed. This completestheproof. â  t _____________________________________________________________________________________________ 15.32 Exercises Exercise15.1 Take the VAR(1) model Y t = AY tâ1 +e t . Assume e t is i.i.d. For each specified matrix A below,checkifY isstrictlystationary.Usemathematicalsoftwaretocomputeeigenvaluesifneeded. t (cid:183) (cid:184) 0.7 0.2 (a) A= 0.2 0.7 (cid:183) (cid:184) 0.8 0.4 (b) A= 0.4 0.8 (cid:183) (cid:184) 0.8 0.4 (c) A= â0.4 0.8 (cid:183) 0.3 0.2 (cid:184) (cid:183) 0.4 â0.1 (cid:184) Exercise15.2 TaketheVAR(2)modelY t =A 1 Y tâ1 +A 2 Y tâ2 +e t withA 1 = 0.2 0.3 andA 2 = â0.1 0.4 . Assumee isi.i.d.IsY strictlystationary?Usemathematicalsoftwareifneeded. t t Exercise15.3 Suppose Y t = AY tâ1 +u t and u t =Bu tâ1 +e t . Show that Y t is a VAR(2) and derive the coefficientmatricesandequationerror. Exercise15.4 SupposeY ,i =1,...,m,areindependentAR(p)processes. Derivetheformoftheirjoint it VARrepresentation. Exercise15.5 In the VAR(1) model Y t = A 1 Y tâ1 +e t find an explicit expression for the h-step moving averagematrixÎ from(15.3). h Exercise15.6 In the VAR(2) model Y t = A 1 Y tâ1 +A 2 Y tâ2 +e t find explicit expressions for the moving averagematrixÎ from(15.3)forh=1,...4. h Exercise15.7 Derive a VAR(1) representation of a VAR(p) process analogously to equation (??) for au- toregressions. Usethistoderiveanexplicitformulafortheh-stepimpulseresponseIRF(h)analogously to(14.42). Exercise15.8 LetY =(Y ,Y ) (cid:48) be2Ã1andconsideraVAR(2)model. SupposeY doesnotGranger- t 1t 2t 2t causeY .WhataretheimplicationsfortheVARcoefficientmatrices A and A ? 1t 1 2 Exercise15.9 Continutingthepreviousexercise,supposethatbothY doesnotGranger-causeY ,and 2t 1t Y doesnotGranger-causeY .WhataretheimplicationsfortheVARcoefficientmatrices A and A ? 1t 2t 1 2",
    "page": 563,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 544 Exercise15.10 Supposethatyouhave20yearsofmonthlyobservationsonm=8variables.Youradvisor recommendsp=12lagstoaccountforannualpatterns.Howmanycoefficientsperequationwillyoube estimating? Howmanyobservationsdoyouhave? Inthiscontextdoesitmakesensetoyoutoestimate aVAR(12)withalleightvariables? Exercise15.11 Lete (cid:98)t betheleastsquaresresidualsfromanestimatedVAR,Î£ (cid:98)betheresidualcovariance matrix,andB(cid:98) =chol(Î£ (cid:98)).ShowthatB(cid:98) canbecalculatedbyrecursiveleastsquaresusingtheresiduals. Exercise15.12 Choleskyfactorization (cid:183) Ï2 ÏÏ Ï (cid:184) (a) DerivetheCholeskydecompositionofthecovariancematrix 1 1 2 . ÏÏ Ï Ï2 1 2 1 (b) Writetheanswerforthecorrelationmatrix(thespecialcaseÏ2=1andÏ2=1). 1 2 (c) Find an upper triangular decomposition for the correlation matrix. That is, an upper-triangular (cid:183) 1 Ï (cid:184) matrixR whichsatisfiesRR (cid:48)= . Ï 1 (cid:183) (cid:184) 1 0 (d) SupposeÎ = ,Ï2=1,andÏ2=1,andÏ=0.8.Findtheorthogonalizedimpulseresponse h 1 1 1 2 OIRF(h)usingtheCholeskydecomposition. (e) Supposethattheorderingofthevariablesisreversed. Thisisequivalenttousingtheuppertrian- gulardecompositionfrompart(c).CalculatetheorthogonalizedimpulseresponseOIRF(h). (f) Comparethetwoorthogonalizedimpulseresponses. Exercise15.13 YoureadanempiricalpaperwhichestimatesaVARinalistedsetofvariablesanddis- playsestimatedorthogonalizedimpulseresponsefunctions. Nocommentismadeinthepaperabout theorderingortheidentificationofthesystem,andyouhavenoreasontobelievethattheorderusedis âstandardâintheliterature.Howshouldyouinterprettheestimatedimpulseresponsefunctions? Exercise15.14 Take the quarterly series gdpc1 (real GDP), gdpctpi (GDP price deflator), and fedfunds (Fed funds interest rate) from FRED-QD. Transform the first two into growth rates as in Section 15.13. Estimatethesamethree-variableVAR(6)usingthesameordering. Theidentificationstrategydiscussed inSection15.23specifiesthesupplyshockastheorthogonalizedshocktotheGDPequation. Calculate theimpulseresponsefunctionofGDP,thepricelevel,andtheFedfundsratewithrespecttothissupply shock. Forthefirsttwothiswillrequirecalculatingthecumulativeimpulseresponsefunction. (Explain why.)Commentontheestimatedfunctions. Exercise15.15 TaketheKilian2009datasetwhichhasthevariablesoil(oilproduction),output(global economic activity), and price (price of crude oil). Estimate an orthogonalized VAR(4) using the same orderingasinKilian(2009)asdescribedinSection15.24. (Asdescribedinthatsection,multiplyâoilâby â1sothatallshocksincreaseprices.) Estimatetheimpulseresponseofoutputwithrespecttothethree shocks.Commentontheestimatedfunctions. Exercise15.16 Takethemonthlyseriespermit(buildingpermits),houst(housingstarts),andrealln(real estate loans) from FRED-MD. The listed ordering is motivated by transaction timing. A developer is requiredtoobtainabuildingpermitbeforetheystartbuildingahouse(thelatterisknownasaâhousing startâ).Arealestateloanisobtainedwhenthehouseispurchased.",
    "page": 564,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 545 (a) Transformreallnintogrowthrates(firstdifferenceoflogs). (b) Selectanappropriatelagorderforthethree-variablesystembycomparingtheAICofVARsoforder 1through8. (c) EstimatetheVARmodelandplottheimpulseresponsefunctionsofhousingstartswithrespectto thethreeshocks. (d) Interpretyourfindings. Exercise15.17 Takethequarterlyseriesgpdic1(RealGrossPrivateDomesticInvestment),gdpctpi(GDP price deflator), gdpc1 (real GDP), and fedfunds (Fed funds interest rate) from FRED-QD. Transform the firstthreeintologs,e.g. gdp=100log(gdpc1). ConsiderastructuralVARbasedonshort-runrestrictions. Useastructureoftheform Ae =Îµ . Imposetherestrictionsthatthefirstthreevariablesdonotreact t t to the fed funds rate, that investment does not respond to prices, and that prices do not respond to investment.Finally,imposethatinvestmentisshort-rununitelasticwithrespecttoGDP(intheequation forinvestment,the AcoefficientonGDPisâ1). (a) Writedownthematrix Asimilarto(15.22),imposingtheidentifyingconstraintsasdefinedabove. (b) Isthemodelidentified?Isthereaconditionforidentification?Explain. (c) Inthismodelareoutputandpricesimultaneous,orrecursiveasintheexampledescribedinSec- tion15.23? (d) EstimatethestructuralVARusing6lagsoradifferentnumberofyourchoosing(justifyyourchoice) andincludeanexogenoustimetrend. Reportyourestimatesofthe A matrix. Canyouinterpret thecoefficients? (e) Estimateandreportthefollowingthreeimpulseresponsefunctions: 1. TheeffectofthefedfundsrateonGDP. 2. TheeffectoftheGDPshockonGDP. 3. TheeffectoftheGDPshockonprices. Exercise15.18 TaketheKilian2009datasetwhichhasthevariablesoil(oilproduction),output(global economicactivity),andprice(priceofcrudeoil). ConsiderastructuralVARbasedonshort-runrestric- tions.Useastructureoftheform Ae =Îµ .Imposetherestrictionsthatoilproductiondoesnotrespond t t tooutputoroilprices, andthatoutputdoesnotrespondtooilproduction. Thelastrestrictioncanbe motivatedbytheobservationthatsupplydisruptionstakemorethanamonthtoreachtheretailmarket sotheeffectoneconomicactivityissimilarlydelayedbyonemonth. (a) Writedownthematrix Asimilarto(15.22)imposingtheidentifyingconstraintsasdefinedabove. (b) Isthemodelidentified?Isthereaconditionforidentification?Explain. (c) EstimatethestructuralVARusing4lagsoradifferentnumberofyourchoosing(justifyyourchoice). (Asdescribedinthatsection,multiplyâoilâbyâ1sothatallshocksincreaseprices.) Reportyour estimatesofthe Amatrix.Canyouinterpretthecoefficients? (d) Estimatetheimpulseresponseofoilpricewithrespecttothethreeshocks. Commentontheesti- matedfunctions.",
    "page": 565,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER15. MULTIVARIATETIMESERIES 546 Exercise15.19 Takethequarterlyseriesgdpc1(realGDP),m1realx(realM1moneystock),andcpiaucsl (CPI)fromFRED-QD.CreatenominalM1(multiplym1realxtimescpiaucsl),andtransformrealGDPand nominalM1togrowthrates. Thehypothesisofmonetaryneutralityisthatthenominalmoneysupply has no effect on real outcomes such as GDP. Strict monetary neutrality states that there is no short or long-termeffect.Long-runneutralitystatesthatthereisnolong-termeffect. (a) To test strict neutrality use a Granger-causality test. Regress GDP growth on four lags of GDP growthandfourlagsofmoneygrowth. Testthehypothesisthatthefourmoneylagsjointlyhave zerocoeffficients.Userobuststandarderrors.Interprettheresults. (b) To test long-run neutrality test if the sum of the four coefficients on money growth equals zero. Interprettheresults. (c) EstimateastructuralVARinrealGDPgrowthandnominalmoneygrowthimposingthelong-run neutralityofmoney.Explainyourmethod. (d) Report estimates of the impulse responses of the levels of GDP and nominal money to the two shocks.Interprettheresults. Exercise15.20 Shapiro and Watson (1988) estimated a structural VAR imposing long-run constraints. Replicateasimplifiedversionoftheirmodel. Takethequarterlyserieshoanbs(hoursworked,nonfarm business sector), gdpc1 (real GDP), and gdpctpi (GDP deflator) from FRED-QD. Transform the first two togrowthratesandforthethird(GDPdeflator)taketheseconddifferenceofthelogarithm(differenced inflation). Shapiro and Watson estimated a structural model imposing the constraints that labor sup- plyhoursarelong-rununaffectedbyoutputandinflationandGDPislong-rununaffectedbydemand shocks.Thisimpliesarecursiveorderinginthevariablesforalong-runrestriction. (a) WritedownthematrixC asin(15.24)imposingtheidentifyingconstraintsasdefinedabove. (b) Isthemodelidentified? (c) UsetheAICtoselectthenumberoflagsforaVAR. (d) EstimatethestructuralVAR.ReporttheestimatedC matrix.Canyouinterpretthecoefficients? (e) Estimate the structural impulse responses of the level of GDP with respect to the three shocks. Interprettheresults.",
    "page": 566,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Chapter 16 Non-Stationary Time Series 16.1 Introduction At the beginning of Chapter 14 we displayed a set of economic time series. Several (real GDP, ex- change rate, interest rate, crude oil price) did not appear to be stationary. In Section 14.23 we intro- duced the non-stationary unit root process which is an autoregressive process with an autoregressive root at unity. Plots of two simulated examples (Figure 14.7) displayed time-paths with wandering be- havior similar to the economic time series. This suggests that perhaps a unit root autoregression is a reasonablemodelfortheseseries. Inthischapterweexploreeconometricestimationandinferencefor non-stationaryunitroottimeseries. 16.2 PartialSumProcessandFunctionalConvergence Takethemultivariaterandomwalk Y t =Y tâ1 +e t where(e ,F )isavectorMDSwithfinitecovariancematrixÎ£. Byback-substitutionwefindY =Y +S t t t 0 t where t (cid:88) S = e t i i=1 isthecumulativesumoftheerrorsuptotimet.WecallS apartialsumprocess. t Thetimeindext rangesfrom0ton. Write1 t =(cid:98)nr(cid:99)asafractionr ofthesamplesizen. Thisallows (cid:112) ustowriteS(cid:98)nr(cid:99)asafunctionofthefractionr.Divideby nsothatthevarianceisstabilized.Withthese modificationswedefinethestandardizedpartialsumprocess. 1 1 (cid:98) (cid:88) nr(cid:99) S n (r)= (cid:112) S(cid:98)nr(cid:99) = (cid:112) e t . n n t=1 TherandomprocessS (r)isascaledversionofthetime-seriesY andisafunctionofthesamplefraction n t r â[0,1]. Itisastochasticprocessmeaningthatitisarandomfunction. Foranyfiniten,S (r)isastep n functionwithnjumps. LetâsconsiderthebehaviorofS n (r)asnincreases.Itâslargestdiscretejumpequalsn â1/2max 1â¤tâ¤n (cid:107)e t (cid:107). Theorem6.16showsthatthisiso (1). ThissuggeststhatthejumpsinS (r)asymptoticallyvanish. We p n wouldliketofinditsasymptoticdistribution. Weexpectthelimitdistributiontobeastochasticprocess aswell. 1Thenotation(cid:98)x(cid:99)meansârounddowntothenearestintegerâ. 547",
    "page": 567,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 548 Todosoweneedtodefinetheasymptoticdistributionofarandomfunction.Theprimarytoolisthe functionalcentrallimittheorem(FCLT)whichisacomponentofempiricalprocesstheory(Chapter18 ofIntroductiontoEconometrics). ItturnsoutthattheFCLTdependsonhowwemeasurethedifference between two functions. The most commonly used measure is the uniform metric. On the space of functionsfrom[0,1]to(cid:82)m itis Ï(Î½ ,Î½ )= sup (cid:107)Î½ (r)âÎ½ (r)(cid:107). 1 2 1 2 0â¤râ¤1 Convergenceindistributionforrandomprocesses(e.g. Definition18.6ofIntroductiontoEconometrics) isdefinedwithrespecttoaspecificmetric. Whilewedonâtrepeatthedetailsheretheimportantconse- quenceisthatcontinuityisdefinedwithrespecttothismetricandthisimpactsapplicationssuchasthe continuousmappingtheorem. TheFunctionalCentralLimitTheorem(Theorem18.9ofIntroductiontoEconometrics)statesthat S (r)ââS(r)asafunctionoverr â[0,1]iftwoconditionshold: n d 1. ThelimitdistributionsofS (r)coincidewiththoseofS(r). n 2. S (r)isstochasticallyequicontinuous. n The first condition means that for any fixed r ,...,r , (S (r ),...,S (r ))ââ(S(r ),...,S(r )). The 1 m n 1 n m 1 m d secondconditionistechnicalbutessentiallyrequiresthatS (r)isasymptoticallycontinuous. n WenowcharacterizethelimitdistributionsofS (r).Therearethreeimportantproperties. n 1. S (0)=0. n 2. Foranyr,S (r)ââN(0,rÎ£). n d 3. Forr <r ,S (r )andS (r )âS (r )areasymptoticallyindependent. 1 2 n 1 n 2 n 1 ThefirstpropertyfollowsfromthedefinitionofS (r).Forthesecond,setN =(cid:98)nr(cid:99).Forr >0,N ââ n asnââ.TheMDSCLT(Theorem14.11)impliesthat (cid:115) (cid:98)nr(cid:99) 1 (cid:88) N (cid:112) S (r)= (cid:112) e ââ rN(0,Î£)=N(0,rÎ£) n t n N t=1 d asclaimed.Forthethirdpropertytheassumptionthate isaMDSimpliesthatS (r )andS (r )âS (r ) t n 1 n 2 n 1 areuncorrelated. Anextensionoftheabovepreviousasymptoticargumentshowsthattheyarejointly asymptoticallynormalwithazerocovarianceandhenceareasymptoticallyindependent. TheabovethreelimitpropertiesofS (r)areasymptoticversionsofthedefinitionofBrownianmo- n tion. Definition16.1 A vector Brownian motion B(r) for r â¥ 0 is defined by the properties: 1. B(0)=0. 2. Foranyr,B(r)â¼N(0,rÎ£). 3. Foranyr â¤r ,B(r )andB(r )âB(r )areindependent. 1 2 1 2 1 WecallÎ£thecovariancematrixofB(r).IfÎ£=I wesaythatB(r)isastandard m BrownianmotionanddenoteitasW(r).TheysatisfyB(r)=Î£1/2W(r).",
    "page": 568,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 549 ABrownianmotionB(r)iscontinuouswithprobabilityonebutisnowheredifferentiable.Inphysics Brownianmotionisusedtodescribethemovementofparticles. Thewanderingpropertiesofparticles suspendedinliquidwasdescribedasfarbackastheRomanpoetLucretius(OntheNatureoftheUniverse, 55 BCE). The name Brownian motion credits the pioneering observational studies of botanist Robert Brown.ThemathematicalprocessisoftencalledaWienerprocesscreditingtheworkofNorbertWiener. TheabovediscussionhasshownthatthelimitdistributionsofthepartialsumprocessS (r)coin- n cidewiththoseofBrownianmotionB(r). InSection16.22wedemonstratethatS (r)isstochastically n equicontinuous.TogetherwiththeFCLTthisestablishesthatS (r)convergesindistributiontoB(r). n Theorem16.1 Weak Convergence of Partial Sum Process If (e ,F ) is a t t strictly stationary and ergodic MDS and Î£=(cid:69)(cid:163) e e (cid:48)(cid:164)<â then as a function t t overr â[0,1],S (r)ââB(r),aBrownianmotionwithcovariancematrixÎ£. n d WeextendTheorem16.1toseriallycorrelatedprocessesinSection16.4. LetâsconnectouranalysisofS (r)withtherandomwalkseriesY .SinceY =Y +S ,wefind n t t 0 t 1 1 (cid:112) Y(cid:98)nr(cid:99) =S n (r)+(cid:112) Y 0 . n n The second term is o (1) when Y is finite with probability one. Thus under this latter assumption p 0 n â1/2Y(cid:98)nr(cid:99) =S n (r)+o p (1)ââB. For simplicity we will frequently implicitly assume Y 0 =0 to simplify d thenotation,asthecasewithY (cid:54)=0doesnotfundamentallychangetheanalysis. 0 16.3 Beveridge-NelsonDecomposition The previous section focused on random walk processes. A unit root process more broadly is an autoregressionwithasinglerootatunity,whichmeansthatthedifferencedprocessâY isseriallycorre- t latedbutstationary. BeveridgeandNelson(1981)introducedacleverwaytodecomposeaunitrootprocessintoaperma- nent(randomwalk)componentandatransitory(stationary)component. Thisallowsastraightforward generalizationofTheorem16.1toincorporateserialcorrelation. RecallthatastationaryprocesshasaWoldrepresentationâY =Î(L)e whereÎ(z)=(cid:80)â Î zj. t t j=0 j Assumption16.1 âY isstrictlystationarywithnodeterministiccomponent, t meanzero,andfinitecovariancematrixÎ£. ThecoefficientsofitsWoldrepre- sentationâY =Î(L)e satisfy t t (cid:176) (cid:176) (cid:88) â (cid:176) (cid:88) â (cid:176) (cid:176) (cid:176) Î (cid:96) (cid:176) (cid:176) <â. (16.1) j=0(cid:176)(cid:96)=j+1 (cid:176) Thecondition(16.1)onthecoefficientsisstrongerthanabsolutesummabilitybutholds(forexam- ple)ifâY isgeneratedbyastationaryARprocess.Itissimilartotheconditionusedfortheautoregressive t Woldrepresentation(Theorem14.19).",
    "page": 569,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 550 Considerthefollowingfactorizationofthelagpolynomial Î(z)=Î(1)+(1âz)Îâ (z) (16.2) whereÎ(1)=(cid:80)â (cid:96)=0 Î (cid:96)andÎâ (z)isthelagpolynomial â Îâ (z)= (cid:88) Îâ zj (16.3) j j=0 â Îâ=â (cid:88) Î (cid:96). (16.4) j (cid:96)=j+1 Attheendofthissectionwedemonstrate(16.2)-(16.4). Assumption(16.1)isthesameas (cid:88) â (cid:176) (cid:176)Îâ (cid:176) (cid:176)<â, (cid:176) j(cid:176) j=0 whichimpliesthatU =Îâ (L)e isconvergent,strictlystationary,andergodic(byTheorem15.4). t t Thefactorization (16.2)meansthatwecanwrite âY t =Î¾ t +U t âU tâ1 . whereÎ¾ =Î(1)e .ThisdecomposesâY intotheinnovatione plusthefirst-differenceofthestochastic t t t t processU .Summingthedifferenceswefind t Y =S +U +V t t t 0 whereS =(cid:80)t Î¾ andV =Y âU . ThisdecomposestheunitrootprocessY intotherandomwalkS , t i=1 t 0 0 0 t t thestationaryprocessU ,andaninitialconditionV . t 0 Wehaveestablishedthefollowing. Theorem16.2 Under Assumption 16.1 then (16.2)-(16.4) holds with (cid:88) â (cid:176) (cid:176)Îâ (cid:176) (cid:176)<â.TheprocessâY satisfies (cid:176) j(cid:176) t j=0 âY t =Î¾ t +U t âU tâ1 and Y =S +U +V t t t 0 whereS =(cid:80)t Î¾ isarandomwalk,Î¾ iswhitenoisewithvarianceÎ(1)Î£Î(1) (cid:48) , t i=1 t t U isstrictlystationary,andV isaninitialcondition. t 0 BeveridgeandNelson(1981)calledS thepermanent(trend)componentofY andU thetransitory t t t component.TheycalledS thepermanentcomponentsinceitdeterminesthelong-runbehaviorofY . t t Asanexample,taketheMA(1)caseâY t =e t +Î 1 e tâ1 . ThishasdecompositionâY t =(I m +Î 1 )e t â Î 1 (e t âe tâ1 ).InthiscaseU t =âÎ 1 e t . TheBeveridge-Nelsondecompositionofaseriesisuniquebutitisnottheonlywaytoconstructa permanent/transitorydecomposition. TheBeveridge-Nelsondecompositionhasthecharacteristicthat theinnovationsdrivingthepermanentandtransitorycomponentsS andU areidentical,sotheperma- t t nentandtransitorycomponentsarecorrelated.Forexample,intheMA(1)caseâS =e andU =âÎ e t t t 1 t areperfectlycorrelated.",
    "page": 570,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 551 Weclosethissectionbyverifying(16.2)-(16.4).Observethattheright-sideof(16.2)is â â â â â â â â (cid:88) Î j â (cid:88) (cid:88) Î (cid:96)zj(1âz)= (cid:88) Î j â (cid:88) (cid:88) Î (cid:96)zj+ (cid:88) (cid:88) Î (cid:96)zj+1 j=0 j=0(cid:96)=j+1 j=0 j=0(cid:96)=j+1 j=0(cid:96)=j+1 â â â â =Î 0 â (cid:88) (cid:88) Î (cid:96)zj+ (cid:88) (cid:88) Î (cid:96)zj j=1(cid:96)=j+1 j=1(cid:96)=j â =Î + (cid:88) Î zj 0 j j=1 whichisÎ(z)asclaimed. 16.4 FunctionalCLT Theorem16.1showedthatarandomwalkprocessconvergesindistributiontoaBrownianmotion. Wenowextendthisresulttothecaseofaunitrootprocesswithcorrelateddifferences. Under Assumption 16.1 a unit root process can be written as Y =S +U +V where S =(cid:80)t Î¾ . t t t 0 t i=1 t DefinethescaledprocessesZ n (r)=n â1/2Y(cid:98)nr(cid:99)andS n (r)=n â1/2S(cid:98)nr(cid:99).Wefind 1 1 Z n (r)=S n (r)+(cid:112) V 0 +(cid:112) U(cid:98)nr(cid:99). n n Iftheerrorse areaMDSwithcovariancematrixÎ£thenbyTheorem16.1,S (r)ââB(r),avectorBrow- t n d nian motion with covariance matrix â¦=Î(1)Î£Î(1) (cid:48) . The initial condition n â1/2V is o (1). The third 0 p (cid:175) (cid:175) term n â1/2U(cid:98)nr(cid:99) is o p (1) if sup 1â¤tâ¤n (cid:175) (cid:175) (cid:112)1 n U t (cid:175) (cid:175) =o p (1), which holds under Theorem 6.16 ifU t has a finite (cid:176) (cid:176) variance. WenowshowthatthisholdsunderAssumption16.1. Thelatterimpliesthat (cid:80)â (cid:176)Îâ(cid:176)<â, j=0(cid:176) j(cid:176) asdiscussedbeforeTheorem16.2.Thisimplies (cid:176) (cid:176) (cid:107)var[U ](cid:107)= (cid:176) (cid:176) (cid:88) â ÎâÎ£Îâ(cid:48) (cid:176) (cid:176)â¤(cid:107)Î£(cid:107)(cid:88) â (cid:176) (cid:176)Îâ (cid:176) (cid:176) 2 â¤(cid:107)Î£(cid:107)max (cid:176) (cid:176)Îâ (cid:176) (cid:176) (cid:88) â (cid:176) (cid:176)Îâ (cid:176) (cid:176)<â t (cid:176) j j (cid:176) (cid:176) j(cid:176) (cid:176) j(cid:176) (cid:176) j(cid:176) (cid:176)j=0 (cid:176) j=0 j j=0 asneeded. Togetherwefindthat Z (r)=S (r)+o (1)ââB(r). n n p d Thevarianceofthelimitingprocessisâ¦=Î(1)Î£Î(1) (cid:48) .Thisistheâlong-runvarianceâofâY . t Theorem16.3 UnderAssumption16.1andinaddition(e ,F )isaMDSwith t t covariancematrixÎ£, thenasafunctionoverr â[0,1], Z (r)ââB(r)avector n d Brownianmotionwithcovariancematrixâ¦. OurderivationusedtheassumptionthatthelinearprojectionerrorsareaMDS.Thisisnotessential forthebasicresult;theFCLTholdsunderavarietyofdependenceconditions. Aflexibleversioncanbe statedusingmixingconditions.",
    "page": 571,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 552 Theorem16.4 IfâY isstrictlystationary,(cid:69)[âY ]=0,withmixingcoefficients t t Î±((cid:96)),andforsomer >2,(cid:69)(cid:107)âY (cid:107)r <âand (cid:80)â Î±((cid:96))1â2/r <â,thenasafunc- t (cid:96)=1 tionoverr â[0,1], Z (r)ââB(r), avectorBrownianmotionwithcovariance n d matrix â (cid:88) â¦= (cid:69)[âY t âY tâ(cid:96)]. (16.5) (cid:96)=ââ ForaproofseeDavidson(2020,Theorems31.5and31.15). Interestingly,Theorem16.4employsex- actly the same assumptions as for Theorem 14.15 (the CLT for mixing processes). This means that we obtainthestrongerresult(theFCLT)withoutstrongerassumptions. Thecovariancematrixâ¦appearingin(16.5)isthelong-runcovariancematrixofâY asdefinedin t Section 14.13. It is useful to observe that we can decompose the long-run variance as â¦=Î£+Î+Î(cid:48) whereÎ£=var[âY ]and t â Î= (cid:88) (cid:69)(cid:163)âY âY (cid:48) (cid:164) . t tâ(cid:96) (cid:96)=1 Thisdecomposesthelong-runvarianceofâY intoitsstatic(one-period)varianceÎ£andasumofco- t variancesÎ.ThematrixÎisnotsymmetric. 16.5 OrdersofIntegration TakeaunivariateseriesY . Theorems16.3and16.4showedthatifâY isstationaryandmeanzero t t thenthelevelprocessY ,suitablyscaled,isasymptoticallyaBrownianmotionwithvarianceÏ2.Forthis t theorytobemeaningfulthisvarianceshouldbestrictlypositivedefinite. Toseewhythisisapotential restriction suppose that Y = a(L)e where the coefficients of a(z) are absolutely convergent and e is t t t i.i.d. (0,Ï2). ThenâY =b(L)e whereb(z)=(1âz)a(z)soÏ2=b(1)2Ï2=0. Thatis,âY hasalong-run t t t varianceof0.WecanthinkoftheprocessâY asover-differenced,sinceY isstrictlystationaryanddoes t t notrequiredifferencingtoachievestationarity. Tomeaningfullydifferentiatebetweenprocesseswhichrequiredifferencingtoachievestationarity weusethefollowingdefinition. Definition16.2 OrderofIntegration. 1. Y â(cid:82)isIntegratedofOrder0,writtenI(0),ifY isweaklystationarywith t t positivelong-runvariance. 2. Y â(cid:82)isIntegratedofOrderd,writtenI(d),ifu =âdY isI(0). t t t I(1)processesincluderandomwalksandunitrootprocessesmorebroadly. I(2)processesrequire double differencing to achieve stationarity. I(â1) processes are stationary but their cumulative sums arealsostationary. I(â1)processesareover-differencedstationaryprocesses,e.g. âY whenY issta- t t tionary. I(0)processesarestationaryprocesseswhicharenotover-differenced. Manymacroeconomic timeseriesinlog-levelsarepotentially I(1)processes. Economictimeserieswhicharepotentially I(2) are log price indices, for their first difference (inflation rates) are potentially non-stationary proceses.",
    "page": 572,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 553 Inthistextbookwefocusoninteger-valuedordersofintegrationbutfractionald arealsowell-defined. InmostapplicationseconomistspresumethateconomicseriesareeitherI(0)orI(1)andoftenusethe shorthandâintegratedâtorefertoI(1)series. Thelong-runvarianceofARMAprocessesisstraightforwardtocalculate. Aswehaveseen,ifâY = t b(L)e where e is white noise with variance Ï2, then Ï2 =b(1)2Ï2. Now suppose a(L)âY =e where t t t t a(z) is invertible. Then b(z)=a(z) â1 and Ï2 =Ï2/a(1)2. For an ARMA process a(L)âY =b(L)e with t t invertiblea(z),thenÏ2=Ï2b(1)2/a(1)2.Hence,ifâY satisfiestheARMAprocessa(L)âY =b(L)e then t t t Y isI(1)ifa(z)isinvertibleandb(1)(cid:54)=0. t Considervectorprocesses. Thelong-runcovariancematrixofâY =Î(L)e isâ¦=Î(1)Î£Î(1) (cid:48) . The t t long-run covariance matrix of A(L)âY =e is â¦= A(1) â1Î£A(1) â1(cid:48) . It is conventional to describe the t t vectorâY as I(0)ifeachelementofâY is I(0)butthisallowsitscovariancematrixtobesingular. To t t excludethelatterweintroducethefollowing. Definition16.3 The vector process Y is full rank I(0) if its long-run covari- t ancematrixâ¦ispositivedefinite. 16.6 Means,LocalMeans,andTrends Theorem16.4showsthatZ (r)ââB(r). Thecontinuousmappingtheoremshowsthatifafunction n d f(x)iscontinuous2then f(Z )ââf(B).Thiscanbeusedtoobtaintheasymptoticdistributionofmany n d statistics of interest. Simple examples are Z (r)2 ââ B(r)2 and (cid:82)1 Z (r)dr ââ (cid:82)1 B(r)dr. The latter n 0 n 0 d d producestheasymptoticdistributionforthesamplemeanaswenowshow. LetY =n â1(cid:80)n Y bethesamplemean.ForsimplicityassumeY =0.Notethatforr â(cid:163)t,t+1(cid:162) , n t=1 t 0 n n 1 (cid:90) (t+1)/n Y =Z (r)=n Z (r)dr. n1/2 t n t/n n Takingtheaveragefort=0tonâ1wefind 1 1 n (cid:88) â1 n (cid:88) â1(cid:90) (t+1)/n (cid:90) 1 Y = Y = Z (r)dr = Z (r)dr. n1/2 n n3/2 t=0 t t=0 t/n n 0 n Thisistheintegral(oraverage)ofZ (r)over[0,1]. n Thecontinuousmappingtheoremcanbeapplied3. Theaboveexpressionconvergesindistribution (cid:82)1 totherandomvariable B(r)dr.ThisistheaverageoftheBrownianmotionover[0,1]. 0 Now consider sub-sample means. Let Y =(n/2) â1(cid:80)n/2â1Y and Y =(n/2) â1(cid:80)nâ1 Y be the 1n t=0 t 2n t=n/2 t samplemeansonthefirst-halfandsecond-halfofthesample,respectively. Byasimilaranalysisasfor thefull-samplemean 1 2 n/ (cid:88) 2â1 (cid:90) 1/2 (cid:90) 1/2 Y = Y =2 Z (r)dr ââ2 B(r)dr n1/2 1n n3/2 t=0 t 0 n d 0 1 2 n (cid:88) â1 (cid:90) 1 (cid:90) 1 Y = Y =2 Z (r)dr ââ2 B(r)dr n1/2 2n n3/2 t=n/2 t 1/2 n d 1/2 2WithrespecttotheuniformmetricÏ. 3Theintegral f(g)=(cid:82)1g(r)dr isacontinuousfunctionofg withrespecttotheuniformmetric. (Smallchangesing result 0 insmallchangesinf.)",
    "page": 573,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 554 whicharetheaveragesofB(r)overtheregions[0,1/2]and[1/2,1]. Thesearedistinctrandomvariables. This gives rise to the prediction that if Y is a unit root process, sample averages will not be constant t (evenapproximatelyinlargesamples)andwillvaryacrosssubsamples. Furthermore, observethatthelimitdistributionswereobtainedafterdividingbyn1/2. Thismeans thatwithoutthisstandardizationthesamplemeanwouldnotbeboundedinprobability. Thisimplies thatthe sample meancanbe (randomly) large. This leads to the rather peculiar property that sample meanswillbelarge,random,andnon-informativeaboutpopulationparameters.Thismeansthatinter- pretingsimplestatisticssuchasmeansistreacherouswhentheseriesmaybeaunitrootprocess. 0 40 80 120 160 200 240 8 6 4 2 0 2â 4â 6â 8â 01â Sample Mean 0 40 80 120 160 200 240 8 6 4 2 0 2â 4â 6â 8â 01â Mean, tÂ£ 120 Mean, t>120 0 40 80 120 160 200 240 (a)FittedMeans 8 6 4 2 0 2â 4â 6â 8â 01â Fitted Trend (b)FittedTrend Figure16.1:RandomWalkwithFittedMean,Sub-SampleMeans,andTrend Toillustrate,Figure16.1(a)displaysasimulatedrandomwalkwithn=240observations.Alsoplotted isthesamplemeanY =â2.98,alongwiththesub-samplemeansY =â0.75andY =â5.21.Aspre- n 1n 2n dicted,themeanandsub-samplemeansarelarge,variable,anduninformativeregardingthepopulation mean. NowconsideralinearregressionofY onalineartimetrend.Themodelforestimationis t Y =Î² +Î² t+e =X (cid:48)Î²+e t 0 1 t t t whereX t =(1,t) (cid:48) . AgainforsimplicityassumethatY 0 =0. TaketheleastsquaresestimatorÎ² (cid:98). Theorem 14.36showsthat 1 (cid:88) n (cid:90) 1 1 tâ rdr = n2 t=1 0 2 1 (cid:88) n t2â (cid:90) 1 r2dr = 1 . n3 t=1 0 3",
    "page": 574,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 555 (cid:183) (cid:184) 1 0 DefineD = .Wecalculatethat n 0 n ï£® 1(cid:88) n 1 (cid:88) n ï£¹ D n â1 n 1 t (cid:88) = n 1 X t X t (cid:48) D n â1= ï£¯ ï£¯ ï£¯ ï£° n 1 t (cid:88) = n 1 1 t n 1 2 (cid:88) t n =1 t t 2 ï£º ï£º ï£º ï£» â (cid:34) (cid:82) 0 1 1 rdr (cid:82) (cid:82) 0 1 0 1 r r 2 d d r r (cid:35) = (cid:90) 0 1 X(r)X(r) (cid:48) dr n2 t=1 n3 t=1 whereX(r)=(1,r). AnapplicationofthecontinuousmappingtheoremwithTheorem16.1yields D â1 1 (cid:88) n X Y = (cid:90) 1 X(r)Z (r)dr ââ (cid:90) 1 X(r)B(r)dr. n n3/2 t=1 t t 0 n d 0 Togetherweobtain (cid:181) n (cid:182)â1(cid:181) n (cid:182) D n n â1/2Î² (cid:98) =D n n â1/2 (cid:88) X t X t (cid:48) (cid:88) X t Y t t=1 t=1 = (cid:181) D â1 1(cid:88) n X X (cid:48) D â1 (cid:182)â1(cid:181) D â1 1 (cid:88) n X Y (cid:182) n n t=1 t t n n n3/2 t=1 t t (cid:181)(cid:90) 1 (cid:182)â1(cid:181)(cid:90) 1 (cid:182) ââ X(r)X(r) (cid:48) dr X(r)B(r)dr . d 0 0 ThisshowsthattheestimatorÎ² (cid:98)hasanasymptoticdistributionwhichisatransformationoftheBrownian (cid:179) (cid:82)1 (cid:48) (cid:180)â1(cid:179) (cid:82)1 (cid:180) motionB(r).Forcompactnessweoftenwritethefinalexpressionas XX XB . 0 0 To illustrate, Figure 16.1(b) displays the random walk from panel (a) along with a fitted trend line. Thefittedtrendappearslargeandsubstantial. Howeveritispurelyrandom,afeatureonlyofthisspe- cific realization, is uninformative about the underlying parameters, and is dangerously misleading for prediction. 16.7 DemeaningandDetrending A common preliminary step in time series analysis is demeaning (subtracting off a mean) and de- trending(subtractingoffalineartrend).Withstationaryprocessesthisdoesnotaffectasymptoticinfer- ence. Incontrast,animportantpropertyofunitrootprocessesisthattheirbehaviorisalteredbythese transformations. Takedemeaning. ThedemeanedversionofY isY â=Y âY . AnimportantobservationisthatY â t t t n t isinvarianttotheinitialconditionY ,sowithoutlossofgeneralitywesimplyassumeY =0. 0 0 Thenormalizedprocessis Z n â (r)= (cid:112) 1 Y(cid:98)nr(cid:99) â(cid:112) 1 Y n =Z n (r)âZ n (1)ââB(r)â (cid:90) 1 B d=ef B â (r). n n d 0 B â (r)isdemeanedBrownianmotion.Ithasthepropertythat (cid:82)1 B â (r)dr =0. 0 Take linear detrending. Based on least squares estimation of a linear trend the detrended series is Y t ââ=Y t âX t (cid:48)Î² (cid:98)where X t =(1,t) (cid:48) . LikethedemeanedseriesthedetrendedseriesisinvarianttoY 0 .The",
    "page": 575,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 556 associatednormalizedprocessis Z n ââ (r)= (cid:112) 1 n Y(cid:98)nr(cid:99) â(cid:112) 1 n X (cid:98) (cid:48) nr(cid:99) Î² (cid:98) =Z n (r)âX((cid:98)nr(cid:99)/n) (cid:48) D n (cid:112) 1 Î² (cid:98) n (cid:181)(cid:90) 1 (cid:182)â1(cid:181)(cid:90) 1 (cid:182) ââB(r)âX(r) (cid:48) XX (cid:48) XB d=ef B ââ (r). d 0 0 B ââ (r) is the continuous-time residual of the Brownian motion B(r) projected orthogonal to X(r) = (cid:48) ââ (1,r).WecallB (r)detrendedBrownianmotion. Thereisanothermethodofdetrendingthroughfirstdifferencing. SupposethatY =Î² +Î² t+Z . t 0 1 t ThefirstdifferenceisâY =Î² +âZ .AnestimatorofÎ² isthesamplemeanofâY : t 1 t 1 t âY = 1 (cid:88) n âY = Y n âY 0 . n t n t=1 n The normalization Z =0 implies Y =Î² so an estimator of Î² is Y . The detrended version of Y is 0 0 0 0 0 t Y(cid:101)t =Y t âY 0 â(t/n)(Y n âY 0 ).Theassociatednormalizedprocessis (cid:98)nr(cid:99) Z(cid:101)n (r)=Z n (r)â Z n (1)ââB(r)ârB(1) d=ef V(r). n d V(r)iscalledaBrownianBridge. Itisalsoknownasatied-downBrownianmotion. Ithastheproperty thatV(0)=V(1)=0. It is also a detrended version of B(r) but is distinct from the linearly detrended â versionB (r). Wesummarizethefindingsinthefollowingtheorem. Theorem16.5 UndertheconditionsofeitherTheorem16.3orTheorem16.4, thenasnââ 1. Z â (r)ââB â (r) n d 2. Z ââ (r)ââB ââ (r) n d 3. Z(cid:101)n (r)ââV(r). d To illustrate, Figure 16.2 displays two detrended versions of the series from Figure 16.1. Panel (a) â showsthelineardetrendedseriesY t . Panel(b)showsthefirst-differencedetrendedseriesY(cid:101)t . Theyare visuallysimilartooneanotherandtoFigure16.1exceptthatthestronglineartrendhasbeenremoved. 16.8 StochasticIntegrals The distribution of the least squares estimator in the regression model Y = X (cid:48)Î²+e requires the t t t nâ1 distributionofthesamplemomentsn â1 (cid:88) X t e t+1 . When X t isnon-stationarythelimitdistributionis t=1 non-standardandequalsastochasticintegral.",
    "page": 576,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 557 0 40 80 120 160 200 240 01 8 6 4 2 0 2â 4â 6â 0 40 80 120 160 200 240 ââ (a)LinearlyDetrendedY t 01 8 6 4 2 0 2â 4â 6â (b)FirstDifferenceDetrendedY(cid:101)t Figure16.2:DetrendedRandomWalk ItmayhelptorecallthedefinitionoftheRiemann-Stieltijesintegral.Overtheregion[0,1]theintegral ofg(x)withrespectto f(x)is (cid:90) 1 N (cid:88) â1 (cid:181) i (cid:182)(cid:181) (cid:181) i+1 (cid:182) (cid:181) i (cid:182)(cid:182) g(x)df(x)= lim g f âf . 0 Nââ i=0 N N N Astochasticintegralisthecasewherethefunction f israndomandisdefinedasaprobabilitylimit. Definition16.4 Thestochasticintegralofvector-valued X(r)withrespectto vector-valuedZ(r)over[0,1]is (cid:90) 1 XdZ (cid:48)= (cid:90) 1 X(r)dZ(r) (cid:48)=plim N (cid:88) â1 X (cid:181) i (cid:182)(cid:181) Z (cid:181) i+1 (cid:182) âZ (cid:181) i (cid:182)(cid:182)(cid:48) . 0 0 Nââ i=0 N N N Nowconsiderthefollowingsetting. Let(X ,e )bevector-valuedsequenceswheree isaMDSwith t t t finitecovarianceandX isnon-stationary.AssumethatforsomescalingsequenceD thescaledprocess t n X n (r)=D n â1X(cid:98)nr(cid:99)satisfiesX n (r)ââX(r)forsomedeterministicorstochasticprocessX(r).Examplesof d X sequencesincludethepartialsumprocessconstructedfrome oranothershock,adetrendedversion t t of a partial sum process, or a deterministic trend proceses. We desire the asymptotic distribution of (cid:80)nâ1X e (cid:48) . Definethepartialsumprocessfore asS (r)=n â1/2(cid:80)(cid:98)nr(cid:99) e . FromTheorem16.1,S ââ t=1 t t+1 t n t=1 t n d B.Wecalculatethat (cid:112) 1 D â1 n (cid:88) â1 X e (cid:48) = n (cid:88) â1 X (cid:181) t (cid:182)(cid:181) S (cid:181) t+1 (cid:182) âS (cid:181) t (cid:182)(cid:182)(cid:48) = (cid:90) 1 X dS (cid:48) . n n t=0 t t+1 t=0 n n n n n n 0 n n TheequalitiesholdbecauseS (r)and X (r)arestepfunctionswithjumpsatr =t/n. Since X (r)and n n n (cid:82)1 (cid:82)1 S (r) converge to X(r) and B(r), by analogy we expect X dS to converge to XdB. This is true, n 0 n n 0",
    "page": 577,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 558 but rather tricky to show since the stochastic integral is not a continuous function of B(r). A general statementoftheconditionshasbeenprovidedbyKurtzandProtter(1991,Theorem2.2). Thefollowing isasimplificationoftheirresult. Theorem16.6 If(e ,F )isamartingaledifferencesequence,(cid:69)(cid:163) e e (cid:48)(cid:164)=Î£<â, t t t t X âF ,and(X (r),S (r))ââ(X(r),B(r))then t t n n d (cid:90) 1 X n dS n (cid:48) = (cid:112) 1 D n â1 n (cid:88) â1 X t e t+1 ââ (cid:90) 1 XdB (cid:48) 0 n t=1 d 0 whereB(r)isaBrownianmotionwithcovariancematrixÎ£. ThebasicapplicationofTheorem16.6istothecase X (r)=S (r). ThusifS =(cid:80)t e ande isa n n t i=1 t t MDSwithcovariancematrixÎ£then 1n (cid:88) â1 S e (cid:48) ââ (cid:90) 1 BdB (cid:48) . n t=1 t t+1 d 0 Wecanextendthisresulttothecaseofseriallycorrelatederrors. Theorem16.7 If Z satisfiestheconditionsofTheorem16.4andS =(cid:80)t Z t t i=1 t then 1n (cid:88) â1 S Z (cid:48) ââ (cid:90) 1 BdB (cid:48)+Î n t=1 t t+1 d 0 whereB(r)isaBrownianmotionwithcovariancematrixâ¦=Î£+Î+Î(cid:48) , Î£= (cid:69)(cid:163) Z t Z t (cid:48)(cid:164) ,andÎ=(cid:80)â j=1 (cid:69)(cid:163) Z tâj Z t (cid:48)(cid:164) . TheproofispresentedinSection16.22. 16.9 EstimationofanAR(1) ConsiderleastsquaresestimationoftheAR(1)parameterÎ±inthemodelY t =Î±Y tâ1 +e t .Thecentered estimatorisÎ± (cid:98) âÎ±=(cid:161)(cid:80)n t= â 1 1Y t 2(cid:162)â1(cid:161)(cid:80)n t= â 1 1Y t e t+1 (cid:162) .Weusethescaling 1n (cid:88) â1 Y t e t+1 n(Î±âÎ±)= n t=1 . (cid:98) 1 n (cid:88) â1 Y2 n2 t=1 t WeexaminethedenominatorandnumeratorseparatelyundertheassumptionÎ±=1. Similarlytoouranalysisofthesamplemeanthedenominatorcanbewrittenasanintegral.Thus 1 n (cid:88) â1 Y2= 1n (cid:88) â1(cid:181) 1 Y (cid:182)2 = (cid:90) 1 Z (r)2dr ââ (cid:90) 1 B(r)2dr =Ï2 (cid:90) 1 W(r)2dr. n2 t=1 t n t=1 n1/2 t 0 n d 0 0",
    "page": 578,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 559 Theconvergenceisbythecontinuousmappingtheorem4. ThefinalequalityrecognizesthatifB(r)has varianceÏ2 thenB(r)2=Ï2W(r)2 whereW(r)isstandardBrownianmotion. Forconcisenessweoften writethefinalintegralas (cid:82)1 W2. 0 ForthenumeratorweappealtoTheorem16.6. 1n (cid:88) â1 Y t e t+1 = (cid:90) 1 Z n dS n ââ (cid:90) 1 BdB=Ï2 (cid:90) 1 WdW. n t=1 0 d 0 0 Thislimitingstochasticintegralisquitefamous.ItisknownasItÃ´âsintegral. Theorem16.8 ItÃ´âsIntegral (cid:82)1 WdW = 1(cid:161) W(1)2â1 (cid:162) . 0 2 IfyouarenotsurprisedbyItÃ´âsintegraltakeanotherlook. Thederivativeof 1W(r)2 isW(r)dW(r). 2 ThusbystandardcalculusandW(0)=0youmightexpect (cid:82)1 WdW = 1W(1)2.Thepresenceoftheextra 0 2 termâ1/2issurprising.ThisarisesbecauseW(r)hasunboundedvariation. TherandomvariableW(1)2 isÏ2 whichhasexpectation1. Thereforetherandomvariable (cid:82)1 WdW 1 0 ismeanzerobutskewed. TheproofofTheorem16.8ispresentedinSection16.22. ReturningtotheleastsquaresestimationproblemwehaveshownthatwhenÎ±=1 Ï2(cid:161) W(1)2â1 (cid:162) (cid:82)1 WdW n(Î±â1)ââ 2 = 0 . (cid:98) d Ï2 (cid:82)1 W2 (cid:82)1 W2 0 0 Theorem16.9 Dickey-FullerCoefficientDistributionIfY t =Î±Y tâ1 +e t with Î±=1,and(e ,F )isastrictlystationaryandergodicmartingaledifferencese- t t quencewithafinitevariance,then (cid:82)1 WdW n(Î±â1)ââ 0 . (cid:98) d (cid:82)1 W2 0 ThelimitdistributioninTheorem16.9isknownastheDickey-FullerDistributionduetothework of Wayne Fuller and David Dickey. Theorem 16.9 shows that the least squares estimator is consistent forÎ±=1andconvergesattheâsuper-consistentârateO (cid:161) n â1(cid:162) . Thelimitdistributionisnon-standard p andiswrittenasafunctionoftheBrownianmotionW(r). Thereisnotaclosed-formexpressionforthe distributionordensityofthestatistic.Mostcommonlyitiscalculatedbysimulation. ThedensityoftheDickey-Fullercoefficientdistributionisdisplayed5 asthesolidline(labeledâNo ConstantorTrendâ)inFigure16.3(a). Youcanseethatthedensityishighskewedwithalonglefttail. 4Thefunctiong(f)=(cid:82)1f(x)2dxiscontinuouswithrespecttotheuniformmetric. 0 5ThedensitiesinFigure16.3wereestimatedfromonemillionsimulationdrawsofthefinitesampledistributionforasample sizen=10,000. Thedensitieswereestimatedusingnonparametrickernelmethods(seeChapter17ofIntroductiontoEcono- metrics).",
    "page": 579,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 560 Youcanseethatmostoftheprobabilitymassofthedistributionisoverthenegativeregion.Thishasthe implicationthatthedensityhasanegativemeanandmedian. Hencetheasymptoticdistributionofthe leastsquaresestimatorisbiasednegatively. ThishasthepracticalimplicationthatwhenÎ±=1theleast squaresestimatorisbiasedawayfromone. We can also examine the limit distribution of the t-ratio. Let e (cid:98)t =Y t âÎ± (cid:98) Y tâ1 be the least squares (cid:113) residual,Ï2=n â1(cid:80) e2 theleastsquaresvarianceestimator,ands(Î±)=Ï/ (cid:80) Y2 theclassicalstandard (cid:98) (cid:98)t (cid:98) (cid:98) t errorforÎ±.Thet-ratioforÎ±isT =(Î±â1)/s(Î±). (cid:98) (cid:98) (cid:98) Theorem16.10 Dickey-FullerTDistributionUndertheassumptionsofThe- orem16.9 Î±â1 (cid:82)1 WdW T = (cid:98) ââ 0 . s(Î± (cid:98) ) d (cid:179) (cid:82)1 W2 (cid:180)1/2 0 ThelimitdistributioninTheorem16.10isknownastheDickey-FullerTdistribution.Theorem16.10 showsthattheclassicalt-ratioconvergestoanon-standardasymptoticdistribution.Thereisnoclosed- formexpressionforthedistributionordensitysoitistypicallycalculatedusingsimulationtechniques. TheproofispresentedinSection16.22. ThedensityoftheDickey-FullerTdistributionisdisplayedasthesolidline(labeledâNoConstantor Trendâ)inFigure16.3(b). Youcanseethatthedensityisskewedbutmuchlesssothanthecoefficient distribution. Thedistributionappearstobeaâfatterâversionoftheconventionalstudenttdistribution. An implication is that conventional inference (confidence intervals and tests) will be inaccurate. We discusstestinginSection16.13. No Constant or Trend No Constant or Trend Constant Included Constant Included Constant Included Constant and Trend Constant and Trend Constant and Trend â30 â25 â20 â15 â10 â5 0 â5 â4 â3 â2 â1 0 1 2 3 0.0 0.1 0.2 0.3 0.4 0.5 (a)DFCoefficientDistribution (b)DFTDistribution (c)KPSSDistribution Figure16.3:UnitRootTestDistributions 16.10 AR(1)EstimationwithanIntercept SupposethatY isarandomwalkandweestimateanAR(1)modelwithanintercept. Theestimated t modelisY t =Âµ+Î±Y tâ1 +e t .BytheFrisch-Waugh-LovellTheorem(Theorem3.5)theleastsquaresestima- torÎ±ofÎ±canbewrittenasthesimpleregressionusingthedemeanedseriesY â .Thatis,thenormalized (cid:98) t",
    "page": 580,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 561 estimatoris 1n (cid:88) â1 â n(Î±â1)= n t=1 Y t e t+1 (cid:98) 1 n (cid:88) â1 Y â2 n2 t=1 t where Y â =Y âY with Y = 1n (cid:88) â1 Y . By Theorems 16.5.1 and 16.6 the calculations from the previous t t n t=1 t sectionshowthat (cid:82)1 â W dW n(Î±â1)ââ 0 . (cid:98) d (cid:82)1 W â2 0 ThisissimilartothedistributioninTheorem16.9. ThisisknownastheDickey-Fullercoefficientdistri- butionforthecaseofanincludedconstant. Similarly if we estimate an AR(1) model with an intercept and trend the estimated model is Y = t Âµ+Î²t+Î±Y tâ1 +e t .BytheFrisch-Waugh-LovellTheoremthisisequivalenttoregressiononthedetrended ââ seriesY .ApplyingTheorems16.5.2and16.6,wefind t (cid:82)1 ââ W dW n(Î±â1)ââ 0 . (cid:98) d (cid:82)1 W ââ2 0 ThisisknownastheDickey-Fullercoefficientdistributionforthecaseofanincludedconstantandlinear trend. Similarresultsariseforthet-ratios.Wesummarizetheresultsinthefollowingtheorem. Theorem16.11 UndertheassumptionsofTheorem16.9,forthecaseofanes- timatedAR(1)withanintercept (cid:82)1 â W dW n(Î±â1)ââ 0 (cid:98) d (cid:82)1 W â2 0 (cid:82)1 â W dW T ââ 0 . d (cid:179) (cid:82)1 W â2 (cid:180)1/2 0 ForthecaseofanestimatedAR(1)withaninterceptandlineartimetrend (cid:82)1 ââ W dW n(Î±â1)ââ 0 (cid:98) d (cid:82)1 W ââ2 0 (cid:82)1 ââ W dW T ââ 0 . d (cid:179) (cid:82)1 W ââ2 (cid:180)1/2 0 ThedensitiesoftheDickey-FullercoefficientdistributionsaredisplayedinFigure16.3(a). Theden- sities are considerably affected by the inclusion of the constant or constant and trend. The effect is twofold: (1)thedistributionsshiftsubstantiallytotheleft;and(2)thedistributionssubstantiallywiden.",
    "page": 581,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 562 Examiningthe âconstantandtrendâversionwe cansee thatthere is very little probability mass above zero. Thismeansthattheasymptoticdistributionisnotonlybiaseddownward,therealizationisnearly always negative. This has the practical implication that the least squares estimator is almost certainly lessthanthetruecoefficientvalue.Thisisastrongformofbias. ThedensitiesoftheDickey-FullerTdistributionsaredisplayedinFigure16.3(b). Theeffectofde- trendingontheTdistributionsisquitedifferentfromtheeffectonthecoefficientdistirbutions.Herewe seethattheprimaryeffectisalocationshiftwithonlyamildimpactondispersion. Thestronglocation shiftisabiasintheasymptoticTdistribution,implyingthatconventionalinferenceswillbeincorrect. 16.11 SampleCovariancesofIntegratedandStationaryProcesses Let (X ,u ) be a sequence where X is non-stationary and u is mean zero and strictly stationary. t t t t AssumethatforsomescalingsequenceD n thescaledprocessX n (r)=D n â1X(cid:98)nr(cid:99)satisfiesX n (r)ââX(r) d whereX(r)iscontinuouswithprobabilityone.Considerthescaledsamplecovariance C = 1 D â1(cid:88) n X u . n n n t=1 t t Theorem16.12 Assumethat X n (r)=D n â1X(cid:98)nr(cid:99) ââX(r)where X(r)isalmost d surelycontinuous.Assumeu ismeanzero,strictlystationaryandergodic,and t (cid:69)|u |<â.ThenC ââ0asnââ. t n p TheproofispresentedinSection16.22. 16.12 AR(p)ModelswithaUnitRoot AssumethatY satisfiesa(L)âY =e wherea(z)isapâ1orderinvertiblelagpolynomialande isa t t t t stationaryMDSwithfinitevarianceÏ2.ThenY canbewrittenastheAR(p)process t Y t =a 1 Y tâ1 +Â·Â·Â·+a p Y tâp +e t (16.6) wherethecoefficientssatisfya +Â·Â·Â·+a =1. Leta betheleastsquaresestimatorofa=(a ,...,a ). We 1 p (cid:98) 1 p nowdescribeitssamplingdistribution. LetB bethe pÃp matrixwhichtransforms(Y tâ1 ,...,Y tâp )to(Y tâ1 ,âY tâ1 ,...,âY tâp+1 ), forexample ï£® ï£¹ 1 0 0 whenp=3thenB= ï£° 1 â1 0 ï£». MakethepartitionB â1(cid:48) a=(Ï,Î²)whereÏâ(cid:82)andÎ²â(cid:82)pâ1. Then 0 1 â1 theAR(p)modelcanbewrittenas Y t =ÏY tâ1 +Î²(cid:48) X tâ1 +e t (16.7) where X tâ1 =(âY tâ1 ,...,âY tâp+1 ). The leading coefficient is Ï =a 1 +Â·Â·Â·+a p =1. This transformation separatestheregressorsintotheunitrootcomponentY tâ1 andthestationarycomponentX tâ1 .",
    "page": 582,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 563 Considertheleastsquaresestimators(Ï (cid:98) ,Î² (cid:98)).Theycanbewrittenundertheassumptionofaunitroot as (cid:181) (cid:112) n n (cid:161) (cid:161) Ï (cid:98) Î² (cid:98) â â 1 Î² (cid:162) (cid:162) (cid:182) = ï£« ï£¬ ï£¬ ï£¬ ï£­ n 1 3/ n 2 1 t 2 = (cid:88) t 1 n = + (cid:88) 1 n p + X p Y tâ t 2 â 1 Y 1 tâ1 n n 1 1 3/ t 2 = (cid:88) t 1 n = (cid:88) + 1 n p + X p Y tâ tâ 1 X 1 X t (cid:48) â t (cid:48) â 1 1 ï£¶ ï£· ï£· ï£· ï£¸ â1ï£« ï£¬ ï£¬ ï£¬ ï£­ (cid:112) n 1 1 n t= t (cid:88) = 1 n (cid:88) 1 n + + p p Y X tâ tâ 1 e 1 e t t ï£¶ ï£· ï£· ï£· ï£¸ . Theorems16.4andtheCMTshowthat 1 (cid:88) n Y2 ââÏ2 (cid:90) 1 W2 n2 t=1+p tâ1 d 0 whereÏ2isthelong-runvarianceofâY whichequalsÏ2=Ï2/a(1)2>0. t Theorem16.12showsthat 1 (cid:88) n n3/2 t=1+p X tâ1 Y tâ1 â p â0. Theorems16.4and16.6showthat 1 (cid:88) n (cid:90) 1 Y tâ1 e t ââÏÏ WdW. n t=1+p d 0 TheWLLNandtheCLTforstationaryprocessesshowthat n 1 t= (cid:88) 1 n +p X tâ1 X t (cid:48) â1 â p âQ 1 (cid:88) n (cid:112) X tâ1 e t ââN(0,â¦) n t=1+p d whereQ=(cid:69)(cid:163) X tâ1 X t (cid:48) â1 (cid:164) andâ¦=(cid:69)(cid:163) X tâ1 X t (cid:48) â1 e t 2(cid:164) .Togetherwehaveestablishedthefollowing. Theorem16.13 AssumethatY satisfiesa(L)âY =e wherea(z)isapâ1order t t t invertiblelagpolynomialand(e ,â )isastationaryMDSwithfinitevariance t t Ï2.Then ï£« (cid:82)1 ï£¶ WdW 0 (cid:181) (cid:112) n n (cid:161) (cid:161) Ï (cid:98) Î² (cid:98) â â 1 Î² (cid:162) (cid:162) (cid:182) â d â ï£¬ ï£¬ ï£¬ ï£­ a(1) (cid:82) 0 1 W2 ï£· ï£· ï£· ï£¸ (16.8) N(0,V) whereV =Q â1â¦Q â1. Thistheoremprovidesanasymptoticdistributiontheoryfortheleastsquaresestimators. Theesti- mator(a (cid:98) ,Î² (cid:98))isconsistent,thecoefficientÎ² (cid:98)onthestationaryvariablesisasymptoticallynormal,andthe coefficientaontheunitrootcomponenthasascaledDickey-Fullerdistribution. (cid:98) The estimator of the representation (16.6) is the linear transformation B (cid:48) (Ï (cid:98) ,Î² (cid:98) (cid:48) ) (cid:48) , and therefore its (cid:48) asymptoticdistributionisthetransformationB of(16.8). Sincetheunitrootcomponentconvergesata fasterO (n â1)ratethanthestationarycomponentitdropsoutoftheasymptoticdistribution.Weobtain p (cid:112) n(aâa)ââN(0,GVG (cid:48) ) (16.9) (cid:98) d",
    "page": 583,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 564 where,inthep=3case ï£® ï£¹ 1 0 G= ï£° â1 1 ï£». 0 â1 TheasymptoticcovariancematrixGVG (cid:48) isdeficientwithrankpâ1. Hencethisisonlyapartialcharac- terizationoftheasymptoticdistribution;equation(16.8)isacompletefirst-ordercharacterization. The implicationof(16.9)isthatindividualcoefficientestimatorsandstandarderrorsof(16.6)haveconven- tionalasymptoticinterpretations. Thisextendstoconventionalhypothesistestswhichdonotinclude thesumofthecoefficients. Formostpurposes(excepttestingtheunitroothypothesis)thismeansthat asymptoticinferenceonthecoefficientsof(16.6)canbebasedontheconventionalnormalapproxima- tionandcanignorethepossiblepresenceofunitroots. 16.13 TestingforaUnitRoot The asymptotic properties of the time series process change discontinuously at the unit root Ï = a +Â·Â·Â·+a =1. It is therefore of standard interest to test the hypothesis of a unit root. We typically 1 p expressthisasthetestof(cid:72) :Ï=1against(cid:72) :Ï<1. Wetypicallyviewthetestasone-sidedasweare 0 1 interestedinthealternativehypothesisthattheseriesisstationary(notthatitisexplosive). Thetestfor(cid:72) vs.(cid:72) isthet-statisticfora +Â·Â·Â·+a =1intheAR(p)model(16.6).Thisisidenticalto 0 1 1 p thet-statisticforÏ=1inreparameterizedform(16.7). Sincethelatterisasimplet-ratiothisisthemost convenientimplementation.ItistypicallycalledtheAugmentedDickey-Fullerstatistic.Itequals Ïâ1 ADF= (cid:98) s (cid:161)Ï(cid:162) (cid:98) wheres (cid:161)Ï(cid:162) isastandarderrorforÏ. Thist-ratioistypicallycalculatedusingaclassical(homoskedastic) (cid:98) (cid:98) standarderror,perhapsforhistoricalreasons,andperhapsbecausetheasymptoticdistributionofADF isinvarianttoconditionalheteroskedasticity.ThestatisticiscalledtheADFstatisticwhentheestimated modelisanAR(p)modelwithp>1;itistypicallycalledtheDickey-Fullerstatisticiftheestimatedmodel isanAR(1). TheasymptoticdistributionofADFdependsonthefitteddeterministiccomponents.Theteststatis- tic is most typically calculated in a model with a fitted intercept or a fitted intercept and time trend, thoughthetheoryisalsopresentedforthecasewithnofittedinterceptandextendstoanypolynomial ordertrend. LetZÎ±denotetheÎ±th quantileoftheDickey-FullerTdistribution(fromTheorem16.10orTheorem 16.11).",
    "page": 584,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 565 Theorem16.14 AssumethatY satisfiesa(L)âY =e wherea(z)isapâ1order t t t invertiblelagpolynomialand(e ,â )isastationaryMDSwithfinitevariance t t Ï2.Then (cid:82)1 UdW ADFââ 0 d (cid:179) (cid:82)1 U2 (cid:180)1/2 0 whereW isBrownianmotion. TheprocessU dependsonthefitteddetermin- isticcomponents: 1. Case1:Nointerceptortrendincluded.U(r)=W(r). 2. Case2:Fittedintercept(demeaneddata).U(r)=W(r)âr (cid:82)1 W. 0 3. Case 3: Fitted intercept and trend (detrended data). U(r) = W(r)â X(r) (cid:48) (cid:179) (cid:82)1 XX (cid:48) (cid:180)â1(cid:179) (cid:82)1 XW (cid:180) whereX(r)=(1,r) (cid:48) . 0 0 ThetestâReject(cid:72) 0 ifADF<ZÎ±âhasasymptoticsizeÎ±. The asymptotic critical values6 are displayed in Table 16.1. This is a one-sided hypothesis test so rejectionsoccurwhentheteststatisticislessthan(morenegativethan)thecriticalvalue.Noticethatthe criticalvaluesarelargerthanthestandardnormalapproximation. Forthecaseofafittedinterceptthe 5%criticalvalueisâ2.9. Forthecaseofafittedtrenditisâ3.4. Thismeansthataweneedtobecareful withourconventionalintuitionwheninterpretingADFestimatesandtests. InmostapplicationsanADFtestisimplementedwithatleastafittedintercept(whichisthemiddle line of the table). Many are implemented with a fitted linear time trend (which is the third line). The choice depends on the nature of the alternative hypothesis. If (cid:72) is that the series is stationary about 1 a constant mean then the case of a fitted intercept is appropriate. Example series for this context are unemploymentandinterestrates. If(cid:72) isthattheseriesisstationaryaboutalineartrendthenthecase 1 ofafittedtrendisappropriate. Examplesforthiscontextarelevelsorlog-levelsofmacroeconomicag- gregates. The ADF test depends on the autoregressive order p. The issue of selection of p is similar to that ofautoregressivemodelselection. Ingeneral,if p istoosmallthanthemodelismis-specifiedandthe ADFstatistichasanasymptoticbias. Ifp istoolargethanthetestcoefficientaisimpreciselyestimated (cid:98) reducingthepowerofthetest. SinceÏisthesumofthep estimatedARcoefficientsinthelevelsmodel (cid:98) theimprecisioncanbequitesensitivetothechoiceofp. AreasonableselectionruleistousetheAIC- selectedARmodel.ImprovedruleshavebeenstudiedbyNgandPerron(2001). We have argued that it is better to report asymptotic p-values than âaccept/rejectâ. For this calcu- lation we need the asymptotic distribution function but this is not available in closed form. A simple approximationisinterpolationofthecriticalvalues. Forexample,supposeADF=â1.9withafittedin- tercept. Thetwoclosestcriticalvaluesarethe30%(â2.0)and50%(â1.6). Linearinterpolationbetween thesevaluesyields 0.30Ã(1.9â1.6)+0.50Ã(2.0â1.9) p= =0.35. 2.0â1.6 Thustheasymptoticp-valueisapproximately35%.Reportingap-valueinsteadoftheâdecisionâofatest improvesinterpretationandcommunication. 6Calculatedbysimulationfromonemillionsimulationdrawsforasampleofsizen=10,000.",
    "page": 585,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 566 Howshouldunitroottestsbeusedinempiricalpractice? Theanswerissubtle. Acommonmistake isâWeuseaunitroottesttodiscoverwhetherornottheserieshasaunitroot.âThisisamistakebecause atestdoesnotrevealthetruth.Rather,itpresentsevidencewhetherornot(cid:72) canberejected.Ifthetest 0 failstoreject(cid:72) thisdoesnotmeanthatâWehavefoundaunitrootâ. Rather,thecorrectconclusionis 0 âWecannotrejectthehypothesisthatithasaunitrootâ. Thuswedonotknow. Ifthetestrejects(cid:72) (if 0 thep-valueisverysmall)thenwecanconcludethattheseriesisunlikelytobeaunitrootprocess; its behaviorismoreconsistentwithastationaryprocess.Anothercommonmistakeistoadopttherule:âIf theADFtestrejectsthenweworkwithY inlevels;iftheADFtestdoesnotrejectthenweworkwiththe t differencedseriesâY .âThisisamistakebecauseitassignsamodelingruletotheresultofastatistical t testwhilethetestisonlydesignedtoanswerthequestionifthereisevidenceagainstthehypothesisofa unitroot.ThechoiceofY versusâY isamodelselectionchoicenotahypothesistestingdecision. t t I believe a reasonable approach is to start with a hypothesis based on theory and context. Does economictheoryleadyoutotreataseriesasstationaryornon-stationary?Isthereareasontobelievethat aseriesshouldbestationaryâthusstableinthemeanâoristherereasontobelievetheserieswillexhibit growth and change? If you have a clear answer to these questions that should be your starting place, yourdefault. Usetheunitroottesttohelpconfirmyourassumptionsratherthantoselectamodeling approach. IfyourassumptionisthatY hasaunitrootbuttheunitrootteststronglyrejects,thenyou t should re-appraise your theory. On the other hand if your assumption is that Y is stationary but the t unitroottestfailstorejectthenullofaunitroot,donotnecessarilydepartfromyourtheoreticalbase. Consider the degree of evidence, the sample size, as well as the point estimates. Use all information togethertobaseyourdecision. ToillustrateapplicationoftheADFtestletâstaketheeightseriesdisplayedinFigures14.1-14.4using thevariablesmeasuredinlevelsorlog-levels.ThevariablesandtransformationsarelistedinTable16.2. Forsixoftheeightseries(allbuttheinterestandunemploymentrates)wetookthelogtransformation. Weincludedaninterceptandlineartimetrendineachregressionandselectedtheautoregressiveorder byminimizingtheAICacrossAR(p)modelswithalineartimetrend.Forthequarterlyseriesweexamined AR(p)modelsuptop=8,forthemonthlyseriesuptop=12. Theselectedvaluesofp areshowninthe table. Thepointestimateofaâ1,itsstandarderror,theADFtstatistic,anditsasymptoticp-valueare (cid:98) shown.Whatweseeisforforsevenoftheeightseries(allbuttheunemploymentrate)thep-valuesarefar fromthecriticalregionindicatingfailuretorejectthenullhypothesisofaunitroot. Thep-valueforthe unemploymentrateis0.01,however,indicatingrejectionofaunitroot.Overall,theresultsareconsistent with the hypotheses that the unemployment rate is stationary and that the other seven variables are possibly(butnotdecisively)unitrootprocesses",
    "page": 586,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Thep-valueforthe unemploymentrateis0.01,however,indicatingrejectionofaunitroot.Overall,theresultsareconsistent with the hypotheses that the unemployment rate is stationary and that the other seven variables are possibly(butnotdecisively)unitrootprocesses. TheADFtestcameintopopularityineconomicswithaseminarpaperbyNelsonandPlosser(1982). TheseauthorsappliedtheADFtoasetofstandardmacroeconomicvariables(similartothoseinTable 16.2)andfoundthattheunitroothypothesiscouldnotberejectedinmostseries.Thisempiricalfinding had a substantial effect on applied economic time series. Before this paper the conventional wisdom wasthateconomicserieswerestationary(possiblyaboutlineartimetrends). Aftertheirworkitbecame moreacceptedtoassumethateconomictimeseriesarebetterdescribedasautoregressiveunitrootpro- cesses.NelsonandPlosser(1982)usedthisempiricalfindingtomakeafurtherandstrongerclaim.They arguedthatKeynesianmacroeconomicmodels(whichwerestandardatthetime)implythateconomic time series are stationary while real business cycle (RBC) models (which were new at the time) imply thateconomictimeseriesareunitrootprocesses.Nelson-Plosserarguedthattheempiricalfindingthat theunitroottestsdonotrejectwasstrongsupportfortheRBCresearchprogram. Theirargumentwas influentialandwasafactormotivatingtheriseoftheRBCliterature.WithhindsightwecanseethatNel- sonandPlosser(1982)madeafundamentalerrorinthislatterargument.TheunitrootbehaviorinRBC modelsisnotinherenttotheirstructure;ratheritisaby-productoftheassumptionsonthetechnology process. (If exogenous technology is a unit root process or a stationary process then macroeconomic",
    "page": 586,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 567 variableswillalsobeunitrootprocessesorstationaryprocesses,respectively.) Similarlythestationary behaviorof1970sKeynesianmodelswasnotinherenttotheirstructurebutratheraby-productofas- sumptions about unobservables. Fundamentally the unit root/stationary distinction says little about theRBC/Keynesiandebate. The ADF test with a fitted intercept can be implemented in Stata by the command dfuller y, lags(q) regress. For a fitted intercept and trend add the option trend. The number of lags âqâ in thecommandisthenumberoffirstdifferencesin(16.7),henceq =pâ1wherep istheautoregressive order. The dfuller command reports the estimated regression, the ADF statistic, asymptotic critical values,andapproximateasymptoticp-value. 16.14 KPSSStationarityTest Kwiatkowski,Phillips,Schmidt,andShin(1992)developedatestofthenullhypothesisofstationarity againstthealternativeofaunitrootwhichhasbecomeknownastheKPSStest.Manyusersfindthisidea attractiveasausefulcounterpointtotheADFtest. Thetestisderivedfromwhatisknownasalocallevelmodel.Thisis Y =Âµ+Î¸S +e t t t S t =S tâ1 +u t where e is a mean zero stationary process and u is i.i.d. (0,Ï2). When Ï2 =0 then Y is stationary. t t u u t WhenÏ2 >0thenY isaunitrootprocess. Thusatestofthenullofstationarityagainstthealternative u t ofaunitrootisatestof(cid:72) :Ï2 =0against(cid:72) :Ï2 >0.Addtheauxillaryassumptionthat(e ,u )arei.i.d 0 u 1 u t t normal.TheLagrangemultipliertestcanbeshowntoreject(cid:72) infavorof(cid:72) forlargevaluesof 0 1 (cid:195) (cid:33)2 1 (cid:88) n (cid:88) i e n2Ï (cid:98) 2 i=1 t=1 (cid:98)t where e =Y âY are the residuals under the null and Ï2 is its sample variance. To generalize to the (cid:98)t t (cid:98) contextofseriallycorrelatede KPSSproposedthestatistic t (cid:195) (cid:33)2 1 (cid:88) n (cid:88) i KPSS = e 1 n2Ï (cid:98) 2 i=1 t=1 (cid:98)t where Ï (cid:98) 2= (cid:96)= (cid:88) M âM (cid:181) 1â M |(cid:96) + | 1 (cid:182) n 1 t (cid:88) = n 1 e (cid:98)t e (cid:98)tâ(cid:96) istheNewey-Westestimatorofthelong-runvarianceÏ2ofY . t Forcontextsallowingforalineartimetrendthelocallevelmodeltakestheform Y =Âµ+Î²t+Î¸S +e t t t whichhasnullleastsquaresestimator Y t =Âµ (cid:101) +Î² (cid:101)t+e (cid:101)t . Noticethate islinearlydetrendedY .TheKPSStestfor(cid:72) against(cid:72) rejectsforlargevaluesof (cid:101)t t 0 1 (cid:195) (cid:33)2 1 (cid:88) n (cid:88) i KPSS = e 2 n2Ï (cid:101) 2 i=1 t=1 (cid:101)t whereÏ2isdefinedasÏ2butwiththedetrendedresidualse . (cid:101) (cid:98) (cid:101)t",
    "page": 587,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 568 Theorem16.15 IfY followsAssumption16.1then t (cid:90) 1 KPSS ââ V2 1 d 0 and (cid:90) 1 KPSS ââ V2 2 2 d 0 where V(r) = W(r)ârW(1) is a Brownian bridge, and V (r) = W(r)â 2 (cid:161)(cid:82)r X(s)ds (cid:162)(cid:48)(cid:179) (cid:82)1 XX (cid:48) (cid:180)(cid:48) (cid:82)1 XdW withX(s)=(1,s) (cid:48) . 0 0 0 TheasymptoticdistributionsinTheorem16.15arenon-standardandaretypicallycalculatedbysim- ulation. TheprocessV (r)isknownasaSecond-levelBrownianBridge. Theasymptoticdistributions 2 aredisplayed7inFigure16.3(c).ThedashedlineisthedensityoftheKPSS asymptoticdistribution.The 1 solid line is the density of the KPSS asymptotic distribution. The densities are skewed with a slowly- 1 decaying right tail. The KPSS distribution is substantially shifted towards the origin compared to the 2 KPSS distribution,indicatingasubstantialeffectofdetrending. 1 The asymptotic critical values are displayed8 in Table 16.3. Rejections occur when the test statis- ticexceedsthecriticalvalue. AsfortheDickey-Fullertestsanapproximateasymptoticp-valuecanbe calculatedfromthistablebylinearinterpolationbetweenadjacentcriticalvalues. TheKPSSstatisticcriticallydependsonthelagorderM usedtoestimatethelong-runvarianceÏ2. Thisisachallengefortestimplementation.IfY isstationarybuthighlypersistent(forexample,anAR(1) t withalargeautoregressivecoefficient)thenthelagtruncationM needstobelargeinordertoaccurately estimateÏ2. However,underthealternativethatY isaunitrootprocess,theestimatorÏ2willincrease t (cid:98) roughlylinearlywithM sothatforanygivensampletheKPSSstatisticcanbemadearbitrarilysmallby selectingM sufficientlylarge. RecallthattheAndrews(1991)referencerule(14.50)is (cid:195) (cid:33)1/3 Ï2 M= 6 n1/3 (cid:161) 1âÏ2 (cid:162)2 whereÏisthefirstautocorrelationofY . FortheKPSStestweshouldnotreplaceÏwithanestimatorÏ t (cid:98) asthelatterconvergesto1under(cid:72) ,leadingtoM âârenderingthetestinconsistent. Insteadwecan 0 useadefaultrulebasedonareasonablealternative.SupposeweconsiderthealternativeÏ=0.8.Theas- sociatedAndrewsâreferenceruleisM=3.1n1/3.ThisleadstoasimpleruleM=3n1/3.Aninterpretation ofthischoiceisthatitshouldapproximatelycontrolthesizeofthetestwhenthetruthisanAR(1)with coefficient0.8butover-rejectformorepersistentARprocesses. Toillustrate,Table16.2reportstheKPSS statisticforthesameeightseriesasexaminedinthepre- 2 vioussection,usingM =3n1/3. Forthefirsttwoquarterlyseriesn=228leadingtoM =18. Forthesix monthlyseriesn=684leadingtoM=26.Forsixoftheeightseries(allbutconsumptionandtheunem- ploymentrate)theKPSSstatisticequalsorexceedsthe1%criticalvalueleadingtoarejectionofthenull hypothesisofstationarityinfavorofthealternativeofaunitroot. ThisisconsistentwiththeADFtest whichfailedtorejectaunitrootfortheseseries",
    "page": 588,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Toillustrate,Table16.2reportstheKPSS statisticforthesameeightseriesasexaminedinthepre- 2 vioussection,usingM =3n1/3. Forthefirsttwoquarterlyseriesn=228leadingtoM =18. Forthesix monthlyseriesn=684leadingtoM=26.Forsixoftheeightseries(allbutconsumptionandtheunem- ploymentrate)theKPSSstatisticequalsorexceedsthe1%criticalvalueleadingtoarejectionofthenull hypothesisofstationarityinfavorofthealternativeofaunitroot. ThisisconsistentwiththeADFtest whichfailedtorejectaunitrootfortheseseries. FortheconsumptionseriestheKPSSstatistichasap-valueof12%,whichdoesnotrejectthehypoth- esisofstationarity.RecallthattheADFtestfailedtorejectthehypothesisofaunitroot.Thusneithertest 7Calculatedbysimulationfromonemillionsimulationdrawsforasampleofsizen=10,000. 8Calculatedbysimulationfromonemillionsimulationdrawsforasampleofsizen=10,000.",
    "page": 588,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 569 Table16.1:ADFTestCriticalValues 1% 2% 3% 4% 5% 7% 10% 15% 20% 30% 50% 70% 90% Case1(NoConstantorTrend) â2.6 â2.3 â2.2 â2.0 â1.9 â1.8 â1.6 â1.4 â1.2 â1.0 â0.5 0.1 0.9 Case2(ConstantIncluded) â3.4 â3.2 â3.1 â3.0 â2.9 â2.7 â2.6 â2.4 â2.2 â2.0 â1.6 â1.1 â0.4 Case3(ConstantandTrendIncluded) â4.0 â3.7 â3.6 â3.5 â3.4 â3.3 â3.1 â2.9 â2.8 â2.6 â2.2 â1.8 â1.2 Table16.2:UnitRootandKPSSTestApplications p aâ1 ADF p-value M KPSS p-value (cid:98) 2 â0.017 log(realGDP) 3 â1.8 0.71 18 0.23 0.01 (.009) â0.029 log(realconsumption) 4 â2.4 0.37 18 0.113 0.12 (.012) â0.009 log(exchangerate) 11 â2.2 0.49 26 0.31 <.01 (.004) â0.005 interestrate 12 â1.5 0.52 26 0.56 <.01 (.004) â0.013 log(oilprice) 2 â2.4 0.35 26 0.23 <.01 (.005) â0.014 unemploymentrate 7 â3.4 0.01 26 0.14 0.06 (.004) â0.001 log(CPI) 11 â1.0 0.95 26 0.55 <.01 (.001) â0.010 log(stockprice) 6 â2.2 0.47 26 0.30 <.01 (.004) Table16.3:KPSSCriticalValues 1% 2% 3% 4% 5% 7% 10% 15% 20% 30% 50% 70% KPSS (ConstantIncluded) 1 0.75 0.62 0.55 0.50 0.46 0.41 0.35 0.28 0.24 0.18 0.12 0.08 KPSS (ConstantandTrendIncluded) 2 0.22 0.19 0.17 0.16 0.15 0.13 0.12 0.10 0.09 0.08 0.06 0.04",
    "page": 589,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 570 leadstoadecisiveresult; asapairthetwotestsareinconclusive. InthiscontextIrecommendstaying withthepredictionofeconomictheory(consumptionisamartingale)asitisnotrejectedbyahypothesis test.TheKPSSfailstorejectstationaritybutthatdoesnotmeanthattheseriesisstationary. Aninterestingcaseistheunemploymentrateseries. IthasKPSS =0.14withap-valueof6%. This 2 isborderlinesignificantforrejectionofstationarity. Ontheotherhand,recallthattheADFtesthadap- valueof1%rejectingtheunitroothypothesis. Theseresultsareborderlineconflicting. Toaugmentour informationwecalculatetheKPSS testastheunemploymentratedoesnotappeartobetrended. We 1 findKPSS =0.19withap-valueof30%.Thisisclearlyinthenon-rejectionregion,failingtoprovideevi- 1 denceagainststationarity.Asawhole,theADFtest(rejectunitroot),theKPSS test(acceptstationarity), 1 andtheKPSS test(borderlinerejectstationarity),takentogetherareconsistentwiththeinterpretation 2 thattheunemploymentrateisastationaryprocess. The KPSS test can be implemented in Stata using the command9 kpss y, maxlag(q). For the 2 KPSS testaddtheoptionnotrend. ThecommandreportstheKPSSstatisticsforM =1,...,q,aswellas 1 asymptoticcriticalvalues.Approximateasymptoticp-valuesarenotreported. 16.15 SpuriousRegression Oneofthemostempiricallyrelevantdiscoveriesfromthetheoryofnon-stationarytimeseriesisthe phenomenonofspuriousregression.Thisisthefindingthattwostatisticallyindependentseries,ifboth unitrootprocesses,arelikelytofooltraditionalstatisticalanalysisbyappearingtobestatisticallyrelated bybotheyeballscrutinyandtraditionalstatisticaltests. Thephenomenonwasobserved10 andnamed byGrangerandNewbold(1974)andexplainedusingthetheoryofnon-stationarytimeseriesbyPhillips (1986). Theprimarylessonisthatitiseasytobetrickedbynon-stationarytime-seriesbuttheproblem disappearsifwepaysuitableattentiontodynamicspecification. Series 1 Labor Force Participation Rate Series 2 Exchange Rate 1980 1985 1990 1995 2000 2005 2010 2015 2020 1960 1970 1980 1990 2000 2010 2020 (a)TwoUnrelatedRandomWalks (b)ExchangeandLaborForceParticipationRates Figure16.4:PlotsofEmpiricalSeries ToillustratetheproblemexamineFigure16.4(a). Displayedaretwotimeseries, monthlyfor1980- 9Thecommandkpssisnotpartofthestandardpackage,butcanbeinstalledbytypingssc install kpss. 10Innumericalsimulations.",
    "page": 590,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 571 2018. Acasualreviewofthegraphsshowsthatbothseriesaregenerallyincreasingover1980-2010with ano-growthperiodaround2000,andtheseriesdisplayadownwardtrendforthefinaldecade. Amore refined perusal may appear to reveal that Series 2 leads Series 1 by about five years, in the sense that Series 2 reaches turning points about five years before Series 1. A casual observer is likely to deduce basedonFigure16.4(a)thatthetwotimeseriesarestronglyrelated. HoweverthetruthisthatSeries1andSeries2arestatisticallyindependentrandomwalksgenerated bycomputersimulation,eachstandardizedtohavemeanzeroandunitvarianceforthepurposeofvisual comparison. Theâfactâthatbothseriesaregenerallyupwardtrendedandhaveâsimilarâturningpoints arestatisticalaccidents.Randomwalkshaveanuncannyabilitytofoolcasualanalysis.Newspaper(and other journalistic) articles containing plots of time series are routinely subject to the tricks of Figure 16.4(a).Economistsarealsoroutinelytrickedandfooled. TraditionalstatisticalexaminationoftheseriesinFigure16.4(a)canalsoleadtoafalseinferenceof a strong relationship. A linear regression of Series 1 on Series 2 yields a slope coefficient of 0.76 with classicalstandarderrorof0.03. Thet-ratioforthetestofazeroslopeisT =26. TheequationR2is0.59. Thesetraditionalstatisticssupporttheincorrectinferencethatthetwoseriesarestronglyrelated. Spuriousrelationshipsofthisformarecommonplaceineconomictimeseries.Anexampleisshown inFigure16.4(b),whichdisplaystheU.S.laborforceparticipationrateandU.S.-Canadaexchangerate, quarterlyfor1960-2018. Asavisualaidbothserieshavebeennormalizedtohavemeanzeroandunit variance. Bothseriesappeartogrowatasimilarratefrom1960-2000,thoughtheexchangerateismore volatile. From 2000-2018 they reverse course, with both series declining. The visual evidence is sup- portedbytraditionalstatistics. Alinearregressionoflaborparticipationontheexchangerateyieldsa slopecoefficientof0.70withaclasicalstandarderrorof0.05. Thet-ratioforthetestofazeroslopeis T =15. TheequationR2 is0.49. Thevisualandstatisticalevidencesupporttheinferencethatthetwo seriesarerelated. Thisempiricalâfindingâthatthelaborparticipationandexchangeratesarerelateddoesnotmake economicsense. Isthisanexampleofaspuriousregressionbetweennon-stationaryvariables? Avisual inspectionofeachseriessupportsthecontentionthateachisnon-stationaryandmaybewellcharac- terizedasaunitrootprocess. WesawinSections16.13and16.14thattheADFandKPSStestssupport thehypothesisthattheexchangerateisaunitrootprocess. Similartestsreachthesameconclusionfor labor force participation. Thus the two series are reasonably characterized as unit root processes and thesetwoseriescouldbeanempiricalexampleofaspuriousregression. ForaformalframeworkassumethattheseriesY andX arerandomwalkprocesses t t Y t =Y tâ1 +e 1t (16.10) X t =X tâ1 +e 2t (16.11) where(e ,e )arei.i.d., meanzero, mutuallyuncorrelated, andnormalizedtohaveunitvariance",
    "page": 591,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Similartestsreachthesameconclusionfor labor force participation. Thus the two series are reasonably characterized as unit root processes and thesetwoseriescouldbeanempiricalexampleofaspuriousregression. ForaformalframeworkassumethattheseriesY andX arerandomwalkprocesses t t Y t =Y tâ1 +e 1t (16.10) X t =X tâ1 +e 2t (16.11) where(e ,e )arei.i.d., meanzero, mutuallyuncorrelated, andnormalizedtohaveunitvariance. Let 1t 2t â â Y andX denotedemeanedversionsofY andX .FromtheFCLTtheysatisfy t t t t (cid:181) (cid:182) (cid:112) 1 Y â ,(cid:112) 1 X â ââ(cid:161) W â (r),W â (r) (cid:162) n (cid:98)nr(cid:99) n (cid:98)nr(cid:99) d 1 2 â â whereW (r)andW (r)aredemeanedBrownianmotions. 1 2 ApplyingtheCMTthesamplecorrelationhastheasymptoticdistribution 1 (cid:80)n Y â X â (cid:82)1 W â W â Ï= n2 i=1 i i ââ 0 1 2 . (cid:98) (cid:161) n 1 2 (cid:80)n i=1 Y i â2(cid:162)1/2(cid:161) n 1 2 (cid:80)n i=1 X i â2(cid:162)1/2 d (cid:179) (cid:82) 0 1 W 1 â2 (cid:180)1/2(cid:179) (cid:82) 0 1 W 2 â2 (cid:180)1/2 Theright-hand-sideisarandomvariable. Furthermoreitisalsonon-degenerate(indeed,itisnon-zero withprobabilityone).ThusthesamplecorrelationÏremainsrandominlargesamples. (cid:98)",
    "page": 591,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 572 Tounderstandmagnitudes,Figure16.5(a)displaystheasymptoticdistributon11ofÏ.Thedensityhas (cid:98) mostprobabilitymassintheinterval[â0.5,0.5],overwhichthedensityisessentiallyflat.Thismeansthat thesamplecorrelationhasadiffusedistribution. Abovewesawthatthetwosimulatedrandomwalks had a sample correlation12 of 0.76 and the two empirical series a sample correlation of 0.70. We can nowseethattheseresultsareconsistentwiththedistributionshowninFigure16.5(a)andaretherefore uninformativeregardingtheunderlyingrelationships. â1.0 â0.5 0.0 0.5 1.0 0 50 100 150 200 Sample Size (a)AsymptoticDensityofSampleCorrelation 7.0 6.0 5.0 4.0 3.0 2.0 1.0 0.0 (b)CoverageProbabilityofNominal95%Interval Figure16.5:PropertiesofSpuriousRegression We can also examine the regression estimators. The slope coefficient from a regression of Y on t X hastheasymptoticdistribution t 1 (cid:80)n Y â X â (cid:82)1 W â W â Î² (cid:98) = n2 i=1 i i ââ 0 1 2 . n 1 2 (cid:80)n i=1 X i â2 d (cid:82) 0 1 W 2 â2 Thisisanon-degeneraterandomvariable. Thustheslopeestimatorremainsrandominlargesamples anddoesnotconvergeinprobability. Nowconsidertheclassicalt-ratioT.Ithastheasymptoticdistribution 1 T = n 1 2 (cid:80)n i=1 Y i â X i â ââ (cid:82) 0 1 W 1 â W 2 â . n1/2 (cid:161) n 1 2 (cid:80)n i=1 X i â2(cid:162)1/2 (cid:179) n 1 2 (cid:80)n i=1 (cid:161) Y i ââX i âÎ² (cid:98) (cid:162)2(cid:180)1/2 d (cid:179) (cid:82)1 W â2 (cid:180)1/2 (cid:181) (cid:82)1 (cid:181) W ââW â (cid:82) 0 1W 1 âW 2 â (cid:182)2(cid:182)1/2 0 2 0 1 2 (cid:82)1Wâ2 0 2 Thisisnon-degenerate.Thusthet-ratiohasanasymptoticdistributiononlyafternormalizationbyn1/2, meaningthattheunnormalizedt-ratiodivergesinprobability! Tounderstandtheutterfailureofclassicalinferencetheoryobservethattheregressionequationis Y =Î±+Î²X +Î¾ (16.12) t t t 11Calculatedbysimulationfromonemillionsimulationdrawsforasampleofsizen=10,000. 12Sincethevariableshavebeenstandardizedtohaveaunitvariancethesamplecorrelationequalstheleastsquaresslope coefficient.",
    "page": 592,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 573 withtruevaluesÎ±=0andÎ²=0.ThismeansthattheerrorÎ¾ =Y isarandomwalk.Thelatterisconsid- t t erablymorestronglyautocorrelatedthanallowedbystationaryregressiontheory,invalidatingconven- tionalstandarderrors. Thelatteraretoosmallbyanorderofmagnituderesultingint-ratioswhichare misleadinglylarge. What this means in practice is that t-ratios from spurious regressions are random and large even whenthereisnorelationship. Thisexplainsthelarget-ratioT =26forthesimulatedseriesandshows thatthevalueT =15fortheempiricalseriesisuninformative.Thereasonforalarget-ratioisnotbecause theseriesarerelatedbutisratherbecausetheseriesareunitrootprocessessoclassicalstandarderrors mis-characterizeestimationvariance. One of the features of the above theory is that it shows that the magnitude of the distortion of the t-ratioincreaseswithsamplesize.Interestingly,theoriginalGranger-Newbold(1974)analysiswasasim- ulationstudywhichconfinedattentiontothecasen=50.Granger-Newboldfoundthe(thensurprising) resultthatt-testssubstantiallyoverrejectunderthenullhypothesisofazerocoefficient. Itwasnâtuntil thetheoreticalanalysisbyPhillips(1986)thatitwasrealizedthatthisdistortionworsenedassamplesize increased.Theseresultsillustratetheinsightâandlimitationsâofsimulationanalysis.Usingsimulation Granger-Newboldpointedoutthattherewasaproblem. Butbyfixingsamplesizeatasinglevaluethey didnotdiscoverthesurprisingeffectofsamplesize. Thefactthatthet-ratiodivergesasn increasesmeansthatthecoverageofclassicalconfidencein- tervalsworsesasnincreases. TocalibratethemagnitudeofthisdistortionexamineFigure16.5(b). This plots13thefinite-samplecoverageprobabilityofclassicalnominal95%confidenceintervalsfortheslope usingstudenttcriticalvaluesplottedasafunctionofsamplesizen. Theobservationsweregenerated asindependentrandomwalkswithnormalinnovations. Youcanseethatthecoveragerangesfrom0.68 (forn=10)to0.2(forn=200).Thesecoverageratesareunacceptablybelowthenominalcoveragelevel of0.95. Theaboveanalysisfocusedonclassicalt-ratiosandconfidenceintervalsconstructedwithold-fashioned homoskedastic standard errors. This may seem to be an out-of-date analysis as we have made the casethatold-fashionedstandarderrorsarenotusedincontemporaryeconometricpractice. However the problem as described carries over to alternative standard error constructions. The common het- eroskedasticstandarderrorsdonotfundamentallychangetheasymptoticdistribution.TheNewey-West standarderrorsreducetheunder-coveragebutonlypartially.Theyaredesignedtoconsistentlyestimate thelong-runvarianceofstationaryseriesbutfailwhentheseriesarenon-stationary. Atthispointletuscollectwhatwehavelearned. Ifwehavetwotime-serieswhichareindependent unit root processes, then by time-series plots, correlation analysis, and simple linear regressions it is easy to make the false inference that they are related. Their sample correlations and regression slope estimateswillberandom,inconsistent,anduninformative",
    "page": 593,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Atthispointletuscollectwhatwehavelearned. Ifwehavetwotime-serieswhichareindependent unit root processes, then by time-series plots, correlation analysis, and simple linear regressions it is easy to make the false inference that they are related. Their sample correlations and regression slope estimateswillberandom,inconsistent,anduninformative. Ourdeductionisthatitisinappropriatetousesimpleinferencetechniqueswhenhandlingpoten- tiallynon-stationarytimeseries.Weneedtobemorecarefulandusebetterinferencemethods. Itturnsoutthatasimplemodificationisoftensufficienttofundamentallyaltertheinferenceprob- lem. Again, suppose we observe the independent series (16.10)-(16.11). A linear regression model is (16.12)witherrorÎ¾ t =Y t . WecanwritethelatterasÎ¾ t =Y tâ1 +e t . Thismeansthatacorrectdynamic specficationoftheregressionmodelis Y t =Î±+Î²X t +Î´Y tâ1 +e t (16.13) with Î± = Î² = 0 and Î´ = 1. If equation (16.13) is estimated the error is no longer a random walk and inferenceonÎ²canproceedconventionally! Inthissimpleexampleasolutionissimplytoincludethe lagged dependent variable Y tâ1 in the estimated regression. More generally, if a trend component is 13Calculatedbysimulationonagridofvaluesfornwithonemillionsimulationreplications.",
    "page": 593,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 574 missingorâY isseriallycorrelateditisnecessarytoincludethetrendtermsand/orsufficientlagsofY t t intheestimatedregression. Forexample,takethesimulatedrandomwalkseriesfromFigure16.4(a). Estimatingmodel(16.13) we find Î² (cid:98) = 0.004 with a standard error of 0.005. Thus by adding the lagged dependent variable the spuriousregressionrelationshiphasbeenbroken.NowtaketheempiricalseriesfromFigure16.4(b).We estimateananalogof(16.13)augmentedwithalineartrend.TheestimateofÎ²inthismodelis0.16with astandarderrorof0.12. Onceagainthespuriousregressionrelationshiphasbeenbrokenbyasimple dynamicre-adjustment. Thisseemslikeastraightforwardsolution. Ifso,whydoesthespuriousregressionproblempersist14 inappliedanalysis?Thereasonispartiallythatnon-specialistsfindthatthesimpleregression(16.12)is easytointerpretwhilethedynamicmodel(16.13)ischallengingtointerpret.Oneofthetasksofaskilled econometricianistounderstandthisfailureofreasoning,toexplaintheproblemtocolleaguesandusers, andtopresentconstructiveusefulalternativemethodsofanalysis. 16.16 NonStationaryVARs LetY beanmÃ1timeseries.SupposethatY satisfiesaVAR(p-1)infirstdifferences,thusD(L)âY = t t t e where D(z) is invertible and Î£ = var[e ] > 0. Then âY has the long-run covariance matrix â¦ = t t t D(1) â1Î£D(1) â1(cid:48) >0. InthiscaseY isavector I(1)processinthesensethateachelementofY is I(1) t t andsoarealllinearcombinationsofY . t ThemodelcanbewrittenasaVARinlevelsas Y t =A 1 Y tâ1 +A 2 Y tâ2 +Â·Â·Â·+A p Y tâp +e t (16.14) where A +A +Â·Â·Â·+A =I .Itcanalsobewritteninthemixedformat 1 2 p m âY t =AY tâ1 +D 1 âY tâ1 +Â·Â·Â·+D pâ1 âY tâp+1 +e t (16.15) where A=0.Theseareequivalentalgebraicrepresentations.Letd=vec (cid:179) (cid:161) D 1 ,...,D pâ1 (cid:162)(cid:48)(cid:180) . Let (cid:161) A(cid:98),d(cid:98) (cid:162) bethemultivariateleastsquaresestimatorof(16.15).SetX t =(âY tâ1 ,...,âY tâp+1 ). Theorem16.16 Assume that âY follows the VAR(p-1) process D(L)âY =e t t t withinvertibleD(z),(cid:69)[e t |F tâ1 ]=0,(cid:69)(cid:107)e t (cid:107)4<â,and(cid:69)(cid:163) e t e t (cid:48)(cid:164)=Î£>0. Thenas nââ (cid:181) (cid:182) ï£« Î£1/2(cid:82)1 dWW (cid:48) (cid:179) (cid:82)1 WW (cid:48) (cid:180)â1 â¦â1/2 ï£¶ (cid:112) nA(cid:98) ââï£¬ 0 0 ï£· n (cid:161) d(cid:98) âd (cid:162) d ï£­ ï£¸ N(0,V) whereW(r)isvectorBrownianmotionand V =(cid:161) I â(cid:69)(cid:163) X X (cid:48)(cid:164)(cid:162)â1â¦(cid:161) I â(cid:69)(cid:163) X X (cid:48)(cid:164)(cid:162)â1 m t t m t t â¦=(cid:69)(cid:163) e e (cid:48)âX X (cid:48)(cid:164) . t t t t 14Anamusingexerciseistoperusenewspaper/magazinearticlesfortimeseriesplotsofhistoricalseries. Moreoftenthan notthedisplayedseriesappeartobe I(1), andmoreoftenthannotthearticledescribestheseriesasârelatedâbasedona combinationofeyeballanalysisandsimplecorrelationstatistics.",
    "page": 594,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 575 ThetopcomponentoftheasymptoticdistributionisamultivariateversionoftheDickey-Fullerco- efficientdistribution.Thebottomcomponentisaconventionalnormaldistribution.Thisshowsthatthe coefficientestimator A(cid:98) isconsistentattheO p (n â1)rate,convergestoanon-standard(biasedandnon- normal)asymptoticdistribution,andthecoefficientestimatord(cid:98)hasaconventionalasymptoticnormal distribution. Parameters of interest, including the coefficients of the levels equation (16.14), impulse response (cid:161) (cid:162) functions,andforecasterrordecompositions,arelinearcombinationoftheestimators A(cid:98),d(cid:98).ForVAR(p) modelswith p â¥2, unlessthelinearcombinationofinterestisinthespanof A(cid:98), theasymptoticdistri- butionofestimatorsaredominatedbytheO p (n â1/2)componentd(cid:98). Thusthesecoefficientestimators have conventional asymptotic normal distributions. Consequently, for most purposes estimation and inferenceonaVARmodelisrobusttothepresenceof(multivariate)unitroots. Therearetwoimportantexceptions.First,inferenceonthesumoflevelscoefficientsA +A +Â·Â·Â·+A 1 2 p isnon-standardastheestimatorofthissumhasthemultivariateDickey-Fullercoefficientdistribution. Thisincludesquestionsconcerningthepresenceofunitrootsandmanyquestionsconcerningthelong- runpropertiesoftheseries. Second,thelong-runimpulsematrixC =A â1=(cid:161) IâA âA âÂ·Â·Â·âA (cid:162)â1 is 1 2 p a(non-linear)functionofthissamesumandthusbytheDeltaMethodisasymptoticallyalineartrans- formation of the multivariate Dickey-Fuller coefficient distribution. This means that the least squares estimator ofC is non-standard (biased and non-normal). AsC is the limit of the CIRF as the horizon tends to infinity this indicates that estimators of the CIRF at long horizons will be non-standard in fi- nitesamples. ConsequentlywhenaVARmodelincludesvariableswhicharepotentiallyunitrootpro- cessestheconventionalconfidenceintervalsfortheCIRFatlonghorizonsarenottrustworthy. Thisisa widespreadissuesincemacroeconomistsroutinelyestimateVARmodelswithmacroeconomicvariables inlevels(forexample,theBlanchard-Perotti(2002)modelpresentedinSection15.25). 16.17 Cointegration Afascinatingtopiciscointegration. TheideaisduetoGranger(1981)andwasarticulatedindetail byEngleandGranger(1987). Apairofunitrootprocessesarecointegratediftheirdifference(orsome linearcombination)isstationary.Thismeansthatthepairâhangtogetherâoverthelongrun. TovisualizeexamineFigure16.6(a). Thisshowstwointerestrateseries. Thesolidlineistheinterest rate(quarterlyfor1959-2017)onten-yearU.S.TreasuryBonds15. Thedashedlineistheinterestrateon 3-monthU.S.TreasuryBonds16.Overthe59-yearperiodthetwoseriesmoveupanddowntogether.The 10-yearrateexceedsthe3-monthrateinmosttimeperiods. Forsomeperiodsthetwolinespullapart but they always come together again. This indicates that the two time series are tightly tied together",
    "page": 595,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". TovisualizeexamineFigure16.6(a). Thisshowstwointerestrateseries. Thesolidlineistheinterest rate(quarterlyfor1959-2017)onten-yearU.S.TreasuryBonds15. Thedashedlineistheinterestrateon 3-monthU.S.TreasuryBonds16.Overthe59-yearperiodthetwoseriesmoveupanddowntogether.The 10-yearrateexceedsthe3-monthrateinmosttimeperiods. Forsomeperiodsthetwolinespullapart but they always come together again. This indicates that the two time series are tightly tied together. Fromourunitrootanalysiswehavealreadydeterminedthatthe10-yearinterestrateisconsistentwitha unitrootprocess;thesamefindingsapplytothe3-monthseries.Thusitappearsthatthesearetwotime serieswhichareindividuallyunitrootprocessesbutjointlytrackeachotherclosely. Toseethisfurtherdefinetheinterestratespreadasthedifferencebetweenthetwointerestrates, long(10-year)minusshort(3-month). ThisseriesisplottedinFigure16.6(b). Themeanoftheseriesis displayedbythedashedline. Whatwecanseeisthatthespreadroughlyappearstobemeanreverting. Withthepossibleexceptionofthefirstdecadeoftheplotweseethatthatthespreadcrossesitsmean multiple times each decade. The fluctuations appear to be stationary. Applying an ADF unit root test withnotrendincludedtothespreadyieldsADF=â4.0whichislessthanthe1%criticalvalue,rejecting thenullhypothesisofaunitroot. Thusthelevelsofthetwointerestratesappeartobenon-stationary whilethespreadisstationary.Thissuggeststhatthetwointerestrateseriesarecointegrated. 15FromFRED-QD,seriesgs10. 16FromFRED-QD,seriestb3ms.",
    "page": 595,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 576 1960 1970 1980 1990 2000 2010 61 41 21 01 8 6 4 2 0 10âYear Interest Rate 3âMonth Interest Rate 1960 1970 1980 1990 2000 2010 (a)InterestRates 4 3 2 1 0 1â 2â l l l ll ll l l l lllllllllll l lll ll l l l l lllllllllllllll l l l lll l l l l ll l l llll l l l ll l l l lll ll l l l ll l l l l l llll lll l l l l ll l l l ll l l lll ll ll l l l l l ll l l l l ll l l l l ll l l ll l l ll l l l l ll l l l ll l l l ll l l l l l l l l l l l l l l l lll l l l l l ll l ll l l llll l l l ll ll l l ll l l l l l l ll ll ll l l lll l l l l l lll l l l llll l l 10âYear Interest Rate (b)InterestRateSpread etaR tseretnI htnoMâ3 0 2 4 6 8 10 12 14 16 61 41 21 01 8 6 4 2 0 l l l l (c)ErrorCorrectionEffect Figure16.6:Cointegration Thisconceptisformalizedinthefollowingdefinition. Definition16.5 ThemÃ1non-deterministicseriesY iscointegratedifthere t existsafullrankmÃm matrix (cid:163)Î²,Î² â¥ (cid:164) suchthatÎ²(cid:48) Y t â(cid:82)r andÎ²(cid:48) â¥ âY t â(cid:82)mâr are I(0).Ther vectorsinÎ²arecalledthecointegratingvectors. Thevariable Z =Î²(cid:48) Y iscalledtheequilibriumerror. t t IntheinterestrateexampleofFigure16.6,therearem=2seriesandr =1cointegratingrelationships. OurdiscussionassumesthatthecointegratingvectorisÎ²=(1,â1) (cid:48) . The cointegrating vectors Î² are not individually identified; only the space spanned by the vectors isidentifiedsoÎ²istypicallynormalized. Whenr =1acommonnormalizationistosetonenon-zero elementequaltoone.AnothercommonnormalizationistosetÎ²tobeorthonormal:Î²(cid:48)Î²=I . r",
    "page": 596,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 577 Theorem16.17 GrangerRepresentationTheorem. Ifnon-deterministicY â t (cid:82)m iscointegratedwithmÃr cointegratingvectorsÎ²and(16.1)holds,then 1. ThecoefficientsoftheWoldrepresentation âY =Î¸+Î(L)e (16.16) t t satisfyÎ(1)=Î² â¥ Î·(cid:48) andÎ¸=Î² â¥ Î³forsomefull-rankmÃ(mâr)matrixÎ· andsome(mâr)Ã1Î³. 2. TheBeveridge-NelsondecompositionofY is t Y t =Î² â¥ (cid:161)Î³t+Î·(cid:48) S t (cid:162)+U t +V 0 (16.17) whereS =(cid:80)t e ,U =Îâ (L)e isastationarylinearprocess,andV = t i=1 t t t 0 Y âU isaninitialcondition. 0 0 3. Suppose that (a) all complex solutions to det(Î(z))=0 are either z =1 or|z|â¥1+Î´forsomeÎ´>0;(b)Î²(cid:48)Îâ (1)Î· â¥ isfullrank,whereÎ· â¥ isafull rank mÃr matrix such that Î·(cid:48)Î· â¥ =0. Then Y t has the (infinite-order) convergentVARrepresentation A(L)Y =a+e (16.18) t t where the coefficients satisfy A(1)=âÎ· â¥ (cid:161)Î²(cid:48)Îâ (1)Î· â¥ (cid:162)â1Î²(cid:48) . All complex solutionstodet(A(z))=0areeitherz=1or|z|â¥1+Î´forsomeÎ´>0. 4. Undertheassumptionsofpart3plus (cid:80)â j=0 (cid:176) (cid:176) (cid:80)â k=0 kÎ j+k (cid:176) (cid:176) 2<âtheVAR representationcanbewritteninerror-correctionform âY t =Î±Î²(cid:48) Y tâ1 +Î(L)âY tâ1 +a+e t (16.19) whereÎ(L)isalagpolynomialwithabsolutelysummablecoefficientma- tricesandÎ±=âÎ· â¥ (cid:161)Î²(cid:48)Îâ (1)Î· â¥ (cid:162)â1 . 5. IfÎ¸=0intheWoldrepresentation(16.16)thenÎ³=0in(16.17)sothere isnolineartrendin(16.17). Theinterceptin(16.18)and(16.19)equals a=Î±ÂµwhereÂµisrÃ1.Equation(16.19)canbewrittenas âY t =Î±(cid:161)Î²(cid:48) Y tâ1 +Âµ(cid:162)+Î(L)âY tâ1 +e t . (16.20) TheproofispresentedinSection16.22. TheGrangerRepresentationTheoremappearsinEngleand Granger(1987).TheassumptiononÎ²(cid:48)Îâ (1)Î· â¥wasintroducedbyJohansen(1995,Theorem4.5). Part 1 shows that the coefficients of the Wold representation sum to a singular matrix in the null spaceofthecointegratingvectors. Part2givestheBeveridge-Nelsonpermanent-transitoryrepresentationofY .Itshowsthatthetrend t Î² â¥ (cid:161)Î³t+Î·(cid:48) S t (cid:162) liesinthenullspaceofthecointegratingvectors.Thusthereisnotrendintherangespace ofthecointegratingvectors. ThisshowsthatthecointegratedvectorY canbethoughtofaspossessing t r âunitrootsandlineartrendsâandmâr âstationaryprocessesâ.",
    "page": 597,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 578 Part3providestheVARrepresentation. ItshowsthattheVARcoefficientssumtoasingularmatrix whichisintherangespaceofthecointegratingvectors. Part 4 is perhaps the most famous result. It shows that a cointegrated system satisfies equation (16.19) which is called the error-correction representation. The error-correction representation is a regressionmodelinstationarytransformationsasthevariablesâY t andÎ²(cid:48) Y tâ1 arestationary.Theequa- tionshowsthatthechangeâY t relatestopastchangesâY tâ1 (asinastandardVAR)aswellastheequilib- riumerrorÎ²(cid:48) Y tâ1 .ThefulltermÎ±Î²(cid:48) Y tâ1 isknownastheâerror-correctiontermâ.Itisthekeycomponent whichgovernshowthecointegratedrelationshipismaintained. Part5examinesthecaseofnolineartrend.TheconditionÎ¸=0ariseswhenthevariablesâY areall t meanzero.Thetheorem(unsuprisingly)showsthatthisimpliesthatthelineartrenddoesnotappearin theBeveridge-Nelsondecomposition. Moreinterestinglythetheoremshowsthatthisconditionimplies thattheerror-correctionmodelcanbewrittentoincorporatetheintercept. To understand the error-correction effect examine Figure 16.6(c). This shows a scatter plot of the historicalvaluesofthetwointerestrateseriesfrompanel(a). Alsoplottedisanestimate17 ofthelinear relationÎ²(cid:48) Y +Âµdisplayedasthesolidline. Thisistheattractorofthesystem. ForvaluesofY onthis line Î²(cid:48) Y +Âµ=0. For values to the southeast Î²(cid:48) Y +Âµ<0, and for values to the northwest Î²(cid:48) Y +Âµ>0. ThecomponentsofÎ±dictatehowthesevaluesimpacttheexpecteddirectionofâY.Thearrowsindicate thesedirections18. WhenÎ²(cid:48) Y +Âµ>0theerrorcorrectiondecreasesthe3-monthrateandincreasesthe 10-yearrate,pushingY towardsthelineofattraction. WhenÎ²(cid:48) Y +Âµ<0theerrorcorrectionincreases the3-monthrateanddecreasesthe10-yearrate,againpushingY towardsthelineofattraction. Inthis particularexamplethetwoeffectsaresimilarinmagnitudesothearrowsshowthatbothvariablesmove towardstheattractorinresponsetodeviations. Theorem16.17showsthatifY iscointegratedthenitsatisfiesaVECM.Thereverseisalsothecase. t Theorem16.18 Granger Representation Theorem, Part II. Suppose that Y t satisfiesaVAR(â)model A(L)Y =a+e withVECMrepresentation t t âY t =Î±Î²(cid:48) Y tâ1 +Î(L)âY tâ1 +a+e t whereÎ²andÎ±aremÃr andfullrank. Supposethat(a)Allcomplexsolutions todet(A(z))=0areeitherz=1or|z|â¥1+Î´forsomeÎ´>0;(b) (cid:80)â j=0 (cid:176) (cid:176) Î j (cid:176) (cid:176) <â; (c)Î±(cid:48) â¥ (I m âÎ(1))Î² â¥isfullrankwhereÎ± â¥andÎ² â¥lieinthenullspacesofÎ±and Î².ThenY iscointegratedwithcointegratingvectorsÎ². t TheproofispresentedinSection16.22. Thisresultforafinite-orderVARfirstappearedinJohansen (1995,Theorem4.2). TheconditionthatÎ±(cid:48) â¥ Î(1)Î² â¥isfullrankisnecessarytoexcludethe(somewhatpathological)possi- bilitythatthesystemisâmulti-cointegratedâ,meaningthatalinearcombinationofÎ²(cid:48) Y tâ1 andâY tâ1 is ofreducedorderofintegration.Together,Theorems16.17and16.18showthataVECMrepresentationis necessaryandsufficientforavectortimeseriestobecointegrated",
    "page": 598,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". t TheproofispresentedinSection16.22. Thisresultforafinite-orderVARfirstappearedinJohansen (1995,Theorem4.2). TheconditionthatÎ±(cid:48) â¥ Î(1)Î² â¥isfullrankisnecessarytoexcludethe(somewhatpathological)possi- bilitythatthesystemisâmulti-cointegratedâ,meaningthatalinearcombinationofÎ²(cid:48) Y tâ1 andâY tâ1 is ofreducedorderofintegration.Together,Theorems16.17and16.18showthataVECMrepresentationis necessaryandsufficientforavectortimeseriestobecointegrated. 17FromTable16.5. 18FromtheestimatesinTable16.6.",
    "page": 598,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 579 16.18 RoleofInterceptandTrend TheroleofinterceptsandtrendsincointegratingVECMsgivesrisetodistinctmodels. Welistsome majoroptions. 1. TrendModel1.Thisspecificationhasnointerceptortrendterms âY t =Î±Î²(cid:48) Y tâ1 +Î(L)âY tâ1 +e t . Thisisconvenientforpedagogybutisnotrelevantforempiricalapplications. InStatauseoption trend(none). 2. TrendModel2. Thisspecificationisappropriatefornon-trendedseriessuchasinterestrates. In thismodeltheinterceptisinthecointegratingrelationship âY t =Î±(cid:161)Î²(cid:48) Y tâ1 +Âµ(cid:162)+Î(L)âY tâ1 +e t . InStatauseoptiontrend(rconstant). 3. TrendModel3.Thisisappropriateforserieswhichhavepossiblelineartrends.Thismodelhasan unconstrainedintercept âY t =Î±Î²(cid:48) Y tâ1 +Î(L)âY tâ1 +a+e t . InthismodelthelevelseriesY isthesumofalineartimetrendandaunitrootprocess.Theequi- t libriumerrorÎ²(cid:48) Y isstationarysoeliminatesthelineartimetrendandtheunitrootcomponent. t InStatauseoptiontrend(constant). 4. TrendModel4. ThismodelextendstheVECMmodeltoallowalineartrendinthecointegrating relationship.Thismodelis âY t =Î±(cid:161)Î²(cid:48) Y tâ1 +Âµt (cid:162)+Î(L)âY tâ1 +a+e t . In this model the level series Y is the sum of a linear time trend and a unit root process. The t equilibriumerrorÎ²(cid:48) Y containsalineartimetrendandastationaryprocess. Thusthecointegrat- t ing vector Î² only eliminates the unit root, not the time trend component. In Stata use option trend(rtrend). 5. TrendModel5.Thisisafurtherextensionallowinganunconstrainedtrendterm âY t =Î±Î²(cid:48) Y tâ1 +Î(L)âY tâ1 +a+bt+e t . InthismodeltheunconstrainedtrendinducesaquadratictimetrendintothelevelsseriesY .This t isnotatypicalmodelingchoiceforappliedeconomictimeseries.InStatauseoptiontrend(trend). 16.19 CointegratingRegression IfY iscointegratedwithasinglecointegratingvector(r =1)thenitturnsoutthatÎ²canbeestimated t by a least squares regression of one component of Y on the others. This approach may be fruitfully t employed when the major focus is the cointegrating vector, the number of variables m is small (e.g. m=2orm=3),anditisknownthatthenumberofcointegratingvectorsr isatmostone.",
    "page": 599,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 580 PartitionY =(Y ,Y )andreparameterizeÎ²as(1,âÎ²). Thusthefirstcomponentofthecointegrat- t 1t 2t ingvectorhasbeennormalizedtoone(thisrequiresthatthetruevalueisnon-zero)andtheremainder multipliedbyâ1.ThecoefficientofinterestisÎ².Leastsquaresisfiteithertotheequation Y =Âµ+Î²(cid:48) Y +u (16.21) 1t 2t 1t (forTrendModels1or2)ortotheequation Y =Âµ+Î¸t+Î²(cid:48) Y +u (16.22) 1t 2t 1t (forTrendModels3or4). Define u = âY , u = (u ,u (cid:48) ) (cid:48) , and the long-run covariance matrix â¦ = Î£+Î+Î(cid:48) where Î£ = 2t 2t t 1t 2t (cid:69)(cid:163) u t u t (cid:48) â(cid:96) (cid:164) andÎ=(cid:80)â (cid:96)=1 (cid:69)(cid:163) u tâ(cid:96)u t (cid:48)(cid:164) .PartitionthecovariancematricesconformablywithY,e.g. (cid:183) â¦ â¦ (cid:184) â¦= 11 12 . â¦ â¦ 21 22 Theorem16.19 Ifu satisfiestheconditionsofTheorem16.4andâ¦ >0then t 22 theleastsquaresestimatorsatisfies (cid:181)(cid:90) 1 (cid:182)â1(cid:181)(cid:90) 1 (cid:182) n (cid:161)Î² (cid:98) âÎ²(cid:162)ââ XX (cid:48) XdB 1 +Î£ 21 +Î 21 d 0 0 whereB(r)=(B (r),B (r))isavectorBrownianmotionwithcovariancematrix 1 2 â¦andX(r)isdeterminedbythemodel: TrendModel1or2estimatedby(16.21): X =B â (demeanedB (r)). 2 2 TrendModel3or4estimatedby(16.22): X =B ââ (detrendedB (r)). 2 2 TheproofispresentedinSection16.22. Theorem 16.19 shows that the estimator converges at the superconsistent O (n â1) rate. This was p discovered by Stock (1987) and the asymptotic distribution derived by Park and Phillips (1988). The asymptoticdistributionisnon-standardduetotheserialcorrelationterms.Takeourempiricalexample. Aleastsquaresregressionofthe3-monthinterestrateonthe10-yearinterestrateyieldstheestimated equationY(cid:98)1t =1.03Y 2t â1.71. Modifications to the least squares estimator which eliminate the non-standard components were introducedbyPhillipsandB.E.Hansen(1990)andStockandWatson(1993). ThePhillips-Hansenes- timator, known as Fully Modified OLS (FM-OLS), eliminates the non-standard components through first-stage estimation of the serial correlation terms. The Stock-Watson estimator, known as Dynamic OLS(DOLS),eliminatesthenon-standardcomponentsbyestimatinganaugmentedregressioninclud- ingleadsandlagsofâY . 2t Weareofteninterestedintestingthehypothesisofnocointegration: (cid:72) :r =0 0 (cid:72) :r >0. 1 Under(cid:72) Z =Î²(cid:48) Y is I(1)yetunder(cid:72) Z is I(0).WhenÎ²isknown(cid:72) canbetestedbyappyinga 0 t t 1 t 0 univariateADFteston Z . Taketheinterestrateexample. Wealreadyconjecturedthattheinterestrate t",
    "page": 600,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 581 spreadisstationarywhichisthesameasthehypothesisthatÎ²=1isthecointegratingcoefficient.Using this value we computed ADF=â4.0 with an asymptotic p-value less than 0.01. Hence we are able to reject the null hypothesis of a unit root in the spread, or equivalently reject the null hypothesis of no cointegration. WhenÎ²isunknown,EngleandGranger(1987)proposedapplyingtheADFtesttotheleastsquares residualu fromeither(16.21)or(16.22). TheasymptoticnulldistributionisdifferentfromtheDickey- (cid:98)1t Fullerdistributionsinceunder(cid:72) theestimatedregressionisspurioussotheleastsquaresestimatoris 0 inconsistent. TheasymptoticdistributionofthestatisticwasworkedoutbyPhillipsandOuliaris(1990) bycombiningthetheoryofspuriousregressionwithDickey-Fullerdistributiontheory. LetEG denote p theEngle-GrangerADFstatisticwithp autoregressivelagsintheADFregression. Theorem16.20 Assumethat(âY ,âY )satisfiestheconditionsofTheorem 1t 2t 16.4andâ¦>0.Ifpââas nââsuchthatp3/nâ0then (cid:179) (cid:180) (cid:82)1 VdV 0 EG ââ p d (cid:179) (cid:82)1 V2 (cid:180)1/2 (1+Î¶(cid:48)Î¶)1/2 0 where, V(r) = W â (r)âÎ¶(cid:48) W â (r) and Î¶ = (cid:179) (cid:82)1 W â W â(cid:48) (cid:180)â1(cid:179) (cid:82)1 W â W â (cid:180) , W(r) = 1 2 0 2 2 0 2 1 â (W (r),W (r)) is vector standard Brownian motion, and W (r) is demeaned 1 2 W(r)if(16.21)isestimatedordetrendedW(r)if(16.22)isestimated. ForaproofseePhillipsandOuliaris(1990). An unusual feature of this Theorem is that it requires p ââ as n ââ even if the true process is afiniteorderARprocessbecausethefirststagespuriousregressioninducesserialcorrelationintothe first-stageresidualswhichneedstobehandledinthesecondstageADFtest. Anotherunusualfeature is the component 1+Î¶(cid:48)Î¶ in the denominator. This is due to the variance estimator component which asymptoticallyisrandombecauseofthefirststagespuriousregression. Table16.4:Engle-GrangerCointegrationTestCriticalValues m 1% 2% 3% 4% 5% 7% 10% 15% 20% 30% 50% 70% RegressionwithIntercept 2 â3.9 â3.7 â3.5 â3.4 â3.3 â3.2 â3.0 â2.8 â2.7 â2.5 â2.1 â1.7 3 â4.3 â4.1 â4.0 â3.9 â3.8 â3.6 â3.5 â3.3 â3.1 â2.9 â2.5 â2.1 4 â4.6 â4.4 â4.3 â4.2 â4.1 â4.0 â3.8 â3.6 â3.5 â3.2 â2.8 â2.4 RegressionwithInterceptandTrend 2 â4.3 â4.1 â4.0 â3.9 â3.8 â3.6 â3.5 â3.3 â3.2 â2.9 â2.5 â2.2 3 â4.7 â4.4 â4.3 â4.2 â4.1 â4.0 â3.8 â3.6 â3.5 â3.3 â2.9 â2.5 4 â5.0 â4.7 â4.6 â4.5 â4.4 â4.3 â4.1 â4.0 â3.8 â3.6 â3.2 â2.8 Theasymptoticcriticalvalues19 aredisplayedinTable16.4. TheEGtestisone-sided,sorejections occurwhentheteststatisticislessthan(morenegativethan)thecriticalvalue. Thecriticalvaluesarea functionofthenumberofvariablesmandthedetrendingmethod. 19Calculatedbysimulationfromonemillionsimulationdrawsforasampleofsizen=10,000.",
    "page": 601,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 582 An important question is which trend model to fit. If the observations are untrended then the in- tercept regression (16.21) should be fit and the âRegression with Interceptâ critical values used. If the observationsaretrendedandnoconstraintsareimposedthenthetrendregression(16.22)shouldbefit andtheâRegressionwithInterceptandTrendâcriticalvaluesused. Acomplicationarisesinthecaseof Model3,whichallowstheobservationstobetrendedbutthetrendisexcludedfromthecointegrating regression. In this cases there are two options. One is to treat the situation as Model 4: estimate re- gression(16.22)andusetheassociatedcriticalvalues. Theotheroptionistoestimate(16.21)sincethe lineartrendisnotinthecointegratingrelationship. Inthiscasetheappropriatecriticalvaluesarefrom theâRegressionwithInterceptandTrendâsectionofthetable,butwiththerowcorrespondingtomâ1. Thisisbecauseoneoftheunitrootprocessesinregression(16.22)isdominatedbyalineartrend. For example,iftherearem=3variablesinthesystemand(16.21)isestimated,thenusethecriticalvalues forâRegressionwithInterceptandTrendâandm=2. Iftherearem=2variablesthenusetheâCase3â ADFcriticalvaluesfromTable16.1. To illustrate, take the interest rate application. These variables are non-trended so we use model (16.21) with the âRegression with Interceptâ critical values. The least squares residuals are u (cid:98)1t =Y(cid:98)1t â 1.03Y â1.7. ApplyinganADFtestwithp=8weobtainEG=â4.0. Thisissmallerthanthe1%asymp- 2t toticcriticalvalueofâ3.9fromTable16.4. Wethereforerejectthehypothesisofnocointegration,sup- portingthehypothesisthatthepairarecointegrated. 16.20 VECMEstimation TheGrangerRepresentationTheorem(Theorems16.17and16.18)showedthatY iscointegratedif t (andonlyif)Y satisfiesanerror-correctionmodel.AVECM(p)modelis t âY t =Î±Î²(cid:48) Y tâ1 +Î 1 âY tâ1 +Â·Â·Â·+Î pâ1 âY tâp+1 +a+e t . (16.23) This is a reduced rank regression as introduced in Section 11.11. The standard estimation method is maximumlikelihoodundertheauxilaryassumptionthate isi.i.d. N(0,Î£),describedinTheorem11.7. t WerepeatthisresultherefortheVECMmodel. Theorem16.21 The MLE for the VECM (16.23) under e â¼N(0,Î£) is given as follows.First,regressâY t andY tâ1 onâY tâ1 ,...,âY tâp+1 andanintercepttoob- taintheresidualvectorsu (cid:98)0t andu (cid:98)1t ,organizedinmatricesasU(cid:98)0 andU(cid:98)1 . The (cid:48) (cid:179) (cid:48) (cid:180)â1 (cid:48) MLEÎ² (cid:98)equalsthefirstr generalizedeigenvectorsof n 1U(cid:98)1 U(cid:98)0 n 1U(cid:98)0 U(cid:98)0 n 1U(cid:98)0 U(cid:98)1 (cid:48) withrespectto n 1U(cid:98)1 U(cid:98)1 correspondingtother largesteigenvaluesÎ» (cid:98)j .Thisuses thenormalizationÎ² (cid:98) (cid:48) n 1U(cid:98) (cid:48) 1 U(cid:98)1 Î² (cid:98) =I r",
    "page": 602,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". The (cid:48) (cid:179) (cid:48) (cid:180)â1 (cid:48) MLEÎ² (cid:98)equalsthefirstr generalizedeigenvectorsof n 1U(cid:98)1 U(cid:98)0 n 1U(cid:98)0 U(cid:98)0 n 1U(cid:98)0 U(cid:98)1 (cid:48) withrespectto n 1U(cid:98)1 U(cid:98)1 correspondingtother largesteigenvaluesÎ» (cid:98)j .Thisuses thenormalizationÎ² (cid:98) (cid:48) n 1U(cid:98) (cid:48) 1 U(cid:98)1 Î² (cid:98) =I r . TheMLEfortheremainingcoefficientsÎ± (cid:98) , Î (cid:98)1 ,...,Î (cid:98)pâ1 ,anda (cid:98) areobtainedbytheleastsquaresregressionofâY t onÎ² (cid:98) (cid:48) Y tâ1 , âY tâ1 ,...,âY tâp+1 ,andanintercept.Themaximizedlog-likelhoodfunctionis (cid:96) n (r)= m 2 (cid:161) nlog(2Ï)â1 (cid:162)â n 2 det (cid:181) n 1 U(cid:98) (cid:48) 0 U(cid:98)0 (cid:182) â n 2 (cid:88) r log (cid:161) 1âÎ» (cid:98)j (cid:162) . j=1 ThisestimationmethodwasdevelopedbyJohansen(1988,1991,1995)asanextensionofthereduced rankregressionofAnderson(1951).",
    "page": 602,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 583 TheVECMisaconstrainedVARsotheVECMestimatescanbeusedforanypurposeforwhichaVARis used.AnadvantageoftheVECMestimationapproachisthatitprovidesacoherentmodelofthesystem, is computationally straightforward, and can handle multiple cointegrating vectors. A disadvantage is thatwhentherearemultiplecointegratingvectors(r >1)theninterpretationofthecointegratingspace (thespacespannedbyÎ²)isdifficult. TheVECMmodelassumesthattheVARorderpandcointegratingrankr areknown.Inpracticedata- basedselectionrulesareused.AICminimizationmaybeusedforselectionofp.Asimpleapproachisto selectp byestimatingunrestrictedVARmodels. Selectionofr istypicallydonebytestingmethods;this isreviewedinthenextsection. Weillustratewiththetwointerestrateseriesalreadyintroduced.AICselectiononlevelsVARsselects aVAR(8);wereporthereaVAR(4)asityieldssimilarresults. ThisimpliesaVECMwith3dynamiclags. SinceinterestratesarenotatrendedseriesweuseTrendModel2. Theestimatedmodelisreportedin Tables16.5and16.6. Table16.5:VECMCointegratingVector Î² s.e. 3-Month 1 10-Year â1.01 0.07 Intercept 1.58 0.46 Table16.6:VectorErrorCorrectionModel â3-Month â10-Year t t Z tâ1 â0.09 0.07 (0.04) (0.03) â3-Month tâ1 0.37 0.04 (0.08) (0.06) â3-Month tâ2 â0.20 â0.08 (0.08) (0.06) â3-Month tâ3 0.28 0.07 (0.08) (0.06) â10-Year tâ1 0.06 0.21 (0.07) (0.08) â10-Year tâ2 â0.19 â0.09 (0.12) (0.08) â10-Year tâ3 0.10 0.06 (0.12) (0.08) Table16.5reportstheestimatedcointegratingvectorÎ². Thecoefficientonthe3-monthinterestrate isnormalizedtoone. Theestimatedcoefficientonthe10-yearrateisnearâ1andtheestimatedinter- ceptisabout1.6. Thelattermeansthatthe3-monthrateisonaverage1.6percentagepointsbelowthe 10-year rate. The coefficients of the estimated VECM are reported in Table 16.6, one column for each variable. ThefirstreportedcoefficientisÎ±, theerror-correctionterm. Thecoefficientforthe3-month (cid:98) rateisnegativeandthatforthe10-yearrateispositiveandtheyareofsimilarmagnitude.Thuswhenthe 3-monthrateexceedsthe10-yearratebymorethanthetypical1.6, the3-monthratetendstofalland the10-yearratetendstorise,movingthetworatesclosertothecointegratingrelation.Thefollowingsix coefficientsarethedynamiccoefficientsoftheVECM.Wecanseethateachvariabletendstorespond",
    "page": 603,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 584 mostlytoitsownlaggedchanges.The3-monthinterestratehasconsiderablylargercoefficientsthanthe 10-yearrateindicatingthatithasstrongerserialcorrelation. Thevaryingsignsofthecoefficientsreveal complicateddynamics.. AnasymptoticdistributionoftheVECMestimatorrequiresanormalizationforthecointegratingvec- tors. ApopularchoiceisÎ²=(I ,Î²â(cid:48) ) (cid:48) . Johansen(1995,Theorem13.5)showsthatundertheassumption r thattheerrorse t arei.i.d.withcovariancematrixÎ£,thecoefficientestimatorsÎ¸ (cid:98) =(Î± (cid:98) ,Î (cid:98))satisfy (cid:112) n (cid:161)Î¸ (cid:98) âÎ¸(cid:162)ââN (cid:161) 0,Î£âQ â1(cid:162) d whereQ =(cid:69)(cid:163) X t X t (cid:48)(cid:164) with X t =(Î²(cid:48) Y tâ1 ,âY tâ1 ,...,âY tâp+1 ),theregressorsgivenÎ². Thisisaclassical(ho- moskedastic)asymptoticdistributionformultivariateregression.Thisresultshowsthatinferenceonthe coefficientsÎ¸canproceedusingconventionalmethods.Thehomoskedasticcovariancematrixisdueto theassumptionthattheerrorsarehomoskedastic.Ifthelatterassumptionisrelaxedthentheasymptotic distributiongeneralizestothecaseofanunrestrictedcovariancematrix. Johansen(1995,Theorem13.3)presentstheasymptoticdistributionofÎ² (cid:98). Heshowsthattheasymp- toticdistributionisnormalwitharandomcovariancematrix. ThelatterisknownasamixedGaussian distribution. From a practical point of view this means that we can treat the asymptotic distribution asnormalsincewhenscaledbyanappropriatestandarderrortheasymptoticdistributionisstandard normal.Forbrevitywedonotpresentthedetails. InStatausethecommandvectoestimateaVECMwithgivencointegratingrankr andVARorderp. 16.21 TestingforCointegrationinaVECM Takethemodel âY t =Î Y tâ1 +Î 1 âY tâ1 +Â·Â·Â·+Î pâ1 âY tâp+1 +a+e t . (16.24) TheGrangerRepresentationTheoremshowsthatY iscointegratedwithr cointegratingvectorsifand t onlyiftherankofÎ equalsr.Thustestingforcointegrationisequaltotestinghypothesesontherankof Î .Writethehypothesisthattherearer cointegratingvectorsas(cid:72)(r):rank(Î )=r. Cointegration is a restriction on the unrestricted model (cid:72)(m). A test for r cointegrating vectors againstanunrestrictedalternativeisatestof(cid:72)(r)against(cid:72)(m). Thelikelihoodratiostatisticfor(cid:72)(r) against(cid:72)(m)is m r m LR(r)=2((cid:96) n (m)â(cid:96) n (r))=ân (cid:88) log (cid:161) 1âÎ» (cid:98)j (cid:162)+n (cid:88) log (cid:161) 1âÎ» (cid:98)j (cid:162)=ân (cid:88) log (cid:161) 1âÎ» (cid:98)j (cid:162) j=1 j=1 j=r+1 whereÎ» (cid:98)j aretheeigenvaluesfromtheestimationproblem(16.21).Thetestaccepts(cid:72)(r)forsmallvalues ofLR(r);thetestrejects(cid:72)(r)forlargevaluesofLR(r). TheasymptoticdistributiontheorywasdevelopedbyJohansen(1988,1991,1995).",
    "page": 604,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 585 Theorem16.22 Assumethatthefinite-lagVECM(16.24)iscorrectlyspecified, theconditionsofTheorem16.18hold,andtheerrorse areaMDS.Underthe t hypothesisthatÎ hasrankr (cid:34) (cid:181)(cid:90) 1 (cid:182)(cid:181)(cid:90) 1 (cid:182)â1(cid:181)(cid:90) 1 (cid:182) (cid:35) LR(r)ââtr dWX (cid:48) XX (cid:48) XdW (cid:48) d 0 0 0 whereW(r) is a mâr dimensional standard Brownian motion and X(r) is a stochasticprocesswhichisafunctionofW(r)dependingonthetrendmodel. 1. TrendModel1. X(r)=W(r) 2. TrendModel2. X(r)=(W(r),1) 3. TrendModel3. X(r)=(W â (r),râ1/2) 1 4. TrendModel4. X(r)=(W â (r),râ1/2) whereW â (r)=W(r)â(cid:82)1 W isdemeanedW(r),andW â (r)isthefirstmârâ1 0 1 â componentsofW (r). AproofofTheorem16.22isalgebraicallytedious.WeprovideasketchinSection16.22.SeeJohansen (1995,Chapter11)forfulldetails. Theorem16.22providestheasymptoticdistributionoftheLRtestforcointegrationrank. Because theasymptoticdistributionequalsthetraceofamultivariateDickey-FullerdistributionthestatisticLR is often referred to as the âtrace testâ or âJohansenâs trace testâ. The asymptotic distribution is a func- tionofthestochasticprocessX(r)whichequalsthetrendcomponentsofY (underthehypothesisofr t cointegratingvectors)projectedorthogonaltotheotherregressors. ForTrendModel2theinterceptis includedinthecointegratingrelationshipsoitisacomponentofX(r). ForTrendModel3thevariables aretrendedwhichdominatestheothercomponentssoappearsintheasymptoticdistribution.Sincethe interceptisexcludedfromthecointegratingrelationshipthecomponentsofX(r)arealldemeaned. For Trend Model 4 the linear trend is included in the cointegrating relationship so it is added to the trend componentswhiletheinterceptisexcludedsotheX(r)processisdemeaned. Theasymptoticdistributionisafunctiononlyofmâr andthetrendspecification. Theasymptotic criticalvalues20aredisplayedinTable16.7formâr upto12forTrendModels2,3,and4. HowaretheteststatisticsLR(r)usedinpractice?Whenthecointegratingrankisunknownthestatis- ticscanbeusedtodeterminer.Theconventionalprocedureisasequentialtest.Startwith(cid:72)(0)(thenull hypothesisofnocointegration)andtheassociatedstatisticLR(0)whichhasmdegreesoffreedom.Ifthe testrejects(ifLR(0)exceedstherowmcriticalvalue)thisisevidencethatthereisatleastonecointegrat- ingvector,orr â¥1. Next,take(cid:72)(1)(thenullhypothesisofonecointegratingvector)andtheassociated statisticLR(1)whichhasmâ1degreesoffreedom.Ifthistestalsorejects(ifLR(1)exceedstherowmâ1 critical value) this is evidence that there is at least two cointegrating vectors, or r â¥2. Continue this sequenceoftestsuntilonefailstoreject. For example, when there are two variables (m = 2) compare the statistic LR(0) against the m = 2 criticalvalue. Ifthetestrejects(ifthestatisticexceedsthecriticalvalue)thisisevidencethattheseries arecointegrated.Ifthetestfailstorejecttheinferenceisuncertain. 20Calculatedbysimulationfromonemillionsimulationdrawsforasampleofsizen=10,000.",
    "page": 605,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 586 Table16.7:VECMCointegrationRankCriticalValues mâr 50% 30% 20% 15% 10% 7% 5% 4% 3% 2% 1% TrendModel2 1 3.5 4.9 5.9 6.6 7.6 8.4 9.2 9.7 10.4 11.3 12.7 2 11.4 13.9 15.5 16.6 18.0 19.2 20.3 21.0 21.9 23.1 25.1 3 23.4 26.8 29.0 30.4 32.3 33.8 35.2 36.1 37.2 38.7 41.3 4 39.4 43.7 46.5 48.3 50.6 52.5 54.1 55.2 56.5 58.4 61.3 5 59.4 64.6 67.9 70.1 72.8 75.0 77.0 78.3 79.8 82 85 6 83 89 93 96 99 102 104 105 107 110 113 7 111 118 123 126 129 132 135 136 138 141 145 8 143 151 156 159 163 167 170 171 174 177 182 9 179 188 194 197 202 205 208 210 213 216 222 10 219 229 235 239 244 248 251 254 256 260 266 11 263 274 281 285 290 295 298 301 304 307 314 12 311 323 330 335 341 345 349 352 355 359 366 TrendModel3 1 0.5 1.1 1.6 2.1 2.7 3.3 3.8 4.2 4.7 5.4 6.6 2 7.7 9.7 11.2 12.1 13.4 14.5 15.5 16.1 17.0 18.1 19.9 3 18.9 22.0 24.0 25.3 27.1 28.5 29.8 30.7 31.7 33.2 35.5 4 34.0 38.0 40.7 42.3 44.5 46.3 47.9 48.9 50.2 51.9 54.7 5 53.1 58.0 61.2 63.2 65.8 67.9 69.8 71.0 72.5 74.5 77.9 6 76 82 86 88 91 94 96 97 99 101 105 7 103 110 114 117 120 123 126 127 129 132 136 8 134 142 147 150 154 157 159 161 163 166 171 9 169 178 183 187 191 194 197 199 202 205 210 10 208 218 224 227 232 236 239 241 244 247 253 11 251 262 269 272 277 282 285 287 290 294 300 12 298 310 317 321 327 331 335 338 341 345 351 TrendModel4 1 5.7 7.4 8.7 9.5 10.7 11.7 12.5 13.1 13.8 14.9 16.6 2 15.9 18.7 20.5 21.7 23.3 24.7 25.9 26.7 27.6 29.0 31.2 3 30.0 33.8 36.2 37.7 39.8 41.4 42.9 43.9 45.1 46.7 49.4 4 48.1 52.8 55.7 57.6 60.1 62.1 63.9 65.0 66.4 68.4 71.5 5 70.2 75.7 79.2 81 84 87 89 90 92 94 98 6 96 103 107 109 113 115 118 119 121 124 128 7 126 134 138 141 145 148 150 152 154 157 162 8 160 168 174 177 181 184 187 189 192 195 200 9 198 207 213 217 221 225 228 230 233 236 242 10 240 250 257 261 266 270 273 275 278 282 288 11 286 297 304 308 314 318 322 325 328 331 338 12 336 348 356 360 366 371 375 378 381 385 392",
    "page": 606,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 587 Thistestingprocedureisappealingwhenm issmall (e.g. m â¤4) butislessappealingforlarge m. Withlargemtheprocedurehasseveralchallenges.Sequentialtestingrequiresmultipletestingforwhich itisdifficulttocontrolTypeIerror.Simultaneouslythetestcanhavelowpowerimplyingthattheproce- dureislikelytoâidentifyâaninappropriatelylowvalueofr. An alternative approach is to use cointegration tests to verify a selected specification. Start with economicmodelingtomotivatethecointegratingrankr. ThelikelihoodratioLR(r)canbeusedtotest thisassumptionagainsttheunrestrictedVAR.Ifthetestrejects(cid:72)(r)thisisevidencethattheproposed modelisincorrect. WeillustrateusingtheinterestrateserieswithaVAR(4)andTrendModel2.ThevalueofLR(0)is31.6. Tocomputethep-valueweuseTable16.7forTrendModel2withmâr =2. Thevalue31.6exceedsthe 1%criticalvalueof25.1sotheasymptoticp-valueofthetestislessthan1%.Thusthenullhypothesisof nocointegrationisstronglyrejectedinfavorofatleastonecointegratingvector.ThevalueofLR(1)is2.8. Thep-valueiscalculatedusingmâr =1.Thevalue2.8issmallerthanthe50%criticalvalueof3.5sothe p-valueislargerthan50%.Thestatisticdoesnotrejectthehypothesisof(cid:72)(1).Togetherthestatisticsare consistentwiththemodelingassumptionthattheseriesareI(1)andmutuallycointegrated. ForabroaderapplicationweexpandtofiveTreasuryinterestrates21: 3-month,6-month,1-year,5- year,and10-year.Ifthefourspreadsaremutuallystationarythenthesystemshouldhavefourcoingrat- ingvectors,thusr =4.Howeverifthethedistributionofthespreadschangeovertimethecointegrating rankcouldbelessthanfour. WereportthelikelihoodratiotestsforcointegrationrankinTable16.8. TheLRtestforr =0is120 whichexceedsthe1%criticalvalueof85.4,andtheLRtestforr =1is68.3whichexceedsthe1%critical value of 61.3, so we safely reject the hypotheses of r =0 and r =1. This suggests that r â¥2. The LR testforr =2is33.6withap-valueof0.07,whichisborderlinesignificant. Thetestsforr =3andr =4 are insignificant. In sum, we cannot reject the models (cid:72)(2), (cid:72)(3), or (cid:72)(4), but (cid:72)(2) is doubtful. Our recommendationinthiscontextistouseeither(cid:72)(3)or(cid:72)(4). Table16.8:TestsforCointegratingRank LR(r) p-value 0 120 <0.01 1 68.3 <0.01 2 33.6 0.07 3 10.8 >0.50 4 2.9 >0.50 InStatausevecranktocalculatetheLRtestsforcointegratingrank.Theoutputisatabledisplaying LR(r)forr =0,...,mâ1alongwiththeasymptotic5%criticalvalues.Thep-valuecanbecalculatedfrom Table16.7. 16.22 TechnicalProofs* ProofofTheorem16.1.InthetextweshowedthatthelimitdistributionsofS coincidewiththoseofB. n ToappealtotheFunctionalCentralLimitTheorem(Theorem18.9ofIntroductiontoEconometrics)we needtoverifythatS isstochasticallyequicontinuous.Forsimplicitywefocusonthescalarcasee â(cid:82). n t AssumewithoutlossofgeneralitythatÏ2=1. Takeany0<Î·<1and0<(cid:178)<1. SetÎ´â¤(cid:178)Î·4/482. Note 21FRED-MDseriesTB3MS,TB6MS,GS1,GS5,andGS10.",
    "page": 607,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 588 that sup |S n (r 2 )âS n (r 1 )|â¤2 sup sup (cid:175) (cid:175)S n (jÎ´+r)âS n (cid:161) jÎ´(cid:162)(cid:175) (cid:175). |r2 âr1 |â¤Î´ 0â¤jâ¤(cid:98)1/Î´(cid:99)0â¤râ¤Î´ Then (cid:34) (cid:35) (cid:34)(cid:98)1/Î´(cid:99) Î· (cid:35) (cid:80) sup |S n (r 2 )âS n (r 1 )|>Î· â¤(cid:80) (cid:91) sup (cid:175) (cid:175)S n (jÎ´+r)âS n (cid:161) jÎ´(cid:162)(cid:175) (cid:175) > |r2 âr1 |â¤Î´ j=0 0â¤râ¤Î´ 2 (cid:98)1/Î´(cid:99) (cid:183) Î·(cid:184) â¤ (cid:88) (cid:80) sup (cid:175) (cid:175)S n (jÎ´+r)âS n (cid:161) jÎ´(cid:162)(cid:175) (cid:175) > j=0 0â¤râ¤Î´ 2 (cid:181) 1 (cid:182) (cid:183) Î·(cid:184) â¤ +1 (cid:80) sup |S (r)|> Î´ 0â¤râ¤Î´ n 2 (cid:34) (cid:175) (cid:175) (cid:35) (cid:181) 1 (cid:182) (cid:175) 1 (cid:88) i (cid:175) Î· = +1 (cid:80) max (cid:175)(cid:112) e (cid:175)> Î´ iâ¤(cid:98)nÎ´(cid:99) (cid:175) (cid:175) n t=1 t(cid:175) (cid:175) 2 (cid:181) 1 (cid:182) (cid:34)(cid:175) (cid:175) 1 (cid:98) (cid:88) nÎ´(cid:99) (cid:175) (cid:175) Î· (cid:35) â¤2 +1 (cid:80) (cid:175)(cid:112) e (cid:175)> . Î´ (cid:175) (cid:175) n t=1 t(cid:175) (cid:175) 4 (cid:112) ThefinalinequalityisBillingsleyâs(B.52)whichholdssinceÎ´<Î·/4 2undertheassumptions.Ourstate- ment(B.52)ofBillingsleyâsinequalityassumesthate isani.i.d. sequence;theresultcanbeextendedto t aMDSsequence. TheCLTimpliesthatn â1/2(cid:80)(cid:98) t n = Î´ 1 (cid:99) e t ââZÎ´ â¼N(0,Î´). Fornsufficientlylargethefinallineisbounded d by Î´ 3 (cid:80) (cid:104) |ZÎ´ |> Î· 4 (cid:105) = Î´ 3 (cid:80) (cid:183) Z Î´ 4> 1 Î· 6 4 2 (cid:184) â¤ Î´ 31 Î· 6 4 2 (cid:69)(cid:163) Z4(cid:164)= 4 Î· 8 4 2 Î´=(cid:178). (16.25) ThefirstinequalityisMarkovâs,thefollowingequality(cid:69)(cid:163) Z4(cid:164)=3Î´2,andthefinalequalityistheassump- Î´ tionÎ´=(cid:178)Î·4/482.ThisshowsthatS satisfiesthedefinitionofstochasticequicontinuity. â  n ProofofTheorem16.7. Z hastheWolddecompositionZ =Î(L)e . Weaddtheadditionalassumption t t t thate t isaMDStosimplifytheproof.BytheBeveridge-NelsondecompositionZ t =Î¾ t +U t âU tâ1 where Î¾ =Î(1)e andU =Îâ (L)e .Then t t t t n 1 t (cid:88) = n 1 S tâ1 Z t (cid:48)= n 1 t (cid:88) = n 1 S tâ1 Î¾(cid:48) t + n 1 t (cid:88) = n 1 S tâ1 U t (cid:48)â n 1 t (cid:88) = n 1 S tâ1 U t (cid:48) â1 = n 1 t (cid:88) = n 1 S tâ1 Î¾(cid:48) t â n 1n t (cid:88) = â 1 1 Z t U t (cid:48)+o p (1)",
    "page": 608,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". â  n ProofofTheorem16.7. Z hastheWolddecompositionZ =Î(L)e . Weaddtheadditionalassumption t t t thate t isaMDStosimplifytheproof.BytheBeveridge-NelsondecompositionZ t =Î¾ t +U t âU tâ1 where Î¾ =Î(1)e andU =Îâ (L)e .Then t t t t n 1 t (cid:88) = n 1 S tâ1 Z t (cid:48)= n 1 t (cid:88) = n 1 S tâ1 Î¾(cid:48) t + n 1 t (cid:88) = n 1 S tâ1 U t (cid:48)â n 1 t (cid:88) = n 1 S tâ1 U t (cid:48) â1 = n 1 t (cid:88) = n 1 S tâ1 Î¾(cid:48) t â n 1n t (cid:88) = â 1 1 Z t U t (cid:48)+o p (1). (cid:82)1 (cid:48) Thefirsttermconvergesto BdB byTheorem16.6.TheBrownianmotionhascovariancematrixequal 0 tothelong-runvarianceofZ ,whichisâ¦.Thesecondtermconvergesinprobabilityto(cid:69)(cid:163) Z U (cid:48)(cid:164) .Making t t t thesubstitutionsU t =Î¾ t+1 +U t+1 âZ t+1 and(cid:69)(cid:163) Z t Î¾(cid:48) t+1 (cid:164)=0thiscanbewrittenas (cid:69)(cid:163) Z U (cid:48)(cid:164)=(cid:69)(cid:163) Z Î¾(cid:48) (cid:164)+(cid:69)(cid:163) Z U (cid:48) (cid:164)â(cid:69)(cid:163) Z Z (cid:48) (cid:164) t t t t+1 t t+1 t t+1 =(cid:69)(cid:163) Z U (cid:48) (cid:164)â(cid:69)(cid:163) Z Z (cid:48) (cid:164) t t+1 t t+1 =(cid:69)(cid:163) Z U (cid:48) (cid:164)â(cid:69)(cid:163) Z Z (cid:48) (cid:164)â(cid:69)(cid:163) Z Z (cid:48) (cid:164) t t+2 t t+2 t t+1 =Â·Â·Â· â â =â (cid:88) (cid:69) (cid:104) Z t Z t (cid:48) +j (cid:105) =â (cid:88) (cid:69)(cid:163) Z tâj Z t (cid:48)(cid:164)=âÎ. j=1 j=1",
    "page": 608,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 589 ThethirdlinemakesthesubstitutionsU t+1 =Î¾ t+2 +U t+2 âZ t+2 and(cid:69)(cid:163) Z t Î¾(cid:48) t+2 (cid:164)=0,andthesubstitutions arerepeateduntilinfinity.Wehaveshowntheresultasclaimed. â  ProofofTheorem16.8.Bythedefinitionofthestochasticintegral (cid:90) 1 N (cid:88) â1 (cid:181) i (cid:182)(cid:181) (cid:181) i+1 (cid:182) (cid:181) i (cid:182)(cid:182) WdW =plim W W âW . (16.26) 0 Nââ i=0 N N N TakeanypositiveintegerN andany j <N.Observethat (cid:181) j+1 (cid:182) (cid:181) j (cid:182) (cid:181) (cid:181) j+1 (cid:182) (cid:181) j (cid:182)(cid:182) W =W + W âW . N N N N Squaringweobtain (cid:181) j+1 (cid:182)2 (cid:181) j (cid:182)2 (cid:181) j (cid:182)(cid:181) (cid:181) j+1 (cid:182) (cid:181) j (cid:182)(cid:182) 1 W âW =2W W âW + Ï . jN N N N N N N whereÏ =N (cid:179) W (cid:179) j+1 (cid:180) âW (cid:179) j (cid:180)(cid:180)2 .NoticethatÏ arei.i.d.across j,distributedasÏ2,andhaveexpec- jN N N jN 1 tation1.Summingover j =0toNâ1weobtain W(1)2=2 N (cid:88) â1 W (cid:181) i (cid:182)(cid:181) W (cid:181) i+1 (cid:182) âW (cid:181) i (cid:182)(cid:182) + 1 N (cid:88) â1 Ï2 . N N N N iN i=0 i=0 Rewriting N (cid:88) â1 W (cid:181) i (cid:182)(cid:181) W (cid:181) i+1 (cid:182) âW (cid:181) i (cid:182)(cid:182) = 1 (cid:195) W(1)2â 1 N (cid:88) â1 Ï2 (cid:33) . N N N 2 N iN i=0 i=0 By(16.26), (cid:82)1 WdW istheprobabilitylimitoftherightside.BytheWLLNthisis1(cid:161) W(1)2â1 (cid:162) asclaimed. â  0 2 ProofofTheorem16.10. (cid:195) 1n (cid:88) â1 (cid:33)2 Ï (cid:98) 2= n 1n t (cid:88) = â 1 1 e (cid:98)t 2 +t = n 1n t (cid:88) = â 1 1 e t 2 +t â n 1 n t 1 =1 n (cid:88) Y â t 1 e Y t+ 2 1 = n 1n t (cid:88) = â 1 1 e t 2 +t +o p (1)â p âÏ2. n2 t=1 t Then 1n (cid:88) â1 T = n t=1 Y t e t+1 ââ Ï2(cid:82) 0 1 WdW = (cid:82) 0 1 WdW . (cid:195) 1 n (cid:88) â1 (cid:33)1/2 d (cid:179) Ï2 (cid:82)1 W2 (cid:180)1/2 Ï (cid:179) (cid:82)1 W2 (cid:180)1/2 Y2 Ï 0 0 n2 t=1 t (cid:98) â  ProofofTheorem16.12.PickÎ·>0and(cid:178)>0.PickÎ´suchthat (cid:195) (cid:33) (cid:80) sup |X(r)âX(s)|>(cid:178) â¤Î· (16.27) |râs|â¤Î´",
    "page": 609,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 590 which is possible since X(r) is almost surely continuous. Set N =(cid:98)1/Î´(cid:99) and t =kn/N. Write X = k nt D â1X .Then n t C = 1 (cid:88) N tk(cid:88)+1 â1 X u = 1 (cid:88) N X tk(cid:88)+1 â1 u + 1 (cid:88) N tk(cid:88)+1 â1 (cid:161) X âX (cid:162) u n n k=0 t=tk nt t n k=0 n,tk t=tk t n k=0 t=tk nt n,tk t and |C |â¤ sup |X (r)|A + sup |X (r)âX (s)|B n n n n n n 0â¤râ¤1 |râs|â¤Î´ where N (cid:175) (cid:175)tk(cid:88)+1 â1 (cid:175) (cid:175) A = max(cid:175) u (cid:175) n (cid:175) t(cid:175) n kâ¤N (cid:175) t=tk (cid:175) B = 1 (cid:88) n |u |. n t n t=1 SinceX ââX andX iscontinuous, n d sup |X (r)|ââ sup |X(r)|<â n 0â¤râ¤1 d 0â¤râ¤1 almostsurely.Thussup 0â¤râ¤1 |X n (r)|=O p (1).SinceX n ââX, d sup |X (r)âX (s)|ââ sup |X(r)âX(s)|â¤(cid:178) n n |râs|â¤Î´ d |râs|â¤Î´ wheretheinequalityholdswithprobabilityexceeding1âÎ·by(16.27). Thusforsufficientlylargen the lefthandsideisboundedby2(cid:178)withthesameprobability,andhenceiso (1). p ForfixedN,A ââ0bytheergodictheorem.Theassumptionthat(cid:69)|u |<âimpliesthatB =O (1). n t n p p Together,wehaveshownthat |C |â¤O (1)o (1)+o (1)O (1)=o (1) n p p p p p asstated. â  ProofofTheorem16.17. Part1: ThedefinitionofcointegrationimpliesthatâY isstationarywithafinitecovariancematrix. By t themultivariateWoldrepresentation(Theorem15.2),âY =Î¸+Î(L)e withtheerrorswhitenoise. Pre- t t multiplicationbyÎ²(cid:48) yieldsÎ²(cid:48)âY =Î²(cid:48)Î¸+Î²(cid:48)Î(L)e whichhaslong-runvarianceÎ²(cid:48)Î(1)Î£Î(1) (cid:48)Î²whereÎ£is t t thecovariancematrixofe . TheassumptionthatÎ²(cid:48) Y is I(0)impliesthatÎ²(cid:48)Î¸=0(elseÎ²(cid:48) Y willhavea t t t timetrend).ThisimpliesÎ¸liesintherangespaceofÎ² â¥,henceÎ¸=Î² â¥ Î³forsomeÎ³.Also,theassumption thatÎ²(cid:48) Y isI(0)impliesthatÎ²(cid:48)âY isI(â1),whichimpliesthatitslong-runcovariancematrixequalszero. t t ThisimpliesthatÎ²(cid:48)Î(1)=0andhenceÎ(1)=Î² â¥ Î·(cid:48) forsomematrixÎ·.TheassumptionthatÎ²(cid:48) â¥ âY t isI(0) impliesthatÎ²(cid:48) â¥ Î(1)Î£Î(1) (cid:48)Î² â¥ >0whichimpliesthatÎ(1)musthaverankmâr andhencesodoesthe matrixÎ·. Part2: TheBeveridge-NelsondecompositionplusÎ(1)=Î² â¥ Î·(cid:48) impliesÎ(L)=Î² â¥ Î·(cid:48)+Îâ (L)(1âL). Ap- pliedtotheWoldrepresentationweobtainâY t =Î² â¥ Î³+Î² â¥ Î·(cid:48) e t +Îâ (L)âe t .Summingwefindthestated representation.",
    "page": 610,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 591 Part3: WithoutlossofgeneralityassumethatH =(cid:163)Î²,Î² â¥ (cid:164) isorthonormal. Alsodefinetheorthonormal matrix HÎ· =(cid:163)Î· â¥,Î·(cid:164) where Î·=Î·(Î·(cid:48)Î·) â1/2. Define X t = H (cid:48) Y t . The Wold representation implies âX t = (cid:181) (cid:182) 0 +C(L)e whereusingtheBeveridge-Nelsondecomposition Î³ t C(L)=H (cid:48)(cid:161)Î² â¥ Î·(cid:48)+Îâ (L)(1âL) (cid:162)= (cid:181) Î·(cid:48)+ Î²(cid:48) Î² Î (cid:48) â Î (L â ) ( ( L 1 ) â (1 L â ) L) (cid:182) . â¥ PartitionX =(X ,X )comformablywithH.Weseethat t 1t 2t (cid:181) âX (cid:182) (cid:181) Î²(cid:48)Îâ (L)(1âL)e (cid:182) âX 1t = Î³+Î·(cid:48) e +Î²(cid:48) Îâ (L)(1 t âL)e . 2t t â¥ t Summingthefirstequationweobtain (cid:181) â X X 1t (cid:182) = (cid:181) Âµ Î³ (cid:182) +D(L)HÎ· (cid:48) e t (16.28) 2t whereÂµ=X âÎ²(cid:48)Îâ (L)e and 1,0 0 (cid:181) Î²(cid:48)Îâ (L) (cid:182) (cid:195) Î²(cid:48)Îâ (L)Î· â¥ Î²(cid:48)Îâ (L)Î· (cid:33) D(L)= Î·(cid:48)+Î²(cid:48) â¥ Îâ (L)(1âL) HÎ· = Î²(cid:48) â¥ Îâ (L)Î· â¥(1âL) (cid:161)Î·(cid:48)Î·(cid:162)1/2+Î²(cid:48) â¥ Îâ (L)Î·(1âL) . Thisisaninvertiblematrixpolynomial.Toseethis,firstobservethat (cid:195) Î²(cid:48)Îâ (1)Î· â¥ Î²(cid:48)Îâ (1)Î· (cid:33) D(1)= 0 (cid:161)Î·(cid:48)Î·(cid:162)1/2 whichisfullrankundertheassumptionthatÎ²(cid:48)Îâ (1)Î· â¥ isfullrank. Thismeansthatdet(D(z))hasno unitroots.Second,(16.28)andthedefinitionofX implythat t (cid:181) 1âz 0 (cid:182) D(z)= HÎ(z)HÎ·. 0 1 SinceH andHÎ·arefullrankthisimpliesthatthesolutionstodet(D(z))=0aresolutionstodet(Î(z))=0 andhencesatisfy|z|â¥1+Î´(sincez(cid:54)=1)bytheassumptiononÎ(z). TogetherwehaveshownthatD(L) isinvertible.Thus(16.28)implies (cid:181) (cid:182) HÎ·D(L) â1 â X X 1t =a+e t (16.29) 2t where (cid:181) Âµ (cid:182) a=HÎ·D(1) â1 Î³ . (16.29)isaVARrepresentationfor (cid:161)Î²(cid:48) Y ,Î²(cid:48) âY (cid:162) withallrootssatisfying|z|â¥1+Î´. ThisimpliesaVAR t â¥ t representationforY whichisequation(16.18)with t (cid:181) Î²(cid:48) (cid:182) A(z)=HÎ·D(z) â1 Î²(cid:48) (1âz) . â¥",
    "page": 611,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 592 Bypartitionedmatrixinversionwecalculate (cid:181) Î²(cid:48) (cid:182) A(1)=HÎ·D(1) â1 0 =(cid:163)Î· â¥,Î·(cid:164) (cid:195) (cid:161)Î²(cid:48)Îâ (1 0 )Î· â¥ (cid:162)â1 â(cid:161)Î²(cid:48)Îâ ( (cid:161) 1 Î· )Î· (cid:48)Î· â¥ (cid:162) (cid:162) â â 1 1 /2 Î²(cid:48)Îâ (1)Î· (cid:33) (cid:181) Î² 0 (cid:48) (cid:182) =Î· â¥ (cid:161)Î²(cid:48)Îâ (1)Î· â¥ (cid:162)â1Î²(cid:48) =âÎ±Î²(cid:48) . asclaimed. Part4.Undertheassumption (cid:80)â j=0 (cid:176) (cid:176) (cid:80)â k=0 kÎ j+k (cid:176) (cid:176) 2<â,Theorem15.3impliesthatthecoefficientsA â k = (cid:80)â j=0 A j+k are absolutely summable. We can then apply the Beveridge-Nelson decomposition A(z)= A(1)+A â (z)(1âz).Applying A(1)=âÎ±Î²(cid:48) andalittlerewritingyields A(z)=I (1âz)âÎ±Î²(cid:48) zâ(cid:161) I +Î±Î²(cid:48)âA â (z) (cid:162) (1âz). m m Appliedto(16.18)weobtainthestatedresultwithÎ(L)=I +Î±Î²(cid:48)âA â (z). ThecoefficientsofÎ(L)are m â absolutelysummablebecausethecoefficients A are. k Part5.TheassumptionÎ¸=0directimpliesÎ³=0.Thisimplies (cid:181) Âµ (cid:182) a=HÎ·D(1) â1 0 =(cid:163)Î· â¥,Î·(cid:164) (cid:195) (cid:161)Î²(cid:48)Îâ (1 0 )Î· â¥ (cid:162)â1 â(cid:161)Î²(cid:48)Îâ ( (cid:161) 1 Î· )Î· (cid:48)Î· â¥ (cid:162) (cid:162) â â 1 1 /2 Î²(cid:48)Îâ (1)Î· (cid:33) (cid:181) Âµ 0 (cid:182) =Î· â¥ (cid:161)Î²(cid:48)Îâ (1)Î· â¥ (cid:162)â1Âµ =Î±Âµ asclaimed. â  ProofofTheorem16.18. WritetheVECMasÎâ (L)âY t âÎ±Î²(cid:48) Y tâ1 =a+e t whereÎâ (z)=I m âÎ(z). Set Î±=Î±(cid:161)Î±(cid:48)Î±(cid:162)â1/2 andorthonormalH=(cid:163)Î±,Î± â¥ (cid:164) .Assumethat (cid:163)Î²,Î² â¥ (cid:164) isorthonormal.DefineZ t =Î²(cid:48) Y t and U =Î²(cid:48) âY .Ourgoalistoshowthat(Z ,U )isI(0)whichisthesameasshowingthatY iscointegrated t â¥ t t t t withcointegratingvectorsÎ². (cid:48) PremultiplyingtheVECMmodelbyH wefindthesystem H (cid:48)(cid:161)Îâ (L)âY t âÎ±Î²(cid:48) Y tâ1 (cid:162)=H (cid:48) a+H (cid:48) e t . UsingtheidentityI m =Î²Î²(cid:48)+Î² â¥ Î²(cid:48) â¥ weseethatâY t =Î²âZ t +Î² â¥U t .Makingthissubstitutionandsetting a=H (cid:48) a v =H (cid:48) e weobtainthesystem t t (cid:181) (cid:182) Z D(L) t =a+v t U t where D(z)= (cid:183) Î±(cid:48) Î± Îâ (cid:48) â¥ ( Î z â )Î² (z ( ) 1 Î² â (1 z â )â z) I m Î± Î± (cid:48) â¥ (cid:48)Î Î â â ( ( z z ) ) Î² Î² â¥ â¥ (cid:184) . Wenowshowthatthisisastationarysystem.First,notethat D(1)= (cid:183) âI 0 m Î± Î± (cid:48) â¥ (cid:48)Î Î â â ( ( 1 1 ) ) Î² Î² â¥ â¥ (cid:184)",
    "page": 612,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 593 whichisfullrankundertheassumptionthatÎ±(cid:48) â¥ Îâ (1)Î² â¥ isfullrank. Thatmeansthatdet(D(z))=0has nosolutionsz=1.Second,D(z)relatesto A(z)bytherelationship D(z)=H (cid:48) A(z) (cid:163)Î²,Î² â¥(1âz) (cid:164) . Thusthesolutionsz(cid:54)=1to det(D(z))=det(H)det(A(z))det (cid:161)(cid:163)Î²,Î² â¥(1âz) (cid:164)(cid:162)=0 areallsolutionstodet(A(z))=0,whichallsatisfy|z|â¥1+Î´byassumption. ThusD(z)isinvertiblewith summablemovingaveragecoefficientmatrices.ThisimpliestheVARsystemfor(Z ,U )isstationary. t t Asdiscussedabove,thisshowsthat(Z ,U )isastationaryprocessandhenceY iscointegratedwith t t t cointegratingvectorÎ². â  ProofofTheorem16.19.SetY â =Y âY .Theestimatorsatisfies 2t 2t 2 n (cid:161)Î² (cid:98) âÎ²(cid:162)= (cid:181) n 1 2 t (cid:88) = n 1 Y 2 â t Y 2 â t (cid:48) (cid:182)â1(cid:181) n 1 t (cid:88) = n 1 Y 2 â t u 1t (cid:182) . SetS t =(cid:80) i t =1 u t . Theorems16.4and16.5implyS(cid:98)nr(cid:99) ââB(r)andY 2 â (cid:98)nr(cid:99) ââB 2 â (r). Bythecontinuous d d mappingtheorem 1 (cid:88) n Y â Y â(cid:48)ââ (cid:90) 1 B â B â(cid:48) . n2 t=1 2t 2t d 0 2 2 ByTheorem16.7andtheWLLN 1 (cid:88) n Y â u = 1 (cid:88) n Y â u + 1 (cid:88) n u u +o (1)ââ (cid:90) 1 B â dB +Î +Î£ . n t=1 2t 1t n t=1 2tâ1 1t n t=1 2t 1t p d 0 2 1 21 21 Togetherweobtainthestatedresult. â  ProofofTheorem16.22(sketch).Forsimplicityabstractfromthedynamicandtrendcoefficientssothat theunconstrainedmodelis âY t =Î±Î²(cid:48) Y tâ1 +e t . wheree isaMDSwithcovariancematrixÎ£. Weexaminetwocasesindetail. First,thecase(cid:72)(0)(which t isrelativelystraightforward)andsecondthecase(cid:72)(r)(whichisalgebraicallymoretedious). First,take(cid:72)(0),inwhichcasetheprocessisâY =e .Thestatisticis t t m m LR(0)=ân (cid:88) log (cid:161) 1âÎ» (cid:98)j (cid:162)(cid:39)n (cid:88) Î» (cid:98)j j=1 j=1 =tr (cid:34) (cid:181) n 1 t (cid:88) = n 1 Y tâ1 e t (cid:48) (cid:182)(cid:181) n 1 t (cid:88) = n 1 e t e t (cid:48) (cid:182)â1(cid:181) n 1 t (cid:88) = n 1 e t Y t (cid:48) â1 (cid:182)(cid:181) n 1 2 t (cid:88) = n 1 Y tâ1 Y t (cid:48) â1 (cid:182)â1 (cid:35) (cid:34) (cid:181)(cid:90) 1 (cid:182)(cid:181)(cid:90) 1 (cid:182)â1(cid:181)(cid:90) 1 (cid:182) (cid:35) ââtr dBB (cid:48) BB (cid:48) BdB (cid:48) d 0 0 0 (cid:34) (cid:181)(cid:90) 1 (cid:182)(cid:181)(cid:90) 1 (cid:182)â1(cid:181)(cid:90) 1 (cid:182) (cid:35) =tr dWW (cid:48) WW (cid:48) WdW (cid:48) 0 0 0 whereB(r)isaBrownianmotionwithcovariancematrixÎ£,andW(r)=Î£â1/2B(r)isstandardBrownian motion.Thisisthestatedresult.",
    "page": 613,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 594 Second, take (cid:72)(r) for 1 < r < m. Define Z t = Î²(cid:48) Y t . The process under (cid:72)(r) is âY t = Î±Z tâ1 +e t . NormalizeÎ²sothat(cid:69)(cid:163) Z Z (cid:48)(cid:164)=I .TheteststatisticisinvarianttolineartransformationsofY sowecan t t r t rescalethedatasothat(cid:69)(cid:163)âY âY (cid:48)(cid:164)=I .NoticethatÎ£=(cid:69)(cid:163) e e (cid:48)(cid:164)=(cid:69)(cid:163)âY âY (cid:48)(cid:164)âÎ±(cid:69)(cid:163) Z Z (cid:48)(cid:164)Î±(cid:48)=I âÎ±Î±(cid:48) . t t m t t t t t t m Thelikelihoodratiostatisticis m m LR(r)=ân (cid:88) log (cid:161) 1âÎ» (cid:98)j (cid:162)(cid:39) (cid:88) Ï (cid:98)j j=r+1 j=r+1 whereÏ (cid:98)j =nÎ» (cid:98)j arethemâr smallestrootsoftheequationdet (cid:161) S(Ï) (cid:162)=0where S(Ï)=Ï n 1 2 t (cid:88) = n 1 Y tâ1 Y t (cid:48) â1 â n 1 t (cid:88) = n 1 Y tâ1 âY t (cid:48) (cid:181) n 1 t (cid:88) = n 1 âY t âY t (cid:48) (cid:182)â1 n 1 t (cid:88) = n 1 âY t Y t (cid:48) â1 . Defineafull-rankmatrixH=[Î²,Î² â¥]whereÎ²(cid:48)Î² â¥ =0.TherootsofÏ (cid:98)j arethesameasthoseofdet (cid:161) S â (Ï) (cid:162)= 0whereS â (Ï)=H (cid:48) S(Ï)H,whichreplacesY tâ1 with(Z tâ1 ,X tâ1 )whereX t =Î²(cid:48) â¥ Y t .Wecalculatethat S â (Ï)=Ï (cid:183) n n 1 1 2 2 (cid:80) (cid:80) n t n t = = 1 1 X Z t t â â 1 1 Z Z t t (cid:48) (cid:48) â â 1 1 n n 1 1 2 2 (cid:80) (cid:80) n t n t = = 1 1 X Z t t â â 1 1 X X t t (cid:48) (cid:48) â â 1 1 (cid:184) â (cid:183) n n 1 1 (cid:80) (cid:80) n t n t = = 1 1 X Z t t â â 1 1 â â Y Y t t (cid:48) (cid:48) (cid:184)(cid:181) n 1 t (cid:88) = n 1 âY t âY t (cid:48) (cid:182)â1(cid:183) n n 1 1 (cid:80) (cid:80) n t n t = = 1 1 X Z t t â â 1 1 â â Y Y t t (cid:48) (cid:48) (cid:184)(cid:48) . We now apply the asymptotictheory for non-stationary theory to each component. The process X = t Î²(cid:48) â¥ Y t isnon-stationaryandsatisfiestheFCLTn â1X(cid:98)nr(cid:99) ââX(r)â¼BM (cid:161)Î²(cid:48) â¥ â¦Î² â¥ (cid:162) whereâ¦isthelong-run d covariance matrix of âY . The sum of the errors satisfy n â1/2(cid:80)(cid:98)nr(cid:99) e ââB(r)â¼BM(Î£). The process t t=1 t d X(r)isalinearfunctionofB(r). Wefindthat n 1 2 (cid:80)n t=1 X tâ1 X t (cid:48) â1 â d â(cid:82) 0 1 XX (cid:48) , n 1 2 (cid:80)n t=1 X tâ1 e t â d â(cid:82) 0 1 XdB (cid:48) , n 1(cid:80)n t=1 Z tâ1 Z t (cid:48) â1 â p âI r , n 1(cid:80)n t=1 âY t âY t (cid:48)â p â I m , n 1(cid:80)n t=1 Z tâ1 âY t (cid:48)â p âÎ±(cid:48) , n 1(cid:80)n t=1 X tâ1 Z t (cid:48) â1 â d âÎ¶forsomerandommatrixbyTheorem16.7,and n 1(cid:80)n t=1 X tâ1 âY t (cid:48)â d â Î¶(cid:48)Î±(cid:48)+(cid:82)1 XdB (cid:48) .Togetherwefindthat 0 ï£® (cid:179) (cid:180) ï£¹ S â (Ï)â d âÏ (cid:183) 0 0 (cid:82) 0 1 XX (cid:48) (cid:184) â ï£° (cid:179) Î¶(cid:48)Î±(cid:48)+(cid:82) Î± 0 1 (cid:48)Î± XdB (cid:48) (cid:180) Î± (cid:179) Î¶(cid:48)Î±(cid:48)+(cid:82) Î± 0 1 (cid:48) X Î± d Î¶ B + (cid:48) (cid:82) (cid:180) 0 (cid:179) 1 Î± d Î¶ B + X (cid:82) (cid:48) 0 1 dBX (cid:48) (cid:180) ï£»",
    "page": 614,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Thusdet (cid:161) S â (Ï) (cid:162) convergesindistributiontothedeterminantoftheright-hand-side,whichequals(using TheoremA.1.5)det (cid:161)Î±(cid:48)Î±(cid:162) multipliedbythedeterminantof Ï (cid:90) 1 XX (cid:48)â (cid:181) Î¶(cid:48)Î±(cid:48)+ (cid:90) 1 XdB (cid:48) (cid:182) (cid:179) I âÎ±(cid:161)Î±(cid:48)Î±(cid:162)â1Î±(cid:48) (cid:180) (cid:181) Î±Î¶+ (cid:90) 1 dBX (cid:48) (cid:182) m 0 0 0 (cid:90) 1 (cid:90) 1 (cid:90) 1 =Ï XX (cid:48)â XdB (cid:48) MÎ± dBX (cid:48) 0 0 0 (cid:90) 1 (cid:90) 1 (cid:90) 1 =Ï XX (cid:48)â XdW (cid:48) H (cid:48) H dWX (cid:48) 1 1 0 0 0 (cid:90) 1 (cid:90) 1 (cid:90) 1 =Ï XX (cid:48)â XdW (cid:48) dWX (cid:48) (16.30) 0 0 0 whereMÎ± =I m âÎ±(cid:161)Î±(cid:48)Î±(cid:162)â1Î±(cid:48) and MÎ±B(r)â¼BM (cid:161) MÎ± (cid:161) I m âÎ±Î±(cid:48)(cid:162) MÎ± (cid:162)=BM(MÎ±)=H 1 W(r)",
    "page": 614,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 595 whereMÎ± =H 1 H 1 (cid:48) ,H 1 (cid:48) H 1 =I mâr andW(r)â¼BM(I mâr ). Thedeterminantof(16.30)hasmâr rootsandtheirsumequals (cid:34) (cid:181)(cid:90) 1 (cid:182)(cid:181)(cid:90) 1 (cid:182)â1(cid:181)(cid:90) 1 (cid:182) (cid:35) (cid:34) (cid:181)(cid:90) 1 (cid:182)(cid:181)(cid:90) 1 (cid:182)â1(cid:181)(cid:90) 1 (cid:182) (cid:35) tr dWX (cid:48) XX (cid:48) XdW (cid:48) =tr dWW (cid:48) WW (cid:48) WdW (cid:48) 0 0 0 0 0 0 sinceX(r)isalinearrotationofW(r).Thisisthestatedresult. â  _____________________________________________________________________________________________ 16.23 Exercises Exercise16.1 TakeS t =S tâ1 +e t withS 0 =0ande t i.i.d.(0,Ï2). (a) Calculate(cid:69)[S ]andvar[S ]. t t (cid:112) (b) SetY =(S â(cid:69)[S ])/ var[S ].Byconstruction(cid:69)[Y ]=0andvar[Y ]=1.IsY stationary? t t t t t t t (c) FindtheasymptoticdistributionofY(cid:98)nr(cid:99)forr â[Î´,1]. Exercise16.2 FindtheBeveridge-NelsondecompositionofâY t =e t +Î 1 e tâ1 +Î 2 e tâ2 . Exercise16.3 SupposeY t =X t +u t whereX t =X tâ1 +e t with(e t ,u t )â¼I(0). (a) IsY I(0)orI(1)?. t (b) Findtheasymptoticfunctionaldistributionofn â1/2Y(cid:98)nr(cid:99). Exercise16.4 LetY =e bei.i.d.andX =âY . t t t t (a) ShowthatY isstationaryandI(0). t (b) ShowthatX isstationarybutnotI(0). t Exercise16.5 LetU t =U tâ1 +e t ,Y t =U t +v t andX t =2U t +w t ,where(e t ,v t ,w t )isani.i.d. sequence. Findthecointegratingvectorfor(Y ,X ). t t Exercise16.6 TaketheAR(1)modelY t =Î±Y tâ1 +e t withi.i.d. e t andt (cid:112) heleastsquaresestimatorÎ± (cid:98) . In Chaper14welearnedthattheasymptoticdistributionwhen|Î±|<1is n(Î±âÎ±)ââN (cid:161) 0,1âÎ±2(cid:162) . How (cid:98) d doyoureconcilethiswithTheorem16.9,especiallyforÎ±closetoone? Exercise16.7 TaketheVECM(1)modelâY t =Î±Î²(cid:48) Y tâ1 +e t .ShowthatZ t =Î²(cid:48) Y t followsanAR(1)process. Exercise16.8 AneconomistestimatesthemodelY t =Î±Y tâ1 +e t andfindsÎ± (cid:98) =0.9withs(Î± (cid:98) )=0.05.They assert:âThet-statisticfortestingÎ±=1is2,soÎ±=1isrejected.âIsthereanerrorintheirreasoning? Exercise16.9 An economist estimates the model Y t = Î±Y tâ1 +e t and finds Î± (cid:98) = 0.9 with s(Î± (cid:98) ) = 0.04. Theyassert: âThe95%confidenceintervalforÎ±is[0.82,0.98]whichdoesnotcontain1. SoÎ±=1isnot consistentwiththedata.âIsthereanerrorintheirreasoning? Exercise16.10 AneconomisttakesY ,detrendstoobtainthedetrendedseriesZ ,appliesaADFtestto t t Z andfindsADF=â2.5. Theyassert: âStataprovidesthe5%criticalvalueâ1.9withp-valuelessthan t 1%.Thuswerejectthenullhypothesisofaunitroot.â Isthereanerrorintheirreasoning?",
    "page": 615,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER16. NON-STATIONARYTIMESERIES 596 Exercise16.11 Aneconomistwantstobuildanautoregressivemodelforthenumberofdailytweetsby aprominantpolitician.ForamodelwithanintercepttheyobtainADF=â2.0.TheyassertâThenumber oftweetsisaunitrootprocess.âIsthereanerrorintheirreasoning? Exercise16.12 ForeachofthefollowingmonthlyseriesfromFRED-MDimplementtheDickey-Fullerunit roottest.Foreach,youneedtoconsidertheARorderp andthetrendspecification. (a) logrealpersonalincome:log(rpi) (b) industrialproductionindex:indpro (c) housingstarts:houst (d) help-wantedindex:hwi (e) civilianlaborforce:clf16ov (f) initialclaims:claims (g) industrialproductionindex(fuels):ipfuels Exercise16.13 For each of the series in the previous exercise implement the KPSS test of stationarity. Foreach,youneedconsiderthelagtruncationM andthetrendspecification. Exercise16.14 For each of the following monthly pairs from FRED-MD test the hypothesis of no coin- tegrationusingtheJohansentracetest. Foreach, youneedtoconsidertheVARorder p andthetrend specification. (a) 3-monthtreasuryinterestrate(tb3ms)and10-yeartreasuryinterestrate(gs10). Note: Inthetext weimplementedthetestonthequarterlyseries,notmonthly. (b) interestrateonAAAbonds(aaa)andinterestrateonBAAbonds(baa). (c) log(industrialproductiondurableconsumergoods)andlog(industrialproductionnondurablecon- sumergoods)(logofipdcongdandipncongd).",
    "page": 616,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Chapter 17 Panel Data 17.1 Introduction Economists traditionally use the term paneldata to refer to data structures consisting of observa- tionsonindividualsformultipletimeperiods. Otherfieldssuchasstatisticstypicallycallthisstructure longitudinaldata. Theobservedâindividualsâcanbe,forexample,people,households,workers,firms, schools,productionplants, industries,regions, states,orcountries. Thedistinguishingfeaturerelative tocross-sectionaldatasetsisthepresenceofmultipleobservationsforeachindividual. Morebroadly, paneldatamethodscanbeappliedtoanycontextwithcluster-typedependence. Thereareseveraldistinctadvantagesofpaneldatarelativetocross-sectiondata.Oneisthepossibil- ityofcontrollingforunobservedtime-invariantendogeneitywithouttheuseofinstrumentalvariables. Asecondisthepossibilityofallowingforbroaderformsofheterogeneity. Athirdismodelingdynamic relationshipsandeffects. Therearetwobroadcategoriesofpaneldatasetsineconomicapplications:micropanelsandmacro panels.Micropanelsaretypicallysurveysoradministrativerecordsonindividualsandarecharacterized by a large number of individuals (often in the 1000âs or higher) and a relatively small number of time periods(often2to20years). Macropanelsaretypicallynationalorregionalmacroeconomicvariables andarecharacterizedbyamoderatenumberofindividuals(e.g. 7-20)andamoderatenumberoftime periods(20-60years). Paneldatawasoncerelativelyesotericinappliedeconomicpractice.Now,itisadominantfeatureof appliedresearch. A typical maintained assumption for micro panels (which we follow in this chapter) is that the in- dividualsaremutuallyindependentwhiletheobservationsforagivenindividualarecorrelatedacross time periods. This means that the observations follow a clustered dependence structure. Because of this,currenteconometricpracticeistousecluster-robustcovariancematrixestimatorswhenpossible. Similar assumptions are often used for macro panels though the assumption of independence across individuals(e.g.countries)ismuchlesscompelling. TheapplicationofpaneldatamethodsineconometricsstartedwiththepioneeringworkofMundlak (1961)andBalestraandNerlove(1966). Several excellent monographs and textbooks have been written on panel econometrics, including Arellano(2003),Hsiao(2003),Wooldridge(2010),andBaltagi(2013). Thischapterwillsummarizesome ofthemainthemesbutforamorein-depthtreatmentseethesereferences. One challenge arising in panel data applications is that the computational methods can require meticulous attention to detail. It is therefore advised to use established packages for routine applica- tions.FormostpaneldataapplicationsineconomicsStataisthestandardpackage. 597",
    "page": 617,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 598 17.2 TimeIndexingandUnbalancedPanels Itistypicaltoindexobservationsbyboththeindividuali andthetimeperiodt,thusY denotesa it variableforindividuali inperiod t. Weindexindividualsasi =1,...,N andtimeperiodsas t =1,...T. ThusN isthenumberofindividualsinthepanelandT isthenumberoftimeseriesperiods. Paneldatasetscaninvolvedataatanytimeseriesfrequencythoughthetypicalapplicationinvolves annualdata.Theobservationsinadatasetwillbeindexedbycalendartimewhichforthecaseofannual observationsistheyear. Fornotationalconvenienceitiscustomarytodenotethetimeperiodsas t = 1,...,T,sothatt=1isthefirsttimeperiodobservedandT isthefinaltimeperiod. Whenobservationsareavailableonallindividualsforthesametimeperiodswesaythatthepanel isbalanced. InthiscasethereareanequalnumberT ofobservationsforeachindividualandthetotal numberofobservationsisn=NT. Whendifferenttimeperiodsareavailablefortheindividualsinthesamplewesaythatthepanelis unbalanced.Thisisthemostcommontypeofpaneldataset.Itdoesnotposeaproblemforapplications butdoesmakethenotationcumbersomeandalsocomplicatescomputerprogramming. Toillustrate,considerthedatasetInvest1993onthetextbookwebpage.Thisisasampleof1962U.S. firmsextractedfromCompustat,assembledbyBronwynHall,andusedintheempiricalworkinHalland Hall(1993). InTable17.1wedisplayasetofvariablesfromthedatasetforthefirst13observations. The first variable is the firm code number. The second variable is the year of the observation. These two variablesareessentialforanypaneldataanalysis. InTable17.1youcanseethatthefirstfirm(#32)is observed for the years 1970 through 1977. The second firm (#209) is observed for 1987 through 1991. Youcanseethattheyearsvaryconsiderablyacrossthefirmssothisisanunbalancedpanel. Forunbalancedpanelsthetimeindext=1,...,T denotesthefullsetoftimeperiods.Forexample,in thedatasetInvest1993thereareobservationsfortheyears1960through1991,sothetotalnumberof timeperiodsisT =32.EachindividualisobservedforasubsetofT periods.Thesetoftimeperiodsfor i (cid:80) individuali isdenotedasS i sothatindividual-specificsums(overtimeperiods)arewrittenas tâSi . Theobservedtimeperiodsforagivenindividualaretypicallycontiguous(forexample,inTable17.1, firm#32isobservedforeachyearfrom1970through1977)butinsomecasesarenon-continguous(if,for example,1973wasmissingforfirm#32).Thetotalnumberofobservationsinthesampleisn=(cid:80)N T . i=1 i Table17.1:ObservationsfromInvestmentDataSet FirmCodeNumber Year I I IË Q Q QË e it i it it i it (cid:98)it 32 1970 0.122 0.155 -0.033 1.17 0.62 0.55 . 32 1971 0.092 0.155 -0.063 0.79 0.62 0.17 -0.005 32 1972 0.094 0.155 -0.061 0.91 0.62 0.29 -0.005 32 1973 0.116 0.155 -0.039 0.29 0.62 -0.33 0.014 32 1974 0.099 0.155 -0.057 0.30 0.62 -0.32 -0.002 32 1975 0.187 0.155 0.032 0.56 0.62 -0.06 0.086 32 1976 0.349 0.155 0.194 0.38 0.62 -0.24 0.248 32 1977 0.182 0.155 0.027 0.57 0.62 -0.05 0.081 209 1987 0.095 0.071 0.024 9.06 21.57 -12.51",
    "page": 618,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". 32 1971 0.092 0.155 -0.063 0.79 0.62 0.17 -0.005 32 1972 0.094 0.155 -0.061 0.91 0.62 0.29 -0.005 32 1973 0.116 0.155 -0.039 0.29 0.62 -0.33 0.014 32 1974 0.099 0.155 -0.057 0.30 0.62 -0.32 -0.002 32 1975 0.187 0.155 0.032 0.56 0.62 -0.06 0.086 32 1976 0.349 0.155 0.194 0.38 0.62 -0.24 0.248 32 1977 0.182 0.155 0.027 0.57 0.62 -0.05 0.081 209 1987 0.095 0.071 0.024 9.06 21.57 -12.51 . 209 1988 0.044 0.071 -0.027 16.90 21.57 -4.67 -0.244 209 1989 0.069 0.071 -0.002 25.14 21.57 3.57 -0.257 209 1990 0.113 0.071 0.042 25.60 21.57 4.03 -0.226 209 1991 0.034 0.071 -0.037 31.14 21.57 9.57 -0.283",
    "page": 618,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 599 17.3 Notation Thischapterfocusesonpaneldataregressionmodelswhoseobservationsarepairs(Y ,X )where it it Y isthedependentvariableandX isak-vectorofregressors.Thesearetheobservationsonindividual it it i fortimeperiodt. Itwillbeusefultoclustertheobservationsattheleveloftheindividual.Weborrowthenotationfrom Section4.23towriteY astheT Ã1stackedobservationsonY fortâS ,stackedinchronologicalorder. i i it i Similarly,wewriteX astheT Ãk matrixofstackedX (cid:48) fortâS ,stackedinchronologicalorder. i i it i Wewillalsosometimesusematrixnotationforthefullsample. Todoso,letY =(Y (cid:48) ,...,Y (cid:48) ) (cid:48) denote 1 N thenÃ1vectorofstackedY ,andsetX =(X (cid:48) ,...,X (cid:48) ) (cid:48) similarly. i 1 N 17.4 PooledRegression Thesimplestmodelinpanelregresionispooledregresssion Y =X (cid:48) Î²+e it it it (cid:69)[X e ]=0. (17.1) it it where Î² is a kÃ1 coefficient vector and e is an error. The model can be written at the level of the it individualas Y =X Î²+e i i i (cid:69)(cid:163) X (cid:48) e (cid:164)=0 i i wheree isT Ã1.TheequationforthefullsampleisY =XÎ²+e wheree isnÃ1. i i ThestandardestimatorofÎ²inthepooledregressionmodelisleastsquares,whichcanbewrittenas (cid:195) (cid:33)â1(cid:195) (cid:33) N N Î² (cid:98)pool = (cid:88) (cid:88) X it X i (cid:48) t (cid:88) (cid:88) X it Y it i=1tâSi i=1tâSi (cid:195) (cid:33)â1(cid:195) (cid:33) N N = (cid:88) X (cid:48) X (cid:88) X (cid:48) Y i i i i i=1 i=1 =(cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48) Y (cid:162) . InthecontextofpaneldataÎ² (cid:98)pool iscalledthepooledregressionestimator. Thevectorofresidualsfor theith individualis (cid:98) e i =Y i âX i Î² (cid:98)pool . Thepooledregressionmodelisideallysuitedforthecontextwheretheerrorse satisfystrictmean it independence: (cid:69)[e |X ]=0. (17.2) it i Thisoccurswhentheerrorse aremeanindependentofallregressorsX foralltimeperiods j =1,...,T. it ij Strictmeanindependenceisstrongerthanpairwisemeanindependence(cid:69)[e |X ]=0aswellaspro- it it jection (17.1). Strict mean independence requires that neither lagged nor future values of X help to it forecast e it . It excludes lagged dependent variables (such as Y itâ1 ) from X it (otherwise e it would be predictablegivenX it+1 ).ItalsorequiresthatX it isexogenousinthesensediscussedinChapter12. WenowdescribesomestatisticalpropertiesofÎ² (cid:98)pool under(17.2). First,noticethatbylinearityand thecluster-levelnotationwecanwritetheestimatoras (cid:195) (cid:33)â1(cid:195) (cid:33) (cid:195) (cid:33)â1(cid:195) (cid:33) N N N N Î² (cid:98)pool = (cid:88) X (cid:48) i X i (cid:88) X (cid:48) i (cid:161) X i Î²+e i (cid:162) =Î²+ (cid:88) X (cid:48) i X i (cid:88) X (cid:48) i e i . i=1 i=1 i=1 i=1",
    "page": 619,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 600 Using(17.2) (cid:195) (cid:33)â1(cid:195) (cid:33) N N (cid:69)(cid:163)Î² (cid:98)pool |X (cid:164)=Î²+ (cid:88) X (cid:48) i X i (cid:88) X (cid:48) i (cid:69)[e i |X i ] =Î² i=1 i=1 soÎ² (cid:98)pool isunbiasedforÎ². Under the additional assumption that the error e is serially uncorrelated and homoskedastic the it covarianceestimatortakesaclassicalformandtheclassicalhomoskedasticvarianceestimatorcanbe used.Iftheerrore isheteroskedasticbutseriallyuncorrelatedthenaheteroskedasticity-robustcovari- it ancematrixestimatorcanbeused. In general, however, we expect the errors e to be correlated across time t for a given individual. it Thisdoesnotnecessarilyviolate(17.2)butinvalidatesclassicalcovariancematrixestimation. Thecon- ventionalsolutionistouseacluster-robustcovariancematrixestimatorwhichallowsarbitrarywithin- clusterdependence.Cluster-robustcovariancematrixestimatorsforpooledregressionequal (cid:195) (cid:33) N V(cid:98)pool =(cid:161) X (cid:48) X (cid:162)â1 (cid:88) X (cid:48) i(cid:98) e i(cid:98) e (cid:48) i X i (cid:161) X (cid:48) X (cid:162)â1 . i=1 Asin(4.50)thiscanbemultipliedbyadegree-of-freedomadjustment.TheadjustmentusedbytheStata regresscommandis (cid:195) (cid:33) V(cid:98)pool = (cid:181) n n â â k 1 (cid:182)(cid:181) N N â1 (cid:182) (cid:161) X (cid:48) X (cid:162)â1 (cid:88) N X (cid:48) i(cid:98) e i(cid:98) e (cid:48) i X i (cid:161) X (cid:48) X (cid:162)â1 . i=1 Thepooledregressionestimatorwithcluster-robuststandarderrorscanbeobtainedusingtheStata commandregress cluster(id)whereidindicatestheindividual. Whenstrictmeanindependence(17.2)failsthepooledleastsquaresestimatorÎ² (cid:98)pool isnotnecessarily consistent for Î². Since strict mean independence is a strong and undesirable restriction it is typically preferredtoadoptoneofthealternativeestimatorsdescribedinthefollowingsections. ToillustratethepooledregressionestimatorconsiderthedatasetInvest1993describedearlier.We considerasimpleinvestmentmodel I it =Î² 1 Q itâ1 +Î² 2 D itâ1 +Î² 3 CF itâ1 +Î² 4 T i +e it (17.3) whereIisinvestment/assets,Qismarketvalue/assets,Dislongtermdebt/assets,CF iscashflow/assets, and T is a dummy variable indicating if the corporationâs stock is traded on the NYSE or AMEX. The regressionalsoincludes19dummyvariablesindicatinganindustrycode. TheQ theoryofinvestment suggeststhatÎ² >0whileÎ² =Î² =0. TheoriesofliquidityconstraintssuggestthatÎ² <0andÎ² >0. 1 2 3 2 3 Wewillbeusingthisexamplethroughoutthischapter.ThevaluesofI andQforthefirst13observations arealsodisplayedinTable17.1. In Table 17.2 we present the pooled regression estimates of (17.3) in the first column with cluster- robuststandarderrors. 17.5 One-WayErrorComponentModel Oneapproachtopaneldataregressionistomodelthecorrelationstructureoftheregressionerror e .Themostcommonchoiceisanerror-componentsstructure.Thesimplesttakestheform it e =u +Îµ (17.4) it i it",
    "page": 620,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 601 Table17.2:EstimatesofInvestmentEquation Pooled RandomEffects FixedEffects Two-Way Hausman-Taylor 0.0024 0.0019 0.0017 0.0016 0.0017 Q itâ1 (0.0010) (0.0009) (0.0008) (0.0008) (0.0008) 0.0096 â0.0092 â0.0139 â0.0140 0.0132 D itâ1 (0.0041) (0.0039) (0.0049) (0.0051) (0.0050) 0.0261 0.0412 0.0491 0.0476 0.0408 CF itâ1 (0.0111) (0.0125) (0.0132) (0.0129) (0.0119) â0.0167 â0.0181 â0.0348 T i (0.0024) (0.0028) (0.0048) IndustryDummies Yes Yes No No Yes TimeEffects No No No Yes Yes Cluster-robuststandarderrorsinparenthesis. whereu isanindividual-specificeffectandÎµ areidiosyncratic(i.i.d.) errors. Thisisknownasaone- i it wayerrorcomponentmodel. Invectornotationwecanwritee =1 u +Îµ where1 isaT Ã1vectorof1âs. i i i i i i Theone-wayerrorcomponentregressionmodelis Y =X (cid:48) Î²+u +Îµ it it i it writtenattheleveloftheobservation,orY =X Î²+1 u +Îµ writtenattheleveloftheindividual. i i i i i Toillustratewhyanerror-componentstructuresuchas(17.4)mightbeappropriate,examineTable 17.1. In the final column we have included the pooled regression residuals e for these observations. (cid:98)it (There is no residual for the first year for each firm due to the lack of lagged regressors for this obser- vation.) Whatisquitestrikingisthattheresidualsforthesecondfirm(#209)areallnegative,clustering aroundâ0.25.Whileinformal,thissuggeststhatitmaybeappropriatetomodeltheseerrorsusing(17.4), expectingthatfirm#209hasalargenegativevalueforitsindividualeffectu. 17.6 RandomEffects Therandomeffectsmodelassumesthattheerrorsu andÎµ in(17.4)areconditionallymeanzero, i it uncorrelated,andhomoskedastic. Assumption17.1 RandomEffects.Model(17.4)holdswith (cid:69)[Îµ |X ]=0 (17.5) it i (cid:69)(cid:163)Îµ2 it |X i (cid:164)=Ï2 Îµ (17.6) (cid:69)(cid:163)Îµ Îµ |X (cid:164)=0 (17.7) it js i (cid:69)[u |X ]=0 (17.8) i i (cid:69)(cid:163) u2|X (cid:164)=Ï2 (17.9) i i u (cid:69)[u Îµ |X ]=0 (17.10) i it i where(17.7)holdsforalls(cid:54)=t.",
    "page": 621,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 602 Assumption17.1isknownasarandomeffectsspecification.Itimpliesthatthevectoroferrorse for i individuali hasthecovariancestructure (cid:69)[e |X ]=0 i i (cid:69)(cid:163) e i e (cid:48) i |X i (cid:164)=1 i 1 (cid:48) i Ï2 u +I i Ï2 Îµ ï£« ï£¶ Ï2+Ï2 Ï2 Â·Â·Â· Ï2 u Îµ u u ï£¬ Ï2 Ï2+Ï2 Â·Â·Â· Ï2 ï£· =ï£¬ u u Îµ u ï£· ï£¬ ï£¬ . . . . . . ... . . . ï£· ï£· ï£­ ï£¸ Ï2 Ï2 Â·Â·Â· Ï2+Ï2 u u u Îµ =Ï2 Îµ â¦ i , say, where I is an identity matrix of dimension T . The matrix â¦ depends on i since its dimension i i i dependsonthenumberofobservedtimeperiodsT . i Assumptions17.1.1and17.1.4statethattheidiosyncraticerrorÎµ andindividual-specificerroru it i arestrictlymeanindependentsothecombinederrore isstrictlymeanindependentaswell. it Therandomeffectsmodelisequivalenttoanequi-correlationmodel.Thatis,supposethattheerror e satisfies it (cid:69)[e |X ]=0 it i (cid:69)(cid:163) e2 |X (cid:164)=Ï2 it i and (cid:69)[e e |X ]=ÏÏ2 is it i for s (cid:54)=t. These conditions imply that e can be written as (17.4) with the components satisfying As- it sumption17.1withÏ2 u =ÏÏ2andÏ2 Îµ =(1âÏ)Ï2.Thusrandomeffectsandequi-correlationareidentical. Therandomeffectsregressionmodelis Y =X (cid:48) Î²+u +Îµ it it i it orY =X Î²+1 u +Îµ wheretheerrorssatisfyAssumption17.1. i i i i i GiventheerrorstructurethenaturalestimatorforÎ²isGLS.SupposeÏ2 u andÏ2 Îµareknown. TheGLS estimatorofÎ²is (cid:195) (cid:33)â1(cid:195) (cid:33) N N Î² (cid:98)gls = (cid:88) X (cid:48) i â¦â i 1X i (cid:88) X (cid:48) i â¦â i 1Y i . i=1 i=1 AfeasibleGLSestimatorreplacestheunknownÏ2 u andÏ2 Îµwithestimators.SeeSection17.15. WenowdescribesomestatisticalpropertiesoftheestimatorunderAssumption17.1.Bylinearity (cid:195) (cid:33)â1(cid:195) (cid:33) N N Î² (cid:98)gls âÎ²= (cid:88) X (cid:48) i â¦â i 1X i (cid:88) X (cid:48) i â¦â i 1e i . i=1 i=1 Thus (cid:195) (cid:33)â1(cid:195) (cid:33) N N (cid:69)(cid:163)Î² (cid:98)gls âÎ²|X (cid:164)= (cid:88) X (cid:48) i â¦â i 1X i (cid:88) X (cid:48) i â¦â i 1(cid:69)[e i |X i ] =0. i=1 i=1 ThusÎ² (cid:98)gls isconditionallyunbiasedforÎ².TheconditionalvarianceofÎ² (cid:98)gls is (cid:195) (cid:33)â1 n V = (cid:88) X (cid:48)â¦â1X . (17.11) gls i i i i=1",
    "page": 622,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 603 Now letâs compare Î² (cid:98)gls with the pooled estimator Î² (cid:98)pool . Under Assumption 17.1 the latter is also conditionallyunbiasedforÎ²andhasconditionalvariance (cid:195) (cid:33)â1(cid:195) (cid:33)â1(cid:195) (cid:33)â1 n n n V = (cid:88) X (cid:48) X (cid:88) X (cid:48)â¦ X (cid:88) X (cid:48) X . (17.12) pool i i i i i i i i=1 i=1 i=1 UsingthealgebraoftheGauss-MarkovTheoremwededucethat V â¤V (17.13) gls pool andthustherandomeffectsestimatorÎ² (cid:98)gls ismoreefficientthanthepooledestimatorÎ² (cid:98)pool underAs- sumption17.1. (SeeExercise17.1.) Thetwovariancematricesareidenticalwhenthereisnoindividual- specificeffect(whenÏ2 u =0)forthenV gls =V pool =(cid:161) X (cid:48) X (cid:162)â1Ï2 Îµ. Undertheassumptionthattherandomeffectsmodelisausefulapproximationbutnotliterallytrue thenwemayconsideracluster-robustcovariancematrixestimatorsuchas (cid:195) (cid:33)â1(cid:195) (cid:33)(cid:195) (cid:33)â1 N N n V(cid:98)gls = (cid:88) X (cid:48) i â¦â i 1X i (cid:88) X (cid:48) i â¦â i 1 (cid:98) e i(cid:98) e (cid:48) i â¦â i 1X i (cid:88) X (cid:48) i â¦â i 1X i (17.14) i=1 i=1 i=1 where (cid:98) e i =Y i âX i Î² (cid:98)gls .Thismaybere-scaledbyadegreeoffreedomadjustmentifdesired. The random effects estimator Î² (cid:98)gls can be obtained using the Stata command xtreg. The default covariancematrixestimatoris(17.11).Forthecluster-robustcovariancematrixestimator(17.14)usethe commandxtreg vce(robust).(Thextsetcommandmustbeusedfirsttodeclarethegroupidentifier. Forexample,cusipisthegroupidentifierinTable17.1.) Toillustrate,inthesecondcolumnofTable17.2wepresenttherandomeffectregressionestimates oftheinvestmentmodel(17.3)withcluster-robuststandarderrors(17.14). Thepointestimatesarerea- sonablydifferentfromthepooledregressionestimator. Thecoefficientondebtswitchesfrompositive tonegative(thelatterconsistentwiththeoriesofliquidityconstraints)andthecoefficientoncashflow increasessignificantlyinmagnitude. Thesechangesappeartobegreaterinmagnitudethanwouldbe expectedifAssumption17.1werecorrect.Inthenextsectionweconsideralessrestrictivespecification. 17.7 FixedEffectModel Considertheone-wayerrorcomponentregressionmodel Y =X (cid:48) Î²+u +Îµ (17.15) it it i it or Y =X Î²+1 u +Îµ . (17.16) i i i i i Inmanyapplicationsitisusefultointerprettheindividual-specificeffectu asatime-invariantunob- i servedmissingvariable.Forexample,inawageregressionu maybetheunobservedabilityofindividual i i.Intheinvestmentmodel(17.3)u maybeafirm-specificproductivityfactor. i Whenu isinterpretedasanomittedvariableitisnaturaltoexpectittobecorrelatedwiththere- i gressorsX .ThisisespeciallythecasewhenX includeschoicevariables. it it Toillustrate,considertheentriesinTable17.1.Thefinalcolumndisplaysthepooledregressionresid- ualse forthefirst13observationswhichweinterpretasestimatesoftheerrore =u +Îµ .Asdescribed (cid:98)it it i it before,whatisparticularlystrikingabouttheresidualsisthattheyareallstronglynegativeforfirm#209, clusteringaroundâ0.25",
    "page": 623,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". i Whenu isinterpretedasanomittedvariableitisnaturaltoexpectittobecorrelatedwiththere- i gressorsX .ThisisespeciallythecasewhenX includeschoicevariables. it it Toillustrate,considertheentriesinTable17.1.Thefinalcolumndisplaysthepooledregressionresid- ualse forthefirst13observationswhichweinterpretasestimatesoftheerrore =u +Îµ .Asdescribed (cid:98)it it i it before,whatisparticularlystrikingabouttheresidualsisthattheyareallstronglynegativeforfirm#209, clusteringaroundâ0.25. Wecaninterpretthisasanestimateofu forthisfirm. Examiningthevalues i",
    "page": 623,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 604 oftheregressorQ forthetwofirmswecanseethatfirm#209hasverylargevalues(inalltimeperiods) forQ. (TheaveragevalueQ forthetwofirmsappearsintheseventhcolumn.) Thusitappears(though i weareonlylookingattwoobservations)thatu andQ arecorrelated. Itisnotreasonabletoinfertoo i it much from these limited observations, but the relevance is that such correlation violates strict mean independence. In the econometrics literature if the stochastic structure of u is treated as unknown and possibly i correlatedwithX thenu iscalledafixedeffect. it i Correlationbetweenu and X willcausebothpooledandrandomeffectestimatorstobebiased. i it Thisisduetotheclassicproblemsofomittedvariablesbiasandendogeneity. Toseethisinagenerated example view Figure 17.1. This shows a scatter plot of three observations (Y ,X ) from three firms. it it The true model is Y =9âX +u . (The true slope coefficient is â1.) The variables u and X are it it i i it highlycorrelatedsothefittedpooledregressionlinethroughthenineobservationshasaslopecloseto +1. (Therandomeffectsestimatorisidentical.) TheapparentpositiverelationshipbetweenY andX is drivenentirelybythepositivecorrelationbetweenX andu. Conditionalonu,however,theslopeisâ1. Thusregressiontechniqueswhichdonotcontrolforu willproducebiasedandinconsistentestimators. i x y l l l 0 5 10 15 51 01 5 0 Firm 1 l Firm 2 Firm 3 Least Squares Fit True Slope Figure17.1:ScatterPlotandPooledRegressionLine Thepresenceoftheunstructuredindividualeffectu meansthatitisnotpossibletoidentifyÎ²under i asimpleprojectionassumptionsuchas(cid:69)[X Îµ ]=0. Itturnsoutthatasufficientconditionforidentifi- it t cationisthefollowing. Definition17.1 TheregressorX isstrictlyexogenousfortheerrorÎµ if it it (cid:69)[X Îµ ]=0 (17.17) is it foralls=1,...,T. Strictexogeneityisastrongprojectioncondition,meaningthatifX foranys(cid:54)=t isaddedto(17.15) is",
    "page": 624,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 605 itwillhaveazerocoefficient.Strictexogeneityisaprojectionanalogofstrictmeanindependence (cid:69)[Îµ |X ]=0. (17.18) it i (17.18) implies (17.17) but not conversely. While (17.17) is sufficient for identification and asymptotic theorywewillalsousethestrongercondition(17.18)forfinitesampleanalysis. While(17.17)and(17.18)arestrongassumptionstheyaremuchweakerthan(17.2)orAssumption 17.1, which require that the individual effect u is also strictly mean independent. In contrast, (17.17) i and(17.18)makenoassumptionsaboutu . i Strictexogeneity(17.17)istypicallyinappropriateindynamicmodels. InSection17.41wediscuss estimationundertheweakerassumptionofpredeterminedregressors. 17.8 WithinTransformation Intheprevioussectionweshowedthatifu andX arecorrelatedthenpooledandrandom-effects i it estimatorswillbebiasedandinconsistent.Ifweleavetherelationshipbetweenu andX fullyunstruc- i it turedthentheonlywaytoconsistentlyestimatethecoefficientÎ²isbyanestimatorwhichisinvariantto u .Thiscanbeachievedbytransformationswhicheliminateu . i i Onesuchtransformationisthewithintransformation. Inthissectionwedescribethistransforma- tionindetail. Definethemeanofavariableforagivenindividualas 1 (cid:88) Y = Y . i it T i tâSi Wecallthistheindividual-specificmeansinceitisthemeanofagivenindividual. Contrarywise,some authorscallthisthetime-averageortime-meansinceitistheaverageoverthetimeperiods. Subtractingtheindividual-specificmeanfromthevariableweobtainthedeviations YË =Y âY . it it i Thisisknownasthewithintransformation. WealsorefertoYË asthedemeanedvaluesordeviations it fromindividualmeans.SomeauthorsrefertoYË asdeviationsfromtimemeans.Whatisimportantis it thatthedemeaninghasoccuredattheindividuallevel. Somealgebramayalsobeuseful. Wecanwritetheindividual-specificmeanasY =(cid:161) 1 (cid:48) 1 (cid:162)â1 1 (cid:48) Y . i i i i i Stackingtheobservationsforindividuali wecanwritethewithintransformationusingthenotation YË =Y â1 Y i i i i =Y â1 (cid:161) 1 (cid:48) 1 (cid:162)â1 1 (cid:48) Y i i i i i i =M Y i i whereM =I â1 (cid:161) 1 (cid:48) 1 (cid:162)â1 1 (cid:48) istheindividual-specificdemeaningoperator. NoticethatM isanidem- i i i i i i i potentmatrix. Similarlyfortheregressorswedefinetheindividual-specificmeansanddemeanedvalues: 1 (cid:88) X = X i it T i tâSi XË =X âX it it i XË =M X . i i i",
    "page": 625,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 606 WeillustratedemeaninginTable17.1.Inthefourthandseventhcolumnswedisplaythefirm-specific meansI andQ andinthefifthandeighthcolumnsthedemeanedvaluesIË andQË . i i it it We can also define the full-sample within operator. Define D = diag (cid:169) 1 ,...,1 (cid:170) and M = I â T1 TN D n D (cid:161) D (cid:48) D (cid:162)â1 D (cid:48) .NotethatM =diag{M ,...,M }.Thus D 1 N ï£« YË ï£¶ ï£« XË ï£¶ 1 1 M D Y =YË =ï£¬ ï£­ . . . ï£· ï£¸ , M D X =XË =ï£¬ ï£­ . . . ï£· ï£¸ . (17.19) YË XË N N Nowapplytheseoperationstoequation(17.15).Takingindividual-specificaveragesweobtain (cid:48) Y =X Î²+u +Îµ (17.20) i i i i whereÎµ i = T 1 i (cid:80) tâSi Îµ it .Subtractingfrom(17.15)weobtain YË =XË(cid:48) Î²+ÎµË (17.21) it it it whereÎµË =Îµ âÎµ .Theindividualeffectu hasbeeneliminated! it it it i Wecanalternativelywritethisinvectornotation.ApplyingthedemeaningoperatorM to(17.16)we i obtain YË =XË Î²+ÎµË . (17.22) i i i Theindividual-effectu iseliminatedsinceM 1 =0.Equation(17.22)isavectorversionof(17.21). i i i The equation (17.21) is a linear equation in the transformed (demeaned) variables. As desired the individualeffectu hasbeeneliminated. Consequentlyestimatorsconstructedfrom(17.21)(orequiva- i lently(17.22))willbeinvarianttothevaluesofu .Thismeansthatthetheendogeneitybiasdescribedin i theprevioussectionwillbeeliminated. Another consequence, however, is that all time-invariant regressors are also eliminated. That is, if theoriginalmodel(17.15)hadincludedanyregressors X =X whichareconstantovertimeforeach it i individual then for these regressors the demeaned values are identically 0. What this means is that if equation(17.21)isusedtoestimateÎ²itwillbeimpossibletoestimate(oridentify)acoefficientonany regressorwhichistimeinvariant.Thisisnotaconsequenceoftheestimationmethodbutratheraconse- quenceofthemodelassumptions.Inotherwords,iftheindividualeffectu hasnoknownstructurethen i itisimpossibletodisentangletheeffectofanytime-invariantregressorX .Thetwohaveobservationally i equivalenteffectsandcannotbeseparatelyidentified. The within transformation can greatly reduce the variance of the regressors. This can be seen in Table17.1whereyoucanseethatthevariationbetweentheelementsofthetransformedvariables IË it andQË islessthanthatoftheuntransformedvariablessincemuchofthevariationiscapturedbythe it firm-specificmeans. Itisnottypicallyneededtodirectlyprogramthewithintransformation,butifitisdesiredthefollow- ingStatacommandseasilydoso. StataCommandsforWithinTransformation * xistheoriginalvariable * idisthegroupidentifier * xdotisthewithin-transformedvariable egenxmean=mean(x),by(id) genxdot=x-xmean",
    "page": 626,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 607 17.9 FixedEffectsEstimator Considerleastsquaresappliedtothedemeanedequation(17.21)orequivalently(17.22).Thisis (cid:195) (cid:33)â1(cid:195) (cid:33) N N Î² (cid:98)fe = (cid:88) (cid:88) XË it XË i (cid:48) t (cid:88) (cid:88) XË it YË it i=1tâSi i=1tâSi (cid:195) (cid:33)â1(cid:195) (cid:33) N N (cid:88) (cid:48) (cid:88) (cid:48) = XË XË XË YË i i i i i=1 i=1 (cid:195) (cid:33)â1(cid:195) (cid:33) N N = (cid:88) X (cid:48) M X (cid:88) X (cid:48) M Y . i i i i i i i=1 i=1 Thisisknownasthefixed-effectsorwithinestimatorofÎ².Itiscalledthefixed-effectsestimatorbecause itisappropriateforthefixedeffectsmodel(17.15).Itiscalledthewithinestimatorbecauseitisbasedon thevariationofthedatawithineachindividual. Theabovedefinitionimplicitlyassumesthatthematrix (cid:80)N XË (cid:48) XË isfullrank. Thisrequiresthatall i=1 i i componentsofX havetimevariationforatleastsomeindividualsinthesample. it Thefixedeffectsresidualsare Îµ (cid:98)it =YË it âXË i (cid:48) t Î² (cid:98)fe Îµ (cid:98)i =YË i âXË i Î² (cid:98)fe . (17.23) Letusdescribesomeofthestatisticalpropertiesoftheestimatorunderstrictmeanindependence (17.18).BylinearityandthefactM 1 =0,wecanwrite i i (cid:195) (cid:33)â1(cid:195) (cid:33) N N Î² (cid:98)fe âÎ²= (cid:88) X (cid:48) i M i X i (cid:88) X (cid:48) i M i Îµ i . i=1 i=1 Then(17.18)implies (cid:195) (cid:33)â1(cid:195) (cid:33) N N (cid:69)(cid:163)Î² (cid:98)fe âÎ²|X (cid:164)= (cid:88) X (cid:48) i M i X i (cid:88) X (cid:48) i M i (cid:69)[Îµ i |X i ] =0. i=1 i=1 ThusÎ² (cid:98)fe isunbiasedforÎ²under(17.18). LetÎ£ =(cid:69)(cid:163)Îµ Îµ(cid:48) |X (cid:164) denotetheT ÃT conditionalcovariancematrixoftheidiosyncraticerrors.The i i i i i i varianceofÎ² (cid:98)fe is (cid:195) (cid:33)â1(cid:195) (cid:33)(cid:195) (cid:33)â1 N N N V fe =var (cid:163)Î² (cid:98)fe |X (cid:164)= (cid:88) XË (cid:48) i XË i (cid:88) XË (cid:48) i Î£ i XË i (cid:88) XË (cid:48) i XË i . (17.24) i=1 i=1 i=1 Thisexpressionsimplifieswhentheidiosyncraticerrorsarehomoskedasticandseriallyuncorrelated: (cid:69)(cid:163)Îµ2 it |X i (cid:164)=Ï2 Îµ (17.25) (cid:69)(cid:163)Îµ Îµ |X (cid:164)=0 (17.26) ij it i forall j (cid:54)=t.Inthiscase,Î£ i =I i Ï2 Îµand(17.24)simplifiesto (cid:195) (cid:33)â1 N V0 fe =Ï2 Îµ (cid:88) XË (cid:48) i XË i . (17.27) i=1",
    "page": 627,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 608 It is instructive to compare the variances of the fixed-effects and pooled estimators under (17.25)- (17.26)andtheassumptionthatthereisnoindividual-specificeffectu =0.Inthiscaseweseethat i (cid:195) (cid:33)â1 (cid:195) (cid:33)â1 N N V0 fe =Ï2 Îµ (cid:88) XË (cid:48) i XË i â¥Ï2 Îµ (cid:88) X (cid:48) i X i =V pool . (17.28) i=1 i=1 The inequality holds since the demeaned variables XË have reduced variation relative to the original i observations X . (SeeExercise17.28.) Thisshowsthecostofusingfixedeffectsrelativetopooledesti- i mation.Theestimationvarianceincreasesduetoreducedvariationintheregressors. Thisreductionin efficiencyisanecessaryby-productoftherobustnessoftheestimatortotheindividualeffectsu . i 17.10 DifferencedEstimator The within transformation is not the only transformation which eliminates the individual-specific effect.Anotherimportanttransformationwhichdoesthesameisfirst-differencing. The first-differencing transformation is âY it =Y it âY itâ1 . This can be applied to all but the first observation(whichisessentiallylost). AttheleveloftheindividualthiscanbewrittenasâY =D Y i i i whereD isthe(T â1)ÃT matrixdifferencingoperator i i i ï£® ï£¹ â1 1 0 Â·Â·Â· 0 0 ï£¯ 0 â1 1 0 0 ï£º D i =ï£¯ ï£¯ ï£¯ . . . ... . . . ï£º ï£º ï£º . ï£° ï£» 0 0 0 Â·Â·Â· â1 1 Applyingthetransformationâto(17.15)or(17.16)weobtainâY =âX (cid:48) Î²+âÎµ or it it it âY =âX Î²+âÎµ . (17.29) i i i Leastsquaresappliedtothedifferencedequationis (cid:195) (cid:33)â1(cid:195) (cid:33) N N Î² (cid:98)â = (cid:88)(cid:88) âX it âX i (cid:48) t (cid:88)(cid:88) âX it âY it i=1tâ¥2 i=1tâ¥2 (cid:195) (cid:33)â1(cid:195) (cid:33) N N = (cid:88) âX (cid:48)âX (cid:88) âX (cid:48)âY i i i i i=1 i=1 (cid:195) (cid:33)â1(cid:195) (cid:33) N N = (cid:88) X (cid:48) D (cid:48) D X (cid:88) X (cid:48) D (cid:48) D Y . (17.30) i i i i i i i i i=1 i=1 (17.30) is called the differencedestimator. For T =2, Î² (cid:98)â =Î² (cid:98)fe equals the fixed effects estimator. See Exercise17.6.Theydiffer,however,forT >2. WhentheerrorsÎµ areseriallyuncorrelatedandhomoskedasticthentheerrorâÎµ =D Îµ in(17.29) it i i i hascovariancematrixHÏ2 Îµwhere ï£« ï£¶ 2 â1 0 0 H=D D (cid:48) = ï£¬ ï£¬ ï£¬ â1 2 ... 0 ï£· ï£· ï£·. (17.31) i i ï£¬ ï£¬ 0 ... ... â1 ï£· ï£· ï£­ ï£¸ 0 0 â1 2",
    "page": 628,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 609 WecanreduceestimationvariancebyusingGLS,whichis (cid:195) (cid:33)â1(cid:195) (cid:33) N N Î² (cid:98)â = (cid:88) âX (cid:48) i H â1âX i (cid:88) âX (cid:48) i H â1âY i i=1 i=1 (cid:195) (cid:33)â1(cid:195) (cid:33) N N = (cid:88) X (cid:48) D (cid:48)(cid:161) D D (cid:48)(cid:162)â1 D X (cid:88) X (cid:48) D (cid:48)(cid:161) D D (cid:48)(cid:162)â1 D Y i i i i i i i i i i i i i=1 i=1 (cid:195) (cid:33)â1(cid:195) (cid:33) N N = (cid:88) X (cid:48) M X (cid:88) X (cid:48) M Y i i i i i i i=1 i=1 whereM =D (cid:48)(cid:161) D D (cid:48)(cid:162)â1 D .Recall,thematrixD is(T â1)ÃT withrankT â1andisorthogonaltothe i i i i i i i i i vectorofones1 . ThismeansM projectsorthogonallyto1 andthusequalsthewithintransformation i i i matrix.HenceÎ² (cid:98)â =Î² (cid:98)fe ,thefixedeffectsestimator! WhatwehaveshownisthatGLSappliedtothefirst-differencedequationpreciselyequalsthefixed effectsestimator.SincetheGauss-MarkovtheoremshowsthatGLShaslowervariancethanleastsquares, thismeansthatthefixedeffectsestimatorismoreefficientthanfirstdifferencingundertheassumption thatÎµ isi.i.d. it Thisargumentextendstoanyothertransformationwhicheliminatesthefixedeffect. GLSapplied aftersuchatransformationisequaltothefixedeffectsestimatorandismoreefficientthanleastsquares appliedafterthesametransformation. ThisshowsthatthefixedeffectsestimatorisGauss-Markoveffi- cientintheclassofestimatorswhicheliminatethefixedeffect. 17.11 DummyVariablesRegression AnalternativewaytoestimatethefixedeffectsmodelisbyleastsquaresofY onX andafullsetof it it dummyvariables,oneforeachindividualinthesample. Itturnsoutthatthisisalgebraicallyequivalent tothewithinestimator. Toseethisstartwiththeerror-componentmodelwithoutaregressor: Y =u +Îµ . (17.32) it i it Consider least squares estimation of the vector of fixed effects u =(u ,...,u ) (cid:48) . Since each fixed effect 1 N u isanindividual-specificmeanandtheleastsquaresestimateoftheinterceptisthesamplemeanit i followsthattheleastsquaresestimateofu isu =Y . TheleastsquaresresidualisthenÎµ =Y âY = i (cid:98)i i (cid:98)it it i YË ,thewithintransformation. it If you would prefer an algebraic argument, let d be a vector of N dummy variables where the ith i element indicates the ith individual. Thus the ith element of d is 1 and the remaining elements are i zero. Noticethatu =d (cid:48) u and(17.32)equalsY =d (cid:48) u+Îµ . Thisisaregressionwiththeregressorsd i i it i it i andcoefficientsu.WecanalsowritethisinvectornotationattheleveloftheindividualasY =1 d (cid:48) u+Îµ i i i i orusingfullmatrixnotationasY =Du+ÎµwhereD=diag (cid:169) 1 ,...,1 (cid:170) . T1 TN Theleastsquaresestimateofuis u=(cid:161) D (cid:48) D (cid:162)â1(cid:161) D (cid:48) Y (cid:162) (cid:98) =diag (cid:161) 1 (cid:48) 1 (cid:162)â1 vec (cid:161) 1 (cid:48) Y (cid:162) i i i i =vec (cid:179) (cid:161) 1 (cid:48) 1 (cid:162)â1 1 (cid:48) Y (cid:180) i i i i (cid:179) (cid:180) =vec Y . i",
    "page": 629,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 610 Theleastsquaresresidualsare Îµ= (cid:179) I âD (cid:161) D (cid:48) D (cid:162)â1 D (cid:48) (cid:180) Y =YË (cid:98) n as shown in (17.19). Thus the least squares residuals from the simple error-component model are the withintransformedvariables. Nowconsidertheerror-componentmodelwithregressors,whichcanbewrittenas Y =X (cid:48) Î²+d (cid:48) u+Îµ (17.33) it it i it sinceu =d (cid:48) uasdiscussedabove.Inmatrixnotation i i Y =XÎ²+Du+Îµ. (17.34) Weconsiderestimationof(Î²,u)byleastsquaresandwritetheestimatesasY =XÎ² (cid:98) +Du (cid:98) +Îµ (cid:98) .Wecall thisthedummyvariableestimatorofthefixedeffectsmodel. BytheFrisch-Waugh-LovellTheorem(Theorem3.5)thedummyvariableestimatorÎ² (cid:98)andresiduals ÎµmaybeobtainedbytheleastsquaresregressionoftheresidualsfromtheregressionofY onD onthe (cid:98) residualsfromtheregressionof X onD. WelearnedabovethattheresidualsfromtheregressiononD arethewithintransformations. ThusthedummyvariableestimatorÎ² (cid:98)andresidualsÎµ (cid:98) maybeobtained fromleastsquaresregressionofthewithintransformedYË onthewithintransformed XË. Thisisexactly thefixedeffectsestimatorÎ² (cid:98)fe .ThusthedummyvariableandfixedeffectsestimatorsofÎ²areidentical. Thisissufficientlyimportantthatwestatethisresultasatheorem. Theorem17.1 ThefixedeffectsestimatorofÎ²algebraicallyequalsthedummy variableestimatorofÎ².Thetwoestimatorshavethesameresiduals. ThismaybethemostimportantpracticalapplicationoftheFrisch-Waugh-LovellTheorem.Itshows that we can estimate the coefficients either by applying the within transformation or by inclusion of dummyvariables(oneforeachindividualinthesample). Thisisimportantbecauseinsomecasesone approachismoreconvenientthantheotheranditisimportanttoknowthatthetwomethodsarealge- braicallyequivalent. WhenN islargeitisadvisabletousethewithintransformationratherthanthedummyvariableap- proach. Thisisbecausethelatterrequiresconsiderablymorecomputermemory. Toseethisconsider thematrixD in(17.34)inthebalancedcase. IthasTN2 elementswhichmustbecreatedandstoredin memory. When N is large thiscanbeexcessive. Forexample, ifT =10 and N =10,000, the matrixD hasonebillionelements! Whetherornotapackagecantechnicallyhandleamatrixofthisdimension dependsonseveralparticulars(systemRAM,operatingsystem,packageversion),butevenifitcanex- ecutethecalculationthecomputationtimeisslow. HenceforfixedeffectsestimationwithlargeN itis recommendedtousethewithintransformationratherthandummyvariableregression. The dummy variable formulation may add insight about how the fixed effects estimator achieves invariancetothefixedeffects",
    "page": 630,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". HenceforfixedeffectsestimationwithlargeN itis recommendedtousethewithintransformationratherthandummyvariableregression. The dummy variable formulation may add insight about how the fixed effects estimator achieves invariancetothefixedeffects. Giventheregressionequation(17.34)wecanwritetheleastsquaresesti- matorofÎ²usingtheresidualregressionformula: Î² (cid:98)fe =(cid:161) X (cid:48) M D X (cid:162)â1(cid:161) X (cid:48) M D Y (cid:162) =(cid:161) X (cid:48) M X (cid:162)â1(cid:161) X (cid:48) M (cid:161) XÎ²+Du+Îµ(cid:162)(cid:162) D D =Î²+(cid:161) X (cid:48) M X (cid:162)â1(cid:161) X (cid:48) M Îµ(cid:162) (17.35) D D sinceM D D=0.Theexpression(17.35)isfreeofthevectoruandthusÎ² (cid:98)fe isinvarianttou.Thisisanother demonstrationthatthefixedeffectsestimatorisinvarianttotheactualvaluesofthefixedeffects, and thusitsstatisticalpropertiesdonotrelyonassumptionsaboutu . i",
    "page": 630,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 611 17.12 FixedEffectsCovarianceMatrixEstimation FirstconsiderestimationoftheclassicalcovariancematrixV0 asdefinedin(17.27).Thisis fe V(cid:98) 0 fe =Ï (cid:98) 2 Îµ (cid:179) XË (cid:48) XË (cid:180)â1 (17.36) with Ï (cid:98) 2 Îµ = nâN 1 âk (cid:88) n (cid:88) Îµ (cid:98) 2 it = nâN 1 âk (cid:88) n Îµ (cid:98) (cid:48) i Îµ (cid:98)i . (17.37) i=1tâSi i=1 The N+k degreeoffreedomadjustmentismotivatedbythedummyvariablerepresentation. Youcan verifythatÏ (cid:98) 2 ÎµisunbiasedforÏ2 Îµunderassumptions(17.18),(17.25)and(17.26).SeeExercise17.8. Noticethattheassumptions(17.18),(17.25),and(17.26)areidenticalto(17.5)-(17.7)ofAssumption 17.1. Theassumptions(17.8)-(17.10)arenotneeded. Thusthefixedeffectmodelweakenstherandom effectsmodelbyeliminatingtheassumptionsonu butretainingthoseonÎµ . i it Theclassicalcovariancematrixestimator(17.36)forthefixedeffectsestimatorisvalidwhentheer- rorsÎµ arehomoskedasticandseriallyuncorrelatedbutisinvalidotherwise.Acovariancematrixestima- it torwhichallowsÎµ tobeheteroskedasticandseriallycorrelatedacrosst isthecluster-robustcovariance it matrixestimator,clusteredbyindividual (cid:195) (cid:33) V(cid:98) c fe luster= (cid:179) XË (cid:48) XË (cid:180)â1 (cid:88) N XË (cid:48) i Îµ (cid:98)i Îµ (cid:98) (cid:48) i XË i (cid:179) XË (cid:48) XË (cid:180)â1 (17.38) i=1 whereÎµ asthefixedeffectsresidualsasdefinedin(17.23).(17.38)wasfirstproposedbyArellano(1987). (cid:98)i cluster Asin(4.50)V(cid:98)fe canbemultipliedbyadegree-of-freedomadjustment.Theadjustmentrecommended bythetheoryofC.Hansen(2007)is (cid:195) (cid:33) V(cid:98) c fe luster= (cid:181) N N â1 (cid:182) (cid:179) XË (cid:48) XË (cid:180)â1 (cid:88) N XË (cid:48) i Îµ (cid:98)i Îµ (cid:98) (cid:48) i XË i (cid:179) XË (cid:48) XË (cid:180)â1 (17.39) i=1 andthatcorrespondingto(4.50)is (cid:195) (cid:33) V(cid:98) c fe luster= (cid:181) nâ n N â1 âk (cid:182)(cid:181) N N â1 (cid:182) (cid:179) XË (cid:48) XË (cid:180)â1 (cid:88) N XË (cid:48) i Îµ (cid:98)i Îµ (cid:98) (cid:48) i XË i (cid:179) XË (cid:48) XË (cid:180)â1 . (17.40) i=1 Theseestimatorsareconvenientbecausetheyaresimpletoapplyandallowforunbalancedpanels. Intypicalmicropanelapplications N isverylargeandk ismodest. Thustheadjustmentin(17.39) isminorwhilethatin(17.40)isapproximatelyT/(T â1)whereT =n/N istheaveragenumberoftime periodsperindividual. WhenT issmallthiscanbeaverylargeadjustment. Hencethechoicebetween (17.38),(17.39),and(17.40)canbesubstantial. Tounderstandifthedegreeoffreedomadjustmentin(17.40)isappropriate,considerthesimplified settingwheretheresidualsareconstructedwiththetrueÎ²butestimatedfixedeffectsu .Thisisauseful i approximationsincethenumberofestimatedslopecoefficientsÎ²issmallrelativetothesamplesizen",
    "page": 631,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". WhenT issmallthiscanbeaverylargeadjustment. Hencethechoicebetween (17.38),(17.39),and(17.40)canbesubstantial. Tounderstandifthedegreeoffreedomadjustmentin(17.40)isappropriate,considerthesimplified settingwheretheresidualsareconstructedwiththetrueÎ²butestimatedfixedeffectsu .Thisisauseful i approximationsincethenumberofestimatedslopecoefficientsÎ²issmallrelativetothesamplesizen. (cid:48) (cid:48) ThenÎµ =ÎµË =M Îµ soXË Îµ =XË Îµ and(17.38)equals (cid:98)i i i i i(cid:98)i i i (cid:195) (cid:33) V(cid:98) c fe luster= (cid:179) XË (cid:48) XË (cid:180)â1 (cid:88) N XË (cid:48) i Îµ i Îµ(cid:48) i XË i (cid:179) XË (cid:48) XË (cid:180)â1 i=1 whichistheidealizedestimatorwiththetrueerrorsratherthantheresiduals. Since(cid:69)(cid:163)Îµ Îµ(cid:48) |X (cid:164)=Î£ it i i i i (cid:104) (cid:105) followsthat(cid:69) V(cid:98) c fe luster|X =V fe andV(cid:98) c fe luster isunbiasedforV fe ! Thusnodegreeoffreedomadjustment",
    "page": 631,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 612 isrequired.ThisisdespitethefactthatN fixedeffectshavebeenestimated.Whilethisanalysisconcerns the idealized case where the residuals have been constructed with the true coefficients Î² so does not translateintoadirectrecommendationforthefeasibleestimator,itstillsuggeststhatthestrongadhoc adjustmentin(17.40)isunwarranted. This (crude) analysis suggests that for the cluster robust covariance estimator for fixed effects re- gressiontheadjustmentrecommendedbyC.Hansen(17.39)isthemostappropriate. Itistypicallywell approximatedbytheunadjustedestimator(17.38). Basedoncurrenttheorythereisnojustificationfor theadhocadjustment(17.40). Themainargumentforthelatteristhatitproducesthelargeststandard errorsandisthusthemostconservativechoice. Incurrentpracticetheestimators(17.38)and(17.40)arethemostcommonlyusedcovariancematrix estimatorsforfixedeffectsestimation. InSections17.22and17.23wediscusscovariancematrixestimationunderheteroskedasticitybutno serialcorrelation. Toillustrate,inTable17.2wepresentthefixedeffectregressionestimatesoftheinvestmentmodel (17.3)inthethirdcolumnwithcluster-robuststandarderrors.ThetradingindicatorT andtheindustry i dummiescannotbeincludedastheyaretime-invariant. Thepointestimatesaresimilartotherandom effectsestimates,thoughthecoefficientsondebtandcashflowincreaseinmagnitude. 17.13 FixedEffectsEstimationinStata ThereareseveralmethodstoobtainthefixedeffectsestimatorÎ² (cid:98)fe inStata. The first method is dummy variable regression. This can be obtained by the Stata regress com- mand,forexamplereg y x i.id, cluster(id)whereidisthegroup(individual)identifier.Inmost cases, asdiscussedinSection17.11, thisisnotrecommendedduetotheexcessivecomputermemory requirements and slow computation. If this command is done it may be useful to suppress display of thefulllistofcoefficientestimates. Todoso,typequietly reg y x i.id, cluster(id)followedby estimates table, keep(x _cons) be se. Thesecondcommandwillreportthecoefficient(s)onx only,notthoseontheindexvariableid.(Otherstatisticscanbereportedaswell.) ThesecondmethodistomanuallycreatethewithintransformedvariablesasdescribedinSection 17.8,andthenuseregress. Thethirdmethodisxtreg fewhichisspecificallywrittenforpaneldata. Thisestimatestheslope coefficients using the partialling-out approach. The default covariance matrix estimator is classical as defined in (17.36). The cluster-robust covariance matrix (17.38) can be obtained using the option vce(robust)orr. Thefourthmethodisareg absorb(id).Thiscommandisanalternativeimplementationofpartialling- outregression.Thedefaultcovariancematrixestimatoristheclassical(17.36).Thecluster-robustcovari- ancematrixestimator(17.40)canbeobtainedusingthecluster(id)option. Theheteroskedasticity- robustcovariancematrixisobtainedwhenrorvce(robust)isspecifiedbutthisisnotrecommended unlessT islargeaswillbediscussedinSection17.22",
    "page": 632,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". The cluster-robust covariance matrix (17.38) can be obtained using the option vce(robust)orr. Thefourthmethodisareg absorb(id).Thiscommandisanalternativeimplementationofpartialling- outregression.Thedefaultcovariancematrixestimatoristheclassical(17.36).Thecluster-robustcovari- ancematrixestimator(17.40)canbeobtainedusingthecluster(id)option. Theheteroskedasticity- robustcovariancematrixisobtainedwhenrorvce(robust)isspecifiedbutthisisnotrecommended unlessT islargeaswillbediscussedinSection17.22. i AnimportantdifferencebetweentheStataxtregandaregcommandsisthattheyimplementdif- ferentcluster-robustcovariancematrixestimators:(17.38)inthecaseofxtregand(17.40)inthecaseof areg. Asdiscussedintheprevioussectiontheadjustmentusedbyaregisadhocandnotwell-justified butproducesthelargestandhencemostconservativestandarderrors. AnotherdifferencebetweenthecommandsishowtheyreporttheequationR2. Thisdifferencecan behugeandstemsfromthefactthattheyareestimatingdistinctpopulationcounter-parts.Fulldummy variableregressionandthearegcommandcalculateR2thesameway:thesquaredcorrelationbetween Y andthefittedregressionwithallpredictorsincludingtheindividualdummyvariables.Thextreg fe it",
    "page": 632,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 613 commandreportsthreevaluesforR2: within,between,andoverall. TheâwithinâR2isidenticaltowhat isobtainedfromasecondstageregressionusingthewithintransformedvariables.(Thesecondmethod describedabove.) TheâoverallâR2 isthesquaredcorrelationbetweenY andthefittedregressionex- it cludingtheindividualeffects. Which R2 should be reported? The answer depends on the baseline model before regressors are added.Ifweviewthebaselineasanindividual-specificmean,thenthewithincalculationisappropriate. Ifthebaselineisasinglemeanforallobservationsthenthefullregression(areg)calculationisappropri- ate.Thelatter(areg)calculationistypicallymuchhigherthanthewithincalculation,asthefixedeffects typicallyâexplainâalargeportionofthevariance. InanyeventasthereisnotasingledefinitionofR2 it isimportanttobeexplicitaboutthemethodifitisreported. Incurrenteconometricpracticebothxtregandaregareused,thougharegappearstobethemore popularchoice.SincethelattertypicallyproducesamuchhighervalueofR2,reportedR2valuesshould beviewedskepticallyunlesstheircalculationmethodisdocumentedbytheauthor. 17.14 BetweenEstimator Thebetweenestimatoriscalculatedfromtheindividual-meanequation(17.20) (cid:48) Y =X Î²+u +Îµ . (17.41) i i i i Estimationcanbedoneatthelevelofindividualsoratthelevelofobservations. Leastsquaresap- pliedto(17.41)attheleveloftheN individualsis (cid:195) (cid:33)â1(cid:195) (cid:33) (cid:88) N (cid:48) (cid:88) N Î² (cid:98)be = X i X i X i Y i . i=1 i=1 Leastsquaresappliedto(17.41)atthelevelofobservationsis (cid:195) (cid:33)â1(cid:195) (cid:33) (cid:195) (cid:33)â1(cid:195) (cid:33) (cid:88) N (cid:88) (cid:48) (cid:88) N (cid:88) (cid:88) N (cid:48) (cid:88) N Î² (cid:101)be = X i X i X i Y i = T i X i X i T i X i Y i . i=1tâSi i=1tâSi i=1 i=1 InbalancedpanelsÎ² (cid:101)be =Î² (cid:98)be buttheydifferonunbalancedpanels. Î² (cid:101)be equalsweightedleastsquares appliedatthelevelofindividualswithweightT . i Undertherandomeffectsassumptions(Assumption17.1)Î² (cid:98)be isunbiasedforÎ²andhasvariance (cid:195) (cid:33)â1(cid:195) (cid:33)(cid:195) (cid:33)â1 V be =var (cid:163)Î² (cid:98)be |X (cid:164)= (cid:88) N X i X (cid:48) i (cid:88) N X i X (cid:48) i Ï2 i (cid:88) N X i X (cid:48) i i=1 i=1 i=1 where Ï2 Ï2=var (cid:163) u +Îµ (cid:164)=Ï2+ Îµ i i i u T i isthevarianceoftheerrorin(17.41).Whenthepanelisbalancedthevarianceformulasimplifiesto V be =var (cid:163)Î² (cid:98)be |X (cid:164)= (cid:195) (cid:88) N X i X (cid:48) i (cid:33)â1(cid:181) Ï2 u + Ï T 2 Îµ (cid:182) . i=1 Under the random effects assumption the between estimator Î² (cid:98)be is unbiased for Î² but is less effi- cientthantherandomeffectsestimatorÎ² (cid:98)gls . Consequentlythereseemslittledirectuseforthebetween estimatorinlinearpaneldataapplications.",
    "page": 633,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 614 Instead,itsprimaryapplicationistoconstructanestimateofÏ2.First,considerestimationof u Ï2= 1 (cid:88) N Ï2=Ï2+ 1 (cid:88) N Ï2 Îµ =Ï2+ Ï2 Îµ b N i=1 i u N i=1 T i u T whereT =N/ (cid:80)N T â1 istheharmonicmeanofT . (InthecaseofabalancedpanelT =T.) Anatural i=1 i i estimatorofÏ2 is b Ï2= 1 (cid:88) N e2 . (17.42) (cid:98)b Nâk (cid:98)bi i=1 (cid:48) wheree (cid:98)bi =Y i âX i Î² (cid:98)be arethebetweenresiduals.(EitherÎ² (cid:98)be orÎ² (cid:101)be canbeused.) FromtherelationÏ2 b =Ï2 u +Ï2 Îµ/T and(17.42)wecandeduceanestimatorforÏ2 u . Wehavealready describedanestimatorÏ (cid:98) 2 ÎµforÏ2 Îµin(17.37)forthefixedeffectsmodel.Sincethefixedeffectsmodelholds underweakerconditionsthantherandomeffectsmodel,Ï (cid:98) 2 Îµ isvalidforthelatteraswell. Thissuggests thefollowingestimatorforÏ2 u Ï2 Ï2 =Ï2â (cid:98)Îµ . (17.43) (cid:98)u (cid:98)b T To summarize, the fixed effect estimator is used for Ï (cid:98) 2 Îµ, the between estimator for Ï (cid:98) 2 b , and Ï (cid:98) 2 u is con- structedfromthetwo. Itispossiblefor(17.43)tobenegative.Itistypicaltousetheconstrainedestimator (cid:183) Ï2(cid:184) Ï2 =max 0,Ï2â (cid:98)Îµ . (17.44) (cid:98)u (cid:98)b T (17.44)isthemostcommonestimatorforÏ2 intherandomeffectsmodel. u ThebetweenestimatorÎ² (cid:98)be canbeobtainedusingtheStatacommandxtreg be. TheestimatorÎ² (cid:101)be canbeobtainedbyxtreg be wls. 17.15 FeasibleGLS Therandomeffectsestimatorcanbewrittenas (cid:195) (cid:33)â1(cid:195) (cid:33) (cid:195) (cid:33)â1(cid:195) (cid:33) N N N N Î² (cid:98)re = (cid:88) X (cid:48) i â¦â i 1X i (cid:88) X (cid:48) i â¦â i 1Y i = (cid:88) X(cid:101) (cid:48) i X(cid:101)i (cid:88) X(cid:101) (cid:48) i Y(cid:101)i (17.45) i=1 i=1 i=1 i=1 whereX(cid:101)i =â¦â i 1/2X i andY(cid:101)i =â¦â i 1/2Y i .Itisinstructivetostudythesetransformations. DefineP =1 (cid:161) 1 (cid:48) 1 (cid:162)â1 1 (cid:48) sothatM =I âP .ThuswhileM isthewithinoperator,P canbecalled i i i i i i i i i i theindividual-meanoperatorsinceP Y =1 Y .Wecanwrite i i i i T Ï2 â¦ i =I i +1 i 1 (cid:48) i Ï2 u /Ï2 Îµ =I i + Ï i 2 u P i =M i +Ïâ i 2P i Îµ where Ï Îµ Ï = . (17.46) i (cid:113) Ï2 Îµ +T i Ï2 u SincethematricesM andP areidempotentandorthogonalwefindthatâ¦â1=M +Ï2P and i i i i i i â¦â1/2=M +Ï P =I â(cid:161) 1âÏ (cid:162) P . (17.47) i i i i i i i",
    "page": 634,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 615 ThereforethetransformationusedbytheGLSestimatoris Y(cid:101)i =(cid:161) I i â(cid:161) 1âÏ i (cid:162) P i (cid:162) Y i =Y i â(cid:161) 1âÏ i (cid:162) 1 i Y i whichisapartialwithintransformation. ThetransformationaswrittendependsonÏ whichisunknown.Itcanbereplacedbytheestimator i Ï Ï = (cid:98)Îµ (17.48) (cid:98)i (cid:113) Ï (cid:98) 2 Îµ +T i Ï (cid:98) 2 u wheretheestimatorsÏ (cid:98) 2 ÎµandÏ (cid:98) 2 u aregivenin(17.37)and(17.44).Weobtainthefeasibletransformations Y(cid:101)i =Y i â(cid:161) 1âÏ (cid:98)i (cid:162) 1 i Y i (17.49) and X(cid:101)i =X i â(cid:161) 1âÏ (cid:98)i (cid:162) 1 i X (cid:48) i . (17.50) Thefeasiblerandomeffectsestimatoris(17.45)using(17.49)and(17.50). IntheprevioussectionwenotedthatitispossibleforÏ (cid:98) 2 u =0.InthiscaseÏ (cid:98)i =1andÎ² (cid:98)re =Î² (cid:98)pool . What this shows is the following. The random effects estimator (17.45) is least squares applied to the transformed variables X(cid:101)i and Y(cid:101)i defined in (17.50) and (17.49). When Ï (cid:98)i =0 these are the within transformations,so X(cid:101)i =XË i ,Y(cid:101)i =YË i ,andÎ² (cid:98)re =Î² (cid:98)fe isthefixedeffectsestimator. WhenÏ (cid:98)i =1thedata areuntransformedX(cid:101)i =X i ,Y(cid:101)i =Y i ,andÎ² (cid:98)re =Î² (cid:98)pool isthepooledestimator. Ingeneral, X(cid:101)i andY(cid:101)i can beviewedaspartialwithintransformations. (cid:113) Recalling the definition Ï (cid:98)i =Ï (cid:98)Îµ/ Ï (cid:98) 2 Îµ +T i Ï (cid:98) 2 u we see that when the idiosyncratic error variance Ï (cid:98) 2 Îµ islargerelativetoT i Ï (cid:98) 2 u thenÏ (cid:98)i â1andÎ² (cid:98)re âÎ² (cid:98)pool . Thuswhenthevarianceestimatessuggestthatthe individualeffectisrelativelysmalltherandomeffectestimatorsimplifiestothepooledestimator.Onthe otherhandwhentheindividualeffecterrorvarianceÏ (cid:98) 2 u islargerelativetoÏ (cid:98) 2 Îµ thenÏ (cid:98)i â0andÎ² (cid:98)re âÎ² (cid:98)fe . Thuswhenthevarianceestimatessuggestthattheindividualeffectisrelativelylargetherandomeffect estimatorisclosetothefixedeffectsestimator. 17.16 InterceptinFixedEffectsRegression Thefixedeffectestimatordoesnotapplytoanyregressorwhichistime-invariantforallindividuals. This includes an intercept. Yet some authors and packages (e.g. Amemiya (1971) and xtreg in Stata) reportanintercept.Toseehowtoconstructanestimatorofanintercepttakethecomponentsregression equationaddinganexplicitintercept Y =Î±+X (cid:48) Î²+u +Îµ . it it i it WehavealreadydiscussedestimationofÎ²byÎ² (cid:98)fe . ReplacingÎ²inthisequationwithÎ² (cid:98)fe andthenesti- matingÎ±byleastsquares,weobtain (cid:48) Î± (cid:98)fe =Y âX Î² (cid:98)fe whereY andX areaveragesfromthefullsample.Thisistheestimatorreportedbyxtreg.",
    "page": 635,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 616 17.17 EstimationofFixedEffects For most applications researchers are interested in the coefficients Î² not the fixed effects u . But i in some cases the fixed effects themselves are interesting. This arises when we want to measure the distributionofu tounderstanditsheterogeneity.Italsoarisesinthecontextofprediction.Asdiscussed i inSection17.11thefixedeffectsestimateuisobtainedbyleastsquaresappliedtotheregression(17.33). (cid:98) Tofindtheirsolution,replaceÎ²in(17.33)withtheleastsquaresminimizerÎ² (cid:98)fe andapplyleastsquares. Sincethisistheindividual-specificinterceptthesolutionis u (cid:98)i = T 1 (cid:88) (cid:161) Y it âX i (cid:48) t Î² (cid:98)fe (cid:162)=Y i âX (cid:48) i Î² (cid:98)fe . (17.51) i tâSi Alternatively,using(17.34)thisis u (cid:98) =(cid:161) D (cid:48) D (cid:162)â1 D (cid:48)(cid:161) Y âXÎ² (cid:98)fe (cid:162) N =diag (cid:169) T i â1(cid:170)(cid:88) d i 1 (cid:48) i (cid:161) Y i âX i Î² (cid:98)fe (cid:162) i=1 (cid:88) N (cid:179) (cid:48) (cid:180) = d i Y i âX i Î² (cid:98)fe i=1 =(u ,...,u ) (cid:48) . (cid:98)1 (cid:98)N Thus the least squares estimates of the fixed effects can be obtained from the individual-specific meansanddoesnotrequirearegressionwithN+k regressors. Ifanintercepthasbeenestimated(asdiscussedintheprevioussection)itshouldbesubtractedfrom (17.51).Inthiscasetheestimatedfixedeffectsare (cid:48) u (cid:98)i =Y i âX i Î² (cid:98)fe âÎ± (cid:98)fe . (17.52) WitheitherestimatorwhenthenumberoftimeseriesobservationsT issmallu willbeanimprecise i (cid:98)i estimatorofu .Thuscalculationsbasedonu shouldbeinterpretedcautiously. i (cid:98)i Thefixedeffects(17.52)maybeobtainedinStataafterivreg, feusingthepredict ucommand orafteraregusingthepredict dcommand. 17.18 GMMInterpretationofFixedEffects Wecanalsointerpretthefixedeffectsestimatorthroughthegeneralizedmethodofmoments. Takethefixedeffectsmodelafterapplyingthewithintransformation(17.21). Wecanviewthisasa system of T equations, one for each time period t. This is a multivariate regressionmodel. Using the notationofChapter11definetheTÃkT regressormatrix ï£« XË(cid:48) 0 Â·Â·Â· 0 ï£¶ i1 X i =ï£¬ ï£­ . . . XË(cid:48) . . . ï£· ï£¸ . (17.53) i2 0 0 Â·Â·Â· XË(cid:48) iT IfwetreateachtimeperiodasaseparateequationwehavethekT momentconditions (cid:69) (cid:104) X (cid:48)(cid:161) YË âXË Î²(cid:162) (cid:105) =0. i i i",
    "page": 636,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 617 ThisisanoveridentifiedsystemofequationswhenT â¥3astherearek coefficientsandkT moments. (However,themomentsarecollinearduetothewithintransformation. Therearek(Tâ1)effectivemo- ments.) Interpretingthismodelinthecontextofmultivariateregression,overidentificationisachieved bytherestrictionthatthecoefficientvectorÎ²isconstantacrosstimeperiods. ThismodelcanbeinterpretedasaregressionofYË on XË usingtheinstruments X . The2SLSesti- i i i matorusingmatrixnotationis (cid:181) (cid:179) (cid:48) (cid:180)(cid:179) (cid:48) (cid:180)â1(cid:179) (cid:48) (cid:180) (cid:182)â1(cid:181) (cid:179) (cid:48) (cid:180)(cid:179) (cid:48) (cid:180)â1(cid:179) (cid:48) (cid:180) (cid:182) Î² (cid:98) = XË X X X X XË XË X X X X YË . Noticethat ï£« XË 0 Â·Â·Â· 0 ï£¶ï£« XË(cid:48) 0 Â·Â·Â· 0 ï£¶ X (cid:48) X = i (cid:88) = n 1 ï£¬ ï£­ 0 . . . i1 XË 0 i2 Â·Â·Â· XË . . . ï£· ï£¸ ï£¬ ï£­ 0 . . . i1 XË 0 i (cid:48) 2 Â·Â·Â· XË . . . (cid:48) ï£· ï£¸ iT iT ï£« (cid:80)n XË XË(cid:48) 0 Â·Â·Â· 0 ï£¶ i=1 i1 i1 =ï£¬ ï£­ . . . (cid:80)n i=1 XË i2 XË i (cid:48) 2 . . . ï£· ï£¸ , 0 0 Â·Â·Â· (cid:80)n XË XË(cid:48) i=1 iT iT ï£« (cid:80)n XË XË(cid:48) ï£¶ i=1 i1 i1 X (cid:48) XË =ï£¬ ï£­ . . . ï£· ï£¸ , (cid:80)n XË XË(cid:48) i=1 iT iT and ï£« (cid:80)n XË YË ï£¶ i=1 i1 i1 X (cid:48) YË =ï£¬ ï£­ . . . ï£· ï£¸ . (cid:80)n XË YË i=1 iT iT Thusthe2SLSestimatorsimplifiesto (cid:195) (cid:195) (cid:33)(cid:195) (cid:33)â1(cid:195) (cid:33)(cid:33)â1 T n n n Î² (cid:98)2sls = (cid:88) (cid:88) XË it XË i (cid:48) t (cid:88) XË it XË i (cid:48) t (cid:88) XË it XË i (cid:48) t t=1 i=1 i=1 i=1 (cid:195) (cid:195) (cid:33)(cid:195) (cid:33)â1(cid:195) (cid:33)(cid:33) T n n n Ã (cid:88) (cid:88) XË XË(cid:48) (cid:88) XË XË(cid:48) (cid:88) XË YË it it it it it it t=1 i=1 i=1 i=1 (cid:195) (cid:33)â1(cid:195) (cid:33) T n T n = (cid:88)(cid:88) XË XË(cid:48) (cid:88)(cid:88) XË YË it it it it t=1i=1 t=1i=1 =Î² (cid:98)fe thefixedeffectsestimator! Thisshowsthatifwetreateachtimeperiodasaseparateequationwithitsseparatemomentequa- tionsothatthesystemisover-identified,andthenestimatebyGMMusingthe2SLSweightmatrix,the resulting GMM estimator equals the simple fixed effects estimator. There is no change by adding the additionalmomentconditions. The2SLSestimatoristheappropriateGMMestimatorwhentheequationerrorisseriallyuncorre- latedandhomoskedastic.Ifweuseatwo-stepefficientweightmatrixwhichallowsforheteroskedasticity",
    "page": 637,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 618 andserialcorrelationtheGMMestimatoris (cid:195) (cid:195) (cid:33)(cid:195) (cid:33)â1(cid:195) (cid:33)(cid:33)â1 T n n n Î² (cid:98)gmm = (cid:88) (cid:88) XË it XË i (cid:48) t (cid:88) XË it XË i (cid:48) t e (cid:98)i 2 t (cid:88) XË it XË i (cid:48) t t=1 i=1 i=1 i=1 (cid:195) (cid:195) (cid:33)(cid:195) (cid:33)â1(cid:195) (cid:33)(cid:33) T n n n Ã (cid:88) (cid:88) XË XË(cid:48) (cid:88) XË XË(cid:48) e2 (cid:88) XË YË it it it it(cid:98)it it it t=1 i=1 i=1 i=1 wheree arethefixedeffectsresiduals. (cid:98)it Notationally,thisGMMestimatorhasbeenwrittenforabalancedpanel. Foranunbalancedpanel thesumsoveri needtobereplacedbysumsoverindividualsobservedduringtimeperiodt. Otherwise nochangesneedtobemade. 17.19 IdentificationintheFixedEffectsModel The identification of the slope coefficient Î² in fixed effects regression is similar to that in conven- tionalregressionbutsomewhatmorenuanced. Itismostusefultoconsiderthewithin-transformedequation,whichcanbewrittenasYË =XË(cid:48) Î²+ÎµË it it it orYË =XË Î²+ÎµË . i i i FromregressiontheoryweknowthatthecoefficientÎ²isthelineareffectofXË onYË . Thevariable it it XË isthedeviationoftheregressorfromitsindividual-specificmeanandsimilarlyforYË .Thusthefixed it it effectsmodeldoesnotidentifytheeffectoftheaveragelevelofX ontheaveragelevelofY ,butrather it it theeffectofthedeviationsinX onY . it it Inanygivensamplethefixedeffectsestimatorisonlydefinedif (cid:80)N XË (cid:48) XË isfullrank. Thepopula- i=1 i i tionanalog(whenindividualsarei.i.d.)is (cid:104) (cid:48) (cid:105) (cid:69) XË XË >0. (17.54) i i Equation (17.54) is the identification condition for the fixed effects estimator. It requires that the re- gressormatrixisfull-rankinexpectationafterapplicationofthewithintransformation. Theregressors cannotcontainanyvariablewhichdoesnothavetime-variationattheindividuallevelnorasetofre- gressorswhosetime-variationattheindividualleveliscollinear. 17.20 AsymptoticDistributionofFixedEffectsEstimator In this section we present an asymptotic distribution theory for the fixed effects estimator in bal- ancedpanels.Unbalancedpanelsareconsideredinthefollowingsection. Weusethefollowingassumptions.",
    "page": 638,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 619 Assumption17.2 1. Y =X (cid:48) Î²+u +Îµ fori =1,...,N andt=1,...,T withT â¥2. it it i it 2. The variables (Îµ ,X ), i = 1,...,N, are independent and identically dis- i i tributed. 3. (cid:69)[X Îµ ]=0foralls=1,...,T. is it (cid:104) (cid:48) (cid:105) 4. Q =(cid:69) XË XË >0. T i i 5. (cid:69)(cid:163)Îµ4 (cid:164)<â. it 6. (cid:69)(cid:107)X (cid:107)4<â. it GivenAssumption17.2wecanestablishasymptoticnormalityforÎ² (cid:98)fe . (cid:112) Theorem17.2 UnderAssumption17.2, as N ââ, N (cid:161)Î² (cid:98)fe âÎ²(cid:162)ââN (cid:161) 0,VÎ² (cid:162) d whereVÎ² =Q â T 1â¦ T Q â T 1andâ¦ T =(cid:69) (cid:104) XË (cid:48) i Îµ i Îµ(cid:48) i XË i (cid:105) . ThisasymptoticdistributionisderivedasthenumberofindividualsN d(cid:112)ivergestoinfini (cid:112) tywhilethe timenumberoftimeperiodsT isheldfixed.Thereforethenormalizationis N ratherthan n(though eithercouldbeusedsinceT isfixed).Thisapproximationisappropriateforthecontextofalargenumber ofindividuals. WecouldalternativelyderiveanapproximationforthecasewherebothN andT diverge toinfinitybutthiswouldnotbeastrongerresult. OnewayofthinkingaboutthisisthatTheorem17.2 doesnotrequireT tobelarge. Theorem 17.2 may appear standard given our arsenal of asymptotic theory but in a fundamental senseitisquitedifferentfromanyotherresultwehaveintroduced.Fixedeffectsregressioniseffectively estimating N+k coefficients â the k slope coefficients Î² plus the N fixed effects u â and the theory specifies that N ââ. Thus the number of estimated parameters is diverging to infinity at the same rateassamplesizeyetthetheestimatorobtainsaconventionalmean-zerosandwich-formasymptotic distribution.InthissenseTheorem17.2isnewandspecial. Wenowdiscusstheassumptions. Assumption 17.2.2 states that the observations are independent across individuals i. This is com- monlyusedforpaneldataasymptotictheory. Animportantimpliedrestrictionisthatitmeansthatwe excludefromtheregressorsanyseriallycorrelatedaggregatetimeseriesvariation. Assumption17.2.3imposesthat X isstrictlyexogeneousforÎµ . Thisisstrongerthansimplepro- it it jectionbutisweakerthanstrictmeanindependence(17.18). Itdoesnotimposeanyconditiononthe individual-specificeffectsu . i Assumption17.2.4istheidentificationconditiondiscussedintheprevioussection. Assumptions17.2.5and17.2.6areneededforthecentrallimittheorem. WenowproveTheorem17.2.Theassumptionsimplythatthevariables(XË ,Îµ )arei.i.d.acrossi and i i havefinitefourthmoments.ThusbytheWLLN 1 (cid:88) N (cid:48) (cid:104) (cid:48) (cid:105) XË XË ââ(cid:69) XË XË =Q . N i=1 i i p i i T",
    "page": 639,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 620 Assumption17.2.3implies (cid:69) (cid:104) XË (cid:48) Îµ (cid:105) = (cid:88) T (cid:69)(cid:163) XË Îµ (cid:164)= (cid:88) T (cid:69)[X Îµ ]â (cid:88) T (cid:88) T (cid:69)(cid:163) X Îµ (cid:164)=0 i i it it it it ij it t=1 t=1 t=1j=1 (cid:48) sotheyaremeanzero.Assumptions17.2.5and17.2.6implythatXË Îµ hasafinitecovariancematrixâ¦ . i i T TheassumptionsfortheCLT(Theorem6.3)hold,thus 1 (cid:88) N (cid:48) (cid:112) XË Îµ ââN(0,â¦ ). i i T N i=1 d Togetherwefind (cid:112) N (cid:161)Î² (cid:98)fe âÎ²(cid:162)= (cid:195) N 1 i (cid:88) N =1 XË (cid:48) i XË i (cid:33)â1(cid:195) (cid:112) 1 N i (cid:88) N =1 XË (cid:48) i Îµ i (cid:33) â d âQ â T 1N(0,â¦ T )=N (cid:161) 0,VÎ² (cid:162) asstated. 17.21 AsymptoticDistributionforUnbalancedPanels Inthissectionweextendthetheoryoftheprevioussectiontocoverunbalancedpanelsunderran- domselection.OurpresentationisbuiltonSection17.1ofWooldridge(2010). Thinkofanunbalancedpanelasashortenedversionofanidealizedbalancedpanelwheretheshort- eningisduetoâmissingâobservationsduetorandomselection. Thussupposethattheunderlying(po- tentiallylatent)variablesareY =(Y ,...,Y ) (cid:48) and X =(X ,...,X ) (cid:48) . Let s =(s ,...,s ) (cid:48) beavector i i1 iT i i1 iT i i1 iT ofselectionindicators,meaningthats =1ifthetimeperiodt isobservedforindividuali ands =0 it it otherwise.Thenwecandescribetheestimatorsalgebraicallyasfollows. LetS =diag(s )andM =S âs (cid:161) s (cid:48) s (cid:162)â1 s (cid:48) ,whichisidempotent. Thewithintransformationscan i i i i i i i i bewrittenasYË =M Y and XË =M X . Theyhavethepropertythatifs =0(sothattimeperiodt is i i i i i i it missing)thenthetth elementofYË andthetth rowof XË areallzeros. Themissingobservationshave i i beenreplacedbyzeros.Consequently,theydonotappearinmatrixproductsandsums. ThefixedeffectsestimatorofÎ²basedontheobservedsampleis (cid:195) (cid:33)â1(cid:195) (cid:33) N N (cid:88) (cid:48) (cid:88) (cid:48) Î² (cid:98)fe = XË i XË i XË i YË i . i=1 i=1 Centeredandnormalized, (cid:112) N (cid:161)Î² (cid:98)fe âÎ²(cid:162)= (cid:195) N 1 i (cid:88) N =1 XË (cid:48) i XË i (cid:33)â1(cid:195) (cid:112) 1 N i (cid:88) N =1 XË (cid:48) i Îµ i (cid:33) . Notationally this appears to be identical to the case of a balanced panel but the difference is that the withinoperatorM incorporatesthesampleselectioninducedbytheunbalancedpanelstructure. i ToderiveadistributiontheoryforÎ² (cid:98)fe weneedtobeexplicitaboutthestochasticnatureofs i . That is,whyaresometimeperiodsobservedandsomenot?Wecouldtakeseveralapproaches: 1. Wecouldtreats asfixed(non-random).Thisistheeasiestapproachbutthemostunsatisfactory. i",
    "page": 640,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 621 2. Wecouldtreats asrandombutindependentof(Y ,X ).Thisisknownasâmissingatrandomâand i i i isacommonassumptionusedtojustifymethodswithmissingobservations. Itisjustifiedwhen thereasonwhyobservationsarenotobservedisindependentoftheobservations. Thisisappro- priate,forexample,inpaneldatasetswhereindividualsenterandexitinâwavesâ. Thestatistical treatmentisnotsubstantiallydifferentfromthecaseoffixeds . i 3. Wecouldtreat(Y ,X ,s )asjointlyrandombutimposeaconditionsufficientforconsistentesti- i i i mationofÎ². Thisistheapproachwetakebelow. Theconditionturnsouttobeaformofmean independence. Theadvantageofthisapproachisthatitislessrestrictivethanfullindependence. Thedisadvantageisthatwemustuseaconditionalmeanrestrictionratherthanuncorrelatedness toidentifythecoefficients. Thespecificassumptionsweimposeareasfollows. Assumption17.3 1. Y =X (cid:48) Î²+u +Îµ fori =1,...,N withT â¥2. it it i it i 2. Thevariables(Îµ ,X ,s ),i =1,...,N,areindependentandidenticallydis- i i i tributed. 3. (cid:69)[Îµ |X ,s ]=0. it i i (cid:104) (cid:48) (cid:105) 4. Q =(cid:69) XË XË >0. T i i 5. (cid:69)(cid:163)Îµ4 (cid:164)<â. it 6. (cid:69)(cid:107)X (cid:107)4<â. it TheprimarydifferencewithAssumption17.2isthatwehavestrengthenedstrictexogeneitytostrict mean independence. This imposes that the regression model is properly specified and that selection does not affect the mean of Îµ . It is less restrictive than full independence since s can affect other it i momentsofÎµ andmoreimportantlydoesnotrestrictthejointdependencebetweens andX . it i i Giventheabovedevelopmentitisstraightforwardtoestablishasymptoticnormality. (cid:112) Theorem17.3 UnderAssumption17.3, as N ââ, N (cid:161)Î² (cid:98)fe âÎ²(cid:162)ââN (cid:161) 0,VÎ² (cid:162) d whereVÎ² =Q â T 1â¦ T Q â T 1andâ¦ T =(cid:69) (cid:104) XË (cid:48) i Îµ i Îµ(cid:48) i XË i (cid:105) . WenowproveTheorem17.3.Theassumptionsimplythatthevariables(XË ,Îµ )arei.i.d.acrossi and i i havefinitefourthmoments.BytheWLLN 1 (cid:88) N (cid:48) (cid:104) (cid:48) (cid:105) XË XË ââ(cid:69) XË XË =Q . N i=1 i i p i i T",
    "page": 641,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 622 (cid:48) TherandomvectorsXË Îµ arei.i.d.ThematrixXË isafunctionof(X ,s )only.Assumption17.3.3andthe i i i i i lawofiteratedexpectationsimplies (cid:104) (cid:48) (cid:105) (cid:104) (cid:48) (cid:105) (cid:69) XË Îµ =(cid:69) XË (cid:69)[Îµ |X ,s ] =0. i i i i i i (cid:48) so that XË Îµ is mean zero. Assumptions 17.3.5 and 17.3.6 and the fact that s is bounded implies that (cid:48) i i i XË Îµ hasafinitecovariancematrix,whichisâ¦ .TheassumptionsfortheCLThold,thus i i T 1 (cid:88) N (cid:48) (cid:112) XË Îµ ââN(0,â¦ ). i i T N i=1 d Togetherweobtainthestatedresult. 17.22 Heteroskedasticity-RobustCovarianceMatrixEstimation We have introduced two covariance matrix estimators for the fixed effects estimator. The classical estimator (17.36) is appropriate for the case where the idiosyncratic errors Îµ are homoskedastic and it serially uncorrelated. The cluster-robust estimator (17.38) allows for heteroskedasticity and arbitrary serialcorrelation. InthisandthefollowingsectionweconsidertheintermediatecasewhereÎµ ishet- it eroskedasticbutseriallyuncorrelated. Assumethat(17.18)and(17.26)holdbutnotnecessarily(17.25).Definetheconditionalvariances (cid:69)(cid:163)Îµ2 |X (cid:164)=Ï2 . (17.55) it i it ThenÎ£ =(cid:69)(cid:163)Îµ Îµ(cid:48) |X (cid:164)=diag (cid:161)Ï2 (cid:162) .Thecovariancematrix(17.24)canbewrittenas i i i i it (cid:195) (cid:33) V = (cid:179) XË (cid:48) XË (cid:180)â1 (cid:88) N (cid:88) XË XË(cid:48) Ï2 (cid:179) XË (cid:48) XË (cid:180)â1 . (17.56) fe it it it i=1tâSi AnaturalestimatorofÏ2 isÎµ2 . ReplacingÏ2 withÎµ2 in(17.56)andmakingadegree-of-freedom it (cid:98)it it (cid:98)it adjustmentweobtainaWhite-typecovariancematrixestimator (cid:195) (cid:33) V(cid:98)fe = nâN n âk (cid:179) XË (cid:48) XË (cid:180)â1 (cid:88) N (cid:88) XË it XË i (cid:48) t Îµ (cid:98) 2 it (cid:179) XË (cid:48) XË (cid:180)â1 . i=1tâSi FollowingtheinsightofWhite(1980)itmayseemappropriatetoexpectV(cid:98)fe tobeareasonableesti- matorofV . UnfortunatelythisisnotthecaseasdiscoveredbyStockandWatson(2008). Theproblem fe is thatV(cid:98)fe is a function of the individual-specific means Îµ i which are negligible only if the number of timeseriesobservationsT arelarge. i Wecanseethisbyasimplebiascalculation. Assumethatthesampleisbalancedandthattheresid- ualsareconstructedwiththetrueÎ².Then 1 (cid:88) T Îµ =ÎµË =Îµ â Îµ . (cid:98)it it it ij T t=1 Using(17.26)and(17.55) (cid:69)(cid:163)Îµ2 |X (cid:164)= (cid:181) Tâ2 (cid:182) Ï2 + Ï2 i (17.57) (cid:98)it i T it T",
    "page": 642,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 623 whereÏ2=T â1(cid:80)T Ï2 .(SeeExercise17.10.)Using(17.57)andsettingk=0weobtain i t=1 it (cid:195) (cid:33) (cid:69)(cid:163) V(cid:98)fe |X (cid:164)= T T â1 (cid:179) XË (cid:48) XË (cid:180)â1 (cid:88) N (cid:88) XË it XË i (cid:48) t (cid:69)(cid:163)Îµ (cid:98) 2 it |X i (cid:164) (cid:179) XË (cid:48) XË (cid:180)â1 i=1tâSi (cid:195) (cid:33) = (cid:181) Tâ2 (cid:182) V + 1 (cid:179) XË (cid:48) XË (cid:180)â1 (cid:88) N XË (cid:48) XË Ï2 (cid:179) XË (cid:48) XË (cid:180)â1 . Tâ1 fe Tâ1 i i i i=1 ThusV(cid:98)fe isbiasedoforderO (cid:161) T â1(cid:162) .UnlessT ââthisbiaswillpersistasN ââ.V(cid:98)fe isunbiasedin twocontexts.ThefirstiswhentheerrorsÎµ arehomoskedastic.ThesecondiswhenT =2.(Toshowthe it latterrequiressomealgebrasoisomitted.) TocorrectthebiasforthecaseT >2,StockandWatson(2008)proposedtheestimator (cid:181) Tâ1 (cid:182) 1 V(cid:101)fe = Tâ2 V(cid:98)fe â Tâ1 B(cid:98)fe (17.58) (cid:195) (cid:33) B(cid:98)fe = (cid:179) XË (cid:48) XË (cid:180)â1 (cid:88) N XË (cid:48) i XË i Ï (cid:98) 2 i (cid:179) XË (cid:48) XË (cid:180)â1 i=1 Ï2= 1 (cid:88) T Îµ2 . (17.59) (cid:98)i Tâ1 t=1 (cid:98)it Youcancheckthat(cid:69)(cid:163)Ï (cid:98) 2 i |X i (cid:164)=Ï2 i and(cid:69)(cid:163) V(cid:101)fe |X i (cid:164)=V fe soV(cid:101)fe isunbiasedforV fe .(SeeExercise17.11.) StockandWatson(2008)showthatV(cid:101)fe isconsistentwithT fixedand N ââ. Insimulationsthey showthatV(cid:101)fe hasexcellentperformance. BecauseoftheStock-WatsonanalysisStatanolongercalculatestheheteroskedasticity-robustcovari- ancematrixestimatorV(cid:98)fe whenthefixedeffectsestimatoriscalculatedusingthextregcommand. In- cluster stead,thecluster-robustestimatorV(cid:98)fe isreportedwhenrobuststandarderrorsarerequested. How- ever,fixedeffectsisoftenimplementedusingthearegcommandwhichreportsthebiasedestimatorV(cid:98)fe ifrobuststandarderrorsarerequested. Theseleadstothepracticalrecommendationthataregshould beusedwiththecluster(id)option. Atpresentthecorrectedestimator(17.58)hasnotbeenprogrammedasaStataoption. 17.23 Heteroskedasticity-RobustEstimationâUnbalancedCase Alimitationwiththebias-correctedrobustcovariancematrixestimatorofStockandWatson(2008) is that it was only derived for balanced panels. In this section we generalize their estimator to cover unbalancedpanels. Theestimatoris (cid:179) (cid:48) (cid:180)â1 (cid:179) (cid:48) (cid:180)â1 V(cid:101)fe = XË XË â¦ (cid:101)fe XË XË (17.60) N (cid:34)(cid:195) T Îµ2 âÏ2(cid:33) (cid:195) T Îµ2 (cid:33) (cid:35) â¦ (cid:101)fe = (cid:88) (cid:88) XË it XË i (cid:48) t i T (cid:98)it â2 (cid:98)i 1 {T i >2}+ T i â (cid:98)i 1 t 1 {T i =2} i=1tâSi i i where Ï2= 1 (cid:88) Îµ2 . (cid:98)i T â1 (cid:98)it i tâSi Tojustifythisestimator,asintheprevioussectionmakethesimplifyingassumptionthattheresiduals areconstructedwiththetrueÎ².Wecalculatethat",
    "page": 643,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 624 (cid:69)(cid:163)Îµ2 |X (cid:164)= (cid:181) T i â2 (cid:182) Ï2 + Ï2 i (17.61) (cid:98)it i T it T i i (cid:69)(cid:163)Ï2|X (cid:164)=Ï2. (17.62) (cid:98)i i i You can show that under these assumptions, (cid:69)(cid:163) V(cid:101)fe |X (cid:164) =V fe and thus V(cid:101)fe is unbiased for V fe . (See Exercise17.12.) InbalancedpanelstheestimatorV(cid:101)fe simplifiestotheStock-Watsonestimator(withk=0). 17.24 HausmanTestforRandomvsFixedEffects The random effects model is a special case of the fixed effects model. Thus we can test the null hypothesisofrandomeffectsagainstthealternativeoffixedeffects. TheHausmantestistypicallyused forthispurpose.Thestatisticisaquadraticinthedifferencebetweenthefixedeffectsandrandomeffects estimators.Thestatisticis H=(cid:161)Î² (cid:98)fe âÎ² (cid:98)re (cid:162)(cid:48) v (cid:99) ar (cid:163)Î² (cid:98)fe âÎ² (cid:98)re (cid:164)â1(cid:161)Î² (cid:98)fe âÎ² (cid:98)re (cid:162) =(cid:161)Î² (cid:98)fe âÎ² (cid:98)re (cid:162)(cid:48)(cid:161) V(cid:98)fe âV(cid:98)re (cid:162)â1(cid:161)Î² (cid:98)fe âÎ² (cid:98)re (cid:162) wherebothV(cid:98)fe andV(cid:98)re taketheclassical(non-robust)form. ThetestcanbeimplementedonasubsetofthecoefficientsÎ². Inparticularthisneedstobedoneif theregressors X containtime-invariantelementssothattherandomeffectsestimatorcontainsmore it coefficients than the fixed effects estimator. In this case the test should be implemented only on the coefficientsonthetime-varyingregressors. Anasymptotic100Î±%testrejectsifH exceedsthe1âÎ±th quantileoftheÏ2 distributionwherek = k dim(Î²). Ifthetestrejectsthisisevidencethattheindividualeffectu iscorrelatedwiththeregressorsso i therandomeffectsmodelisnotappropriate. Ontheotherhandifthetestfailstorejectthisevidence saysthattherandomeffectshypothesiscannotberejected. It is tempting to use the Hausman test to select whether to use the fixed effects or random effects estimator. OnecouldimagineusingtherandomeffectsestimatoriftheHausmantestfailstorejectthe randomeffectshypothesisandusingthefixedeffectsestimatorotherwise. Thisisnot,however,awise approach.Thisprocedureâselectinganestimatorbasedonatestâisknownasapretestestimatorand isbiased.Thebiasarisesbecausetheresultofthetestisrandomandcorrelatedwiththeestimators. Instead,theHausmantestcanbeusedasaspecificationtest. Ifyouareplanningtousetherandom effectsestimator(andbelievethattherandomeffectsassumptionsareappropriateinyourcontext)the Hausmantestcanbeusedtocheckthisassumptionandprovideevidencetosupportyourapproach. 17.25 RandomEffectsorFixedEffects? We have presented the random effects and fixed effects estimators of the regression coefficients",
    "page": 644,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Instead,theHausmantestcanbeusedasaspecificationtest. Ifyouareplanningtousetherandom effectsestimator(andbelievethattherandomeffectsassumptionsareappropriateinyourcontext)the Hausmantestcanbeusedtocheckthisassumptionandprovideevidencetosupportyourapproach. 17.25 RandomEffectsorFixedEffects? We have presented the random effects and fixed effects estimators of the regression coefficients. Whichshouldbeusedinpractice?Howshouldweviewthedifference? Thebasicdistinctionisthattherandomeffectsestimatorrequirestheindividualerroru tosatisfy i theconditionalmeanassumption(17.8).Thefixedeffectsestimatordoesnotrequire(17.8)andisrobust toitsviolation.Inparticular,theindividualeffectu canbearbitrarilycorrelatedwiththeregressors.On i theotherhandtherandomeffectsestimatorisefficientunderrandomeffects(Assumption17.1). Currenteconometricpracticeistopreferrobustnessoverefficiency. Consequently,currentpractice is (nearly uniformly) to use the fixed effects estimator for linear panel data models. Random effects",
    "page": 644,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 625 estimators are only used in contexts where fixed effects estimation is unknown or challenging (which occursinmanynonlinearmodels). Thelabelsârandomeffectsâandâfixedeffectsâaremisleading. Thesearelabelswhicharoseinthe early literature and we are stuck with these labels today. In a previous era regressors were viewed as âfixedâ.Viewingtheindividualeffectasanunobservedregressorleadstothelabeloftheindividualeffect asâfixedâ. Today,werarelyrefertoregressorsasâfixedâwhendealingwithobservationaldata. Weview allvariablesasrandom.Consequentlydescribingu asâfixedâdoesnotmakemuchsenseanditishardly i acontrastwiththeârandomeffectâlabelsinceundereitherassumptionu istreatedasrandom. Once i again,thelabelsareunfortunatebutthekeydifferenceiswhetheru iscorrelatedwiththeregressors. i 17.26 TimeTrends In general we expect that economic agents will experience common shocks during the same time period.Forexample,businesscyclefluctations,inflation,andinterestratesaffectallagentsintheecon- omy.Thereforeitisoftendesirabletoincludetimeeffectsinapanelregressionmodel. Thesimplestspecificationisalineartimetrend Y =X (cid:48) Î²+Î³t+u +Îµ . it it i it ForaintroductiontotimetrendsseeSection14.42.Moreflexiblespecifications(suchasaquadratic)can alsobeused. Forestimationitisappropriatetoincludethetimetrendt asanelementoftheregressor vectorX andthenapplyfixedeffects. it Insomecasesthetimetrendsmaybeindividual-specific. Seriesmaybegrowingordecliningatdif- ferentrates.Alineartimetrendspecificationonlyextractsacommontimetrend.Toallowforindividual- specifictimetrendsweneedtoincludeaninteractioneffect.Thiscanbewrittenas Y =X (cid:48) Î²+Î³ t+u +Îµ . it it i i it Inafixedeffectsspecificationthecoefficients(Î³ ,u )aretreatedaspossiblycorrelatedwiththere- i i gressors. Toeliminatethemfromthemodelwetreatthemasunknownparametersandestimateallby leastsquares. BytheFWLtheoremtheestimatorforÎ²equalsleastsquaresofYË on XË wheretheirele- mentsaretheresidualsfromtheleastsquaresregressionsonalineartimetrendfitseparatelyforeach individualandvariable. 17.27 Two-WayErrorComponents In the previous section we discussed inclusion of time trends and individual-specific time trends. Thefunctionalformsimposedbylineartimetrendsarerestrictive.Thereisnoeconomicreasontoexpect theâtrendâofaseriestobelinear. Businesscycleâtrendsâarecyclic. Thissuggeststhatitisdesirableto bemoreflexiblethanalinear(orevenpolynomial)specifications. Inthissectionweconsiderthemost flexible specification where the trend is allowed to take any arbitrary shape but will require that it is commonratherthanindividual-specific. Themodelweconsideristhetwo-wayerrorcomponentmodel Y =X (cid:48) Î²+v +u +Îµ . (17.63) it it t i it Inthismodelu isanunobservedindividual-specificeffect,v isanunobservedtime-specificeffect,and i t Îµ isanidiosyncraticerror. it",
    "page": 645,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 626 Thetwo-waymodel(17.63)canbehandledeitherusingrandomeffectsorfixedeffects.Inarandom effectsframeworktheerrorsv andu aremodeledasinAssumption17.1. Whenthepanelisbalanced t i thecovariancematrixoftheerrorvectore=vâ1 +1 âu+Îµis N T var[e]=â¦=(cid:161) I T â1 N 1 (cid:48) N (cid:162)Ï2 v +(cid:161) 1 T 1 (cid:48) T âI N (cid:162)Ï2 u +I n Ï2 Îµ. (17.64) When the panel is unbalanced a similar but cumbersome expression for (17.64) can be derived. This variance(17.64)canbeusedforGLSestimationofÎ². Moretypically(17.63)ishandledusingfixedeffects. Thetwo-waywithintransformationsubtracts bothindividual-specificmeansandtime-specificmeanstoeliminatebothv andu fromthetwo-way t i model(17.63).ForavariableY wedefinethetime-specificmeanasfollows.LetS bethesetofindivid- it t ualsi forwhichtheobservationtisincludedinthesampleandletN bethenumberoftheseindividuals. t Thenthetime-specificmeanattimet is 1 (cid:88) Y(cid:101)t = Y it . N t iâSt ThisistheaverageacrossallvaluesofY observedattimet. it Forthecaseofbalancedpanelsthetwo-waywithintransformationis YÂ¨ it =Y it âY i âY(cid:101)t +Y (17.65) whereY =n â1(cid:80)N (cid:80)T Y isthefull-samplemean.IfY satisfiesthetwo-waycomponentmodel i=1 t=1 it it Y =v +u +Îµ it t i it thenY i =v+u i +Îµ i ,Y(cid:101)t =v t +u+Îµ (cid:101)t andY =v+u+Îµ.Hence YÂ¨ =v +u +Îµ â(cid:161) v+u +Îµ (cid:162)â(cid:161) v +u+Îµ (cid:162)+v+u+Îµ it t i it i i t (cid:101)t =Îµ âÎµ âÎµ +Îµ=ÎµÂ¨ it i (cid:101)t it sotheindividualandtimeeffectsareeliminated. Thetwo-waywithintransformationappliedto(17.63)yields YÂ¨ =XÂ¨(cid:48) Î²+ÎµÂ¨ (17.66) it it it whichisinvarianttobothv andu .Thetwo-waywithinestimatorisleastsquaresappliedto(17.66). t i Fortheunbalancedcasetherearetwocomputationalapproachestoimplementtheestimator. Both arebasedontherealizationthattheestimatorisequivalenttoincludingdummyvariablesforalltime periods. LetÏ beasetofT dummyvariableswherethetth indicatesthetth timeperiod. Thusthetth t element of Ï is 1 and the remaining elements are zero. Set v =(v ,...,v ) (cid:48) as the vector of time fixed t 1 T effects.Noticethatv =Ï(cid:48) v.Wecanwritethetwo-waymodelas t t Y =X (cid:48) Î²+Ï(cid:48) v+u +Îµ . (17.67) it it t i it Thisisthedummyvariablerepresentationofthetwo-wayerrorcomponentsmodel. Model(17.67)canbeestimatedbyone-wayfixedeffectswithregressors X andÏ andcoefficient it t vectorsÎ²andv. Thiscanbeimplementedbystandardone-wayfixedeffectsmethodsincludingxtreg orareginStata. ThisproducesestimatesoftheslopesÎ²aswellasthetimeeffectsv. Toachieveidenti- ficationonetimedummyvariableisomittedfromÏ sotheestimatedtimeeffectsareallrelativetothis t baselinetimeperiod. Thisisthemostcommonmethodinpracticetoestimateatwo-wayfixedeffects model.Asthenumberoftimeperiodsistypicallymodestthisisacomputationallyattractiveapproach.",
    "page": 646,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 627 Thesecondcomputationalapproachistoeliminatethetimeeffectsbyresidualregression. Thisis donebythefollowingsteps.First,subtractindividual-specificmeansfor(17.67).Thisyields YË =XË(cid:48) Î²+ÏË (cid:48) v+ÎµË . it it t it Second,regressYË onÏË toobtainaresidualYÂ¨ andregresseachelementofXË onÏË toobtainaresidual it t it it t XÂ¨ .Third,regressYÂ¨ onXÂ¨ toobtainthewithinestimatorofÎ². Thesestepseliminatethefixedeffects it it it v sotheestimatorisinvarianttotheirvalue. Whatisimportantaboutthistwo-stepprocedureisthat t thesecondstepisnotawithintransformationacrossthetimeindexbutratherstandardregression. If the two-way within estimator is used then the regressors X cannot include any time-invariant it variables X orcommontimeseriesvariables X . Bothareeliminatedbythetwo-waywithintransfor- i t mation. Coefficientsareonlyidentifiedforregressorswhichhavevariationbothacrossindividualsand acrosstime. Ifdesiredtherelevanceofthetimeeffectscanbetestedbyanexclusiontestonthecoefficientsv. If thetestrejectsthehypothesisofzerocoefficientsthenthisindicatesthatthetimeeffectsarerelevantin theregressionmodel. Thefixedeffectsestimatorof(17.63)isinvarianttothevaluesofv andu ,thusnoassumptionsneed t i tobemadeconcerningtheirstochasticproperties. Toillustrate,thefourthcolumnofTable17.2presentsfixedeffectsestimatesoftheinvestmentequa- tion,augmentedtoincludedyeardummyindicators,andisthusatwo-wayfixedeffectsmodel. Inthis examplethecoefficientestimatesandstandarderrorsarenotgreatlyaffectedbytheinclusionoftheyear dummyvariables. 17.28 InstrumentalVariables Takethefixedeffectsmodel Y =X (cid:48) Î²+u +Îµ . (17.68) it it i it We say X is exogenous for Îµ if (cid:69)[X Îµ ]=0, and we say X is endogenous for Îµ if (cid:69)[X Îµ ](cid:54)=0. it it it it it it it it In Chapter 12 we discussed several economic examples of endogeneity and the same issues apply in the panel data context. The primary difference is that in the fixed effects model we only need to be concernediftheregressorsarecorrelatedwiththeidiosyncraticerrorÎµ ,ascorrelationbetweenX and it it u isallowed. i AsinChapter12iftheregressorsareendogenousthefixedeffectsestimatorwillbebiasedandin- consistentforthestructuralcoefficientÎ². Thestandardapproachtohandlingendogeneityistospecify instrumental variables Z which are both relevant (correlated with X ) yet exogenous (uncorrelated it it withÎµ ). it Let Z be an (cid:96)Ã1 instrumental variable where (cid:96)â¥k. As in the cross-section case, Z may con- it it tain both included exogenous variables (variables in X that are exogenous) and excluded exogenous it variables(variablesnotin X ). Let Z bethestackedinstrumentsbyindividualand Z bethestacked it i instrumentsforthefullsample. ThedummyvariableformulationofthefixedeffectsmodelisY =X (cid:48) Î²+d (cid:48) u+Îµ whered isan it it i it i NÃ1vectorofdummyvariables,oneforeachindividualinthesample. Themodelinmatrixnotation forthefullsampleis Y =XÎ²+Du+Îµ",
    "page": 647,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". As in the cross-section case, Z may con- it it tain both included exogenous variables (variables in X that are exogenous) and excluded exogenous it variables(variablesnotin X ). Let Z bethestackedinstrumentsbyindividualand Z bethestacked it i instrumentsforthefullsample. ThedummyvariableformulationofthefixedeffectsmodelisY =X (cid:48) Î²+d (cid:48) u+Îµ whered isan it it i it i NÃ1vectorofdummyvariables,oneforeachindividualinthesample. Themodelinmatrixnotation forthefullsampleis Y =XÎ²+Du+Îµ. (17.69) Theorem17.1showsthatthefixedeffectsestimatorforÎ²canbecalculatedbyleastsquaresestimation of(17.69).ThusthedummiesD shouldbeviewedasincludedexogenousvariables.",
    "page": 647,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 628 Consider 2SLS estimation of Î² using the instruments Z for X. Since D is an included exogenous variableitshouldalsobeusedasaninstrument.Thus2SLSestimationofthefixedeffectsmodel(17.68) isalgebraically2SLSoftheregression(17.69)ofY on(X,D)usingthepair(Z,D)asinstruments. SincethedimensionofDcanbeexcessivelylarge,asdiscussedinSection17.11,itisadvisabletouse residualregressiontocomputethe2SLSestimatoraswenowdescribe. InSection12.12, wedescribedseveralalternativerepresentationsfor the2SLSestimator. The fifth (equation(12.32))showsthatthe2SLSestimatorforÎ²equals Î² (cid:98)2sls = (cid:179) X (cid:48) M D Z (cid:161) Z (cid:48) M D Z (cid:162)â1 Z (cid:48) M D X (cid:180)â1(cid:179) X (cid:48) M D Z (cid:161) Z (cid:48) M D Z (cid:162)â1 Z (cid:48) M D Y (cid:180) whereM =I âD (cid:161) D (cid:48) D (cid:162)â1 D (cid:48) . Thelatteristhematrixwithinoperator,thusM Y =YË,M X =XË,and D n D D M Z =ZË.Itfollowsthatthe2SLSestimatoris D (cid:181) (cid:48) (cid:179) (cid:48) (cid:180)â1 (cid:48) (cid:182)â1(cid:181) (cid:48) (cid:179) (cid:48) (cid:180)â1 (cid:48) (cid:182) Î² (cid:98)2sls = XË ZË ZË ZË ZË XË XË ZË ZË ZË ZË YË . Thisisconvenient. Itshowsthatthe2SLSestimatorforthefixedeffectsmodelcanbecalculatedby applying2SLStothewithin-transformedY it ,X it ,andZ it .The2SLSresidualsare (cid:98) e=YËâXËÎ² (cid:98)2sls . ThisestimatorcanbeobtainedusingtheStatacommandxtivreg fe.Itcanalsobeobtainedusing theStatacommandivregressaftermakingthewithintransformations. Thepresentationabovefocusedforclarityontheone-wayfixedeffectsmodel.Thereisnosubstantial changeinthetwo-wayfixedeffectsmodel Y =X (cid:48) Î²+u +v +Îµ . it it i t it Theeasiestwaytoestimatethetwo-waymodelistoaddT â1time-perioddummiestotheregression modelandincludethesedummyvariablesasbothregressorsandinstruments. 17.29 IdentificationwithInstrumentalVariables TounderstandtheidentificationofthestructuralslopecoefficientÎ²inthefixedeffectsmodelitis necessarytoexaminethereducedformequationfortheendogenousregressorsX .Thisis it X =ÎZ +W +Î¶ it it i it whereW isakÃ1vectoroffixedeffectsforthek regressorsandÎ¶ isanidiosyncraticerror. i it ThecoefficientmatrixÎisthelineareffectofZ onX holdingthefixedeffectsW constant. Thus it it i ÎhasasimilarinterpretationasthecoefficientÎ²inthefixedeffectsregressionmodel. Itistheeffectof thevariationinZ aboutitsindividual-specificmeanonX . it it The2SLSestimatorisafunctionofthewithintransformedvariables.Applyingthewithintransforma- tiontothereducedformwefindXË =ÎZË +Î¶Ë .ThisshowsthatÎistheeffectofthewithin-transformed it it it instruments on the regressors. If there is no time-variation in the within-transformed instruments or thereisnocorrelationbetweentheinstrumentsandtheregressorsafterremovingtheindividual-specific meansthenthecoefficientÎwillbeeithernotidentifiedorsingular. IneithercasethecoefficientÎ²will notbeidentified. Thusforidentificationofthefixedeffectsinstrumentalvariablesmodelweneed (cid:104) (cid:48) (cid:105) (cid:69) ZË ZË >0 (17.70) i i",
    "page": 648,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 629 and (cid:179) (cid:104) (cid:48) (cid:105)(cid:180) rank (cid:69) ZË XË =k. (17.71) i i Condition(17.70)isthesameastheconditionforidentificationinfixedeffectsregressionâtheinstru- ments must have full variation after the within transformation. Condition (17.71) is analogous to the relevance condition for identification of instrumental variable regression in the cross-section context butappliestothewithin-transformedinstrumentsandregressors. Condition(17.71)showsthattoexamineinstrumentvalidityinthecontextoffixedeffects2SLSitis importanttoestimatethereducedformequationusingfixedeffects(within)regression. Standardtests forinstrumentvalidity(F testsontheexcludedinstruments)canbeapplied.However,sincethecorrela- tionstructureofthereducedformequationisingeneralunknownitisappropriatetouseacluster-robust covariancematrix,clusteredattheleveloftheindividual. 17.30 AsymptoticDistributionofFixedEffects2SLSEstimator Inthissectionwepresentanasymptoticdistributiontheoryforthefixedeffectsestimator. Wepro- videaformaltheoryforthecaseofbalancedpanelsanddiscussanextensiontotheunbalancedcase. Weusethefollowingassumptionsforbalancedpanels. Assumption17.4 1. Y =X (cid:48) Î²+u +Îµ fori =1,...,N andt=1,...,T withT â¥2. it it i it 2. Thevariables(Îµ ,X ,Z ),i =1,...,N,areindependentandidenticallydis- i i i tributed. 3. (cid:69)[Z Îµ ]=0foralls=1,...,T. is it (cid:104) (cid:48) (cid:105) 4. Q =(cid:69) ZË ZË >0. ZZ i i 5. rank (cid:161) Q (cid:162)=k whereQ =(cid:69) (cid:104) ZË (cid:48) XË (cid:105) . ZX ZX i i 6. (cid:69)(cid:163)Îµ4 (cid:164)<â. it 7. (cid:69)(cid:107)X (cid:107)2<â. it 8. (cid:69)(cid:107)Z (cid:107)4<â. it GivenAssumption17.4wecanestablishasymptoticnormalityforÎ² (cid:98)2sls . (cid:112) Theorem17.4 UnderAssumption17.4,asN ââ, N (cid:161)Î² (cid:98)2sls âÎ²(cid:162)ââN (cid:161) 0,VÎ² (cid:162) d where VÎ² =(cid:161) Q (cid:48) ZX â¦â Z 1 Z Q ZX (cid:162)â1(cid:161) Q (cid:48) ZX â¦â Z 1 Z â¦ ZÎµ â¦â Z 1 Z Q ZX (cid:162)(cid:161) Q (cid:48) ZX â¦â Z 1 Z Q ZX (cid:162)â1 â¦ ZÎµ =(cid:69) (cid:104) ZË (cid:48) i Îµ i Îµ(cid:48) i ZË i (cid:105) .",
    "page": 649,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 630 The proof of the result is similar to Theorem 17.2 so is omitted. The key condition is Assumption 17.4.3,whichstatesthattheinstrumentsarestrictlyexogenousfortheidiosyncraticerrors.Theidentifi- cationconditionsareAssumptions17.4.4and17.4.5,whichwerediscussedintheprevioussection. Thetheoremisstatedforbalancedpanels. Forunbalancedpanelswecanmodifythetheoremasin Theorem17.3byaddingtheselectionindicatorss andreplacingAssumption17.4.3with(cid:69)[Îµ |Z ,s ]= i it i i 0,whichstatesthattheidiosyncraticerrorsaremeanindependentoftheinstrumentsandselection. IftheidiosyncraticerrorsÎµ arehomoskedasticandseriallyuncorrelatedthenthecovariancematrix it simplifiesto VÎ² =(cid:161) Q (cid:48) ZX â¦â Z 1 Z Q ZX (cid:162)â1Ï2 Îµ. In this case a classical homoskedastic covariance matrix estimator can be used. Otherwise a cluster- robustcovariancematrixestimatorcanbeused,andtakestheform V(cid:98)Î²(cid:98) = (cid:181) XË (cid:48) ZË (cid:179) ZË (cid:48) ZË (cid:180)â1 ZË (cid:48) XË (cid:182)â1(cid:179) XË (cid:48) ZË (cid:180)(cid:179) ZË (cid:48) ZË (cid:180)â1 (cid:195) (cid:88) N ZË (cid:48) i Îµ (cid:98)i Îµ (cid:98) (cid:48) i ZË i (cid:33) i=1 (cid:179) (cid:48) (cid:180)â1(cid:179) (cid:48) (cid:180) (cid:181) (cid:48) (cid:179) (cid:48) (cid:180)â1 (cid:48) (cid:182)â1 Ã ZË ZË ZË XË XË ZË ZË ZË ZË XË . As for the case of fixed effects regression, the heteroskedasticity-robust covariance matrix estimator is notrecommendedduetobiaswhenT issmall,andabias-correctedversionhasnotbeendeveloped. TheStatacommandxtivreg, febydefaultreportstheclassicalhomoskedasticcovariancematrix estimator.Toobtainacluster-robustcovariancematrixuseoptionvce(robust)orvce(cluster id). 17.31 LinearGMM Considerthejust-identified2SLSestimator.ItsolvestheequationZË (cid:48)(cid:161) YËâXËÎ²(cid:162)=0.Thesearesample analogsofthepopulationmomentcondition(cid:69) (cid:104) ZË (cid:48)(cid:161) YË âXË Î²(cid:162) (cid:105) =0. Thesepopulationconditionshold i i i atthetrueÎ²since ZË (cid:48) u =Z (cid:48) MDu =0sinceu liesinthenullspaceofD, and(cid:69) (cid:104) ZË (cid:48) Îµ (cid:105) =0isimpliedby i Assumption17.4.3. Thepopulationorthogonalityconditionsholdintheoveridentifiedcaseaswell.Inthiscaseanalter- nativeto2SLSisGMM.Letâ¦ (cid:98)i beanestimatorofW =(cid:69) (cid:104) ZË (cid:48) i Îµ i Îµ(cid:48) i ZË i (cid:105) ,forexample W(cid:99) = N 1 (cid:88) N ZË (cid:48) i Îµ (cid:98)i Îµ (cid:98) (cid:48) i ZË i (17.72) i=1 whereÎµ arethe2SLSfixedeffectsresiduals.TheGMMfixedeffectsestimatoris (cid:98)i Î² (cid:98)gmm = (cid:179) XË (cid:48) ZËW(cid:99) â1 ZË (cid:48) XË (cid:180)â1(cid:179) XË (cid:48) ZËW(cid:99) â1 ZË (cid:48) YË (cid:180) . (17.73) Theestimator(17.73)-(17.72)doesnothaveaStatacommandbutcanbeobtainedbygeneratingthe withintransformedvariablesXË,ZË andYË,andthenestimatingbyGMMaregressionofYË onXË usingZË asinstrumentsusingaweightmatrixclusteredbyindividual",
    "page": 650,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". (17.73) Theestimator(17.73)-(17.72)doesnothaveaStatacommandbutcanbeobtainedbygeneratingthe withintransformedvariablesXË,ZË andYË,andthenestimatingbyGMMaregressionofYË onXË usingZË asinstrumentsusingaweightmatrixclusteredbyindividual. 17.32 EstimationwithTime-InvariantRegressors One of the disappointments with the fixed effects estimator is that it cannot estimate the effect of regressorswhicharetime-invariant.Theyarenotidentifiedseparatelyfromthefixedeffectandareelim- inatedbythewithintransformation. Incontrast,therandomeffectsestimatorallowsfortime-invariant",
    "page": 650,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 631 regressorsbutdoessoonlybyassumingstrictexogeneitywhichisstrongerthantypicallydesiredineco- nomicapplications. Itturnsoutthatwecanconsideranintermediatecasewhichmaintainsthefixedeffectsassumptions forthetime-varyingregressorsbutusesstrongerassumptionsonthetime-invariantregressors. Forour expositionwewilldenotethetime-varyingregressorsbythekÃ1vector X andthetime-invariantre- it gressorsbythe(cid:96)Ã1vectorZ . i Considerthelinearregressionmodel Y =X (cid:48) Î²+Z (cid:48)Î³+u +Îµ . it it i i it Attheleveloftheindividualthiscanbewrittenas Y =X Î²+Z Î³+Ä± u +Îµ i i i i i i whereZ =Ä± Z (cid:48) .Forthefullsampleinmatrixnotationwecanwritethisas i i i Y =XÎ²+ZÎ³+u+Îµ. (17.74) WemaintaintheassumptionthattheidiosyncraticerrorsÎµ areuncorrelatedwithboth X and Z it it i atalltimehorizons: (cid:69)[X Îµ ]=0 (17.75) is it (cid:69)[Z Îµ ]=0. (17.76) i it InthissectionweconsiderthecasewhereZ isuncorrelatedwiththeindividual-levelerroru ,thus i i (cid:69)[Z u ]=0, (17.77) i i but the correlation of X and u is left unrestricted. In this context we say that Z is exogenous with it i i respect to the fixed effect u while X is endogenous with respect to u . Note that this is a different i it i typeofendogeneitythanconsideredinthesectionsoninstrumentalvariables:thereendogeneitymeant correlationwiththeidiosyncraticerrorÎµ .Hereendogeneitymeanscorrelationwiththefixedeffectu . it i We consider estimation of (17.74) by instrumental variables and thus need instruments which are uncorrelatedwiththeerroru +Îµ .Thetime-invariantregressorsZ satisfythisconditiondueto(17.76) i it i and(17.77),thus (cid:69)(cid:163) Z (cid:48)(cid:161) Y âX Î²âZ Î³(cid:162)(cid:164)=0. i i i i While the time-varying regressors X are correlated with u the within transformed variables XË are it i it uncorrelatedwithu +Îµ under(17.75),thus i it (cid:69) (cid:104) XË (cid:48)(cid:161) Y âX Î²âZ Î³(cid:162) (cid:105) =0. i i i i Therefore we can estimate (Î²,Î³) by instrumental variable regression using the instrument set (XË,Z). Specifically,regressionofY onX andZ treatingX asendogenous,Z asexogenous,andusingtheinstru- ment XË. Writethisestimatoras(Î² (cid:98),Î³ (cid:98) ). ThiscanbeimplementedusingtheStataivregresscommand afterconstructingthewithintransformedXË. Thisinstrumentalvariablesestimatorisalgebraicallyequaltoasimpletwo-stepestimator. Thefirst step Î² (cid:98) = Î² (cid:98)fe is the fixed effects estimator. The second step sets Î³ (cid:98) = (cid:161) Z (cid:48) Z (cid:162)â1(cid:161) Z (cid:48) u (cid:98) (cid:162) , the least squares coefficient from the regression of the estimated fixed effect u on Z . To see this equivalence observe (cid:98)i i thattheinstrumentalvariablesestimatorestimatorsolvesthesamplemomentequations XË (cid:48)(cid:161) Y âXÎ²âZÎ³(cid:162)=0 (17.78) Z (cid:48)(cid:161) Y âXÎ²âZÎ³(cid:162)=0. (17.79)",
    "page": 651,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 632 NoticethatXË (cid:48) Z =XË (cid:48) Ä± Z (cid:48)=0soXË (cid:48) Z =0. Thus(17.78)isthesameasXË (cid:48)(cid:161) Y âXÎ²(cid:162)=0whosesolutionis i i i i i Î² (cid:98)fe .Pluggingthisintotheleft-sideof(17.79)weobtain (cid:179) (cid:180) Z (cid:48)(cid:161) Y âXÎ² (cid:98)fe âZÎ³(cid:162)=Z (cid:48) Y âXÎ² (cid:98)fe âZÎ³ =Z (cid:48)(cid:161) u (cid:98) âZÎ³(cid:162) (cid:48) whereY andX arethestackedindividualmeansÄ± Y andÄ± X .Setequalto0andsolvingweobtainthe i i i i leastsquaresestimatorÎ³=(cid:161) Z (cid:48) Z (cid:162)â1(cid:161) Z (cid:48) u (cid:162) asclaimed. ThisequivalencewasfirstobservedbyHausman (cid:98) (cid:98) andTaylor(1981). Forstandarderrorcalculationitisrecommendedtoestimate(Î²,Î³)jointlybyinstrumentalvariable regression and use a cluster-robust covariance matrix clustered at the individual level. Classical and heteroskedasticity-robustestimatorsaremisspecifiedduetotheindividual-specificeffectu . i Theestimator(Î² (cid:98),Î³ (cid:98) )isaspecialcaseoftheHausman-Taylorestimatordescribedinthenextsection. (ForanunknownreasontheaboveestimatorcannotbeestimatedusingStataâsxthtaylorcommand.) 17.33 Hausman-TaylorModel HausmanandTaylor(1981)considerageneralizationofthepreviousmodel.Theirmodelis Y =X (cid:48) Î² +X (cid:48) Î² +Z (cid:48) Î³ +Z (cid:48) Î³ +u +Îµ it 1it 1 2it 2 1i 1 2i 2 i it where X and X are time-varying and Z and Z are time-invariant. Let the dimensions of X , 1it 2it 1i 2i 1it X ,Z ,andZ bek ,k ,(cid:96) ,and(cid:96) ,respectively. 2it 1i 2i 1 2 1 2 Writethemodelinmatrixnotationas Y =X Î² +X Î² +Z Î³ +Z Î³ +u+Îµ. (17.80) 1 1 2 2 1 1 2 2 Let X and X denote conformable matrices of individual-specific means and let XË = X âX and 1 2 1 1 1 XË =X âX denotethewithin-transformedvariables. 2 2 2 TheHausman-Taylormodelassumesthatallregressorsareuncorrelatedwiththeidiosyncraticerror Îµ atalltimehorizonsandthatX andZ areexogenouswithrespecttothefixedeffectu sothat it 1it 1i i (cid:69)[X u ]=0 1it i (cid:69)[Z u ]=0. 1i i TheregressorsX andZ ,however,areallowedtobecorrelatedwithu . 2it 2i i Set X = (X ,X ,Z ,Z ) and Î² = (cid:161)Î² ,Î² ,Î³ ,Î³ (cid:162) . The assumptions imply the following population 1 2 1 2 1 2 1 2 momentconditions (cid:69) (cid:104) XË (cid:48) (cid:161) Y âXÎ²(cid:162) (cid:105) =0 1 (cid:69) (cid:104) XË (cid:48) (cid:161) Y âXÎ²(cid:162) (cid:105) =0 2 (cid:69) (cid:104) X (cid:48) (cid:161) Y âXÎ²(cid:162) (cid:105) =0 1 (cid:69)(cid:163) Z (cid:48) (cid:161) Y âXÎ²(cid:162)(cid:164)=0. 1 Thereare2k +k +(cid:96) momentconditionsandk +k +(cid:96) +(cid:96) coefficients.Identificationrequiresk â¥(cid:96) : 1 2 1 1 2 1 2 1 2 thatthereareatleastasmanyexogenoustime-varyingregressorsasendogenoustime-invariantregres- sors.(Thisincludesthemodeloftheprevioussectionwherek =(cid:96) =0.) 1 2",
    "page": 652,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 633 GiventhemomentconditionsthecoefficientsÎ²=(cid:161)Î² ,Î² ,Î³ ,Î³ (cid:162) canbeestimatedby2SLSregression 1 2 1 2 of(17.80)usingtheinstrumentsZ =(XË ,XË ,X ,Z )orequivalentlyZ =(X ,XË ,X ,Z ).Thisis2SLSre- 1 2 1 1 1 2 1 1 gressiontreatingX andZ asexogenousandX andZ asendogenoususingtheexcludedinstruments 1 1 2 2 XË andX . 2 1 It is recommended to use cluster-robust covariance matrix estimation clustered at the individual level. Neitherconventionalnorheteroskedasticity-robustcovariancematrixestimatorsshouldbeused astheyaremisspecifiedduetotheindividual-specificeffectu . i Whenthemodelisjust-identifiedtheestimatorssimplifyasfollows. Î² (cid:98)1 andÎ² (cid:98)2 arethefixedeffects estimator. Î³ and Î³ equal the 2SLS estimator from a regression of u on Z and Z using X as an (cid:98)1 (cid:98)2 (cid:98)i 1i 2i 1i instrumentforZ .(SeeExercise17.14.) 2i Whenthemodelisover-identifiedtheequationcanalsobeestimatedbyGMMwithacluster-robust weightmatrixusingthesameequationsandinstruments. Thisestimatorwithcluster-robuststandarderrorscanbecalculatedusingtheStataivregress cluster(id) commandafterconstructingthetransformedvariablesXË andX . 2 1 The2SLSestimatordescribedabovecorrespondswiththeHausmanandTaylor(1981)estimatorin thejust-identifiedcasewithabalancedpanel. HausmanandTaylorderivedtheirestimatorunderthestrongerassumptionthattheerrorsÎµ and it u are strictly mean independent and homoskedastic and consequently proposed a GLS-type estima- i tor which is more efficient when these assumptions are correct. Define â¦=diag(â¦ ) where â¦ =I + i i i 1 i 1 (cid:48) i Ï2 u /Ï2 ÎµandÏ2 ÎµandÏ2 u arethevariancesoftheerrorcomponentsÎµ it andu i .Defineaswellthetrans- formedvariablesY(cid:101) =â¦â1/2Y,X(cid:101) =â¦â1/2X andZ(cid:101) =â¦â1/2Z.TheHausman-Taylorestimatoris Î² (cid:98)ht = (cid:179) X (cid:48)â¦â1Z (cid:161) Z (cid:48)â¦â1Z (cid:162)â1 Z (cid:48)â¦â1X (cid:180)â1(cid:179) X (cid:48)â¦â1Z (cid:161) Z (cid:48)â¦â1Z (cid:162)â1 Z (cid:48)â¦â1Y (cid:180) (cid:181) (cid:48) (cid:179) (cid:48) (cid:180)â1 (cid:48) (cid:182)â1(cid:181) (cid:48) (cid:179) (cid:48) (cid:180)â1 (cid:48) (cid:182) = X(cid:101) Z(cid:101) Z(cid:101) Z(cid:101) Z(cid:101) X(cid:101) X(cid:101) Z(cid:101) Z(cid:101) Z(cid:101) Z(cid:101) Y(cid:101) . Recallfrom(17.47)thatâ¦â1/2=M +Ï P whereÏ isdefinedin(17.46).Thus i i i i i Y(cid:101)i =Y i â(cid:161) 1âÏ i (cid:162) Y i X(cid:101)1i =X 1i â(cid:161) 1âÏ i (cid:162) X 1i X(cid:101)2i =X 2i â(cid:161) 1âÏ i (cid:162) X 2i Z(cid:101)1i =Ï i Z 1i Z(cid:101)2i =Ï i Z 2i X(cid:101)Ë =XË 1i 1i X(cid:101)Ë =XË . 2i 2i ItfollowsthattheHausman-Taylorestimatorcanbecalculatedby2SLSregressionofY(cid:101)i on(X(cid:101)1i ,X(cid:101)2i ,Ï i Z 1i ,Ï i Z 2i ) (cid:179) (cid:180) usingtheinstruments XË ,XË ,Ï X ,Ï Z . 1i 2i i 1i i 2i WhenthepanelisbalancedthecoefficientsÏ allequalandscaleoutfromtheinstruments",
    "page": 653,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". 2i 2i ItfollowsthattheHausman-Taylorestimatorcanbecalculatedby2SLSregressionofY(cid:101)i on(X(cid:101)1i ,X(cid:101)2i ,Ï i Z 1i ,Ï i Z 2i ) (cid:179) (cid:180) usingtheinstruments XË ,XË ,Ï X ,Ï Z . 1i 2i i 1i i 2i WhenthepanelisbalancedthecoefficientsÏ allequalandscaleoutfromtheinstruments. Thus i the estimator can be calculated by 2SLS regression of Y(cid:101)i on (X(cid:101)1i ,X(cid:101)2i ,Z 1i ,Z 2i ) using the instruments (cid:179) (cid:180) XË ,XË ,X ,Z . 1i 2i 1i 2i InpracticeÏ isunknown.Itcanbeestimatedasin(17.48)withthemodificationthattheerrorvari- i ance is estimated from the untransformed 2SLS regression. Under the homoskedasticity assumptions usedbyHausmanandTaylortheestimatorÎ² (cid:98)ht hasaclassicalasymptoticcovariancematrix.Whenthese assumptionsarerelaxedthecovariancematrixcanbeestimatedusingcluster-robustmethods.",
    "page": 653,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 634 TheHausman-Taylorestimatorwithcluster-robuststandarderrorscanbeimplementedinStataby the command xthtaylor vce(robust). This Stata command, for an unknown reason, requires that thereisatleastoneexogenoustime-invariantvariable((cid:96) â¥1)andatleastoneexogenoustime-varying 1 variable(k â¥1),evenwhenthemodelisidentified.Otherwise,theestimatorcanbeimplementedusing 1 theinstrumentalvariablemethoddescribedabove. The Hausman-Taylor estimator wasrefinedby Amemiya and MaCurdy (1986) and Breusch, Mizon andSchmidt(1989)whoproposedmoreefficientversionsusingadditionalinstrumentswhicharevalid understrongerorthogonalityconditions. Theobservationthatintheunbalancedcasetheinstruments shouldbeweightedbyÏ wasmadebyGardner(1998). i In the over-identified case it is unclear if it is preferred to use the simpler 2SLS estimator Î² (cid:98)2sls or theGLS-typeHausman-TaylorestimatorÎ² (cid:98)ht .TheadvantagesofÎ² (cid:98)ht arethatitisasymptoticallyefficient undertheirstatedhomoskedasticityandserialcorrelationconditionsandthatthereisanavailablepro- gram in Stata. The advantage of Î² (cid:98)2sls is that it is much simpler to program (ifdoing so yourself), may havebetterfinitesampleproperties(sinceitavoidsvariance-componentestimation),andisthenatural estimatorfromthethemodernGMMviewpoint. To illustrate, the final column of Table 17.2 contains Hausman-Taylor estimates of the investment modeltreatingQ itâ1 ,D itâ1 ,andT i asendogenousforu i andCF itâ1 andtheindustrydummiesasex- ogenous. Relative to the fixed effects models this allows estimation of the coefficients on the trading indicatorT .Themostinterestingchangerelativetothepreviousestimatesisthatthecoefficientonthe i tradingindicatorT doublesinmagnituderelativetotherandomeffectsestimate.Thisisconsistentwith i thehypothesisthatT iscorrelatedwiththefixedeffectandhencetherandomeffectsestimateisbiased. i 17.34 JackknifeCovarianceMatrixEstimation Asanalternativetoasymptoticinferencethedelete-clusterjackknifecanbeusedforcovariancema- trixcalculation.Inthecontextoffixedeffectsestimationthedelete-clusterestimatorstaketheform (cid:195) (cid:33)â1(cid:195) (cid:33) (cid:195) (cid:33)â1 N (cid:88) (cid:48) (cid:88) (cid:48) (cid:88) (cid:48) (cid:48) Î² (cid:98)(âi) = XË j XË j XË j YË j =Î² (cid:98)fe â XË i XË i XË i(cid:101) e i . j(cid:54)=i j(cid:54)=i i=1 where (cid:181) (cid:179) (cid:48) (cid:180)â1 (cid:48) (cid:182)â1 e = I âXË XË XË XË e (cid:101)i i i i i i (cid:98)i (cid:98) e i =YË i âXË i Î² (cid:98)fe . Thedelete-clusterjackknifeestimatorofthevarianceofÎ² (cid:98)fe is V(cid:98) j Î²(cid:98) ack= N N â1 i (cid:88) N =1 (cid:179) Î² (cid:98)(âi) âÎ² (cid:180)(cid:179) Î² (cid:98)(âi) âÎ² (cid:180)(cid:48) 1 (cid:88) N Î²= Î² (cid:98)(âi) . N i=1 jack Thedelete-clusterjackknifeestimatorV(cid:98) issimilartothecluster-robustcovariancematrixestimator. Î²(cid:98) ForparameterswhicharefunctionsÎ¸ (cid:98)fe =r(Î² (cid:98)fe )ofthefixedeffectsestimatorthedelete-clusterjack-",
    "page": 654,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 635 knifeestimatorofthevarianceofÎ¸ (cid:98)fe is V(cid:98) j Î¸(cid:98) ack= N N â1 i (cid:88) N =1 (cid:179) Î¸ (cid:98)(âi) âÎ¸ (cid:180)(cid:179) Î¸ (cid:98)(âi) âÎ¸ (cid:180)(cid:48) Î¸ (cid:98)(âi) =r(Î² (cid:98)(âi) ) 1 (cid:88) N Î¸= Î¸ (cid:98)(âi) . N i=1 TheestimatorV(cid:98) jack issimilartothedelta-methodcluster-robustcovariancematrixestimatorforÎ¸ (cid:98). Î¸(cid:98) Asinthecontextofi.i.d.samplesoneadvantageofthejackknifecovariancematrixestimatorsisthat they do not require the user to make a technical calculation of the asymptotic distribution. A down- sideisanincreaseincomputationcostasN separateregressionsareeffectivelyestimated. Thiscanbe particularlycostlyinmicropanelswhichhavealargenumberN ofindividuals. InStatajackknifestandarderrorsforfixedeffectsestimatorsareobtainedbyusingeitherxtreg fe vce(jackknife) or areg absorb(id) cluster(id) vce(jackknife) where id is the cluster vari- able.Forthefixedeffects2SLSestimatorusextivreg fe vce(jackknife). 17.35 PanelBootstrap Bootstrap methods can also be applied to panel data by a straightforward application of the pairs cluster bootstrap which samples entire individuals rather than single observations. In the context of paneldatawecallthisthepanelnonparametricbootstrap. ThepanelnonparametricbootstrapsamplesN individualhistories(Y ,X )tocreatethebootstrap i i sample. Fixed effects (or any other estimation method) is applied to the bootstrap sample to obtain thecoefficientestimates. ByrepeatingB times,bootstrapstandarderrorsforcoefficientsestimates,or functions of the coefficient estimates, can be calculated. Percentile-type and percentile-t confidence intervalscanbecalculated.TheBC intervalrequiresanestimatoroftheaccelerationcoefficientawhich a is a scaled jackknife estimate of the third moment of the estimator. In panel data the delete-cluster jackknifeshouldbeusedforestimationofa. InStata,toobtainbootstrapstandarderrorsandconfidenceintervalsuseeitherxtreg, vce(bootstrap, reps(#))orareg, absorb(id) cluster(id) vce(bootstrap, reps(#))whereidistheclustervari- ableand#isthenumberofbootstrapreplications.Forthefixedeffects2SLSestimatorusextivreg, fe vce(bootstrap, reps(#)). 17.36 DynamicPanelModels Themodelssofarconsideredinthischapterhavebeenstaticwithnodynamicrelationships.Inmany economiccontextsitisnaturaltoexpectthatbehavioranddecisionsaredynamic,explicitlydepending onpastbehavior.Inourinvestmentequation,forexample,economicmodelspredictthatafirmâsinvest- mentinanygivenyearwilldependoninvestmentdecisionsfrompreviousyears. Theseconsiderations leadustoconsiderexplicitlydynamicmodels. Theworkhorsedynamicmodelinapanelframeworkisthepth-orderautoregressionwithregressors andaone-wayerrorcomponentstructure.Thisis Y it =Î± 1 Y i,tâ1 +Â·Â·Â·+Î± p Y i,tâp +X i (cid:48) t Î²+u i +Îµ it . (17.81) where Î± are the autoregressive coefficients, X is a k vector of regressors, u is an individual effect, j it i and Îµ is an idiosyncratic error",
    "page": 655,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Theseconsiderations leadustoconsiderexplicitlydynamicmodels. Theworkhorsedynamicmodelinapanelframeworkisthepth-orderautoregressionwithregressors andaone-wayerrorcomponentstructure.Thisis Y it =Î± 1 Y i,tâ1 +Â·Â·Â·+Î± p Y i,tâp +X i (cid:48) t Î²+u i +Îµ it . (17.81) where Î± are the autoregressive coefficients, X is a k vector of regressors, u is an individual effect, j it i and Îµ is an idiosyncratic error. It is conventional to assume that the errors u and Îµ are mutually it i it",
    "page": 655,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 636 independentandtheÎµ areseriallyuncorrelatedandmeanzero. Forthepresentwewillassumethat it theregressorsX arestrictlyexogenous(17.17).InSection17.41wediscusspredeterminedregressors. it FormanyillustrationswewillfocusontheAR(1)model Y it =Î±Y i,tâ1 +u i +Îµ it (17.82) The dynamics should be interpreted individual-by-individual. The coefficient Î± in (17.82) equals the first-order autocorrelation. When Î± = 0 the series is serially uncorrelated (conditional on u ). Î± > 0 i meansY ispositivelyseriallycorrelated. Î±<0meansY isnegativelyseriallycorrelated. Anautore- it it gressive unit root holds when Î±=1, which means that Y follows a random walk with possible drift. it Sinceu isconstantforagivenindividualitshouldbetreatedasanindividual-specificintercept. The i idiosyncraticerrorÎµ playstheroleoftheerrorinastandardtimeseriesautoregression. it If|Î±|<1themodel(17.82)isstationary.Bystandardautoregressivebackwardsrecursionwecalculate that â â Y it = (cid:88) Î±j(u i +Îµ it )=(1âÎ±) â1u i + (cid:88) Î±jÎµ i,tâj . (17.83) j=0 j=0 Thusconditionalonu i themeanandvarianceofY it are(1âÎ±) â1u i and (cid:161) 1âÎ±2(cid:162)â1Ï2 Îµ,respectively. The kth autocorrelation(conditionalonu )isÎ±k. Noticethattheeffectofcross-sectionvariationinu isto i i shiftthemeanbutnotthevarianceorserialcorrelation. Thisimpliesthatifweviewtimeseriesplots ofY againsttimeforasetofindividualsi,theseriesY willappeartohavedifferentmeansbuthave it it similarvariancesandserialcorrelation. As with the case with time series data, serial correlation (large Î±) can proxy for other factors such astimetrends. Thusinapplicationsitwilloftenbeusefultoincludetimeeffectstoeliminatespurious serialcorrelation. 17.37 TheBiasofFixedEffectsEstimation Toestimatethepanelautoregression(17.81)itmayappearnaturaltousethefixedeffects(within) estimator. Indeed,thewithintransformationeliminatestheindividualeffectu . Thetroubleisthatthe i within operator induces correlation between the AR(1) lag and the error. The result is that the within estimatorisinconsistentforthecoefficientswhenT isfixed. AthoroughexplanationappearsinNickell (1981).WedescribethebasicprobleminthissectionfocusingontheAR(1)model(17.82). Applyingthewithinoperatorto(17.82)weobtain YË it =Î±YË itâ1 +ÎµË it fort â¥2. Asexpectedtheindividualeffectiseliminated. Thedifficultyisthat(cid:69)(cid:163) YË itâ1 ÎµË it (cid:164)(cid:54)=0sinceboth YË itâ1 andÎµË it arefunctionsoftheentiretimeseries. To see this clearly in a simple example, suppose we have a balanced panel with T =3. There are twoobservedpairs(Y it ,Y itâ1 )perindividualsothewithinestimatorequalsthedifferencedestimator. Applyingthedifferencingoperatorto(17.82)fort=3wefind âY =Î±âY +âÎµ . (17.84) i3 i2 i3 Becauseofthelaggeddependentvariableanddifferencingthereiseffectivelyoneobservationperindi- vidual.Noticethattheindividualeffecthasbeeneliminated",
    "page": 656,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". To see this clearly in a simple example, suppose we have a balanced panel with T =3. There are twoobservedpairs(Y it ,Y itâ1 )perindividualsothewithinestimatorequalsthedifferencedestimator. Applyingthedifferencingoperatorto(17.82)fort=3wefind âY =Î±âY +âÎµ . (17.84) i3 i2 i3 Becauseofthelaggeddependentvariableanddifferencingthereiseffectivelyoneobservationperindi- vidual.Noticethattheindividualeffecthasbeeneliminated. ThefixedeffectsestimatorofÎ±isequaltotheleastsquaresestimatorappliedto(17.84),whichis (cid:195) (cid:33)â1(cid:195) (cid:33) (cid:195) (cid:33)â1(cid:195) (cid:33) N N N N Î± = (cid:88) âY2 (cid:88) âY âY =Î±+ (cid:88) âY2 (cid:88) âY âÎµ . (cid:98)fe i2 i2 i3 i2 i2 i3 i=1 i=1 i=1 i=1",
    "page": 656,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 637 Thedifferencedregressoranderrorarenegativelycorrelated.Indeed (cid:69)[âY âÎµ ]=(cid:69)[(Y âY )(Îµ âÎµ )] i2 i3 i2 i1 i3 i2 =(cid:69)[Y Îµ ]â(cid:69)[Y Îµ ]â(cid:69)[Y Îµ ]+(cid:69)[Y Îµ ] i2 i3 i1 i3 i2 i2 i1 i2 =0â0âÏ2 Îµ +0 =âÏ2 Îµ. UsingthevarianceformulaforAR(1)models(assuming|Î±|<1)wecalculatethat(cid:69)(cid:163) (âY i2 )2(cid:164)=2Ï2 Îµ/(1+ Î±).ItfollowsthattheprobabilitylimitofthefixedeffectsestimatorÎ± ofÎ±in(17.84)is (cid:98)fe (cid:69)[âY âÎµ ] 1+Î± plim(Î± âÎ±)= i2 i3 =â . (17.85) Nââ (cid:98)fe (cid:69)(cid:163) (âY i2 )2(cid:164) 2 Itistypicaltocall(17.85)theâbiasâofÎ± ,thoughitistechnicallyaprobabilitylimit. (cid:98)fe Thebiasfoundin(17.85)islarge. ForÎ±=0thebiasisâ1/2andincreasestowards1asÎ±â1. Thus foranyÎ±<1theprobabilitylimitofÎ± isnegative!Thisisextremebias. (cid:98)fe FromNickellâs(1981)expressionsandsomealgebra,wecancalculatethattheprobabilitylimitofthe fixedeffectsestimatorfor|Î±|<1andgeneralT is 1+Î± plim(Î± âÎ±)= . (17.86) (cid:98)fe 2Î± Tâ1 Nââ â 1âÎ± 1âÎ±Tâ1 ItfollowsthatthebiasisoforderO(1/T). ItisoftenassertedthatitisokaytousefixedeffectsifT issufficientlylarge, e.g. T â¥30. However, from(17.86)wecancalculatethatforT =30thebiasofthefixedeffectsestimatorisâ0.056whenÎ±=0.5 and the bias is â0.15 when Î±=0.9. For T =60 and Î±=0.9 the bias is â0.05. These magnitudes are unacceptablylarge. Thisincludesthelongertimeseriesencounteredinmacropanels. ThustheNickell biasproblemappliestobothmicroandmacropanelapplications. Theconclusionfromthisanalysisisthatthefixedeffectsestimatorshouldnotbeusedformodels withlaggeddependentvariablesevenifthetimeseriesdimensionT islarge. 17.38 Anderson-HsiaoEstimator AndersonandHsiao(1982)madeanimportantbreakthroughbyshowingthatasimpleinstrumental variablesestimatorisconsistentfortheparametersof(17.81). Themethodfirsteliminatestheindividualeffectu byfirst-differencing(17.81)fortâ¥p+1 i âY it =Î± 1 âY i,tâ1 +Î± 2 âY i,tâ2 +Â·Â·Â·+Î± p âY i,tâp +âX i (cid:48) t Î²+âÎµ it . (17.87) Thiseliminatestheindividualeffectu . Thechallengeisthatfirst-differencinginducescorrelationbe- i tweenâY itâ1 andâÎµ it : (cid:69)(cid:163)âY i,tâ1 âÎµ it (cid:164)=(cid:69)(cid:163)(cid:161) Y i,tâ1 âY i,tâ2 (cid:162) (Îµ it âÎµ itâ1 ) (cid:164)=âÏ2 Îµ. TheotherregressorsarenotcorrelatedwithâÎµ it . For s >1,(cid:69)[âY itâs âÎµ it ]=0,andwhen X it isstrictly exogenous(cid:69)[âX âÎµ ]=0. it it ThecorrelationbetweenâY itâ1 andâÎµ it isendogeneity.Onesolutiontoendogeneityistouseanin- strument.Anderson-HsiaopointedoutthatY itâ2 isavalidinstrumentsinceitiscorrelatedwithâY i,tâ1 yetuncorrelatedwithâÎµ . it (cid:69)(cid:163) Y i,tâ2 âÎµ it (cid:164)=(cid:69)(cid:163) Y i,tâ2 Îµ it (cid:164)â(cid:69)(cid:163) Y i,tâ2 Îµ itâ1 (cid:164)=0. (17.88)",
    "page": 657,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 638 The Anderson-Hsiao estimator is IV using Y i,tâ2 as an instrument for âY i,tâ1 . Equivalently, this is IV usingtheinstruments(Y i,tâ2 ,...,Y i,tâpâ1 )for(âY i,tâ1 ,...,âY i,tâp ).TheestimatorrequiresT â¥p+2. Toshowthatthisestimatorisconsistent,forsimplicityassumewehaveabalancedpanelwithT =3, p=1,andnoregressors.InthiscasetheAnderson-HsiaoIVestimatoris (cid:195) (cid:33)â1(cid:195) (cid:33) (cid:195) (cid:33)â1(cid:195) (cid:33) N N N N (cid:88) (cid:88) (cid:88) (cid:88) Î± = Y âY Y âY =Î±+ Y âY Y âÎµ . (cid:98)iv i1 i2 i1 i3 i1 i2 i1 i3 i=1 i=1 i=1 i=1 Under the assumption that Îµ is serially uncorrelated, (17.88) shows that (cid:69)[Y âÎµ ] =0. In general, it i1 i3 (cid:69)[Y âY ](cid:54)=0.AsN ââ i1 i2 (cid:69)[Y âÎµ ] Î± ââÎ±â i1 i3 =Î±. (cid:98)iv p (cid:69)[Y âY ] i1 i2 ThustheIVestimatorisconsistentforÎ±. TheAnderson-HsiaoIVestimatorreliesontwocriticalassumptions. First,thevalidityoftheinstru- ment(uncorrelatednesswiththeequationerror)reliesontheassumptionthatthedynamicsarecorrectly specifiedsothatÎµ isseriallyuncorrelated.Forexample,manyapplicationsuseanAR(1).Ifinsteadthe it truemodelisanAR(2)thenY itâ2 isnotavalidinstrumentandtheIVestimateswillbebiased. Second, therelevanceoftheinstrument(correlatednesswiththeendogenousregressor)requires(cid:69)[Y âY ](cid:54)=0. i1 i2 ThisturnsouttobeproblematicandisexploredfurtherinSection17.40. Theseconsiderationssuggest thatthevalidityandaccuracyoftheestimatorarelikelytobesensitivetotheseunknownfeatures. 17.39 Arellano-BondEstimator Theorthogonalitycondition(17.88)isoneofmanyimpliedbythedynamicpanelmodel.Indeed,all lagsY itâ2 ,Y itâ3 ,...arevalidinstruments.IfT >p+2thesecanbeusedtopotentiallyimproveestimation efficiency. ThiswasfirstpointedoutbyHoltz-Eakin,NeweyandRosen(1988)andfurtherdevelopedby ArellanoandBond(1991). Usingtheseextrainstrumentshasacomplicationthatthereareadifferentnumberofinstruments foreachtimeperiod.ThesolutionistoviewthemodelasasystemofT equationsasinSection17.18. Itwillbeusefultofirstwritethemodelinvectornotation.Stackthedifferencedregressors(âY i,tâ1 ,...âY i,tâp ,âX i (cid:48) t ) intoamatrixâX andthecoefficientsintoavectorÎ¸.Wecanwrite(17.87)asâY =âX Î¸+âÎµ .Stacking i i i i allN individualsthiscanbewrittenasâY =âXÎ¸+âÎµ. Forperiodt =p+2wehavep+k validinstruments (cid:163) Y i1 ...,Y ip ,âX i,p+2 (cid:164) . Forperiodt =p+3there arep+1+kvalidinstruments (cid:163) Y i1 ...,Y ip+1 ,âX i,p+3 (cid:164) .Forperiodt=p+4therearep+2+kinstruments. Ingeneral,foranyt â¥p+2therearetâ2instruments (cid:163) Y i1 ,...,Y i,tâ2 ,âX it (cid:164) . Similarlyto(17.53)wecan definetheinstrumentmatrixforindividuali as ï£® (cid:104) (cid:105) ï£¹ Y ...,Y ,âX (cid:48) 0 0 i1 ip i,p+2 ï£¯ (cid:104) (cid:105) ï£º Z i = ï£¯ ï£¯ ï£¯ ï£¯ ï£¯ 0 Y i1 ...,Y i,p+1 ,âX i (cid:48) ,p+3 ... 0 ï£º ï£º ï£º ï£º ï£º",
    "page": 658,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Forperiodt =p+3there arep+1+kvalidinstruments (cid:163) Y i1 ...,Y ip+1 ,âX i,p+3 (cid:164) .Forperiodt=p+4therearep+2+kinstruments. Ingeneral,foranyt â¥p+2therearetâ2instruments (cid:163) Y i1 ,...,Y i,tâ2 ,âX it (cid:164) . Similarlyto(17.53)wecan definetheinstrumentmatrixforindividuali as ï£® (cid:104) (cid:105) ï£¹ Y ...,Y ,âX (cid:48) 0 0 i1 ip i,p+2 ï£¯ (cid:104) (cid:105) ï£º Z i = ï£¯ ï£¯ ï£¯ ï£¯ ï£¯ 0 Y i1 ...,Y i,p+1 ,âX i (cid:48) ,p+3 ... 0 ï£º ï£º ï£º ï£º ï£º . (17.89) ï£° (cid:104) (cid:105) ï£» 0 0 Y i1 ,Y i2 ,...,Y i,Tâ2 ,âX i (cid:48) ,T Thisis (cid:161) Tâpâ1 (cid:162)Ã(cid:96)where(cid:96)=k (cid:161) Tâpâ1 (cid:162)+(cid:161) (Tâ2)(Tâ1)â(cid:161) pâ2 (cid:162)(cid:161) pâ1 (cid:162)(cid:162) /2.Thisinstrumentmatrix consistsofalllaggedvaluesY i,tâ2 ,Y i,tâ3 ,...whichareavailableinthedatasetplusthedifferencedstrictly exogenousregressors. The(cid:96)momentconditionsare (cid:69)(cid:163) Z (cid:48) (âY ââX Î±) (cid:164)=0. (17.90) i i i",
    "page": 658,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 639 If T >p+2 then (cid:96)>p and the model is overidentified. Define the (cid:96)Ã(cid:96) covariance matrix for the momentconditions â¦=(cid:69)(cid:163) Z (cid:48)âÎµ âÎµ(cid:48) Z (cid:164) . i i i i LetZ denoteZ stackedintoa (cid:161) Tâpâ1 (cid:162) NÃ(cid:96)matrix.TheefficientGMMestimatorofÎ±is i Î± =(cid:161)âX (cid:48) Zâ¦â1Z (cid:48)âX (cid:162)â1(cid:161)âX (cid:48) Zâ¦â1Z (cid:48)âY (cid:162) . (cid:98)gmm IftheerrorsÎµ areconditionallyhomoskedasticthen it â¦=(cid:69)(cid:163) Z (cid:48) i HZ i (cid:164)Ï2 Îµ whereH isgivenin(17.31).Inthiscaseset N â¦ (cid:98)1 = (cid:88) Z (cid:48) i HZ i i=1 asa(scaled)estimateofâ¦.UndertheseassumptionsanasymptoticallyefficientGMMestimatoris Î± (cid:98)1 =(cid:161)âX (cid:48) Zâ¦ (cid:98) â 1 1Z (cid:48)âX (cid:162)â1(cid:161)âX (cid:48) Zâ¦ (cid:98) â 1 1Z (cid:48)âY (cid:162) . (17.91) Estimator(17.91)isknownastheone-stepArellano-BondGMMestimator. UndertheassumptionthattheerrorÎµ ishomoskedasticandseriallyuncorrelated, aclassicalco- it variancematrixestimatorforÎ± is (cid:98)1 V(cid:98) 0 1 =(cid:161)âX (cid:48) Zâ¦ (cid:98) â 1 1Z (cid:48)âX (cid:162)â1Ï (cid:98) 2 Îµ (17.92) whereÏ (cid:98) 2 Îµisthesamplevarianceoftheone-stepresidualsÎµ (cid:98)i =âY i ââX i Î± (cid:98) .Acovariancematrixestimator whichisrobusttoviolationoftheseassumptionsis V(cid:98)1 =(cid:161)âX (cid:48) Zâ¦ (cid:98) â 1 1Z (cid:48)âX (cid:162)â1(cid:161)âX (cid:48) Zâ¦ (cid:98) â 1 1Z (cid:48)â¦ (cid:98)2 Zâ¦ (cid:98) â 1 1Z (cid:48)âX (cid:162)(cid:161)âX (cid:48) Zâ¦ (cid:98) â 1 1Z (cid:48)âX (cid:162)â1 (17.93) where N â¦ (cid:98)2 = (cid:88) Z (cid:48) i Îµ (cid:98)i Îµ (cid:98) (cid:48) i Z i i=1 isa(scaled)cluster-robustestimatorofâ¦usingtheone-stepresiduals. Anasymptoticallyefficienttwo-stepGMMestimatorwhichallowsheterskedasticityis Î± (cid:98)2 =(cid:161)âX (cid:48) Zâ¦ (cid:98) â 2 1Z (cid:48)âX (cid:162)â1(cid:161)âX (cid:48) Zâ¦ (cid:98) â 2 1Z (cid:48)âY (cid:162) . (17.94) Estimator(17.94)isknownasthetwo-stepArellano-BondGMMestimator. Anappropriaterobustco- variancematrixestimatorforÎ± is (cid:98)2 V(cid:98)2 =(cid:161)âX (cid:48) Zâ¦ (cid:98) â 2 1Z (cid:48)âX (cid:162)â1(cid:161)âX (cid:48) Zâ¦ (cid:98) â 2 1Z (cid:48)â¦ (cid:98)3 Zâ¦ (cid:98) â 2 1Z (cid:48)âX (cid:162)(cid:161)âX (cid:48) Zâ¦ (cid:98) â 2 1Z (cid:48)âX (cid:162)â1 (17.95) where N â¦ (cid:98)3 = (cid:88) Z (cid:48) i Îµ (cid:98)i Îµ (cid:98) (cid:48) i Z i i=1 isa(scaled)cluster-robustestimatorofâ¦usingthetwo-stepresidualsÎµ =âY ââX Î± .Asymptotically, (cid:98)i i i(cid:98)2 V(cid:98)2 isequivalentto V(cid:101)2 =(cid:161)âX (cid:48) Zâ¦ (cid:98) â 2 1Z (cid:48)âX (cid:162)â1 . (17.96)",
    "page": 659,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 640 TheGMMestimatorcanbeiterateduntilconvergencetoproduceaniteratedGMMestimator. TheadvantageoftheArellano-BondestimatorovertheAnderson-HsiaoestimatoristhatwhenT > p+2theadditional(overidentified)momentconditionsreducetheasymptoticvarianceoftheestimator andstabilizeitsperformance.ThedisadvantageisthatwhenT islargeusingthefullsetoflagsasinstru- mentsmaycauseaâmanyweakinstrumentsâproblem.Theadvisedcompromiseistolimitthenumber oflagsusedasinstruments. Theadvantageoftheone-stepArellano-Bondestimatoristhattheweightmatrixâ¦ (cid:98)1 doesnotdepend onresidualsandisthereforelessrandomthanthetwo-stepweightmatrixâ¦ (cid:98)2 . Thiscanresultinbetter performance by the one-step estimator in small to moderate samples especially when the errors are approximately homoskedastic. The advantage of the two-step estimator is that it achieves asymptotic efficiencyallowingforheteroskedasticity andisthusexpectedtoperformbetterinlargesampleswith non-homoskedasticerrors. To summarize, the Arellano-Bond estimator applies GMM to the first-differenced equation (17.87) usingasetofavailablelagsY i,tâ2 ,Y i,tâ3 ,...asinstrumentsforâY i,tâ1 ,...,âY i,tâp . TheArellano-BondestimatormaybeobtainedinStatausingeitherthextabondorxtdpdcommand. Thedefaultsettingistheone-stepestimator(17.91)andnon-robuststandarderrors(17.92).Forthetwo- stepestimatorandrobuststandarderrorsusethetwostep vce(robust)options. Reportedstandard errors in Stata are based on Windmeijerâs (2005) finite-sample correction to the asymptotic estimator (17.96).Therobustcovariancematrix(17.95)northeiteratedGMMestimatorareimplemented. 17.40 WeakInstruments BlundellandBond(1998)pointedoutthattheAnderson-HsiaoandArellano-Bondestimatorssuffer fromweakinstruments.ThiscanbeseeneasiestintheAR(1)modelwiththeAnderson-Hsiaoestimator whichusesY i,tâ2 asaninstrumentforâY i,tâ1 .ThereducedformequationforâY itâ1 is âY i,tâ1 =Y i,tâ2 Î³+v it . The reduced form coefficient Î³ is defined by projection. Using âY i,tâ1 =(Î±â1)Y i,tâ2 +u i +Îµ i,tâ1 and (cid:69)(cid:163) Y itâ2 Îµ i,tâ1 (cid:164)=0wecalculatethat Î³= (cid:69)(cid:163) Y i,tâ2 âY i,tâ1 (cid:164) =(Î±â1)+ (cid:69)(cid:163) Y i,tâ2 u i (cid:164) . (cid:69)(cid:163) Y i 2 tâ2 (cid:164) (cid:69) (cid:104) Y i 2 ,tâ2 (cid:105) Assumingstationaritysothat(17.83)holds, (cid:69)(cid:163) Y i,tâ2 u i (cid:164)=(cid:69) (cid:34)(cid:195) 1 u â i Î± + (cid:88) â Î±jÎµ i,tâ2âj (cid:33) u i (cid:35) = 1 Ï â 2 u Î± j=0 and (cid:69) (cid:104) Y i 2 ,tâ2 (cid:105) =(cid:69) (cid:34)(cid:195) 1 u â i Î± + j (cid:88) â =0 Î±jÎµ itâ2âj (cid:33)2(cid:35) = (1â Ï2 u Î±)2 + (cid:161) 1â Ï2 Îµ Î±2 (cid:162) whereÏ2 u =(cid:69)(cid:163) u i 2(cid:164) andÏ2 Îµ =(cid:69)(cid:163)Îµ2 it (cid:164) . Usingtheseexpressionsandafairamountofalgebra,Blundelland Bond(1998)foundthatthereducedformcoefficientequals (cid:181) (cid:182) k Î³=(Î±â1) (17.97) k+Ï2 u /Ï2 Îµ wherek=(1âÎ±)/(1+Î±).",
    "page": 660,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 641 The Anderson-Hsiao instrument Y i,tâ2 is weak if Î³ is close to zero. From (17.97) we see that Î³=0 wheneitherÎ±=1(aunitroot)orÏ2 u /Ï2 Îµ =â(theidiosyncraticeffectissmallrelativetotheindividual- specificeffect). IneithercasethecoefficientÎ±isnotidentified. Weknowfromourearlierstudyofthe weakinstrumentsproblem(Section12.36)thatwhenÎ³isclosetozerothenÎ±isweaklyidentifiedand theestimatorswillperformpoorly. ThismeansthatwhentheautoregressivecoefficientÎ±islargeorthe individual-specific effect dominates the idiosyncratic effect these estimators will be weakly identified, have poor performance, and conventional inference methods will be misleading. Since the value of Î± andtherelativevariancesareunknownapriorithismeansthatweshouldgenericallytreatthisclassof estimatorsasweaklyidentified. AnalternativeestimatorwhichhasimprovedperformanceisdiscussedinSection17.42. 17.41 DynamicPanelswithPredeterminedRegressors Theassumptionthatregressorsarestrictlyexogenousisrestrictive. Alessrestrictiveassumptionis that the regressors are predetermined. Dynamic panel methods can be modified to handle predeter- minedregressorsbyusingtheirlagsasinstruments. Definition17.2 TheregressorX ispredeterminedfortheerrorÎµ if it it (cid:69)(cid:163) X i,tâs Îµ it (cid:164)=0 (17.98) forallsâ¥0. Thedifferencebetweenstrictlyexogenousandpredeterminedregressorsisthatfortheformer(17.98) holdsforalls notjustsâ¥0. Onewayofinterpretingaregressionmodelwithpredeterminedregressors isthatthemodelisaprojectiononthecompletepasthistoryoftheregressors. Under(17.98),leadsofX it canbecorrelatedwithÎµ it ,thatis(cid:69)[X it+s Îµ it ](cid:54)=0forsâ¥1,orequivalently X it canbecorrelatedwithlagsofÎµ ij ,thatis(cid:69)[X it Îµ itâs ](cid:54)=0forsâ¥1. Thismeansthat X it canrespond dynamicallytopastvaluesofY ,asin,forexample,anunrestrictedvectorautoregression. it Considerthedifferencedequation(17.87) âY it =Î± 1 âY i,tâ1 +Î± 2 âY i,tâ2 +Â·Â·Â·+Î± p âY i,tâp +âX i (cid:48) t Î²+âÎµ it . WhentheregressorsarepredeterminedbutnotstrictlyexogenousX andÎµ areuncorrelatedbutâX it it it andâÎµ arecorrelated.Toseethis, it (cid:69)[âX it âÎµ it ]=(cid:69)[X it Îµ it ]â(cid:69)(cid:163) X i,tâ1 Îµ it (cid:164)â(cid:69)(cid:163) X it Îµ i,tâ1 (cid:164)+(cid:69)(cid:163) X i,tâ1 Îµ i,tâ1 (cid:164) =â(cid:69)(cid:163) X it Îµ i,tâ1 (cid:164)(cid:54)=0. ThismeansthatifwetreatâX asexogenousthecoefficientestimateswillbebiased. it TosolvethecorrelationproblemwecanuseinstrumentsforâX it . AvalidinstrumentisX i,tâ1 since itisgenerallycorrelatedwithâX yetuncorrelatedwithâÎµ .Indeed,foranysâ¥1 it it (cid:69)(cid:163) X i,tâs âÎµ it (cid:164)=(cid:69)(cid:163) X i,tâs Îµ it (cid:164)â(cid:69)(cid:163) X i,tâs Îµ i,tâ1 (cid:164)=0. Consequently, Arellano and Bond (1991) recommend the instrument set (X i1 ,X i2 ,...,X itâ1 ). When the numberoftimeperiodsislargeitisadvisedtolimitthenumberofinstrumentlagstoavoidthemany weakinstrumentsproblem.",
    "page": 661,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 642 Algebraically,GMMestimationisthesameastheestimatorsdescribedinSection17.39,exceptthat theinstrumentmatrix(17.89)ismodifiedto ï£® (cid:104) (cid:105) ï£¹ (cid:48) (cid:48) Y ,...,Y ,X ,..,X 0 0 i1 ip i1 i,p+1 ï£¯ (cid:104) (cid:105) ï£º Z i = ï£¯ ï£¯ ï£¯ ï£¯ ï£¯ 0 Y i1 ,...,Y i,p+1 ,X i (cid:48) 1 ,..,X i (cid:48) ,p+2 ... 0 ï£º ï£º ï£º ï£º ï£º . ï£° (cid:104) (cid:105) ï£» (cid:48) (cid:48) 0 0 Y i1 ,...,Y i,Tâ2 ,X i1 ,..,X i,Tâ1 (17.99) Tounderstandhowthemodelisidentifiedweexaminethereducedformequationfortheregressor. Fort=p+2andusingthefirstlagasaninstrumentthereducedformis âX it =Î³ 1 Y i,tâ2 +Î 2 X i,tâ1 +Î¶ it . ThemodelisidentifiedifÎ isfullrank. Thisisvalid(ingeneral)when X isstationary. Identification 2 it fails,however,whenX hasaunitroot.Thisindicatesthatthemodelwillbeweaklyidentifiedwhenthe it predeterminedregressorsarehighlypersistent. Themethodgeneralizestohandlemultiplelagsofthepredeterminedregressors. Toseethis,write themodelexplicitlyas Y it =Î± 1 Y i,tâ1 +Â·Â·Â·+Î± p Y i,tâp +X i (cid:48) t Î² 1 +Â·Â·Â·+X i (cid:48) ,tâq Î² q +u i +Îµ it . Infirstdifferencesthemodelis âY it =Î± 1 âY i,tâ1 +Â·Â·Â·+Î± p âY i,tâp +âX i (cid:48) t Î² 1 +Â·Â·Â·+âX i (cid:48) ,tâq Î² q +âÎµ it . Asufficientsetofinstrumentsfortheregressorsare(X itâ1 ,âX i,tâ1 ,...,âX i,tâq )orequivalently(X i,tâ1 ,X i,tâ2 ,...,X i,tâqâ1 ). In many cases it is more reasonable to assume that X itâ1 is predetermined but not X it , since X it andÎµ maybeendogenous.This,forexample,isthestandardassumptioninvectorautoregressions.In it thiscasetheestimationmethodismodifiedtousetheinstruments(X i,tâ2 ,X i,tâ3 ,...,X i,tâqâ1 ).Whilethis weakenstheexogeneityassumptionitalsoweakenstheinstrumentsetasnowthereducedformusesthe secondlagX i,tâ2 topredictâX it . Theadvantageobtainedbytreatingaregressoraspredetermined(ratherthanstrictlyexogenous)is thatitisasubstantialrelaxationofthedynamicassumptions.Otherwisetheparameterestimateswillbe inconsistentduetoendogeneity. Themajordisadvantageoftreatingaregressoraspredeterminedisthatitsubstantiallyreducesthe strengthofidentificationespeciallywhenthepredeterminedregressorsarehighlypersistent. In Stata the xtabond command by default treats independent regressors as strictly exogenous. To treattheregressorsaspredeterminedusetheoptionpre.Bydefaultallregressorlagsareusedasinstru- ments,butthenumbercanbelimitedifspecified. 17.42 Blundell-BondEstimator ArellanoandBover(1995)andBlundellandBond(1998)introducedasetoforthogonalityconditions whichreducetheweakinstrumentproblemdiscussedintheSection17.40andimproveperformancein finitesamples. Consider the levels AR(1) model with no regressors (17.82). Recall, least squares (pooled) regres- sionisinconsistentbecausetheregressorY i,tâ1 iscorrelatedwiththeerroru i . Thisraisesthequestion: Is there an instrument Z it which solves this problem in the sense that Z it is correlated with Y i,tâ1 yet",
    "page": 662,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 643 uncorrelatedwithu it +Îµ it ? Blundell-BondproposetheinstrumentâY i,tâ1 . Clearly, âY i,tâ1 andY i,tâ1 arecorrelatedsoâY i,tâ1 satisfiestherelevancecondition.Also,âY i,tâ1 isuncorrelatedwiththeidiosyn- craticerrorÎµ whenthelatterisseriallyuncorrelated. ThusthekeytotheBlundell-Bondinstrumentis it whetherornot (cid:69)[âY itâ1 u i ]=0. (17.100) BlundellandBond(1998)showthatasufficientconditionfor(17.100)is (cid:104)(cid:179) u (cid:180) (cid:105) (cid:69) Y â i u =0. (17.101) i1 1âÎ± i Recall that u /(1âÎ±) is the conditional mean of Y under stationarity. Condition (17.101) states that i it thedeviationoftheinitialconditionY fromthisconditionalmeanisuncorrelatedwiththeindividual i1 effectu .Condition(17.101)isimpliedbystationaritybutissomewhatweaker. i Toseethat(17.101)implies(17.100),byapplyingrecursionto(17.87)wefindthat tâ3 âY i,tâ1 =Î±tâ3âY i2 + (cid:88) Î±jâÎµ i,tâ1âj . j=0 Also, (cid:179) u (cid:180) âY =(Î±â1)Y +u +Îµ =(Î±â1) Y â i +Îµ . i2 i1 i i2 i1 1âÎ± i2 Hence (cid:69)(cid:163)âY i,tâ1 u i (cid:164)=(cid:69) (cid:34)(cid:195) Î±tâ3(Î±â1) (cid:179) Y i1 â 1 u â i Î± (cid:180) +Î±tâ3Îµ i2 + t (cid:88) â3 Î±jâÎµ i,tâ1âj (cid:33) u i (cid:35) j=0 =Î±tâ3(Î±â1)(cid:69) (cid:104)(cid:179) Y â u i (cid:180) u (cid:105) i1 1âÎ± i =0 under(17.101),asclaimed. Nowconsiderthefullmodel(17.81)withpredeterminedregressors. Considertheassumptionthat theregressorshaveconstantcorrelationwiththeindividualeffect (cid:69)[X u ]=(cid:69)[X u ] it i is i foralls.Thisimplies (cid:69)[âX u ]=0 (17.102) it i whichmeansthatthedifferencedpredeterminedregressorsâX canalsobeusedasinstrumentsforthe it levelequation. Using(17.100)and(17.102)BlundellandBondproposethefollowingmomentconditionsforGMM estimation (cid:69)(cid:163)âY i,tâ1 (cid:161) Y it âÎ± 1 Y i,tâ1 âÂ·Â·Â·âÎ± p Y i,tâp âX i (cid:48) t Î²(cid:162)(cid:164)=0 (17.103) (cid:69)(cid:163)âX i,t (cid:161) Y it âÎ± 1 Y i,tâ1 âÂ·Â·Â·âÎ± p Y i,tâp âX i (cid:48) t Î²(cid:162)(cid:164)=0 (17.104) fort =p+2,...,T. Noticethattheseareforthelevels(undifferenced)equationwhiletheArellano-Bond (17.90)momentsareforthedifferencedequation(17.87). Wecanwrite(17.103)-(17.104)invectornota- tionifwesetZ 2i =diag(âY i2 ,...,âY iTâ1 ,âX i3 ,...,âX iT ).Then(17.103)-(17.104)equals (cid:69)[Z (Y âX Î¸)]=0. (17.105) 2i i i",
    "page": 663,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 644 Blundell and Bond proposed combining the (cid:96) Arellano-Bond moments with the levels moments. This can be done by stacking the moment conditions (17.90) and (17.105). Recall from Section 17.39 thevariablesâY ,âX ,and Z . DefinethestackedvariablesY =(cid:161)âY (cid:48) ,Y (cid:48)(cid:162)(cid:48) , X =(cid:161)âX (cid:48) ,X (cid:48)(cid:162)(cid:48) and Z = i i i i i i i i i i diag(Z ,Z ).Thestackedmomentconditionsare i 2i (cid:104) (cid:179) (cid:180)(cid:105) (cid:69) Z Y âX Î¸ =0. i i i TheBlundell-BondestimatorisfoundbyapplyingGMMtothisequation. Theycallthisasystems GMMestimator.LetY,X,andZ denoteY i ,X i ,andZ i stackedintomatrices.DefineH=diag(H,I Tâ2 ) whereH isfrom(17.31)andset (cid:88) N (cid:48) â¦ (cid:98)1 = Z i HZ i . i=1 TheBlundell-Bondone-stepGMMestimatoris Î¸ (cid:98)1 = (cid:179) X (cid:48) Zâ¦ (cid:98) â 1 1Z (cid:48) X (cid:180)â1(cid:179) X (cid:48) Zâ¦ (cid:98) â 1 1Z (cid:48) Y (cid:180) . (17.106) ThesystemsresidualsareÎµ (cid:98)i =Y i âX i Î¸ (cid:98)1 .Arobustcovariancematrixestimatoris V(cid:98)1 = (cid:179) X (cid:48) Zâ¦ (cid:98) â 1 1Z (cid:48) X (cid:180)â1(cid:179) X (cid:48) Zâ¦ (cid:98) â 1 1Z (cid:48) â¦ (cid:98)2 Zâ¦ (cid:98) â 1 1Z (cid:48) X (cid:180)(cid:179) X (cid:48) Zâ¦ (cid:98) â 1 1Z (cid:48) X (cid:180)â1 (17.107) where â¦ (cid:98)2 = (cid:88) N Z (cid:48) i Îµ (cid:98)i Îµ (cid:98) (cid:48) i Z i . i=1 TheBlundell-Bondtwo-stepGMMestimatoris Î¸ (cid:98)2 = (cid:179) X (cid:48) Zâ¦ (cid:98) â 2 1Z (cid:48) X (cid:180)â1(cid:179) X (cid:48) Zâ¦ (cid:98) â 2 1Z (cid:48) Y (cid:180) . (17.108) Thetwo-stepsystemsresidualsareÎµ (cid:98)i =Y i âX i Î¸ (cid:98)2 .Arobustcovariancematrixestimatoris V(cid:98)2 = (cid:179) X (cid:48) Zâ¦ (cid:98) â 2 1Z (cid:48) X (cid:180)â1(cid:179) X (cid:48) Zâ¦ (cid:98) â 2 1Z (cid:48) â¦ (cid:98)3 Zâ¦ (cid:98) â 2 1Z (cid:48) X (cid:180)(cid:179) X (cid:48) Zâ¦ (cid:98) â 2 1Z (cid:48) X (cid:180)â1 (17.109) where â¦ (cid:98)3 = (cid:88) N Z (cid:48) i Îµ (cid:98)i Îµ (cid:98) (cid:48) i Z i . i=1 Asymptotically,V(cid:98)2 isequivalentto V(cid:101)2 = (cid:179) X (cid:48) Zâ¦ (cid:98) â 2 1Z (cid:48) X (cid:180)â1 . (17.110) TheGMMestimatorcanbeiterateduntilconvergencetoproduceaniteratedGMMestimator. SimulationexperimentsreportedinBlundellandBond(1998)indicatethattheirsystemsGMMesti- matorperformssubstantiallybetterthantheArellano-Bondestimator,especiallywhenÎ±isclosetoone orthevarianceratioÏ2 u /Ï2 Îµislarge.Theexplanationisthattheorthogonalitycondition(17.103)doesnot suffertheweakinstrumentprobleminthesecases. The advantage of the Blundell-Bond estimator is that the added orthogonality condition (17.103) greatlyimprovesperformancerelativetotheArellano-Bondestimatorwhenthelatterisweaklyidenti- fied.AdisadvantageoftheBlundell-Bondestimatoristhattheirorthogonalityconditionisjustifiedbya stationaritycondition(17.101)andviolationofthelattermayinduceestimationbias",
    "page": 664,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". The advantage of the Blundell-Bond estimator is that the added orthogonality condition (17.103) greatlyimprovesperformancerelativetotheArellano-Bondestimatorwhenthelatterisweaklyidenti- fied.AdisadvantageoftheBlundell-Bondestimatoristhattheirorthogonalityconditionisjustifiedbya stationaritycondition(17.101)andviolationofthelattermayinduceestimationbias. Theadvantagesanddisadvantagesoftheone-stepversustwo-stepBlundell-Bondestimatorsarethe sameasdescribedfortheArellano-BondestimatorasdescribedinSection17.39.Alsoasdescribedthere",
    "page": 664,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 645 whenT islargeitmaybedesiredtolimitthenumberoflagstouseasinstrumentsinordertoavoidthe manyweakinstrumentsproblem. The Blundell-Bond estimator may be obtained in Stata using either the xtdpdsys or xtdpd com- mand. The default setting is the one-step estimator (17.106) and non-robust standard errors. For the two-stepestimatorandrobuststandarderrorsusethetwostep vce(robust)options. Statastandard errorsareWindmeijerâs(2005)finite-samplecorrectiontotheasymptoticestimate(17.110). Therobust covariancematrixestimator(17.109)northeiteratedGMMestimatorareimplemented. 17.43 ForwardOrthogonalTransformation ArellanoandBover(1995)proposedanalternativetransformationwhicheliminatestheindividual- specificeffectandmayhaveadvantagesindynamicpanelmodels. Theforwardorthogonaltransfor- mationis (cid:181) (cid:182) Y i â t =c it Y it â T 1 ât (cid:161) Y i,t+1 +Â·Â·Â·+Y iTi (cid:162) (17.111) i wherec2 =(T ât)/(T ât+1).Thiscanbeappliedtoallbutthefinalobservation(whichislost).Essen- it i i â tially,Y subtractsfromY theaverageoftheremainingvaluesandthenrescalessothatthevariance it it isconstantundertheassumptionofhomoskedasticerrors. Thetransformation (17.111)wasoriginally proposedfortime-seriesobservationsbyHayashiandSims(1983). AttheleveloftheindividualthiscanbewrittenasY â=A Y whereA isthe(T â1)ÃT orthogonal i i i i i i deviationoperator ï£® 1 â 1 â 1 Â·Â·Â· â 1 â 1 â 1 ï£¹ Ti â1 Ti â1 Ti â1 Ti â1 Ti â1 ï£¯ 0 1 â 1 Â·Â·Â· â 1 â 1 â 1 ï£º (cid:195)(cid:115) T â1 (cid:114) 1 (cid:33)ï£¯ ï£¯ . . T . i â2 T . i â2 T . i â2 T . i â2 ï£º ï£º A i =diag i ,..., ï£¯ ï£¯ . . . . . . . . . . . . ï£º ï£º . T i 2 ï£¯ ï£¯ 0 0 0 Â·Â·Â· 1 â1 â1 ï£º ï£º ï£° 2 2 ï£» . 0 0 . . 0 Â·Â·Â· 0 â1 1 Importantpropertiesofthematrix A arethat A 1 =0(soiteliminatesindividualeffects), A (cid:48) A =M , i i i i i i and A i A (cid:48) i =I Ti â1 .Thesecanbeverifiedbydirectmultiplication. Applyingthetransformation A to(17.81)weobtain i Y â=Î± Y â +Â·Â·Â·+Î± Y â +X â(cid:48)Î²+Îµâ . (17.112) it 1 i,tâ1 p i,tâp it it fort=p+1,...,Tâ1.Thisisequivalenttofirstdifferencing(17.87)whenT =3butdiffersforT >3. What is special about the transformed equation (17.112) is that under the assumption that Îµ are it seriallyuncorrelatedandhomoskedastictheerrorvectorÎµâ i hasvarianceÏ2 ÎµA i A (cid:48) i =Ï2 ÎµI Ti â1 .Thismeans thatÎµâ hasthesamecovariancestructureasÎµ .Thustheorthogonaltransformationoperatoreliminates i i thefixedeffectwhilepreservingthecovariancestructure.Thisisincontrastto(17.87)whichhasserially correlatederrorsâÎµ . it ThetransformederrorÎµâ it isafunctionofÎµ it ,Îµ it+1 ,...,Îµ iT . ThusvalidinstrumentsareY itâ1 ,Y itâ2 ,.... Usingtheinstrumentmatrix Z from(17.89)inthecaseofstrictlyexogenousregressorsor(17.99)with i predeterminedregressorsthe(cid:96)momentconditionscanbewrittenusingmatrixnotationas (cid:69)(cid:163) Z (cid:48)(cid:161) Y ââX âÎ¸(cid:162)(cid:164)=0. (17.113) i i i Definethe(cid:96)Ã(cid:96)covariancematrix â¦=(cid:69)(cid:163) Z (cid:48)ÎµâÎµâ(cid:48) Z (cid:164) . i i i i",
    "page": 665,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 646 IftheerrorsÎµ it areconditionallyhomoskedasticthenâ¦=(cid:69)(cid:163) Z (cid:48) i Z i (cid:164)Ï2 Îµ.Thusanasymptoticallyefficient GMM estimator is 2SLS applied to the orthogonalized equation using Z as an instrument. In matrix i notation, Î¸ (cid:98)1 = (cid:179) X â(cid:48) Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) X â (cid:180)â1 Y â Thisistheone-stepGMMestimator. GiventheresidualsÎµ (cid:98)i =Y â i âX â i Î¸ (cid:98)1 thetwo-stepGMMestimatorwhichisrobusttoheteroskedasticity andarbitraryserialcorrelationis Î¸ (cid:98)2 =(cid:161) X â(cid:48) Zâ¦ (cid:98) â 2 1Z (cid:48) X â(cid:162)â1(cid:161) X â(cid:48) Zâ¦ (cid:98) â 2 1Z (cid:48) Y â(cid:162) where N â¦ (cid:98)2 = (cid:88) Z (cid:48) i Îµ (cid:98)i Îµ (cid:98) (cid:48) i Z i . i=1 StandarderrorsforÎ¸ (cid:98)1 andÎ¸ (cid:98)2 canbeobtainedusingcluster-robustmethods. Forwardorthogonalizationmayhaveadvantagesoverfirstdifferencing. First,theequationerrorsin (17.112)haveascalarcovariancestructureunderi.i.d.idiosyncraticerrorswhichisexpectedtoimprove estimationprecision.Italsoimpliesthattheone-stepestimatoris2SLSratherthanGMM.Second,while therehasnotbeenaformalanalysisoftheweakinstrumentpropertiesoftheestimatorsafterforward orthogonalizationitappearsthatifT >p+2themethodislessaffectedbyweakinstrumentsthanfirst differencing. Thedisadvantagesofforwardorthogonalizationarethatittreatsearlyobservationsasym- metricallyfromlateobservations,itislessthoroughlystudiedthanfirstdifferencing,andisnotavailable withseveralpopularestimationmethods. The Stata command xtdpd includes forward orthogonalization as an option but not when levels (Blundell-Bond) instruments are included or if there are gaps in the data. An alternative is the down- loadableStatapackagextabond2. 17.44 EmpiricalIllustration We illustrate the dynamic panel methods with the investment model (17.3). Estimates from two models are presented in Table 17.3. Both are estimated by Blundell-Bond two-step GMM with lags 2 through6asinstruments,acluster-robustweightmatrix,andclusteredstandarderrors. The first column presents estimates of an AR(2) model. The estimates show that the series has a moderateamountofpositiveserialcorrelationbutappearstobewellmodeledasanAR(1)astheAR(2) coefficientisclosetozero.Thispatternofserialcorrelationisconsistentwiththepresenceofinvestment projectswhichspantwoyears. The second column presents estimates of the dynamic version of the investment regression (17.3) excludingthetradingindicator.Twolagsareincludedofthedependentvariableandeachregressor.The regressorsaretreatedaspredeterminedincontrasttothefixedeffectsregressionswhichtreatedthere- gressorsasstrictlyexogenous.Theregressorsarenotcontemporaneouswiththedependentvariablebut laggedoneandtwoperiods. Thisisdonesothattheyarevalidpredeterminedvariables. Contempora- neousvariablesarelikelyendogenoussoshouldnotbetreatedaspredetermined. The estimates in the second column of Table 17.3 complement the earlier results",
    "page": 666,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Thisisdonesothattheyarevalidpredeterminedvariables. Contempora- neousvariablesarelikelyendogenoussoshouldnotbetreatedaspredetermined. The estimates in the second column of Table 17.3 complement the earlier results. The evidence showsthatinvestmenthasamoderatedegreeofserialdependence,ispositivelyrelatedtothefirstlagof Q,andisnegativelyrelatedtolaggeddebt.Investmentappearstobepositivelyrelatedtochangeincash flow,ratherthanthelevel.Thusanincreaseincashflowinyeartâ1leadstoinvestmentinyeart. _____________________________________________________________________________________________",
    "page": 666,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 647 Table17.3:EstimatesofDynamicInvestmentEquation AR(2) AR(2)withRegressors 0.3191 0.2519 I itâ1 (0.0172) (0.0220) 0.0309 0.0137 I itâ2 (0.0112) (0.0125) 0.0018 Q itâ1 (0.0007) â0.0000 Q itâ2 (0.0003) â0.0154 D itâ1 (0.0058) â0.0043 D itâ2 (0.0054) 0.0400 CF itâ1 (0.0091) â0.0290 CF itâ2 (0.0051) Two-stepGMMestimates.Cluster-robuststandarderrorsinparenthesis. Allregressionsincludetimeeffects.GMMinstrumentsincludelags2through6. 17.45 Exercises Exercise17.1 (a) Show(17.11)and(17.12). (b) Show(17.13). Exercise17.2 Is(cid:69)[Îµ it |X it ]=0sufficientforÎ² (cid:98)fe tobeunbiasedforÎ²?Explainwhyorwhynot. Exercise17.3 Showthatvar (cid:163) XË (cid:164)â¤var[X ]. it it Exercise17.4 Show(17.24). Exercise17.5 Show(17.28). Exercise17.6 ShowthatwhenT =2thedifferencedestimatorequalsthefixedeffectsestimator. Exercise17.7 InSection17.14itisdescribedhowtoestimatetheindividual-effectvarianceÏ2 usingthe u betweenresiduals. DevelopanalternativeestimatorofÏ2 u onlyusingthefixedeffectserrorvarianceÏ (cid:98) 2 Îµ andthelevelserrorvarianceÏ (cid:98) 2 e =n â1(cid:80) i N =1 (cid:80) tâSi e (cid:98)i 2 t wheree (cid:98)it =Y it âX i (cid:48) t Î² (cid:98)fe arecomputedfromthelevels variables. Exercise17.8 VerifythatÏ (cid:98) 2 Îµdefinedin(17.37)isunbiasedforÏ2 Îµunder(17.18),(17.25)and(17.26).",
    "page": 667,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 648 Exercise17.9 Develop a version of Theorem 17.2 for the differenced estimator Î² (cid:98)â. Can you weaken Assumption17.2.3?Stateanappropriateversionwhichissufficientforasymptoticnormality. Exercise17.10 Show(17.57). Exercise17.11 (a) ForÏ2definedin(17.59)show(cid:69)(cid:163)Ï2|X (cid:164)=Ï2. (cid:98)i (cid:98)i i i (b) ForV(cid:101)fe definedin(17.58)show(cid:69)(cid:163) V(cid:101)fe |X (cid:164)=V fe . Exercise17.12 (a) Show(17.61). (b) Show(17.62). (c) ForV(cid:101)fe definedin(17.60)show(cid:69)(cid:163) V(cid:101)fe |X (cid:164)=V fe . Exercise17.13 Take the fixed effects model Y = X Î² +X2Î² +u +Îµ . A researcher estimates the it it 1 it 2 i it modelbyfirstobtainingthewithintransformedYË and XË andthenregressingYË on XË and XË2. Is it it it it it thecorrectestimationmethod?Ifnot,describethecorrectfixedeffectsestimator. Exercise17.14 InSection17.33verifythatinthejust-identifiedcasethe2SLSestimatorÎ² (cid:98)2sls simplifies asclaimed:Î² (cid:98)1 andÎ² (cid:98)2 arethefixedeffectsestimator.Î³ (cid:98)1 andÎ³ (cid:98)2 equalthe2SLSestimatorfromaregression ofuonZ andZ usingX asaninstrumentforZ . (cid:98) 1 2 1 2 Exercise17.15 InthisexerciseyouwillreplicateandextendtheempiricalworkreportedinArellanoand Bond(1991)andBlundellandBond(1998).Arellano-Bondgatheredadatasetof1031observationsfrom anunbalancedpanelof140U.K.companiesfor1976-1984andisinthedatafileAB1991onthetextbook webpage.Thevariableswewillbeusingarelogemployment(N),logrealwages(W),andlogcapital(K). Seethedescriptionfilefordefinitions. (a) EstimatethepanelAR(1)K it =Î±K itâ1 +u i +v t +Îµ it usingArellano-Bondone-stepGMMwithclus- teredstandarderrors.Notethatthemodelincludesyearfixedeffects. (b) Re-estimateusingBlundell-Bondone-stepGMMwithclusteredstandarderrors. (c) Explainthedifferenceintheestimates. Exercise17.16 Thisexerciseusesthesamedatasetasthepreviousquestion. BlundellandBond(1998) estimatedadynamicpanelregressionoflogemploymentN onlogrealwagesW andlogcapitalK.The followingspecification1usedtheArellano-Bondone-stepestimator,treatingW i,tâ1 andK i,tâ1 asprede- termined. N it = .7075 N i,tâ1 â .7088 W it + .5000 W i,tâ1 + .4660 K it â .2151 K i,tâ1 . (17.114) (.0842) (.1171) (.1113) (.1010) (.0859) Thisequationalsoincludedyeardummiesandthestandarderrorsareclustered. 1BlundellandBond(1998),Table4,column3.",
    "page": 668,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER17. PANELDATA 649 (a) Estimate(17.114)usingtheArellano-Bondone-stepestimatortreatingW andK asstrictlyex- it it ogenous. (b) Estimate(17.114)treatingW i,tâ1 andK i,tâ1 aspredetermindtoverifytheresultsin(17.114).What isthedifferencebetweentheestimatestreatingtheregressorsasstrictlyexogenousversusprede- termined? (c) EstimatetheequationusingtheBlundell-Bondone-stepsystemsGMMestimator. (d) Interpretthecoefficientestimatesviewing(17.114)asafirm-levellabordemandequation. (e) DescribetheimpactonthestandarderrorsoftheBlundell-Bondestimatesinpart(c)ifyouforget touseclustering.(Youdonothavetolistallthestandarderrors,butdescribethemagnitudeofthe impact.) Exercise17.17 UsethedatafileInvest1993onthetextbookwebpage. Youwillbeestimatingthepanel AR(1)D it =Î±D i,tâ1 +u i +Îµ it forD=debt/assets(thisisdebtainthedatafile).Seethedescriptionfilefor definitions. (a) EstimatethemodelusingArellano-BondtwostepGMMwithclusteredstandarderrors. (b) Re-estimateusingBlundell-BondtwostepGMM. (c) Experimentwithyourresults, tryingtwostepversusonestep, AR(1)versusAR(2), numberoflags usedasinstruments,andclassicalversusrobuststandarderrors. Whatmakesthemostdifference forthecoefficientestimates?Forthestandarderrors? Exercise17.18 UsethedatafileInvest1993onthetextbookwebpage.Youwillbeestimatingthemodel D it =Î±D i,tâ1 +Î² 1 I i,tâ1 +Î² 2 Q i,tâ1 +Î² 3 CF i,tâ1 +u i +Îµ it . Thevariablesaredebta,inva,vala,andcfainthedatafile.Seethedescriptionfilefordefinitions. (a) EstimatetheaboveregressionusingArellano-Bondtwo-stepGMMwithclusteredstandarderrors treatingallregressorsaspredetermined. (b) Re-estimateusingBlundell-Bondtwo-stepGMMtreatingallregressorsaspredetermined. (c) Experimentwithyourresults,tryingtwo-stepversusone-step,numberoflagsusedasinstruments, and classical versus robust standard errors. What makes the most difference for the coefficient estimates?Forthestandarderrors?",
    "page": 669,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Chapter 18 Difference in Differences 18.1 Introduction Oneofthemostpopularwaystoestimatetheeffectofapolicychangeisthemethodofdifference indifferences,oftencalledâdiffindiffsâ. Estimationistypicallyatwo-waypaneldataregressionwitha policyindicatorasaregressor.Clusteredvarianceestimationisgenerallyrecommendedforinference. Inordertointrepretadifferenceindifferenceestimateasapolicyeffecttherearethreekeycondi- tions.First,thattheestimatedregressionisthecorrectconditionalmean.Inparticular,thisrequiresthat alltrendsandinteractionsareproperlyincluded. Second,thatthepolicyisexogenousâitsatisfiescon- ditionalindependence. Third,therearenootherrelevantunincludedfactorscoincidentwiththepolicy change.Iftheseassumptionsaresatisfiedthedifferenceindifferenceestimandisavalidcausaleffect. 18.2 MinimumWageinNewJersey Themostwell-knownapplicationofthedifferenceindifferencemethodologyisCardandKrueger (1994) who investigated the impact of New Jerseyâs 1992 increase of the minimum hourly wage from $4.25to$5.05.Classicaleconomicsteachesthatanincreaseintheminimumwagewillleadtodecreases inemploymentandincreasesinprices.Toinvestigatethemagnitudeofthisimpacttheauthorssurveyed apanelof331fastfoodrestaurantsinNewJerseyduringtheperiod2/15/1992-3/4/1992(beforetheen- actmentoftheminimumwageincrease)andthenagainduringtheperiod11/5/1992-12/31/1992(after the enactment). Fast food restaurants were selected for investigation as they are a major employer of minimumwageemployees. Beforethechangeabout30%ofthesampledworkerswerepaidthemini- mumwageof$4.25. Table18.1:AverageEmploymentatFastFoodRestaurants NewJersey Pennsylvania Difference BeforeIncrease 20.43 23.38 2.95 AfterIncrease 20.90 21.10 0.20 Difference 0.47 â2.28 2.75 ThedatafileCK1994isextractedfromtheoriginalCard-Kruegerdatasetandispostedonthetext- bookwebpage. 650",
    "page": 670,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER18. DIFFERENCEINDIFFERENCES 651 Table18.1(firstcolumn)displaysthemeannumber1offull-timeequivalentemployees2atNewJer- seyfastfoodrestaurantsbeforeandaftertheminimumwageincrease. Beforetheincreasetheaverage numberofemployeeswas20.4. Aftertheincreasetheaveragenumberofemployeeswas20.9. Contrary tothepredictionsofconventionaltheoryemploymentslightlyincreased(by0.5employeesperrestau- rant)ratherthandecreased. Thisestimateâthechangeinemploymentâcouldbecalledadifferenceestimator. Itisthechange inemploymentcoincidentwiththechangeinpolicy.Adifficultyininterpretationisthatallemployment changeisattributedtothepolicy.Itdoesnotprovidedirectevidenceofthecounterfactualâwhatwould havehappenediftheminimumwagehadnotbeenincreased. Adifferenceindifferenceestimatorimprovesonadifferenceestimatorbycomparingthechangein thetreatmentsamplewithacomparablechangeinacontrolsample. CardandKruegerselectedeasternPennsylvaniafortheircontrolsample. Theminimumwagewas constantat$4.25anhourinthestateofPennsylvaniaduring1992. Atthebeginningoftheyearstarting wages at fast food restaurants in the two states were similar. The two areas (New Jersey and eastern Pennsylvania)sharefurthersimilarities.Anytrendsoreconomicshockswhichaffectonestatearelikely toaffectboth. ThereforeCardandKruegerarguedthatitisappropriatetotreateasternPennsylvaniaas acontrol. Thismeansthatintheabsenceofaminimumwageincreasetheyexpectedthesamechanges inemploymenttooccurinbothNewJerseyandeasternPennsylvania. Card and Krueger surveyed a panel of 79 fast food restaurants in eastern Pennsylvania simultane- ouslywhilesurveyingtheNewJerseyrestaurants. Theaveragenumberoffull-timeequivalentemploy- ees is displayed in the second column of Table 18.1. Before the policy change the average number of employeeswas23.4.Afterthepolicychangetheaveragenumberwas21.1.ThusinPennsylvaniaaverage employmentdecreasedby2.3employeesperrestaurant. Treating Pennsylvania as a control means comparing the change in New Jersey (0.5) with that in Pennsylvnia(â2.3). Thedifference(2.75employeesperrestaurant)isthedifference-in-differenceesti- mateoftheimpactoftheminimumwageincrease.Incompletecontradictiontoconventionaleconomic theorytheestimateindicatesanincreaseinemploymentratherthanadecrease.Thissurprisingestimate hasbeenwidelydiscussedamongeconomists3andthepopularpress. Itisconstructivetore-writetheestimatesfromTable18.1inregressionformat. LetY denoteem- it ploymentatrestauranti surveyedattime t. LetState beadummyvariableindicatingthestate, with i State =1 for New Jersey and State =0 for Pennsylvania. Let Time be a dummy variable indicating i i t the time period, with Time =0 for the period before the policy change and Time =1 for the period t t afterthepolicychange. LetD denoteatreatmentdummy,withD =1iftheminimumwageequals it it $5.05andD =0iftheminimumwageequals$4.25.Inthisapplicationitequalstheinteractiondummy it D =State Time",
    "page": 671,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". LetState beadummyvariableindicatingthestate, with i State =1 for New Jersey and State =0 for Pennsylvania. Let Time be a dummy variable indicating i i t the time period, with Time =0 for the period before the policy change and Time =1 for the period t t afterthepolicychange. LetD denoteatreatmentdummy,withD =1iftheminimumwageequals it it $5.05andD =0iftheminimumwageequals$4.25.Inthisapplicationitequalstheinteractiondummy it D =State Time . it i t Table18.1isasaturatedregressioninthetwodummyvariablesandcanthereforebewrittenasthe regressionequation Y =Î² +Î² State +Î² Time +Î¸D +Îµ . (18.1) it 0 1 i 2 t it it IndeedthecoefficientscanbewrittenintermsofTable18.1bythefollowingcorrespondence: 1Ourcalculationsdroprestaurantsiftheyweremissingthenumberoffull-typeequivalentemployeesineithersurvey. 2FollowingCardandKruegerfull-timeequivalentemployeesisdefinedasthesumofthenumberoffull-timeemployees, managers,andassistantmanagers,plusone-halfofthenumberofpart-timeemployees. 3Mosteconomistsdonottaketheestimateliterallyâtheydonotbelievethatincreasingtheminimumwagewillcause employmentincreases. Insteadithasbeeninterpretedasevidencethatsmallchangesintheminimumwagemayhaveonly minorimpactsonemploymentlevels.",
    "page": 671,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER18. DIFFERENCEINDIFFERENCES 652 NewJersey Pennsylvania Difference BeforeIncrease Î² +Î² Î² Î² 0 1 0 1 AfterIncrease Î² +Î² +Î² +Î¸ Î² +Î² Î² +Î¸ 0 1 2 0 2 1 Difference Î² +Î¸ Î² Î¸ 2 2 Weseethatthecoefficientsintheregression(18.1)correspondtointerpretabledifferenceanddiffer- enceindifferenceestimands.Î² isthedifferenceestimandoftheeffectofâNewJerseyvs.Pennsylvaniaâ 1 in the period before the policy change. Î² is the difference estimand of the time effect in the control 2 state. Î¸ is the difference in difference estimand â the change in New Jersey relative to the change in Pennsylvania. Ourestimateoftheregression(18.1)is Y = 23.4 â 2.9 State â 2.3 Time + 2.75 D +Îµ . (18.2) it i t it it (1.4) (1.5) (1.2) (1.34) Thestandarderrorsareclusteredbyrestaurant. AsexpectedthecoefficientÎ¸ (cid:98)onthetreatmentdummy preciselyequalsthedifferenceindifferenceestimatefromTable18.1. Thecoefficientestimatescanbe interpretedasdescribed. Thepre-changedifferencebetweenNewJerseyandPennsylvaniaisâ2.9and thetimeeffectisâ2.3. Thedifferenceindifferenceeffectis2.75. Thet-statistictotestthehypothesisof zeroeffectisjustabove2withanasymptoticp-valueof0.04. SincetheobservationsaredividedintothegroupsState =0andState =1andTime isequivalent i i t toatimeindexthisregressionisidenticaltoatwo-wayfixedeffectsregressionofY onD withstate it it andtimefixedeffects. Furthermore,sincetheregressorD doesnotvaryacrossindividualswithinthe it state this fixed effects regression is unchanged if restaurant-level fixed effects are included instead of statefixedeffects. (Restaurantfixedeffectsareorthogonaltoanyvariabledemeanedatthestatelevel. SeeExercise18.1.)Thustheaboveregressionisidenticaltothetwo-wayfixedeffectsregression Y =Î¸D +u +v +Îµ (18.3) it it i t it whereu isarestaurantfixedeffectandv isatimefixedeffect.Thesimplestmethodtoimplementthis i t isbyaone-wayfixedeffectsregressionwithtimedummies.Theestimatesare Y = 2.75 D â 2.3 Time +u +Îµ (18.4) it it t i it (1.34) (1.2) whichareidenticaltothepreviousregression. Equation(18.3)isthebasicdifference-in-differencemodel.Itisatwo-wayfixedeffectsregressionof theresponseY onabinarypolicyD .ThecoefficientÎ¸correspondstothedoubledifferenceinsample it it means and can be interpreted as the policy impact (also called the treatment effect) of D on Y. (We discussidentificationinthenextsection.)Ourpresentation(andtheCard-Kruegerexample)focuseson thebasiccaseoftwoaggregateunits(states)andtwotimeperiods. Theregressionformulation(18.3)is convenientasitcanbeeasilygeneralizedtoallowformultiplestatesandtimeperiods.Doingsoprovides moreconvincingevidenceofanidentifiedpolicyeffect. Theequation(18.3)canalsobegeneralizedby changingthetrendspecificationandbyusingacontinuoustreatmentvariable. AnothercommongeneralizationistoaugmenttheregressionwithcontrolsX .Thismodelis it Y =Î¸D +X (cid:48) Î²+u +v +Îµ (18.5) it it it i t it",
    "page": 672,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER18. DIFFERENCEINDIFFERENCES 653 Many empirical studies report estimates both of the basic model and regressions with controls. For examplewecouldaugmenttheCard-Kruegerregressiontoincludethevariablehoursopen,thenumber ofhoursadaytherestaurantisopen.Arestaurantwithlongerhourswilltendtohavemoreemployees. Y = 2.84 D â 2.2 Time + 1.2 hoursopen +u +Îµ . it it t it i it (1.31) (1.2) (0.4) Theestimatedeffectisthatarestaurantemploysanadditional1.2employeesforeachhouropenand thiseffectisstatisticallysignificant.Theestimatedtreatmenteffectisnotmeaningfullychanged. 18.3 Identification Considerthedifference-in-differenceequation(18.5)fori =1,...,N andt=1,...,T.Weareinterested inconditionsunderwhichthecoefficientÎ¸isthecausalimpactofthetreatmentD ontheoutcomeY . it it TheanswercanbefoundbyapplyingTheorem2.12fromSection2.30. In Section 2.30 we introduced the potential outcomes framework which writes the outcome as a functionofthetreatment,controls,andunobservables. Theoutcome(e.g. employmentatarestaurant) iswrittenasY =h(D,X,e)whereDistreatment(minimumwagepolicy),X arecontrols,andeisavector ofunobservedfactors. Model(18.5)specifiesthath(D,X,e)isseparableandlinearinitsargumentsand thattheunobservablesconsistofindividual-specific,time-specific,andidiosyncraticeffects. We now present sufficient conditions under which the coefficient Î¸ can be interpreted as a causal effect.Recallthetwo-waywithintransformation(17.65)andsetZÂ¨ =(cid:161) DÂ¨ ,XÂ¨(cid:48) (cid:162)(cid:48) . it it it Theorem18.1 Supposethefollowingconditionshold: 1. Y =Î¸D +X (cid:48) Î²+u +v +Îµ . it it it i t it 2. (cid:69)(cid:163) ZÂ¨ ZÂ¨(cid:48) (cid:164)>0. it it 3. (cid:69)[X Îµ ]=0forallt ands. it is 4. ConditionalonX ,X ,...,X therandomvariablesD andÎµ aresta- i1 i2 iT it is tisticallyindependentforallt ands. ThenthecoefficientÎ¸in(18.5)equalstheaveragecausaleffectforDonY con- ditionalonX. Condition1statesthattheoutcomeequalsthespecifiedlinearregressionmodelwhichisadditively separableintheobservables,individualeffect,andtimeeffect. Condition2statesthatthetwo-waywithintransformedregressorshaveanon-singulardesignmatrix. ThisrequiresthatallelementsofD andX varyacrosstimeandindividuals. it it Condition3isthestandardexogeneityasumptionforregressorsinafixed-effectsmodel. Condition4statesthatthetreatmentvariableisconditionallyindependentoftheidiosyncraticerror. Thisistheconditionalindependenceassumptionforfixedeffectsregression. ToshowTheorem18.1applythetwo-waywithintransformation(17.65)to(18.5).Weobtain YÂ¨ =Î¸DÂ¨ +XÂ¨(cid:48) Î²+ÎµÂ¨ . it it it it",
    "page": 673,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER18. DIFFERENCEINDIFFERENCES 654 UnderCondition2theprojectioncoefficients(Î¸,Î²)areuniquelydefinedandunderConditions3and4 theyequalthelinearregressioncoefficients. ThusÎ¸istheregressionderivativewithrespecttoD. Con- dition4impliesthatconditionalon XÂ¨ therandomvariablesDÂ¨ andÎµÂ¨ arestatisticallyindependent. it it is Theorem2.12showsthattheregressionderivativeÎ¸equalstheaveragecausaleffectasstated. The assumption that D and Îµ are independent is the fundamental exogeneity assumption. To in- trepret Î¸ as a treatment effect it is important that D is defined as the treatment and not simply as an interaction(timeandstate)dummy. Thisissubtle. Examineequation(18.5)recallingthatD isdefined asthetreatment(anincreaseintheminimumwage).InthisequationtheerrorÎµ containsallvariables it andeffectsnotincludedintheregression.ThusifthereareotherchangesinNewJerseywhicharecoin- cidentwiththeminimumwageincreasetheassumptionthatDandÎµareindependentmeansthatthose coincidentchangesareindependentofÎµandthusdonotaffectemployment. Thisisastrongassump- tion. Onceagain,Condition4statesthatallothereffectswhicharecoincidentwiththeminimumwage increasehavenoeffectonemployment. Withoutthisassumptionitwouldnotbepossibletoclaimthat thediff-in-diffregressionidentifiesthecausaleffectofthetreatment. Furthermore, independence of D and Îµ means that neither can be affected by the other. This it is meansthatthepolicy(treatment)wasnotenactedinresponsetoknowledgeabouttheresponsevari- ableineitherperiodanditmeansthattheoutcome(employment)didnotchangeinthefirstperiodin anticipationoftheupcomingpolicychange. ItisdifficulttoknowiftheexogeneityofDisareasonableassumption.Itissimilartoinstrumentexo- geneityininstrumentalvariableregression.Itsvalidityhingesonawell-articulatedstructuralargument. An empirical investigation based on a difference-in-difference specification needs to make an explicit caseforexogeneityofD similartothatforIVregression. InthecaseoftheCard-Kruegerapplicationtheauthorsarguethatthepolicywasexogeneousbecause itwasadoptedtwoyearsbeforetakingeffect. Atthetimeofthepassageofthelegislationtheeconomy wasinanexpansionbutbythetimeofadoptiontheeconomyhasslippedintorecession. Thissuggests thatitiscredibletoassumethatthepolicy decisionin1990wasnotaffectedby employmentlevelsin 1992. Furthermore, concernabouttheimpactoftheincreasedminimumwageduringarecessionled to a serious discussion about reversing the policy, meaning that there was uncertainty about whether or not the policy would actually be enacted at the time of the first survey. It thus seems credible that employmentdecisionsatthattimewerenotdeterminedinanticipationoftheupcomingminimumwage increase. Theauthorsdonotdiscuss,however,whetherornottherewereothercoincidenteventsintheNew JerseyorPennsylvaniaeconomiesduring1992whichcouldhaveaffectedemploymentdifferentiallyin thetwostates. Itseemsplausiblethattherecouldhavebeenmanysuchcoincidentevents. Thisseems tobethegreatestweaknessintheiridentificationargument",
    "page": 674,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". It thus seems credible that employmentdecisionsatthattimewerenotdeterminedinanticipationoftheupcomingminimumwage increase. Theauthorsdonotdiscuss,however,whetherornottherewereothercoincidenteventsintheNew JerseyorPennsylvaniaeconomiesduring1992whichcouldhaveaffectedemploymentdifferentiallyin thetwostates. Itseemsplausiblethattherecouldhavebeenmanysuchcoincidentevents. Thisseems tobethegreatestweaknessintheiridentificationargument. Identification(theconditionsforTheorem18.1)alsorequiresthattheregressionmodeliscorrectly specified. This means that the true model is linear in the specified variables and all interactions are included. Since the basic 2Ã2 specification is a saturated dummy variable model it is necessarily a conditionalmeanandthuscorrectlyspecified. Thisisnotthecaseinapplicationswithmorethantwo statesortimeperiodsandthusmodelspecificationneedstobecarefullyconsideredinsuchcases. 18.4 MultipleUnits The basic difference-in-difference model has two aggregate units (e.g. states) and two time peri- ods. Additionalinformationcanbeobtainediftherearemultipleunitsormultipletimeperiods. Inthis sectionwefocusonthecaseofmultipleunits. Therecanbemultipletreatmentunits,multiplecontrol units,orboth. InthissectionwesupposethatthenumberofperiodsisT =2. LetN â¥1bethenumber 1",
    "page": 674,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER18. DIFFERENCEINDIFFERENCES 655 ofuntreated(control)units,andN â¥1bethenumberoftreatedunits,withN =N +N . 2 1 2 Thebasicregressionmodel Y =Î¸D +u +v +Îµ it it i t it imposestwostrongrestrictions. First,thatallunitsareequallyaffectedbytimeasv iscommonacross t i.Second,thatthetreatmenteffectÎ¸iscommonacrossalltreatedunits. The Card-Krueger data set only contains observations from two states but the authors did record additionalvariablesincludingtheregionofthestate.TheydividedNewJerseyintothreeregions(North, Central,andSouth)andeasternPennsylvaniaintotworegions(1fornortheastPhiladelphiasuburbsand 2fortheremainder). Table18.2displaysthemeannumberoffull-timeequivalentemployeesbyregion,beforeandafter theminimumwageincrease. WeobservethattwoofthethreeNewJerseyregionshadnearlyidentical increasesinemploymentandallthreechangesaresmall. WecanalsoobservethatbothofthePennsyl- vaniaregionshademploymentdecreasesthoughwithdifferentmagnitudes. We can test the assumption of equal treatment effect Î¸ by a regression exclusion test. This can be donebyaddinginteractiondummiestotheregressionandtestingfortheexclusionoftheinteractions. AstherearethreetreatedregionsinNewJerseyweincludetwoofthethreeNewJerseyregiondummies interactedwiththetimeindex. IngeneralwewouldincludeN â1suchinteractions. Thesecoefficients 2 measurethetreatmenteffectdifferenceacrossregions. Testingthatthesetwocoefficientsarezerowe obtainap-valueof0.60whichisfarfromsignificant. Thusweacceptthehypothesisthatthetreatment effectÎ¸iscommonacrosstheNewJerseyregions. Incontrast, whenthetreatmenteffectÎ¸ varieswecallthisaheterogeneoustreatmenteffect. Itis notaviolationofthetreatmenteffectframeworkbutitcanbeconsiderablymorecomplicatedtoana- lyze.(Amodelwhichincorrectlyimposesahomogeneoustreatmenteffectismisspecifiedandproduces inconsistentestimates.) Amoreseriousproblemarisesifthecontroleffectisheterogeneous.Thecontroleffectisthechange inthecontrolgroup. Table18.2breaksdowntheestimatedcontroleffectacrossthetwoPennsylvania regions. Whilebothestimatesarenegativetheyaresomewhatdifferentfromoneanother. Iftheeffects aredistinctthereisnotahomogeneouscontroleffect.Wecantesttheassumptionofequalcontroleffects byaregressionexclusiontest. AstherearetwoPennsylvaniaregionsweincludetheinteractionofone ofthePennsylvaniaregionswiththetimeindex. (IngeneralwewouldincludeN â1interactions.) This 1 coefficientmeasuresthedifferenceinthecontroleffectacrosstheregions.Wetestthatthiscoefficientis zeroobtainingat-statisticof1.2andap-valueof0.23. Itisnotstatisticallysignificant,meaningthatwe cannotrejectthehypothesisthatthecontroleffectishomogeneous. In contrast, if the control effect were heterogeneous then the difference-in-difference estimation strategyismisspecified.Themethodreliesontheabilitytoidentifyacrediblecontrolsample.Therefore if a test for equal control effects rejects the hypothesis of homogeneous control effects this should be takenasevidenceagainstinterpretationofthedifference-in-differenceparameterasatreatmenteffect",
    "page": 675,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Itisnotstatisticallysignificant,meaningthatwe cannotrejectthehypothesisthatthecontroleffectishomogeneous. In contrast, if the control effect were heterogeneous then the difference-in-difference estimation strategyismisspecified.Themethodreliesontheabilitytoidentifyacrediblecontrolsample.Therefore if a test for equal control effects rejects the hypothesis of homogeneous control effects this should be takenasevidenceagainstinterpretationofthedifference-in-differenceparameterasatreatmenteffect. Table18.2:AverageEmploymentatFastFoodRestaurants SouthNJ CentralNJ NorthNJ PA1 PA2 BeforeIncrease 16.6 22.0 22.0 24.8 22.2 AfterIncrease 17.3 21.4 22.7 21.0 21.2 Difference 0.7 â0.6 0.7 â3.8 â1.0",
    "page": 675,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER18. DIFFERENCEINDIFFERENCES 656 18.5 DoPoliceReduceCrime? DiTella and Schargrodsky (2004) use a difference-in-difference approach to study the question of whetherthestreetpresenceofpoliceofficersreducescartheft. Rationalcrimemodelspredictthatthe thepresenceofanobservablepoliceforcewillreducecrimerates(atleastlocally)duetodeterrence.The causaleffectisdifficulttomeasure,however,aspoliceforcesarenotallocatedexogenously,butrather areallocatedinanticipationofneed. Adifference-in-differenceestimatorrequiresanexogenousevent whichchangespoliceallocations.TheinnovationinDiTella-Schargrodskywastousethepoliceresponse toaterroristattackasexogenousvariation. InJuly1994therewasahorrificterroristattackonthemainJewishcenterinBuenosAires,Argentina. WithintwoweeksthefederalgovernmentprovidedpoliceprotectiontoallJewishandMuslimbuildings in the country. DiTella and Schargrodsky (2004) hypothesized that their presence, while allocated to deteraterrororreprisalattack,wouldalsodeterotherstreetcrimessuchasautomobiletheftlocallyto thedeployedpolice.Theauthorscollecteddetailedinformationoncartheftsinselectedneighborhoods of Buenos Aires for April-December 1994, resulting in a panel for 876 city blocks. They hypothesized that the terrorist attack and the governmentâs response were exogenous to auto thievery and is thus a validtreatment. Theypostulatedthatthedeterrenceeffectwouldbestrongestforanycityblockwhich containedaJewishinstitution(andthuspoliceprotection).Potentialcarthiefswouldbedeterredfroma burglaryduetothethreatofbeingcaught.Thedeterrenceeffectwasexpectedtoweakenasthedistance fromtheprotectedsitesincreased. Theauthorsthereforeproposedadifference-in-differenceestimator basedontheaveragenumberofcartheftsperblock,beforeandaftertheterroristattack,andbetween cityblockswithandwithoutaJewishinstitution.Theirsamplehas37blockswithJewishinstitutions(the treatmentsample)and839blockswithoutaninstitution(thecontrolsample). ThedatafileDS2004isaslightlyrevisedversionoftheauthorâsAERreplicationfileandispostedon thetextbookwebpage. Table18.3:NumberofCarTheftsbyCityBlock SameBlock NotonSameBlock Difference April-June 0.112 0.095 â0.017 August-December 0.035 0.105 0.070 Difference â0.077 0.010 â0.087 Table18.3displaystheaveragenumberofcartheftsperblock,separatelyforthemonthsbeforethe JulyattackandthemonthsaftertheJulyattack,andseparatelyforcityblockswhichhaveaJewishinsti- tution(andthereforereceivedpoliceprotectionstartinginlateJuly)andforothercityblocks.Wecansee thattheaveragenumberofcartheftsdramaticallydecreasedintheprotectedcityblocks,from0.112per monthto0.035,whiletheaveragenumberinnon-protectedblockswasnear-constant,risingfrom0.095 to0.105.Takingthedifferenceindifferencewefindthattheeffectofpolicepresencedecreasedcarthefts by0.087,whichisabout78%",
    "page": 676,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Ageneralwaytoestimateadiff-in-diffmodelisaregressionoftheform(18.3)whereY isthenum- it berofcartheftsonblocki duringmontht,andu andv areblockandmonthfixedeffects.Thisregres- i t sion4yieldsthesameestimateof0.087sincethepanelisbalancedandtherearenocontrolvariables. Themodel(18.3)makesthestrongassumptionthatthetreatmenteffectisconstantacrossthefive treatedmonths.WeinvestigatethisassumptioninTable18.4whichbreaksdownthecartheftsbymonth. Forthecontrolsamplethenumberofcartheftsisnearconstantacrossthemonths.Forsevenoftheeight 4WeomittheobservationsforJulyasthecartheftdataisonlyforthefirsthalfofthemonth.",
    "page": 676,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER18. DIFFERENCEINDIFFERENCES 657 Table18.4:NumberofCarTheftsbyCityBlock SameBlock NotonSameBlock Difference Pre-Attack April 0.112 0.110 â0.012 May 0.088 0.100 0.012 June 0.128 0.076 â0.052 Post-Attack August 0.047 0.111 0.064 September 0.014 0.099 0.085 October 0.061 0.108 0.047 November 0.027 0.100 0.073 December 0.027 0.106 0.079 monthstheaveragenumberperblockrangesfrom0.10to0.11,withonlyonemonth(June)abitlower at0.08. Inthetreatmentsampletheaveragenumberoftheftsperblockinthethreemonthsbeforethe terrorist atack are similar to the averages in the control sample. But in the five months following the attackthenumberofcartheftsisuniformlyreduced. Theaveragesrangefrom0.014to0.061. Ineach month after the attack the control sample has lower thefts with averages ranging from 0.047 to 0.085. Giventhesmallsamplesize(37)ofthetreatmentsamplethisisstrikinglyuniformevidence. Wecanformallytestthehomogeneityofthetreatmenteffectbyincludingfourdummyvariablesfor theinteractionsoffourpost-attackmonthswiththetreatmentsampleandthentestingtheexclusionof thesevariables.Thep-valueforthistestis0.81,exceedinglyfarfromsignificant.Thusthereisnoreason inthedatatobesuspiciousofthehomogeneityassumption. Thegoalwastoestimatethecausaleffectofpolicepresenceasadeterrenceforcrime.Letusevaluate thecaseforidentification.Itseemsreasonabletotreattheterroristattackasexogenous.Thegovernment responsealsoappearsexogenous. Neitherisreasonablyrelatedtotheautotheftrate. Wealsoobserve thattheevidenceinTables18.3and18.4indicatesthattheftratesweresimilarinthepre-attacktreatment andcontrolsamples. Thustheadditionalpoliceprotectionseemscrediblyprovidedforthepurposeof attackpreventionratherthanasanexcuseforcrimeprevention. Thegeneralhomogeneityofthetheft rateacrossmonths,onceallowingforthetreatmenteffect,givescredibilitytotheclaimthatthepolice responsewasacausaleffect.Theterrorattackitselfdidnotreducecartheftratesasthereseemstobeno measurableeffectoutsideofthetreatmentsample. Finally,whilethepaperdoesnotexplicitlyaddress whetherornottherewasanyothercoincidenteventinJuly1994whichmayhaveeffectedthesespecific cityblocksitisdifficulttoconceiveofanalternativeexplanationforsuchalargeeffect. Ourconclusion isthatthisisastrongidentificationargument.Policepresencegreatlyreducestheincidenceofcartheft. Theauthorsassertedtheinferencethatpolicepresencedeterscrimemorebroadly.Thisisatenuous extensionasthepaperdoesnotprovidedirectevidenceofthisclaim. Whileitmayseemreasonablewe shouldbecautiousaboutmakinggeneralizationswithoutsupportingevidence. Overall, DiTella and Schargrodsky (2004) is an excellent example of a well-articulated and credibly identifieddifference-in-differenceestimateofanimportantpolicyeffect. 18.6 TrendSpecification Someapplications(includingthetwointroducedearlierinthischapter)applytoashortperiodof timesuchasoneyearinwhichcasewemaynotexpectthevariablestobetrended",
    "page": 677,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Whileitmayseemreasonablewe shouldbecautiousaboutmakinggeneralizationswithoutsupportingevidence. Overall, DiTella and Schargrodsky (2004) is an excellent example of a well-articulated and credibly identifieddifference-in-differenceestimateofanimportantpolicyeffect. 18.6 TrendSpecification Someapplications(includingthetwointroducedearlierinthischapter)applytoashortperiodof timesuchasoneyearinwhichcasewemaynotexpectthevariablestobetrended. Otherapplications covermanyyearsordecadesinwhichcasethevariablesarelikelytobetrended.Thesetrendcanreflect long-term growth, business cycle effects, changing tastes, or many other features. If trends are incor-",
    "page": 677,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER18. DIFFERENCEINDIFFERENCES 658 rectlyspecifiedthenthemodelwillbemisspecifiedandtheestimatedpolicyeffectwillbeinconsistent duetoomittedvariablebias.Considerthedifference-in-differenceequation(18.5).Thismodelimposes thestrongassumptionthatthetrendsinY areentirelyexplainedbytheincludedcontrolsX andthe it it commonunobservedtimecomponent v . Thiscanbequiterestrictive. Itisreasonabletoexpectthat t trendsmaydifferacrossunitsandarenotfullycapturedbyobservedcontrols. One way to think about this is in terms of overidentification. For simplicity suppose there are no controlsandthepanelisbalanced. ThenthereareNT observations. Thetwo-waymodelwithapolicy effecthasN+T coefficients. UnlessN =T =2thismodelisoveridentified. Inadditiontoconsidering heterogeneoustreatmenteffectsitisreasonabletoconsiderheterogeneoustrends. Onegeneralizationistoincludeinteractionsofalineartrendwithacontrolvariable.Thismodelis Y =Î¸D +X (cid:48) Î²+Z (cid:48)Î´t+u +v +Îµ . it it it i i t it ItspecifiesthatthetrendinY differsacrossunitsdependingonthecontrolsZ . it i Abroadergeneralizationistoincludeunit-specificlineartimetrends.Thismodelis Y =Î¸D +X (cid:48) Î²+u +v +tw +Îµ . (18.6) it it it i t i it Inthismodelw isatimetrendfixedeffectwhichvariesacrossunits.Iftherearenocontrolsthismodel i has2N+T coefficientsandisidentifiedaslongasT â¥4. Estimationofmodel(18.6)canbedoneoneofthreeways. If N issmall(forexample, applications withstate-leveldata)theregressioncanbeestimatedusingtheexplicitdummyvariableapproach. Let d andS bedummyvariablesindicatingtheith unitandtth timeperiod. Setd =d t,theinteraction i t it i oftheindividualdummywiththetimetrend.TheequationisestimatedbyregressionofY onD ,X , it it it d ,S ,andd .Equivalently,onecanapplyone-wayfixedeffectswithregressorsD ,X ,S ,andd . i t it it it t it When N islargeacomputationallymoreefficientapproachistouseresidualregression. Foreach uniti,estimateatimetrendmodelforeachvariableY ,D ,X andS .Thatis,foreachi estimate it it it t Y =Î± +Î± t+YË . it (cid:98)0 (cid:98)1 it Thisisageneralizedwithintransformation. TheresidualsYË areusedinplaceoftheoriginalobserva- it tions.RegressYË onDË ,XË ,andSË toobtaintheestimatesof(18.6). it it it t The relevance of the trend fixed effects v can be assessed by a significance test. Specifically, the t hypothesisthatthecoefficientsontheperioddummiesarezerocanbetestedusingastandardexclusion test. Similarly, trendinteractiontermscanbetestedforsignificanceusingstandardexclusiontests. If thetestsarestatisticallysignificantthisindicatesthattheirinclusionisrelevantforcorrectspecification. Unfortunately,theunit-specificlineartimetrendscannotbetestedforsignificancewhenthecovariance matrixisclusteredattheunitlevel.Thisissimilartotheproblemoftestingthesignificanceofadummy variablewithasingleobservation.Theunit-specifictimetrendscanonlybetestedforsignificanceifthe covariancematrixisclusteredatafinerlevel. Otherwisethecovariancematrixestimateissingularand biaseddownwards.NaÃ¯vetestswillover-statesignificance",
    "page": 678,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Unfortunately,theunit-specificlineartimetrendscannotbetestedforsignificancewhenthecovariance matrixisclusteredattheunitlevel.Thisissimilartotheproblemoftestingthesignificanceofadummy variablewithasingleobservation.Theunit-specifictimetrendscanonlybetestedforsignificanceifthe covariancematrixisclusteredatafinerlevel. Otherwisethecovariancematrixestimateissingularand biaseddownwards.NaÃ¯vetestswillover-statesignificance. 18.7 DoBlueLawsAffectLiquorSales? HistoricallymanyU.S.statesprohibitedorlimitedthesaleofalcoholicbeveragesonSundays.These laws are known as âblue lawsâ. In recent years these laws have been relaxed. Have these changes led toincreasedconsumptionofalcoholicbeverages? Bernheim,MeerandNovarro(2016)investigatedthis question using a detailed panel on alcohol consumption and sales hours. It is possible that observed changescoincidentwithchangesinthelawmightreflectunderlyingtrends.Thefactthatdifferentstates",
    "page": 678,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER18. DIFFERENCEINDIFFERENCES 659 changedtheirlawsduringdifferentyearsallowsforadifference-in-differencemethodologytoidentify thetreatmenteffect. Thepaperfocusesondistilledliquorsalesthoughwineandbeersalesarealsoincludedintheirdata. AnabridgedversionoftheirdatasetBMN2016ispostedonthetextbookwebpage. Liquorismeasured inpercapitagallonsofpureethanolequivalent. Thedataarestate-levelfor47U.S.statesfortheyears 1970-2007,unbalanced. TheauthorscarefullygatheredinformationontheallowablehoursthatalcoholcanbesoldonaSun- day. Theymakeadistinctionbetweenoff-premisesales(liquorstores,supermarkets)whereconsump- tionisoff-premise,andon-premisesales(restaurants,bars)whereconsumptionison-premise. LetY it denotethenaturallogarithmofper-capitaliquorsalesinstatei inyear t. Asimplifiedversionoftheir basicmodelis Y = 0.011 OnHours + 0.003 OffHours â 0.013 UR (18.7) it it it it (0.003) (0.003) (0.004) + 0.029 OnOutFlows â 0.000 OffOutFlows +u +v +Îµ . it it i t it (0.008) (0.010) OnHoursandOffHoursarethenumberofallowableSundayon-premisesandoff-premisessalehours. URisthestateunemplomentrate.OnOutFlows(OffOutFlows)istheweightednumberofon(off)-premises salehourslessthanneighborstates. Theseareaddedtoadjustforpossiblecross-bordertransactions. Themodelincludesbothstateandyearfixedeffects.Thestandarderrorsareclusteredbystate. Theestimatesindicatethatincreasedon-premisesalehoursleadtoasmallincreaseinliquorsales. Thisisconsistentwithalcoholbeingacomplementarygoodinsocial(restaurantandbar)settings. The smallandinsignificantcoefficientonOffHoursindicatesthatincreasedoff-premisesalehoursdoesnot leadtoanincreaseinliquorsales.Thisisconsistentwithrationalconsumerswhoadjusttheirpurchases toknownhours.Thenegativeeffectoftheunemploymentratemeansthatliquorsalesarepro-cyclical. The authors were concerned whether their dynamic and trend specifications were correctly speci- fiedsotriedsomealternativespecificationsandinteractions. Tounderstandthetrendissueweplotin Figure18.1thetime-seriespathofthelogofper-capitaliquorsalesforthreestates:California,Iowa,and NewYork. Youcanseethatallthreeexhibitadownwardtrendfrom1970untilabout1995andthenan increasingtrend. Theslopesofthethreetrends, however, arenotidentical. Thissuggeststhatthereis bothanationalcommoncomponentaswellasalocalizedcomponent. Ifweaugmentthebasicmodeltoincludestate-specificlineartrendstheestimatesareasfollows. Y = 0.000 OnHours + 0.002 OffHours â 0.015 UR (18.8) it it it it (0.002) (0.002) (0.004) + 0.005 OnOutFlows â 0.005 OffOutFlows +tw +u +v +Îµ . it it i i t it (0.005) (0.005) TheestimatedcoefficientforOnHoursdropstozeroandbecomesinsignificant",
    "page": 679,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Theslopesofthethreetrends, however, arenotidentical. Thissuggeststhatthereis bothanationalcommoncomponentaswellasalocalizedcomponent. Ifweaugmentthebasicmodeltoincludestate-specificlineartrendstheestimatesareasfollows. Y = 0.000 OnHours + 0.002 OffHours â 0.015 UR (18.8) it it it it (0.002) (0.002) (0.004) + 0.005 OnOutFlows â 0.005 OffOutFlows +tw +u +v +Îµ . it it i i t it (0.005) (0.005) TheestimatedcoefficientforOnHoursdropstozeroandbecomesinsignificant. Theotherestimatesdo notchangemeaningfully.Theauthorsonlydiscussthisregressioninafootnotestatingthataddingstate- specifictrendsâdemandsagreatdealfromthedataandleavestoolittlevariationtoidentifytheeffects ofinterest.âThisisanunfortunateclaimasactuallythestandarderrorshavedecreased,notincreased, indicatingthattheeffectsarebetteridentified.ThetroubleisthatOnHoursandOffHoursaretrendedand thetrendsvarybystate. Thismeansthatthesevariablesarecorrelatedwiththestate-trendinteraction.",
    "page": 679,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER18. DIFFERENCEINDIFFERENCES 660 4. 2. 0 2.- 4.- 6.- 8.- 1970 1980 1990 2000 2010 (a)California 4. 2. 0 2.- 4.- 6.- 8.- 1970 1980 1990 2000 2010 (b)Iowa 4. 2. 0 2.- 4.- 6.- 8.- 1970 1980 1990 2000 2010 (c)NewYork Figure18.1:LogofPer-CapitaLiquorSales Omittingthetrendinteractioninducedomittedvariablebias.Thatexplainswhythecoefficientestimates changewhenthetrendspecificationchanges. Bernheim,MeerandNovarro(2016)isanexcellentexampleofmeticulousempiricalworkwithcare- ful attention to detail and isolating a treatment strategy. It is also a good example of how attention to trendspecificationcanaffectresults. 18.8 CheckYourCode: DoesAbortionImpactCrime? In a highly-discussed paper, Donohue and Levitt (2001) used a difference-in-difference approach todevelopanunusualtheory. CrimeratesfelldramaticallythroughouttheUnitedStatesinthe1990s. Donohue and Levitt postulated that one contributing explanation was the landmark 1973 legalization ofabortion. Thelattermightaffectthecrimeratethroughtwopotentialchannels. First,itreducedthe cohortsizeofyoungmales.Second,itreducedthecohortsizeofyoungmalesatriskforcriminalbehav- ior. Thissuggeststhesubstantialincreaseinabortionsintheearly1970swilltranslateintoasubstantial reductionincrime20yearslater. Asyoumightimaginethispaperwascontroversialonseveraldimensions.Thepaperwasalsometic- ulousinitsempiricalanalysis,investigatingthepotentiallinksusingavarietyoftoolsanddifferinglevels ofgranularity.Themostdetailed-orientedregressionswerepresentedattheveryendofthepaperwhere theauthorsexploiteddifferencesacrossagegroups.Theseregressionstooktheform log(Arrests )=Î²Abortion +u +Î» +Î¸ +Îµ itb ib i tb it itb wherei,t,andbindexstate,year,andbirthcohort.Arrestsistherawnumberofarrestsforagivencrime andAbortionistheratioofabortionsperlivebirths. Theregressionincludesstatefixedeffects,cohort- yearinteractions,andstate-yearinteractions. Byincludingalltheseinteractioneffectstheregressionis estimatingatriple-difference,andisidentifyingtheabortionimpactonwithin-statecross-cohortvaria- tion,whichisamuchstrongeridentificationargumentthanasimplecross-statediff-in-diffregression. DonohueandLevittreportedanestimateofÎ²equallingâ0.028withasmallstandarderror. Basedon theseestimatesDonohueandLevittsuggestthatlegalizingabortionreducedcrimebyabout15-25%. Unfortunately,theirestimatescontainedanerror. InanattempttoreplicateDonohue-Levittâswork FooteandGoetz(2008)discoveredthatDonohue-Levittâscomputercodeinadvertentlyomittedthestate- yearinteractionsÎ¸ . ThiswasanimportantomissionaswithoutÎ¸ theestimatesarebasedonamix it it",
    "page": 680,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER18. DIFFERENCEINDIFFERENCES 661 ofcross-stateandcross-cohortvariationratherthanjustcross-cohortvariationasclaimed. Footeand Goetzre-estimatedtheregressionandfoundanestimateofÎ²equallingâ0.010. Whilestillstatistically differentfromzero,thereductioninmagnitudesubstantiallydecreasedtheestimatedimpact.Footeand Gootzincludemoreextensiveempiricalanalysisaswell. RegardlessoftheerrorsandpoliticalramificationstheDonohue-Levittpaperisaverycleverandcre- ativeuseofthedifference-in-differencemethod. Itisunfortunatethatthiscreativeworkwassomewhat overshadowedbyadebateovercomputercode. Ibelievetherearetwoimportantmessagesfromthisepisode.First,includetheappropriatecontrols! IntheDonohue-Levittregressiontheywerecorrecttoadvocatefortheregressionwhichincludesstate- year interactions as this allows the most precise measurement of the desired causal impact. Second, checkyourcode! Computationerrorsarepervasiveinappliedeconomicwork. Itisveryeasytomake errors; it is very difficult to clean them out of lengthy code. Errors in most papers are ignored as the detailsreceiveminorattention. Importantandinfluentialpapers,however,arescrutinized. Ifyouever aresoblessedastowriteapaperwhichreceivessignificantattentionyouwillfinditmostembarrassing ifacodingerrorisfoundafterpublication.Thesolutionistobepro-activeandvigilant. 18.9 Inference Manydifference-in-differenceapplicationsusehighlyaggregate(e.g. statelevel)databecausethey areinvestigatingtheimpactofpolicychangeswhichoccuratanaggregatelevel.Ithasbecomecustom- aryintherecentliteraturetouseclusteringmethodstocalculatestandarderrorswithclusteringapplied atahighlevelofaggregation. Tounderstandthemotivationforthischoiceitisusefultoreviewthetraditionalargumentforclus- teredvarianceestimation. Supposethattheerrore forindividuali ingroup g isindependentofthe ig regressors, has variance Ï2, and has correlation Ï across individuals within the group. If the number ofindividualsineachgroupisN thentheexactvarianceoftheleastsquaresestimator(recallequation (4.48))is V =(cid:161) X (cid:48) X (cid:162)â1Ï2(cid:161) 1+Ï(Nâ1) (cid:162) Î²(cid:98) asoriginallyderivedbyMoulton(1990). Thisinflatestheâusualâvariancebythefactor (cid:161) 1+Ï(Nâ1) (cid:162) . EvenifÏisverysmall,ifN ishugethenthisinflationfactorcanbelargeaswell. Theclusteredvarianceestimatorimposesnostructureontheconditionalvariancesandcorrelations withineachgroup.Itallowsforarbitraryrelationships.Theadvantageisthattheresultingvarianceesti- matorsarerobusttoabroadrangeofcorrelationstructures.Thedisadvantageisthattheestimatorscan be much less precise. Effectively, clustered variance estimators should be viewed as constructed from thenumberofgroups. IfyouareusingU.S.statesasyourgroups(asiscommonlyseeninapplications) then the number of groups is (at most) 51. This means that you are estimating the covariance matrix using51observationsregardlessofthenumberofâobservationsâinthesample",
    "page": 681,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Effectively, clustered variance estimators should be viewed as constructed from thenumberofgroups. IfyouareusingU.S.statesasyourgroups(asiscommonlyseeninapplications) then the number of groups is (at most) 51. This means that you are estimating the covariance matrix using51observationsregardlessofthenumberofâobservationsâinthesample. Oneimplicationisthat ifyouareestimatingmorethan51coefficientsthesamplecovariancematrixestimatorwillnotbefull rankwhichcaninvalidatepotentiallyrelevantinferencemethods. The case for clustered standard errors was made convincingly in an influential paper by Bertrand, Duflo,andMullainathan(2004). Theseauthorsdemonstratedtheirpointbytakingthewell-knownCPS datasetandthenaddingrandomlygeneratedregressors.Theyfoundthatifnon-clusteredvarianceesti- matorswereusedthenstandarderrorswouldbemuchtoosmallandaresearcherwouldinappropriately concludethattherandomlygeneratedâvariableâhasasignificanteffectinaregression. Thefalserejec- tionscouldbeeliminatedbyusingclusteredstandarderrors, clusteredatthestatelevel. Basedonthe recommendationsfromthispaper,researchersineconomicsnowroutinelyclusteratthestatelevel.",
    "page": 681,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER18. DIFFERENCEINDIFFERENCES 662 Therearelimitations,however.TaketheCard-Krueger(1994)exampleintroducedearlier.Theirsam- plehadonlytwostates(NewJerseyandPennsylvania). Ifthestandarderrorsareclusteredatthestate level then there are only two effective observations available for standard error calculation, which is muchtoofew. Forthisapplicationclusteringatthestatelevelisimpossible. Oneimplicationmightbe thatthiscastsdoubtsonapplicationsinvolvingjustahandfulofstates. Ifwecannotruleoutclustered dependence structures, and cannot use clustering methods due to the small number of states, then it maybeinappropriatetotrustthereportedstandarderrors. Anotherchallengeariseswhentreatment(D =1)appliestoonlyasmallnumberofunits.Themost it extremecaseiswherethereisonlyonetreatedunit. Thiscouldarise,forexample,whenyouareinter- estedinmeasuringtheeffectofapolicywhichonlyonestatehasadopted. Thissituationisparticularly treacherous and is algebraically identical to the problem of robust covariance matrix estimation with sparse dummy variables. (See Section 4.18.) As we learned from that analysis, in the extreme case of asingletreatedunittherobustcovariancematrixestimatorissingularandhighlybiasedtowardszero. Theproblemisbecausethevarianceofthesub-groupisestimatedfromasingleobservation. The same analysis applies to cluster-variance estimators. If there is a single treated unit then the standardclusteredcovariancematrixestimatorwillbesingular. Ifyoucalculateastandarderrorforthe sub-group mean it will be algebraically zero despite being the most imprecisely estimated coefficient. The treatment effect will have a non-zero reported standard error but it will be incorrect and highly biasedtowardszero. ForamoredetailedanalysisandrecommendationsforinferenceseeConleyand Taber(2011). _____________________________________________________________________________________________ 18.10 Exercises Exercise18.1 Inthetextitwasclaimedthatinabalancedsampleindividual-levelfixedeffectsareor- thogonaltoanyvariabledemeanedatthestatelevel. (a) Showthisclaim. (b) Doesthisclaimholdinunbalancedsamples? (c) Explainwhythisclaimimpliesthattheregressions Y =Î² +Î² State +Î² Time +Î¸D +Îµ it 0 1 i 2 t it it and Y =Î¸D +u +Î´ +Îµ it it i t it yieldidenticalestimatesofÎ¸. Exercise18.2 Inregression(18.1)withT =2andN =2supposethetimevariableisomitted. Thusthe estimatingequationis Y =Î² +Î² State +Î¸D +Îµ . it 0 1 i it it whereD =State Time isthetreatmentindicator. it i t (a) FindanalgebraicexpressionfortheleastsquaresestimatorÎ¸ (cid:98). (b) Show that Î¸ (cid:98)is a function only of the treated sub-sample and is not a function of the untreated sub-sample.",
    "page": 682,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER18. DIFFERENCEINDIFFERENCES 663 (c) IsÎ¸ (cid:98)adifference-in-differenceestimator? (d) UnderwhichassumptionsmightÎ¸ (cid:98)beanappropriateestimatorofthetreatmenteffect? Exercise18.3 Takethebasicdifference-in-differencemodel Y =Î¸D +u +Î´ +Îµ . it it i t it Instead of assuming that D and Îµ are independent, assume we have an instrumental variable Z it it it whichisindependentofÎµ butiscorrelatedwithD .DescribehowtoestimateÎ¸. it it Hint:ReviewSection17.28. Exercise18.4 For the specification tests of Section 18.4 explain why the regression test for homoge- neoustreatmenteffectsincludesonlyN â1interactiondummyvariablesratherthanallN interaction 2 2 dummies. AlsoexplainwhytheregressiontestforequalcontroleffectsincludesonlyN â1interaction 1 dummyvariablesratherthanallN interactiondummies. 1 Exercise18.5 AneconomistisinterestedintheimpactofWisconsinâs2011âAct10âlegislationonwages. (Forbackground,Act10reducedthepoweroflaborunions.) Shecomputesthefollowingstatistics5 for averagewageratesinWisconsinandtheneighboringstateofMinnesotaforthedecadesbeforeandafter Act10wasenacted. Years AverageWage Wisconsin 2001-2010 15.23 Wisconsin 2010-2020 16.72 Minnesota 2001-2010 16.42 Minnesota 2010-2020 18.10 (a) Basedonthisinformation,whatisherpointestimateoftheimpactofAct10onaveragewages? (b) The numbers in the above table were calculated as county-level averages. (The economist was giventheaveragewageineachcounty. Shecalculatedtheaverageforthestatebytakingtheaver- ageacrossthecounties.) Nowsupposethatsheestimatesthefollowinglinearregression,treating individualcountiesasobservations. wage=Î±+Î²Act10+Î³Wisconsin+Î´Post2010+e ThethreeregressorsaredummyvariablesforâAct10ineffectinthestateâ,âcountyisinWisconsinâ, andâtimeperiodis2011-2020.â WhatvalueofÎ² (cid:98)doesshefind? (c) WhatvalueofÎ³doesshefind? (cid:98) Exercise18.6 Use the datafile CK1994 on the textbook webpage. Classical economics teaches that in- creasingtheminimumwagewillincreaseproductprices. YoucanthereforeusetheCard-Kruegerdiff- in-diffmethodologytoestimatetheeffectofthe1992NewJerseyminimumwageincreaseonproduct prices. Thedatafilecontainsthevariablespriceentree,pricefryandpricesoda. Createthevariableprice asthesumofthesethree,indicatingthecostofatypicalmeal. 5Thisnumbersarecompletelyfictitious.",
    "page": 683,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER18. DIFFERENCEINDIFFERENCES 664 (a) Some values of price are missing. Delete these observations. This will produce an unbalanced panelaspricemaybemissingforonlyoneofthetwosurveys. Balancethepanelbydeletingthe pairedobservation.ThiscanbeaccomplishedinStatabythecommands: â¢ drop if price == . â¢ bys store: gen nperiods = [_N] â¢ keep if nperiods == 2 (b) CreateananalogofTable18.1butwiththepriceofamealratherthanthenumberofemployees. Interprettheresults. (c) Estimateananalogofregression(18.2)withpriceasthedependentvariable. (d) Estimateananalogofregression(18.4)withstatefixedeffectsandpriceasthedependentvariable. (e) Estimate an analog of regression (18.4) with restaurant fixed effects and price as the dependent variable. (f) Aretheresultsoftheseregressionsthesame? (g) CreateananalogofTable18.2forthepriceofameal.Interprettheresults. (h) Testforhomogeneoustreatmenteffectsacrossregions. (i) Testforequalcontroleffectsacrossregions. Exercise18.7 UsethedatafileDS2004onthetextbookwebpage.Theauthorsarguedthatanexogenous policepresencewoulddeterautomobiletheft. Theevidencepresentedinthechaptershowedthatcar theft was reduced for city blocks which received police protection. Does this deterrence effect extend beyondthesameblock? Thedatasethasthedummyvariableoneblockwhichindicatesifthecityblock isoneblockawayfromaprotectedinstitution. (a) CalculateananalogofTable18.3whichshowsthedifferencebetweencityblockswhichareone block away from a protected institution and those which are more than one block away from a protectedinstitution. (b) Estimatearegressionwithblockandmonthfixedeffectswhichincludestwotreatmentvariables: forcityblockswhichareonthesameblockasaprotectedinstitution,andforcityblockswhichare oneblockaway,bothinteractedwithapost-Julydummy.ExcludeobservationsforJuly. (c) Commentonyourfindings.Doesthedeterrenceeffectextendbeyondthesamecityblock? Exercise18.8 UsethedatafileBMN2016onthetextbookwebpage. Theauthorsreportresultsforliquor sales. Thedatafilecontainsthesameinformationforbeerandwinesales. Foreitherbeerorwinesales, estimate diff-in-diff models similar to (18.7) and (18.8) and interpret your results. Some relevant vari- ablesareid(stateidentification),year,unempw(unemploymentrate).Forbeertherelevantvariablesare logbeer (logofbeersales),beeronsun(numberofhoursofallowedon-premisesales),beeroffsun(num- berofhoursofallowedoff-premisesales),beerOnOutflows,beerOffOutflows.Forwinethevariableshave similarnames.",
    "page": 684,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Part V Nonparametric Methods 665",
    "page": 685,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Chapter 19 Nonparametric Regression 19.1 Introduction Wenowturntononparametricestimationoftheconditionalexpectationfunction (cid:69)[Y |X =x]=m(x). Unlessaneconomicmodelrestrictstheformofm(x)toaparametricfunctionm(x)cantakeanynonlin- earshapeandisthereforenonparametric. Inthischapterwediscussnonparametrickernelsmoothing estimatorsofm(x).ThesearerelatedtothenonparametricdensityestimatorsofChapter17ofIntroduc- tiontoEconometrics.InChapter20ofthistextbookweexploreestimationbyseriesandsievemethods. There are many excellent monographs written on nonparametric regression estimation, including HÃ¤rdle(1990),FanandGijbels(1996),PaganandUllah(1999),andLiandRacine(2007). Togetstarted,supposethatthereisasinglereal-valuedregressorX.Weconsiderthecaseofvector- valuedregressorslater.Thenonparametricregressionmodelis Y =m(X)+e (cid:69)[e|X]=0 (cid:69)(cid:163) e2|X (cid:164)=Ï2(X). Weassumethatwehaven observationsforthepair(Y,X). Thegoalistoestimatem(x)eitherata singlepointxoratasetofpoints.Formostofourtheorywefocusonestimationatasinglepointxwhich isintheinteriorofthesupportofX. In addition to the conventional regression assumptions we assume that both m(x) and f(x) (the marginaldensityof X)arecontinuousinx. Forourtheoreticaltreatmentweassumethattheobserva- tionsarei.i.d.Themethodsextendtodependentobservationsbutthetheoryismoreadvanced.SeeFan andYao(2003).WediscussclusteredobservationsinSection19.20. 19.2 BinnedMeansEstimator Forclarity,fixthepointx andconsiderestimationofm(x). ThisisthemeanofY forrandompairs (Y,X) such that X = x. If the distribution of X were discrete then we could estimate m(x) by taking theaverageofthesub-sampleofobservationsY forwhich X =x.Butwhen X iscontinuousthenthe i i probability is zero that X exactly equals x. So there is no sub-sample of observations with X =x and this estimation idea is infeasible. However, if m(x) is continuous then it should be possible to get a goodapproximationbytakingtheaverageoftheobservationsforwhichX isclosetox,perhapsforthe i 666",
    "page": 686,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 667 observationsforwhich|X âx|â¤hforsomesmallh>0.Asforthecaseofdensityestimationwecallha i bandwidth.Thisbinnedmeansestimatorcanbewrittenas n (cid:88)1 {|X âx|â¤h}Y i i m(x)= i=1 . (19.1) (cid:98) n (cid:88)1 {|X âx|â¤h} i i=1 Thisisanstepfunctionestimatoroftheregressionfunctionm(x). x y l l l l l l l ll l l l l ll l l l l l l lll l l l ll l ll l l l l l ll l l l lll l l l l l ll l l l l lll l l l l l l l l ll l l ll l l lll l ll l l l l l l ll l l l l l l l l l l l 0 1 2 3 4 5 6 7 8 9 10 5.1 0.1 5.0 0.0 5.0â Binned Means Rolling Binned NadarayaâWatson x (a)Nadaraya-Watson y l l l l l l l ll l l l l ll l l l l l l lll l l l ll l ll l l l l l ll l l l lll l l l l l ll l l l l lll l l l l l l l l ll l l ll l l lll l ll l l l l l l ll l l l l l l l l l l l 0 1 2 3 4 5 6 7 8 9 10 5.1 0.1 5.0 0.0 5.0â Binned Regression Rolling Binned Regression Local Linear Regression (b)LocalLinear Figure19.1:Nadaraya-WatsonandLocalLinearRegression Tovisualize,Figure19.1(a)displaysascatterplotof100randompairs(Y ,X )generatedbysimula- i i tion. Theobservationsaredisplayedastheopencircles. Theestimator(19.1)ofm(x)atx=1withh=1 istheaverageoftheY fortheobservationssuchthatX fallsintheinterval[0â¤X â¤2].Thisestimator i i i ism(1)andisshownonFigure19.1bythefirstsolidsquare. Werepeatthecalculation(19.1)forx =3, (cid:98) 5,7,and9,whichisequivalenttopartitioningthesupportofX intothebins[0,2],[2,4],[4,6],[6,8],and [8,10].ThesebinsareshowninFigure19.1(a)bytheverticaldottedlinesandtheestimates(19.1)bythe solidsquares. The binned estimator m(x) is the step function which is constant within each bin and equals the (cid:98) binned mean. In Figure 19.1(a) it is displayed by the horizontal dashed lines which pass through the solidsquares.Thisestimateroughlytracksthecentraltendencyofthescatteroftheobservations(Y ,X ). i i However,thehugejumpsattheedgesofthepartitionsaredisconcerting,counter-intuitive,andclearly anartifactofthediscretebinning. Ifwetakeanotherlookattheestimationformula(19.1)thereisnoreasonwhyweneedtoevaluate (19.1)onlyonacoursegrid.Wecanevaluatem(x)foranysetofvaluesofx.Inparticular,wecanevaluate (cid:98) (19.1)onafinegridofvaluesofx andtherebyobtainasmootherestimateoftheCEF.Thisestimatoris displayedinFigure19.1(a)withthesolidline. WecallthisestimatorâRollingBinnedMeansâ. Thisisa generalization of the binned estimator and by construction passes through the solid squares. It turns out that this is a special case of the Nadaraya-Watson estimator considered in the next section. This estimator,whilelessabruptthantheBinnedMeansestimator,isstillquitejagged.",
    "page": 687,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 668 19.3 KernelRegression Onedeficiencywiththeestimator(19.1)isthatitisastepfunctioninxevenwhenevaluatedonafine grid. ThatiswhyitsplotinFigure19.1isjagged. Thesourceofthediscontinuityisthattheweightsare discontinuousindicatorfunctions. Ifinsteadtheweightsarecontinuousfunctionsthenm(x)willalso (cid:98) becontinuousinx. Definition19.1 A(second-order)kernelfunctionK(u)satisfies 1. 0â¤K(u)â¤K <â, 2. K(u)=K(âu), 3. (cid:82)â K(u)du=1, ââ 4. (cid:82)â |u|rK(u)du<âforallpositiveintegersr. ââ Essentially, a kernel function is a bounded probability density function which is symmetric about zero. Assumption19.1.4isnotessentialformostresultsbutisaconvenientsimplificationanddoesnot excludeanykernelfunctionusedinstandardempiricalpractice.Someofthemathematicalexpressions aresimplifiedifwerestrictattentiontokernelswhosevarianceisnormalizedtounity. Definition19.2 Anormalizedkernelfunctionsatisfies (cid:82)â u2K(u)du=1. ââ TherearealargenumberoffunctionswhichsatisfyDefinition19.1, andmanyareprogrammedas optionsinstatisticalpackages. WelistthemostimportantinTable19.1below: theRectangular,Gaus- sian, Epanechnikov, andTriangularkernels. Inpracticeitisunnecessary toconsiderkernelsbeyond thesefour. FornonparametricregressionwerecommendeithertheGaussianorEpanechnikovkernel, andeitherwillgivesimilarresults.InTable19.1weexpressthekernelsinnormalizedform. FormorediscussiononkernelfunctionsseeChapter17ofIntroductiontoEconometrics. Ageneralizationof(19.1)isobtainedbyreplacingtheindicatorfunctionwithakernelfunction: (cid:88) n (cid:181) X i âx (cid:182) K Y i h m (x)= i=1 . (19.2) (cid:98)nw (cid:88) n (cid:181) X i âx (cid:182) K h i=1 The estimator (19.2) is known as the Nadaraya-Watson estimator, the kernelregression estimator, or thelocalconstantestimator,andwasintroducedindependentlybyNadaraya(1964)andWatson(1964). The rolling binned means estimator (19.1) is the Nadarya-Watson estimator with the rectangular kernel. The Nadaraya-Watson estimator (19.2) can be used with any standard kernel and is typically estimatedusingtheGaussianorEpanechnikovkernel. IngeneralwerecommendtheGaussiankernel sinceitproducesanestimatorm (x)whichpossessesderivativesofallorders. (cid:98)nw Thebandwidthh playsasimilarroleinkernelregressionasinkerneldensityestimation. Namely, largervaluesofh willresultinestimatesm (x)whicharesmootherin x,andsmallervaluesofh will (cid:98)nw resultinestimateswhicharemoreerratic. Itmightbehelpfultoconsiderthetwoextremecaseshâ0 andhââ.Ashâ0wecanseethatm (X )âY (ifthevaluesofX areunique),sothatm (x)issim- (cid:98)nw i i i (cid:98)nw plythescatterofY onX .Incontrast,ashââthenm (x)âY,thesamplemean. Forintermediate i i (cid:98)nw valuesofh,m (cid:98)nw (x)willsmoothbetweenthesetwoextremeca(cid:112)ses. Theestimator(19.2)usingtheGaussiankernelandh=1/ 3isalsodisplayedinFigure19.1withthe long dashes. As you can see, this estimator appears to be much smoother than that using the binned",
    "page": 688,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 669 Table19.1:CommonNormalizedSecond-OrderKernels Kernel Formula R K ï£± 1 (cid:112) ï£´ (cid:112) if |u|< 3 ï£´ ï£² 2 3 1 Rectangular K(u)= (cid:112) ï£´ 2 3 ï£´ ï£³ 0 otherwise 1 (cid:181) u2(cid:182) 1 Gaussian K(u)= (cid:112) exp â (cid:112) 2Ï 2 2 Ï ï£± ï£´ ï£´ ï£´ (cid:112) 3 (cid:181) 1â u2(cid:182) if |u|< (cid:112) 5 (cid:112) Epanechnikov K(u)= ï£² 4 5 5 3 5 ï£´ 25 ï£´ ï£´ ï£³ 0 otherwise ï£± 1 (cid:181) |u|(cid:182) (cid:112) ï£´ ï£´ (cid:112) 1â(cid:112) if |u|< 6 (cid:112) Triangular K(u)= ï£² 6 6 6 ï£´ 9 ï£´ ï£³ 0 otherwise (cid:112) estimatorbuttracksexactlythesamepath. Thebandwidthh=1/ 3fortheGaussiankernelisequiv- alenttothebandwidthh=1forthebinnedestimatorbecausethelatterisakernelestimatorusingthe rectangularkernelscaledtohaveastandarddeviationof1/3. 19.4 LocalLinearEstimator TheNadaraya-Watson(NW)estimatorisoftencalledalocalconstantestimatorasitlocally(about x) approximates m(x) as a constant function. One way to see this is to observe that m(x) solves the (cid:98) minimizationproblem m (x)=argmin (cid:88) n K (cid:181) X i âx (cid:182) (Y âm)2. (cid:98)nw i m i=1 h ThisisaweightedregressionofY onaninterceptonly. ThismeansthattheNWestimatorismakingthelocalapproximationm(X)(cid:39)m(x)forX (cid:39)x,which meansitismakingtheapproximation Y =m(X)+e(cid:39)m(x)+e. TheNWestimatorisalocalestimatorofthisapproximatemodelusingweightedleastsquares. Thisinterpretationsuggeststhatwecanconstructalternativenonparametricestimatorsofm(x)by alternative local approximations. Many such local approximations are possible. A popular choice is the Local Linear (LL) approximation. Instead of the approximation m(X) (cid:39) m(x), LL uses the linear approximationm(X)(cid:39)m(x)+m (cid:48) (x)(Xâx).Thus Y =m(X)+e(cid:39)m(x)+m (cid:48) (x)(Xâx)+e.",
    "page": 689,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 670 TheLLestimatorthenappliesweightedleastsquaressimilarlyasinNWestimation. OnewaytorepresenttheLLestimatorisasthesolutiontotheminimizationproblem (cid:169) m (x),m (cid:48) (x) (cid:170)=argmin (cid:88) n K (cid:181) X i âx (cid:182) (cid:161) Y âÎ±âÎ²(X âx) (cid:162)2 . (cid:98)LL (cid:98)LL Î±,Î² i=1 h i i Anotheristowritetheapproximatingmodelas Y (cid:39)Z(X,x) (cid:48)Î²(x)+e whereÎ²(x)=(cid:161) m(x),m (cid:48) (x) (cid:162)(cid:48) and (cid:181) (cid:182) 1 Z(X,x)= . Xâx This is a linear regression with regressor vector Z (x)= Z(X ,x) and coefficient vector Î²(x). Applying i i weightedleastsquareswiththekernelweightsweobtaintheLLestimator Î² (cid:98)LL (x)= (cid:195) (cid:88) n K (cid:181) X i âx (cid:182) Z i (x)Z i (x) (cid:48) (cid:33)â1 (cid:88) n K (cid:181) X i âx (cid:182) Z i (x)Y i h h i=1 i=1 =(cid:161) Z (cid:48) KZ (cid:162)â1 Z (cid:48) KY whereK =diag{K((X âx)/h),...,K((X âx)/h)}, Z isthestacked Z (x) (cid:48) ,andY isthestackedY . This 1 n i i expressiongeneralizestheNadaraya-WatsonestimatorasthelatterisobtainedbysettingZ (x)=1. No- i ticethatthematricesZ andK dependonxandh. ThelocallinearestimatorwasfirstsuggestedbyStone(1977)andcameintoprominencethroughthe workofFan(1992,1993). Tovisualize,Figure19.1(b)displaysthescatterplotofthesame100observationsfrompanel(a)di- videdintothesamefivebins. Alinearregressionisfittotheobservationsineachbin. Thesefivefitted regressionlinesaredisplayedbytheshortdashedlines. Thisâbinnedregressionestimatorâproducesa flexibleappromationforthemeanfunctionbuthaslargejumpsattheedgesofthepartitions. Themid- pointsofeachofthesefiveregressionlinesaredisplayedbythesolidsquaresandcouldbeviewedasthe targetestimateforthebinnedregressionestimator. Arollingversionofthebinnedregressionestimator movestheseestimationwindowscontinuouslyacrossthesupportofX andisdisplayedbythesoli(cid:112)dline. Thiscorrespondstothelocallinearestimatorwitharectangularkernelandabandwidthofh=1/ 3.By constructionthislinepassesthroughthesolidsquares. Toobtainasmo(cid:112)otherestimatorwereplacethe rectangularwiththeGaussiankernel(usingthesamebandwidthh=1/ 3). Wedisplaytheseestimates withthelongdashes.Thishasthesameshapeastherectangularkernelestimate(rollingbinnedregres- sion)butisvisuallymuchsmoother. WelabelthistheâLocalLinearâestimatorsinceitisthestandard implementation. OneinterestingfeatureisthatashââtheLLestimatorapproachesthefull-sampleleastsquares estimatorm (cid:98)LL (x)âÎ± (cid:98) +Î² (cid:98)x.Thatisbecauseashââallobservationsreceiveequalweight.Inthissense theLLestimatorisaflexiblegeneralizationofthelinearOLSestimator. AnotherusefulpropertyoftheLLestimatoristhatitsimultaneouslyprovidesestimatesoftheregres- (cid:48) sionfunctionm(x)anditsslopem (x)atx. 19.5 LocalPolynomialEstimator TheNWandLLestimatorsarebothspecialcasesofthelocalpolynomialestimator. Theideaisto approximatetheregressionfunctionm(x)byapolynomialoffixeddegreep,andthenestimatelocally usingkernelweights.",
    "page": 690,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 671 Theapproximatingmodelisapth orderTaylorseriesapproximation Y =m(X)+e (cid:39)m(x)+m (cid:48) (x)(Xâx)+Â·Â·Â·+m(p)(x) (Xâx)p +e p! =Z(X,x) (cid:48)Î²(x)+e i where ï£« ï£¶ 1 ï£« ï£¶ m(x) ï£¬ ï£¬ Xâx ï£· ï£· ï£¬ m (cid:48) (x) ï£· Z(X,x)=ï£¬ ï£¬ ï£¬ . . . ï£· ï£· ï£· Î²(x)=ï£¬ ï£¬ ï£¬ . . . ï£· ï£· ï£· . ï£¬ (Xâx)p ï£· ï£­ ï£¸ ï£­ ï£¸ m(p)(x) p! Theestimatoris Î² (cid:98)LP (x)= (cid:195) (cid:88) n K (cid:181) X i âx (cid:182) Z i (x)Z i (x) (cid:48) (cid:33)â1(cid:195) (cid:88) n K (cid:181) Y i âx (cid:182) Z i (x)Y i (cid:33) h h i=1 i=1 =(cid:161) Z (cid:48) KZ (cid:162)â1 Z (cid:48) KY where Z (x)=Z(X ,x)NoticethatthisexpressionincludestheNadaraya-Watsonandlocallinearesti- i i matorsasspecialcaseswithp=0andp=1,respectively. Thereisatrade-offbetweenthepolynomialorder p andthelocalsmoothingbandwidthh. Byin- creasing p we improve the model approximation and thereby can use a larger bandwidth h. On the otherhand,increasingp increasesestimationvariance. 19.6 AsymptoticBias Since(cid:69)[Y |X =x]=m(x),theconditionalmeanoftheNadaraya-Watsonestimatoris (cid:88) n K (cid:181) X i âx (cid:182) (cid:69)[Y |X ] (cid:88) n K (cid:181) X i âx (cid:182) m(X ) i i i h h (cid:69)[m (x)|X]= i=1 = i=1 . (19.3) (cid:98)nw (cid:88) n (cid:181) X i âx (cid:182) (cid:88) n (cid:181) X i âx (cid:182) K K h h i=1 i=1 Wecansimplifythisexpressionasnââ. The following regularity conditions will be maintained through the chapter. Let f(x) denote the marginaldensityofX andletÏ2(x)=(cid:69)(cid:163) e2|X =x (cid:164) denotetheconditionalvarianceofe=Y âm(X). Assumption19.1 1. hâ0. 2. nhââ. 3. m(x), f(x),andÏ2(x)arecontinuousinsomeneighborhoodN ofx. 4. f(x)>0.",
    "page": 691,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 672 Theseconditionsaresimilartothoseusedfortheasymptotictheoryforkerneldensityestimation. Theassumptionshâ0andnhââmeansthatthebandwidthgetssmallyetthenumberofobserva- tionsintheestimationwindowdivergestoinfinity. Assumption19.1.3areminimalsmoothnesscondi- tionsontheconditionalmeanm(x),marginaldensity f(x),andconditionalvarianceÏ2(x).Assumption 19.1.4specifiesthatthemarginaldensityisnon-zero.Thisisrequiredsinceweareestimatingthecondi- tionalmeanatx,sothereneedstobeanon-trivialnumberofobservationsforX nearx. i (cid:48)(cid:48) (cid:48) Theorem19.1 SupposeAssumption19.1holdsandm (x)and f (x)arecon- tinuousinN .Then (cid:179)(cid:113) (cid:180) 1. (cid:69)[m (x)|X]=m(x)+h2B (x)+o (cid:161) h2(cid:162)+O h (cid:98)nw nw p p n where B (x)= 1 m (cid:48)(cid:48) (x)+f(x) â1f (cid:48) (x)m (cid:48) (x). nw 2 (cid:179)(cid:113) (cid:180) 2. (cid:69)[m (x)|X]=m(x)+h2B (x)+o (cid:161) h2(cid:162)+O h (cid:98)LL LL p p n where B (x)= 1 m (cid:48)(cid:48) (x). LL 2 TheprooffortheNadaraya-WatsonestimatorispresentedinSection19.26. Foraproofforthelocal linearestimatorseeFanandGijbels(1996). Wecallthetermsh2B (x)andh2B (x)theasymptoticbiasoftheestimators. nw LL Theorem19.1showsthattheasymptoticbiasoftheNadaraya-Watsonandlocallinearestimatorsis proportional to the squared bandwidth h2 (the degree of smoothing) and to the functions B (x) and nw B (x).Theasymptoticbiasofthelocallinearestimatordependsonthecurvature(secondderivative)of LL theCEFfunctionm(x)similarlytotheasymptoticbiasofthekerneldensityestimatorinTheorem17.1 of Introduction to Econometrics. When m (cid:48)(cid:48) (x)<0 then m (x) is downwards biased. When m (cid:48)(cid:48) (x)>0 (cid:98)LL thenm (x)isupwardsbiased.Localaveragingsmoothsm(x),inducingbias,andthisbiasisincreasing (cid:98)LL inthelevelofcurvatureofm(x).Thisiscalledsmoothingbias. TheasymptoticbiasoftheNadaraya-Watsonestimatoraddsasecondtermwhichdependsonthe firstderivativesofm(x)and f(x). ThisisbecausetheNadaraya-Watsonestimatorisalocalaverage. If thedensityisupwardslopedatx(if f (cid:48) (x)>0)thenthereare(onaverage)moreobservationstotheright ofx thantotheleftsoalocalaveragewillbebiasedifm(x)hasanon-zeroslope. Incontrastthebiasof (cid:48) thelocallinearestimatordoesnotdependonthelocalslopem (x)sinceitlocallyfitsalinearregression. ThefactthatthebiasofthelocallinearestimatorhasfewertermsthanthebiasoftheNadaraya-Watson (cid:48) estimator(andisinvarianttotheslopem (x))justifiestheclaimthatthelocallinearestimatorhasgener- icallyreducedbiasrelativetoNadaraya-Watson. WeillustrateasymptoticsmoothingbiasinFigure19.2(a).Thesolidlineisthetrueconditionalmean forthedatadisplayedinFigure19.1. Thedashedlinesaretheasymptoticapproximationstotheexpec- tationm(x)+h2B(x)forbandwidthsh=1/2,h=1,andh=3/2",
    "page": 692,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". ThefactthatthebiasofthelocallinearestimatorhasfewertermsthanthebiasoftheNadaraya-Watson (cid:48) estimator(andisinvarianttotheslopem (x))justifiestheclaimthatthelocallinearestimatorhasgener- icallyreducedbiasrelativetoNadaraya-Watson. WeillustrateasymptoticsmoothingbiasinFigure19.2(a).Thesolidlineisthetrueconditionalmean forthedatadisplayedinFigure19.1. Thedashedlinesaretheasymptoticapproximationstotheexpec- tationm(x)+h2B(x)forbandwidthsh=1/2,h=1,andh=3/2. (TheasymptoticbiasesoftheNWand LLestimatorsarethesamesince X hasauniformdistribution.) Youcanseethatthereisminimalbias forthesmallestbandwidthbutconsiderablebiasforthelargest.Thedashedlinesaresmoothedversions oftheconditionalmean,attenuatingthepeaksandvalleys.",
    "page": 692,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 673 Smoothingbiasisanaturalby-productofnon-parametricestimationofnonlinearfunctions. Itcan onlybereducedbyusingasmallbandwidth. Asweseeinthefollowingsectionthiswillresultinhigh estimationvariance. x ) ( xm 0 1 2 3 4 5 6 7 8 9 10 5.1 0.1 5.0 0.0 5.0â m(x) h=1/2 h=1 h=3/2 x (a)SmoothingBias y l l llll l ll l l l ll l l ll l l l l l lll l l l l l l ll l l ll l l l l l l l l ll l l l l l l l ll l ll l l l l l lll l l l l l ll lll l l l l l ll l lll l l l ll l ll l l l 0 1 2 3 4 5 6 7 8 9 10 21 01 8 6 4 2 0 2â m(x) NadarayaâWatson, h=1 NadarayaâWatson, h=2 (b)BoundaryBias Figure19.2:AsymptoticBias 19.7 AsymptoticVariance From(19.3)wededucethat (cid:88) n (cid:181) X i âx (cid:182) K e i h m (x)â(cid:69)[m (x)|X]= i=1 . (cid:98)nw (cid:98)nw (cid:88) n (cid:181) X i âx (cid:182) K h i=1 SincethedenominatorisafunctiononlyofX andthenumeratorislinearine wecancalculatethatthe i i finitesamplevarianceofm (x)is (cid:98)nw (cid:88) n K (cid:181) X i âx (cid:182)2 Ï2(X ) i h var[m (x)|X]= i=1 . (19.4) (cid:98)nw (cid:195) (cid:88) n (cid:181) X i âx (cid:182) (cid:33)2 K h i=1 Wecansimplifythisexpressionasnââ. LetÏ2(x)=(cid:69)(cid:163) e2|X =x (cid:164) denotetheconditionalvariance ofe=Y âm(X).",
    "page": 693,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 674 Theorem19.2 UnderAssumption19.1, R Ï2(x) (cid:181) 1 (cid:182) 1. var[m (x)|X]= K +o . (cid:98)nw p f(x)nh nh R Ï2(x) (cid:181) 1 (cid:182) 2. var[m (x)|X]= K +o . (cid:98)LL p f(x)nh nh Intheseexpressions (cid:90) â R = K(u)2du K ââ istheroughnessofthekernelK(u). TheprooffortheNadaraya-WatsonestimatorispresentedinSection19.26. Forthelocallinearesti- matorseeFanandGijbels(1996). WecalltheleadingtermsinTheorem19.2theasymptoticvarianceoftheestimators. Theorem19.2 showsthattheasymptoticvarianceofthetwoestimatorsareidentical. Theasymptoticvarianceispro- portionaltotheroughnessR ofthekernelK(u)andtotheconditionalvarianceÏ2(x)oftheregression K error.Itisinverselyproportionaltotheeffectivenumberofobservationsnhandtothemarginaldensity f(x). This expression reflects the fact that the estimators are local estimators. The precision of m(x) (cid:98) islowforregionswheree hasalargeconditionalvarianceand/or X hasalowdensity(wherethereare relativelyfewobservations). 19.8 AIMSE We define theasymptoticMSE(AMSE) of an estimator m(x) as the sum of itssquaredasymptotic (cid:98) biasandasymptoticvariance. UsingTheorems19.1and19.2fortheNadaraya-Watsonandlocallinear estimators,weobtain R Ï2(x) AMSE(x) d=ef h4B(x)2+ K nhf(x) whereB(x)=B (x)fortheNadaraya-WatsonestimatorandB(x)=B (x)forthelocallinearestimator. nw LL ThisistheasymptoticMSEfortheestimatorm(x)forasinglepointx. (cid:98) AglobalmeasureoffitcanbeobtainedbyintegratingAMSE(x). ItisstandardtoweighttheAMSE by f(x)w(x) for some integrable weight function w(x). This is called the asymptoticintegratedMSE (AIMSE).LetSbethesupportofX (theregionwhere f(x)>0). (cid:90) (cid:90) (cid:181) R Ï2(x) (cid:182) R AIMSE d=ef AMSE(x)f(x)w(x)dx= h4B(x)2+ K f(x)w(x)dx=h4B+ KÏ2 (19.5) nhf(x) nh S S where (cid:90) B= B(x)2f(x)w(x)dx S (cid:90) Ï2= Ï2(x)w(x)dx. S The weight function w(x) can be omitted if S is bounded. Otherwise, a common choice is w(x) = 1 {Î¾ â¤xâ¤Î¾ }. Anintegrableweightfunctionisneededwhen X hasunboundedsupporttoensurethat 1 2 Ï2<â.",
    "page": 694,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 675 TheformoftheAIMSEissimilartothatforkerneldensityestimation(Theorem17.3ofIntroduction toEconometrics). Ithastwoterms(squaredbiasandvariance). Thefirstisincreasinginthebandwidth h andthesecondisdecreasinginh. Thusthechoiceofh affectsAIMSEwithatrade-offbetweenthese twocomponents. Similarlytodensityestimationwecancalculatethebandwidthwhichminimizesthe AIMSE.(SeeExercise19.2.)Thesolutionisgiveninthefollowingtheorem. Theorem19.3 ThebandwidthwhichminimizestheAIMSE(19.5)is h = (cid:195) R K Ï2 (cid:33)1/5 n â1/5. (19.6) 0 4B Withhâ¼n â1/5thenAIMSE[m(x)]=O (cid:161) n â4/5(cid:162) . (cid:98) ThisresultcharacterizestheAIMSE-optimalbandwidth.Thisbandwidthsatisfiestherateh=cn â1/5 whichisthesamerateasforkerneldensityestimation. Theoptimalconstantc dependsonthekernel K(x), the weighted average squared bias B, and the weighted average variance Ï2. The constant c is different,however,fromthatfordensityestimation. Inserting(19.6)into(19.5)plussomealgebrawefindthattheAIMSEusingtheoptimalbandwidthis AIMSE (cid:39)1.65 (cid:179) R4BÏ8 (cid:180)1/5 n â4/5. 0 K ThisdependsonthekernelK(u)onlythroughtheconstantR . SincetheEpanechnikovkernelhasthe K smallestvalue1 ofR itisalsothekernelwhichproducesthesmallestAIMSE.Thisistrueforboththe K NWandLLestimators. Theorem19.4 TheAIMSE(19.5)oftheNadaraya-WatsonandLocalLinearre- gressionestimatorsisminimizedbytheEpanechnikovkernel. Theefficiencylossbyusingtheotherstandardkernels,however,issmall. Therelativeefficiency2 of (cid:161) (cid:161) (cid:162)(cid:162)2/5 estimation using the another kernel is R /R Epanechnikov . Using the values of R from Table K K K 19.1wecalculatethattheefficiencylossfromusingtheTriangle,Gaussian,andRectangularkernelsare 1%,2%,and3%,respectively,whichareminimal.SincetheGaussiankernelproducesthesmoothestes- timates,whichisimportantforestimationofmarginaleffects,ouroverallrecommendationistheGaus- siankernel. 19.9 ReferenceBandwidth TheNW,LLandLPestimatorsdependonabandwidthandwithoutanempiricalruleforselection of h the methods are incomplete. It is useful to have a reference bandwith which mimics the optimal bandwidthinasimplifiedsettingandprovidesabaselineforfurtherinvestigations. 1SeeTheorem17.4ofIntroductiontoEconometrics. 2MeasuredbyrootAIMSE.",
    "page": 695,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 676 Theorem19.3andalittlere-writingrevealsthattheoptimalbandwidthequals (cid:181) R (cid:182)1/5 (cid:195) Ï2 (cid:33)1/5 (cid:195) Ï2 (cid:33)1/5 h = K (cid:39)0.58 (19.7) 0 4 nB nB wheretheapproximationholdsforallsingle-peakedkernelsbysimilarcalculations3asinSection17.9of IntroductiontoEconometrics. Areferenceapproachcanbeusedtodeveloparule-of-thumbforregressionestimation.Inparticular, FanandGijbels(1996,Section4.2)developwhattheycalltheROT(ruleofthumb)bandwidthforthelocal linearestimator.Wenowdescribetheirderivation. First,setw(x)=1 {Î¾ â¤xâ¤Î¾ }. Second,formapilotorpreliminaryestimatoroftheregressionfunc- 1 2 tionm(x)usingaqth-orderpolynomialregression m(x)=Î² +Î² x+Î² x2+Â·Â·Â·+Î² xq 0 1 2 q forq â¥2. (FanandGijbels(1996)suggestq =4butthisisnotessential.) Byleastsquaresweobtainthe coefficientestimatesÎ² (cid:98)0 ,...,Î² (cid:98)q andimpliedsecondderivativem (cid:98) (cid:48)(cid:48) (x)=2Î² (cid:98)2 +6Î² (cid:98)3 x+12Î² (cid:98)4 x2+Â·Â·Â·+q(qâ 1)Î² (cid:98)q xqâ2.Third,noticethatB canbewrittenasanexpectation B=(cid:69)(cid:163) B(X)2w(X) (cid:164)=(cid:69) (cid:183)(cid:181) 1 m (cid:48)(cid:48) (X) (cid:182)2 1 {Î¾ â¤X â¤Î¾ } (cid:184) . 1 2 2 Amomentestimatoris B(cid:98) = 1 (cid:88) n (cid:181) 1 m (cid:98) (cid:48)(cid:48) (X i ) (cid:182)2 1 {Î¾ 1 â¤X i â¤Î¾ 2 }. (19.8) n 2 i=1 Third,assumethattheregressionerrorishomoskedastic(cid:69)(cid:163) e2|X (cid:164)=Ï2sothatÏ2=Ï2(Î¾ âÎ¾ ).Estimate 2 1 Ï2bytheerrorvarianceestimateÏ2fromthepreliminaryregression.Pluggingtheseinto(19.7)weobtain (cid:98) thereferencebandwidth (cid:181)Ï2(Î¾ âÎ¾ ) (cid:182)1/5 h =0.58 (cid:98) 2 1 . (19.9) rot nB(cid:98) FanandGijbels(1996)callthistheRule-of-Thumb(ROT)bandwidth. FanandGijbelsdevelopedsimilarrulesforhigher-orderoddlocalpolynomialestimatorsbutnotfor thelocalconstant(Nadaraya-Watson)estimator. However, wecanderiveaROTfortheNWaswellby usingareferencemodelforthemarginaldensity f(x).Aconvenientchoiceistheuniformdensityunder which f (cid:48) (x)=0andtheoptimalbandwidthsforNWandLLcoincide. Thismotivatesusing(19.9)asa ROTbandwidthforboththeLLandNWestimators. Aswementionedabove,FanandGijbelssuggestusinga4th-orderpolynomialforthepilotestimator butthisspecificchoiceisnotessential.InapplicationsitmaybeprudenttoassesssensitivityoftheROT bandwithtothechoiceofq andtoexaminetheestimatedpilotregressionforprecisionoftheestimated higher-orderpolynomialterms. We now comment on the choice of the weight region [Î¾ ,Î¾ ]. When X has bounded support then 1 2 [Î¾ ,Î¾ ]canbesetequaltothissupport. Otherwise,[Î¾ ,Î¾ ]canbesetequaltotheregionofinterestfor 1 2 1 2 m(x),ortheendpointscanbesettoequalfixedquantiles(e.g.0.05and0.95)ofthedistributionofX. (cid:98) To illustrate, take the data shown in Figure 19.1. If we fit a 4th order polynomial we find m(x) = (cid:98) .49+.70xâ.28x2â.033x3â.0012x4 whichimpliesm (cid:48)(cid:48) (x)=â.56â.20xâ.014x2. Setting[Î¾ ,Î¾ ]=[0,10] (cid:98) 1 2 fromthesupportofX wefindB(cid:98) =0.00889",
    "page": 696,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". When X has bounded support then 1 2 [Î¾ ,Î¾ ]canbesetequaltothissupport. Otherwise,[Î¾ ,Î¾ ]canbesetequaltotheregionofinterestfor 1 2 1 2 m(x),ortheendpointscanbesettoequalfixedquantiles(e.g.0.05and0.95)ofthedistributionofX. (cid:98) To illustrate, take the data shown in Figure 19.1. If we fit a 4th order polynomial we find m(x) = (cid:98) .49+.70xâ.28x2â.033x3â.0012x4 whichimpliesm (cid:48)(cid:48) (x)=â.56â.20xâ.014x2. Setting[Î¾ ,Î¾ ]=[0,10] (cid:98) 1 2 fromthesupportofX wefindB(cid:98) =0.00889. Theresidualsfromthepolynomialregressionhavevariance Ï2=0.0687.Pluggingtheseinto(19.9)wefindh =0.551whichissimilarthatusedinFigure19.1. (cid:98) rot 3Theconstant(RK/4)1/5isboundedbetween0.58and0.59.",
    "page": 696,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 677 19.10 EstimationataBoundary OneadvantageofthelocallinearovertheNadaraya-WatsonestimatoristhattheLLhasbetterper- formanceattheboundaryofthesupportofX.TheNWestimatorhasexcessivesmoothingbiasnearthe boundaries. Inmanycontextsineconometricstheboundariesareofgreatinterest. Insuchcasesitis stronglyrecommendedtousethelocallinearestimator(oralocalpolynomialestimatorwithpâ¥1). TounderstandtheproblemitmaybehelpfultoexamineFigure19.2(b). Thisshowsascatterplotof 100observationsgeneratedasX â¼U[0,10]andY â¼N(X,1)sothatm(x)=x. Supposeweareinterested in the conditional mean m(0) at the lower boundary x =0. The Nadaraya-Watson estimator equals a weightedaverageoftheY observationsforsmallvaluesof|X|. Since X â¥0, theseareallobservations forwhichm(X)â¥m(0),andthereforem (0)isbiasedupwards. Symmetrically,theNadaraya-Watson (cid:98)nw estimatorattheupperboundary x =10isaweightedaverageofobservationsforwhichm(X)â¤m(10) andthereforem (10)isbiaseddownwards. (cid:98)nw Incontrast,thelocallinearestimatorsm (0)andm (10)areunbiasedinthisexamplesincem(x) (cid:98)LL (cid:98)LL islinearinx.Thelocallinearestimatorfitsalinearregressionline.Sincethemeaniscorrectlyspecified thereisnoestimationbias. Theexactbias4oftheNWestimatorisshowninFigure19.2(b)bythedashedlines. Thelongdashes isthemean(cid:69)[m (x)]forh=1andtheshortdashesisthemean(cid:69)[m (x)]forh=2. Wecanseethat (cid:98)nw (cid:98)nw thebiasissubstantial. Forh =2thebiasisvisibleforallvaluesof x. Forthesmallerbandwidthh =1 thebiasisminimalfor x inthecentralrangeofthesupport,butisstillquitesubstantialfor x nearthe boundaries. TocalculatetheasymptoticsmoothingbiaswecanrevisittheproofofTheorem19.1.1whichcalcu- latedtheasymptoticbiasatinteriorpoints. Equation(19.29)calculatesthebiasofthenumeratorofthe estimatorexpressedasanintegraloverthemarginaldensity. Evaluatedatalowerboundarythedensity ispositiveonlyforuâ¥0sotheintegralisoverthepositiveregion[0,â).Thisappliesaswelltoequation (19.31)andtheequationswhichfollow. Inthiscasetheleadingtermofthisexpansionisthefirstterm (19.32) which is proportional to h rather than h2. Completing the calculations we find the following. Definem(x+)=limm(z)andm(xâ)=limm(z). zâx zâx Theorem19.5 SupposeAssumption19.1holds.SetÂµ =2 (cid:82)â K(u)du.Letthe K 0 supportofX beS=[x,x]. Ifm (cid:48)(cid:48) (x+),Ï2(x+)and f (cid:48) (x+)exist,and f(x+)>0then ï£«(cid:115) ï£¶ (cid:69)(cid:163) m (cid:98)nw (x)|X (cid:164)=m(x)+hm (cid:48) (x)Âµ K +o p (h)+O pï£­ h ï£¸. n Ifm (cid:48)(cid:48) (xâ),Ï2(xâ)and f (cid:48) (xâ)exist,and f(xâ)>0then ï£«(cid:115) ï£¶ (cid:69)(cid:163) m (cid:98)nw (x)|X (cid:164)=m(x)âhm (cid:48) (x)Âµ K +o p (h)+O pï£­ h ï£¸. n Theorem19.5showsthattheasymptoticbiasoftheNWestimatorattheboundaryisO(h)andde- pends on the slope of m(x) at the boundary. When the slope is positive the NW estimator is upward 4Calculatedbysimulationfrom10,000simulationreplications.",
    "page": 697,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 678 biasedatthelowerboundaryanddownwardbiasedattheupperboundary.Thestandardinterpretation ofTheorem19.5isthattheNWestimatorhashighbiasnearboundarypoints. SimilarlywecanevaluatetheperformanceoftheLLestimator. Wesummarizetheresultswithout derivation(astheyaremoretechnicallychallenging)andinsteadreferinterestedreaderstoCheng,Fan andMarron(1997)andImbensandKalyahnaraman(2012). DefinethekernelmomentsÎ½ =(cid:82)â ujK(u)du,Ï =(cid:82)â ujK(u)2du,andprojectedkernel j 0 j 0 K â (u)=(cid:163) 1 0 (cid:164) (cid:183) Î½ 0 Î½ 1 (cid:184)â1(cid:183) 1 (cid:184) K(u)= Î½ 2 âÎ½ 1 u K(u). Î½ Î½ u Î½ Î½ âÎ½2 1 2 0 2 1 Defineitssecondmoment (cid:90) â Î½2âÎ½ Î½ Ï2 Kâ = u2K â (u)du= Î½ 2 Î½ â 1 Î½ 3 2 0 0 2 1 androughness (cid:90) â Î½2Ï â2Î½ Î½ Ï +Î½2Ï R â = K â (u)2du= 2 0 1 2 1 1 2 . K 0 (cid:161)Î½ 0 Î½ 2 âÎ½2 1 (cid:162)2 Theorem19.6 UndertheassumptionsofTheorem19.5,ataboundarypointx 1. (cid:69)(cid:163) m (x)|X (cid:164)=m(x)+ h2m (cid:48)(cid:48) (x)Ï2 Kâ +o (cid:161) h2(cid:162)+O (cid:179)(cid:113) h (cid:180) (cid:98)LL 2 p p n 2. var (cid:163) m (x)|X (cid:164)= R K âÏ2(x) +o (cid:181) 1 (cid:182) (cid:98)LL p f(x)nh nh Theorem19.6showsthattheasymptoticbiasoftheLLestimatorataboundaryisO(h2), thesame asatinteriorpointsandisinvarianttotheslopeofm(x). Thetheoremalsoshowsthattheasymptotic variancehasthesamerateasatinteriorpoints. TakingTheorems19.1,19.2,19.5,and19.6togetherweconcludethatthelocallinearestimatorhas superiorasymptoticpropertiesrelativetotheNWestimator. Atinteriorpointsthetwoestimatorshave the same asymptotic variance. The bias of the LL estimator is invariant to the slope of m(x) and its asymptoticbiasonlydependsonthesecondderivativewhilethebiasoftheNWestimatordependson both the first and second derivatives. At boundary points the asymptotic bias of the NW estimator is O(h)whichisofhigherorderthantheO(h2)biasoftheLLestimator. Forthesereasonswerecommend thelocallinearestimatorovertheNadaraya-Watsonestimator. TheasymptoticbiasandvarianceoftheLLestimatorattheboundaryisslightlydifferentthaninthe interior.Thedifferenceisthatthebiasandvariancedependonthemomentsofthekernel-likefunction â K (u)ratherthantheoriginalkernelK(u). Aninterestingquestionistofindtheoptimalkernelfunctionforboundaryestimation. Bythesame â â calculations as for Theorem 19.4 we find that the optimal kernel K (u) minimizes the roughness R K given the second moment Ï2 and as argued for Theorem 19.4 this is achieved when K â (u) equals a Kâ â quadraticfunctioninu. SinceK (u)istheproductofK(u)andalinearfunctionthismeansthatK(u) must be linear in |u|, implying that the optimal kernel K(u) is the Triangular kernel. See Cheng, Fan, and Marron (1997). Calculations similar to those following Theorem 19.4 show that efficiency loss5 of estimationusingtheEpanechnikov,Gaussian,andRectangularkernelsare1%,1%,and3%,respectively. 5MeasuredbyrootAIMSE.",
    "page": 698,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 679 19.11 NonparametricResidualsandPredictionErrors Givenanynonparametricregressionestimatorm(x)thefittedregressionatx=X ism(X )andthe (cid:98) i (cid:98) i fittedresidual is e =Y âm(X ). Asa general rule, butespecially whenthebandwidth h is small, itis (cid:98)i i (cid:98) i hardtoviewe asagoodmeasureofthefitoftheregression. FortheNWandLLestimators, ash â0 (cid:98)i thenm(X )âY andthereforee â0.Thisisclearoverfittingasthetrueerrore isnotzero. Ingeneral, (cid:98) i i (cid:98)i i sincem(X )isalocalaveragewhichincludesY ,thefittedvaluewillbenecessarilyclosetoY andthe (cid:98) i i i residuale small,andthedegreeofthisoverfittingincreasesashdecreases. (cid:98)i Astandardsolutionistomeasurethefitoftheregressionatx=X byre-estimatingthemodelexclud- i ing the ith observation. Let m (cid:101)âi (x) be the leave-one-out nonparametric estimator computed without observationi.Forexample,forNadaraya-Watsonregression,thisis (cid:181)X âx(cid:182) (cid:88) j K Y j h j(cid:54)=i Y(cid:101)i =m (cid:101)âi (x)= (cid:181)X âx(cid:182) . (cid:88) j K h j(cid:54)=i Notationally,theââiâsubscriptisusedtoindicatethattheith observationisomitted. Theleave-one-outpredictedvalueforY i atx=X i isY(cid:101)i =m (cid:101)âi (X i )andtheleave-one-outprediction erroris e (cid:101)i =Y i âY(cid:101)i . (19.10) SinceY(cid:101)i isnotafunctionofY i thereisnotendencyforY(cid:101)i tooverfitforsmallh.Consequently,e (cid:101)i isagood measureofthefitoftheestimatednonparametricregression. Whenpossibletheleave-one-outpredictionerrorsshouldbeusedinsteadoftheresidualse . (cid:98)i 19.12 Cross-ValidationBandwidthSelection Themostpopularmethodinappliedstatisticstoselectbandwidthsiscross-validation. Thegeneral idea is to estimate the model fit based on leave-one-out estimation. Here we describe the method as typicallyappliedforregressionestimation.ThemethodappliestoNW,LL,andLPestimation,aswellas othernonparametricestimators. Tobeexplicitaboutthedependenceoftheestimatoronthebandwidthletuswriteanestimatorof m(x)withagivenbandwidthhasm(x,h). (cid:98) Ideally,wewouldliketoselecth tominimizetheintegratedmean-squarederror(IMSE)ofm(x,h) (cid:98) asaestimatorofm(x): (cid:90) IMSE (h)= (cid:69)(cid:163) (m(x,h)âm(x))2(cid:164) f(x)w(x)dx n (cid:98) S where f(x)isthemarginaldensityof X and w(x)isanintegrableweightfunction. Theweight w(x)is thesameasusedin(19.5)andcanbeomittedwhenX hasboundedsupport. The difference m(x,h)âm(x) at x = X can be estimated by the leave-one-out prediction errors (cid:98) i (19.10) e (cid:101)i (h)=Y i âm (cid:101)âi (X i ,h) wherewearebeingexplicitaboutthedependenceonthebandwidthh.AreasonableestimatorofIMSE (h) n istheweightedaveragemeansquaredpredictionerrors CV(h)= 1 (cid:88) n e (h)2w(X ). (19.11) (cid:101)i i n i=1",
    "page": 699,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 680 Thisfunctionofhisknownasthecross-validationcriterion.Onceagain,ifX hasboundedsupportthen theweightsw(X )canbeomittedandthisistypicallydoneinpractice. i Itturnsoutthatthecross-validationcriterionisanunbiasedestimatoroftheIMSEplusaconstant forasamplewithnâ1observations. Theorem19.7 (cid:69)[CV(h)]=Ï2+IMSE nâ1 (h) (19.12) whereÏ2=(cid:69)(cid:163) e2w(X) (cid:164) . TheproofofTheorem19.7ispresentedinSection19.26. SinceÏ2isaconstantindependentofthebandwidthh,(cid:69)[CV(h)]issimplyashiftedversionofIMSE nâ1 (h). Inparticular,theh whichminimizes(cid:69)[CV(h)]andIMSE nâ1 (h)areidentical. Whenn islargetheband- widthwhichminimizesIMSE nâ1 (h)andIMSE n (h)arenearlyidenticalsoCV(h)isessentiallyunbiased as an estimator of IMSE (h)+Ï2. This considerations lead to the recommendation to select h as the n valuewhichminimizesCV(h). Thecross-validationbandwidthh(cid:98)isthevaluewhichminimizesCV(h) h =argminCV(h) (19.13) cv hâ¥h(cid:96) forsomeh(cid:96) >0.Therestrictionhâ¥h(cid:96)canbeimposedsothatCV(h)isnotevaluatedoverunreasonably smallbandwidths. Thereisnotanexplicitsolutiontotheminimizationproblem(19.13), soitmustbesolvednumer- ically. One method is grid search. Create a grid of values for h, e.g. [h ,h ,...,h ], evaluate CV(h ) for 1 2 J j j =1,...,J,andset h = argmin CV(h). cv hâ[h1,h2,...,hJ] Evaluation using a coarse grid is typically sufficient for practical application. Plots of CV(h) against h areausefuldiagnostictooltoverifythattheminimumofCV(h)hasbeenobtained. Acomputationally moreefficientmethodforobtainingthesolution(19.13)isGolden-SectionSearch. SeeSection12.4of IntroductiontoEconometrics. Itispossibleforthesolution(19.13)tobeunbounded,thatis,CV(h)isdecreasingforlargehsothat h =â.Thisisokay. Itsimplymeansthattheregressionestimatorsimplifiestoitsfull-sampleversion. cv ForNadaraya-Watsonestimatorthisism (cid:98)nw (x)=Y.Forthelocallinearestimatorthisism (cid:98)LL (x)=Î± (cid:98) +Î² (cid:98)x. ForNWandLLestimation,thecriterion(19.11)requiresleave-one-outestimationoftheconditional mean at each observation X . This is different from calculation of the estimator m(x) as the latter is i (cid:98) typicallydoneatasetoffixedvaluesofxforpurposesofdisplay. Toillustrate,Figure19.3(a)displaysthecross-validationcriteriaCV(h)fortheNadaraya-Watsonand LocalLinearestimatorsusingthedatafromFigure19.1,bothusingtheGaussiankernel. TheCVfunc- tionsarecomputedonagridon[h /3,3h ]with200gridpoints. TheCV-minimizingbandwidthsare rot rot h =0.830fortheNadaraya-Watsonestimatorandh =0.764forthelocallinearestimator. Theseare nw LL somewhathigherthantheruleofthumbh =0.551valuecalculatedearlier. Figure19.3(a)showsthe rot minimizingbandwidthsbythearrows. The CV criterion can also be used to select between different nonparametric estimators. The CV- selectedestimatoristheonewiththelowestminimizedCVcriterion.Forexample,inFigure19.3(a),you canseethattheLLestimatorhasaminimizedCVcriterionof0.0699whichislowerthantheminimum",
    "page": 700,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 681 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 870.0 670.0 470.0 270.0 070.0 860.0 h )h(VC NadarayaâWatson Local Linear 0 2 4 6 8 10 (a)Cross-ValidationCriterion 5.1 0.1 5.0 0.0 5.0â x y l l m(x) l l h h R C O V T l l l ll l l l l ll l l l l l l lll l l l ll l ll l l l l l ll l l l lll l l l l l ll l l l l lll l l l l l l l l ll l l ll l l lll l ll l l l l l l ll l l l l l l l l l l l (b)NonparametricEstimates Figure19.3:BandwidthSelection 0.0703obtainedbytheNWestimator. SincetheLLestimatorachievesalowervalueoftheCVcriterion, LL is the CV-selected estimator. The difference, however, is small, indicating that the two estimators achievesimilarIMSE. Figure19.3(b)displaysthelocallinearestimatesm(x)usingtheROTandCVbandwidthsalongwith (cid:98) the true conditional mean m(x). The estimators track the true function quite well, and the difference betweenthebandwidthsisrelativelyminorinthisapplication. 19.13 AsymptoticDistribution Wefirstprovideaconsistencyresult. Theorem19.8 Under Assumption 19.1, m (x) ââ m(x) and m (x) ââ (cid:98)nw (cid:98)LL p p m(x). AprooffortheNadaraya-WatsonestimatorispresentedinSection19.26.Forthelocallinearestima- torseeFanandGijbels(1996). Theorem19.8showsthattheestimatorsareconsistentform(x)undermildcontinuityassumptions. Inparticular,nosmoothnessconditionsonm(x)arerequiredbeyondcontinuity. Wenextpresentanasymptoticdistributionresult.Thefollowingshowsthatthekernelregressiones- timatorsareasymptoticallynormalwithanon-parametricrateofconvergence,anon-trivialasymptotic bias,andanon-degenerateasymptoticvariance.",
    "page": 701,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 682 Theorem19.9 Suppose Assumption 19.1 holds. Assume in addition that m (cid:48)(cid:48) (x)and f (cid:48) (x)arecontinuousinN ,thatforsomer >2andxâN , (cid:69)(cid:163)|e|r |X =x (cid:164)â¤Ï<â, (19.14) and nh5=O(1). (19.15) Then (cid:112) nh (cid:161) m (x)âm(x)âh2B (x) (cid:162)ââN (cid:181) 0, R K Ï2(x) (cid:182) . (19.16) (cid:98)nw nw d f(x) Similarly, (cid:112) nh (cid:161) m (x)âm(x)âh2B (x) (cid:162)ââN (cid:181) 0, R K Ï2(x) (cid:182) . (cid:98)LL LL d f(x) AprooffortheNadaraya-WatsonestimatorappearsinSection19.26. Forthelocallinearestimator seeFanandGijbels(1996). RelativetoTheorem19.8,Theorem19.9requiresstrongersmoothnessconditionsontheconditional meanandmarginaldensity. Therearealsotwotechnicalregularityconditions. Thefirstisaconditional momentbound(19.14)(whichisusedtoverifytheLindebergconditionfortheCLT)andthesecondisthe bandwidthboundnh5=O(1). Thelattermeansthatthebandwidthmustdeclinetozeroatleastatthe raten â1/5 andisused6 toensurethathigher-orderbiastermsdonotentertheasymptoticdistribution (19.16). Thereareseveralinterestingfeaturesabouttheasymptoticdistributionw(cid:112)hicharen (cid:112) oticeablydiffer- e(cid:112)ntthanforparametrices (cid:112) timators. First,theestimatorsconvergeattherate nh not n.Sincehâ0, nhdivergesslowerthan n,thusthenonparametricestimatorsconvergemoreslowlythanaparamet- ricestimator.Second,theasymptoticdistributioncontainsanon-negligiblebiastermh2B(x).Third,the distribution(19.16)isidenticalinformtothatforthekerneldensityestimator(Theorem17.7ofIntro- ductiontoEconometrics). (cid:112) Thefactthattheestimatorsconvergeattherate nh hasledtotheinterpretationofnh astheâef- fectivesamplesizeâ.Thisisbecausethenumberofobservationsbeingusedtoconstructm(x)ispropor- (cid:98) tionaltonh,notnasforaparametricestimator. Itishelpfultounderstandthatthenonparametricestimatorhasareducedconvergenceraterelative toparametricasymptotictheorybecausetheobjectbeingestimatedâm(x)âisnonparametric. Thisis harderthanestimatingafinitedimensionalparameter,andthuscomesatacost. Unlikeparametricestimationtheasymptoticdistributionofthenonparametricestimatorincludes a term representing the bias of the estimator. The asymptotic distribution (19.16) shows the form of thisbias. Itisproportionaltothesquaredbandwidthh2 (thedegreeofsmoothing)andtothefunction B (x)orB (x)whichdependsontheslopeandcurvatureoftheCEFm(x).Interestingly,whenm(x)is nw LL constantthenB (x)=B (x)=0andthekernelestimatorhasnoasymptoticbias.Thebiasisessentially nw LL increasinginthecurvatureoftheCEFfunctionm(x).Thisisbecausethelocalaveragingsmoothsm(x), andthesmoothinginducesmorebiaswhenm(x)iscurved. 6Thiscouldbeweakenedifstrongersmoothnessconditionsareassumed.Forexample,ifm(4)(x)andf(3)(x)arecontinuous then(19.15)canbeweakenedtonh9=O(1),whichmeansthatthebandwidthmustdeclinetozeroatleastattheraten â1/9.",
    "page": 702,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 683 Theasymptoticvarianceofm(x)isinverselyproportionaltothemarginaldensity f(x). Thismeans (cid:98) that m(x) has relatively low precision for regions where X has a low density. This makes sense since (cid:98) theseareregionswheretherearerelativelyfewobservations. Animplicationisthatthenonparametric estimatorm(x)willberelativelyinaccurateinthetailsofthedistributionofX. (cid:98) 19.14 Undersmoothing Thebiastermintheasymptoticdistributionofthekerneldensityestimatorcanbetechnicallyelim- inated if the bandwidth is selected to converge to zero faster than the optimal rate n â1/5, thus h = o (cid:161) n â1/5(cid:162) .Thisiscalledanunder-smoothingbandwidth.Byusingasmallbandwidththebiasisreduced andthevarianceisincreased.Thustherandomcomponentdominatesthebiascomponent(asymptoti- cally).Thefollowingisthetechnicalstatement. Theorem19.10 UndertheconditionsofTheorem19.9,andnh5=o(1), (cid:112) (cid:181) R Ï2(x) (cid:182) nh(m (x)âm(x))ââN 0, K (cid:98)nw d f(x) (cid:112) (cid:181) R Ï2(x) (cid:182) nh(m (x)âm(x))ââN 0, K . (cid:98)LL d f(x) Theorem19.10hastheadvantageofnobiasterm. Consequentlythistheoremispopularwithsome authors. Therearealsoseveraldisadvantages. First,theassumptionofanundersmoothingbandwidth doesnotreallyeliminatethebias, itsimplyassumesitaway. Thusinanyfinitesamplethereisalways bias. Second,itisnotclearhowtosetabandwidthsothatitisundersmoothing.Third,aundersmooth- ingbandwidthimpliesthattheestimatorhasincreasedvarianceandisinefficient. Finally,thetheoryis simplymisleadingasacharacterizationofthedistributionoftheestimator. 19.15 ConditionalVarianceEstimation Theconditionalvarianceis Ï2(x)=var[Y |X =x]=(cid:69)(cid:163) e2|X =x (cid:164) . ThereareanumberofcontextswhereitisdesirabletoestimateÏ2(x)includingpredictionintervalsand confidence intervals for the estimated mean function. In general the conditional variance function is nonparametricaseconomicmodelsrarelyspecifytheformofÏ2(x).ThusestimationofÏ2(x)istypically donenonparametrically. SinceÏ2(x)istheCEFofe2 given X itcanbeestimatedbynonparametricregression. Forexample, theidealNWestimator(ifewereobserved)is (cid:88) n K (cid:181) X i âx (cid:182) e2 h i Ï2(x)= i=1 . (cid:88) n (cid:181) X i âx (cid:182) K h i=1",
    "page": 703,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 684 Since the errors e are not observed, we need to replace them with an estimator. A simple choice are theresidualse (cid:98)i =Y i âm (cid:98) (X i ). Abetterchoicearetheleave-one-outpredictionerrorse (cid:101)i =Y i âm (cid:98)âi (X i ). The latter are recommended for variance estimation as they are not subject to overfitting. With this substitutiontheNWestimatoroftheconditionalvarianceis (cid:88) n K (cid:181) X i âx (cid:182) e2 h (cid:101)i Ï2(x)= i=1 . (19.17) (cid:98) (cid:88) n (cid:181) X i âx (cid:182) K h i=1 Thisestimatordependsonabandwidthh butthereisnoreasonforthisbandwidthtobethesame asthatusedtoestimatetheconditionalmean. TheROTorcross-validationusinge2 asthedependent (cid:101)i variablecanbeusedtoselectthebandwidthforestimationofÏ2(x)separatelyfromthechoiceforesti- (cid:98) mationofm(x). (cid:98) There is one subtle difference between CEF and conditional variance estimation. The conditional variance is inherently non-negative Ï2(x)â¥0 and it is desirable for the estimator to satisfy this prop- erty. The NW estimator (19.17) is necessarily non-negative since it is a smoothed average of the non- negativesquaredresiduals. TheLLestimator, however, isnotguaranteedtobenon-negativeforall x. Furthermore,theNWestimatorhasasaspecialcasethehomoskedasticestimatorÏ2(x)=Ï2(fullsam- (cid:98) (cid:98) plevariance)whichmaybearelevantselection. Forthesereasons,theNWestimatormaybepreferred forconditionalvarianceestimation. FanandYao(1998)derivetheasymptoticdistributionoftheestimator(19.17). Theyobtainthesur- prisingresultthattheasymptoticdistributionofthetwo-stepestimatorÏ2(x)isidenticaltothatofthe (cid:98) one-stepidealizedestimatorÏ2(x). 19.16 VarianceEstimationandStandardErrors It is relatively straightforward to calculate the exact conditional variance of the Nadaraya-Watson, locallinear,orlocalpolynomialestimator.Theestimatorscanbewrittenas Î² (cid:98)(x)=(cid:161) Z (cid:48) KZ (cid:162)â1(cid:161) Z (cid:48) KY (cid:162)=(cid:161) Z (cid:48) KZ (cid:162)â1(cid:161) Z (cid:48) Km (cid:162)+(cid:161) Z (cid:48) KZ (cid:162)â1(cid:161) Z (cid:48) Ke (cid:162) wheremisthenÃ1vectorofmeansm(X ).Thefirstcomponentisafunctiononlyoftheregressorsand i thesecondislinearintheerrore.ThusconditionallyontheregressorsX, V Î²(cid:98) (x)=var (cid:163)Î² (cid:98) |X (cid:164)=(cid:161) Z (cid:48) KZ (cid:162)â1(cid:161) Z (cid:48) KDKZ (cid:162)(cid:161) Z (cid:48) KZ (cid:162)â1 whereD=diag (cid:161)Ï2(X ),...Ï2(X ) (cid:162) . 1 n AWhite-typeestimatorcanbeformedbyreplacingÏ2(X )withthesquaredresidualse2orprediction i (cid:98)i errorse2 (cid:101)i (cid:195) (cid:33) V(cid:98)Î²(cid:98) (x)=(cid:161) Z (cid:48) KZ (cid:162)â1 (cid:88) n K (cid:181) X i h âx (cid:182)2 Z i (x)Z i (x) (cid:48) e (cid:101)i 2 (cid:161) Z (cid:48) KZ (cid:162)â1 . (19.18) i=1 Alternatively,Ï2(X )couldbereplacedwithanestimatorsuchas(19.17)evaluatedatÏ2(X )orÏ2(x)",
    "page": 704,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". 1 n AWhite-typeestimatorcanbeformedbyreplacingÏ2(X )withthesquaredresidualse2orprediction i (cid:98)i errorse2 (cid:101)i (cid:195) (cid:33) V(cid:98)Î²(cid:98) (x)=(cid:161) Z (cid:48) KZ (cid:162)â1 (cid:88) n K (cid:181) X i h âx (cid:182)2 Z i (x)Z i (x) (cid:48) e (cid:101)i 2 (cid:161) Z (cid:48) KZ (cid:162)â1 . (19.18) i=1 Alternatively,Ï2(X )couldbereplacedwithanestimatorsuchas(19.17)evaluatedatÏ2(X )orÏ2(x). i (cid:98) i (cid:98) Asimpleoptionistheasymptoticformula R Ï2(x) V(cid:98)m(cid:98)(x) = n K h (cid:98) f(cid:98)(x)",
    "page": 704,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 685 withÏ (cid:98) 2(x)from(19.17)and f(cid:98)(x)adensityestimatorsuchas f(cid:98)(x)= 1 (cid:88) n K (cid:181) X i âx (cid:182) (19.19) nb b i=1 wherebisabandwidth.(SeeChapter17ofIntroductiontoEconometrics.) Ingeneralwerecommend(19.18)calculatedwithpredictionerrorsasthisistheclosestanalogofthe finitesamplecovariancematrix. ForlocallinearandlocalpolynomialestimatorstheestimatorV(cid:98)m(cid:98)(x) isthefirstdiagonalelementof thematrixV(cid:98)Î²(cid:98) (x).Foranyofthevarianceestimatorsastandarderrorform (cid:98) (x)isthesquarerootofV(cid:98)m(cid:98)(x) . 19.17 ConfidenceBands Wecanconstructasymptoticconfidenceintervals.An95%intervalform(x)is (cid:113) m (cid:98) (x)Â±1.96 V(cid:98)m(cid:98)(x) . (19.20) Thisconfidenceintervalcanbeplottedalongwithm(x)toassessprecision. (cid:98) It should be noted, however, that this confidence interval has two unusual properties. First, it is pointwiseinx,meaningthatitisdesignedtohavecoverageprobabilityateachx notuniformlyacross x.Thustheyaretypicallycalledpointwiseconfidenceintervals. Second, becauseitdoesnotaccountforthebiasitisnotanasymptoticallyvalidconfidenceinter- val for m(x). Rather, it is an asymptotically valid confidence interval for the pseudo-true (smoothed) value,e.g.m(x)+h2B(x).Onewayofthinkingaboutthisisthattheconfidenceintervalsaccountforthe variance of the estimator but not its bias. A technical trick which solves this problem is to assume an undersmoothingbandwidth. Inthiscasetheaboveconfidenceintervalsaretechnicallyasymptotically valid.Thisisonlyatechnicaltrickasitdoesnotreallyeliminatethebiasonlyassumesitaway.Theplain fact is that once we honestly acknowledge that the true CEF is nonparametric it then follows that any finitesampleestimatorwillhavefinitesamplebiasandthisbiaswillbeinherentlyunknownandthus difficulttoincorporateintoconfidenceintervals. Despitetheseunusualpropertieswecanstillusetheinterval(19.20)todisplayuncertaintyandasa checkontheprecisionoftheestimates. 19.18 TheLocalNatureofKernelRegression Thekernelregressionestimators(Nadaraya-Watson,LocalLinear,andLocalPolynomial)arealles- sentiallylocalestimatorsinthatgivenhtheestimatorm(x)isafunctiononlyofthesub-sampleforwhich (cid:98) X iscloseto x. Theotherobservationsdonotdirectlyaffecttheestimator. Thisisreflectedinthedis- tributiontheoryaswell. Theorem19.8showsthatm(x)isconsistentform(x)ifthelatteriscontinuous (cid:98) atx. Theorem19.9showsthattheasymptoticdistributionofm(x)dependsonlyonthefunctionsm(x), (cid:98) f(x)andÏ2(x)atthepointx.Thedistributiondoesnotdependontheglobalbehaviorofm(x). Global features do affect the estimator m(x), however, through the bandwidth h. The bandwidth (cid:98) selectionmethodsdescribedhereareglobalinnatureastheyattempttominimizeAIMSE.Localband- widths(designedtominimizetheAMSEatasinglepointx)canalternativelybeemployedbuttheseare lesscommonlyused, inpartbecausesuchbandwidthestimatorshavehighimprecision. Pickinglocal bandwidthsaddsextranoise",
    "page": 705,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Global features do affect the estimator m(x), however, through the bandwidth h. The bandwidth (cid:98) selectionmethodsdescribedhereareglobalinnatureastheyattempttominimizeAIMSE.Localband- widths(designedtominimizetheAMSEatasinglepointx)canalternativelybeemployedbuttheseare lesscommonlyused, inpartbecausesuchbandwidthestimatorshavehighimprecision. Pickinglocal bandwidthsaddsextranoise. Furthermore,selectedbandwidthsmaybemeaningfullylargesothattheestimationwindowmaybe alargeportionofthesample.Inthiscaseestimationisneitherlocalnorfullyglobal.",
    "page": 705,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 686 19.19 ApplicationtoWageRegression WeillustratethemethodswithanapplicationtothetheCPSdataset. Weareinterestedinthenon- parametricregressionoflog(wage)onexperience.ToillustratewetakethesubsampleofBlackmenwith 12yearsofeducation(highschoolgraduates).Thissamplehas762observations. Wefirstneedtodecideontheregionofinterest(rangeofexperience)forwhichwewillcalculatethe regression estimator. We select the range [0,40] since most observations (90%) have experience levels below40years. Toavoidboundarybiasweusethelocallinearestimator. WenextcalculatetheFan-Gijbelsrule-of-thumbbandwidth(19.9)andfindh =5.14. Wethencal- rot culatethecross-validationcriterionusingtherule-of-thumbasabaseline.TheCVcriterionisdisplayed inFigure19.4(a).Theminimizerish =4.32whichissomewhatsmallerthantheROTbandwidth. cv We calculate the local linear estimator using both bandwidths and display the estimates in Figure 19.4(b). Theregressionfunctionsare increasingforexperience levels upto20 yearsandthenbecome flat. While the functions are roughly concave they are noticably different than a traditional quadratic specification. Comparingtheestimates,thesmallerCV-selectedbandwidthproducesaregressionesti- matewhichisabittoowavywhiletheROTbandwidthproducesaregressionestimatewhichismuch smootheryetcapturesthesameessentialfeatures.Basedonthisinspectionweselecttheestimatebased ontheROTbandwidth(thesolidlineinpanel(b)). 0132.0 5032.0 0032.0 5922.0 h )h(VC 2 3 4 5 6 7 8 9 10 11 (a)Cross-ValidationCriterion 7.2 6.2 5.2 4.2 3.2 2.2 1.2 0.2 experience )egaw(gol hROT hCV 0 5 10 15 20 25 30 35 40 (b)LocalLinearRegression Figure19.4:log(wage)regressiononexperience Wenextconsiderestimationoftheconditionalvariancefunction.WecalculatetheROTbandwidth foraregressionusingthesquaredpredictionerrorsandfindh =6.77whichislargerthantheband- rot widthusedforconditionalmeanestimation.Wenextcalculatethecross-validationfunctionsforcondi- tionalvarianceestimation(regressionofsquaredpredictionerrorsonexperience)usingbothNWandLL regression. TheCVfunctionsaredisplayedinFigure19.5(a). TheCVplotsarequiteinteresting. Forthe LLestimatortheCVfunctionhasalocalminimumaroundh=5buttheglobalminimizerisunbounded. TheCVfunctionfortheNWestimatorisgloballydecreasingwithanunboundedminimizer.TheNWalso achievesaconsiderablylowerCVvaluethantheLLestimator.ThismeansthattheCV-selectedvariance",
    "page": 706,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 687 estimatoristheNWestimatorwithh=â,whichisthesimplefull-sampleestimatorÏ2calculatedwith (cid:98) thepredictionerrors. Wenextcomputestandarderrorsfortheregressionfunctionestimatesusingformula(19.18).InFig- ure19.5(b)wedisplaytheestimatedregression(thesameasFigure19.4usingtheROTbandwidth)along with95%asymptoticconfidencebandscomputedasin(19.20). Bydisplayingtheconfidencebandswe canseethatthereisconsiderableimprecisionintheestimatorforlowexperiencelevels.Wecanstillsee thattheestimatesandconfidencebandsshowthattheexperienceprofileisincreasinguptoabout20 yearsofexperienceandthenflattensabove20years.Theestimatesimplythatforthispopulation(Black menwhoarehighschoolgraduates)theaveragewagerisesforthefirst20yearsofworkexperience(from 18to38yearsofage)andthenflattenswithnofurtherincreasesinaveragewagesforthenext20yearsof workexperience(from38to58yearsofage). 5 10 15 20 25 30 35 40 212.1 012.1 802.1 602.1 402.1 h )h(VC NadarayaâWatson Local Linear (a)Cross-ValidationforConditionalVariance 7.2 6.2 5.2 4.2 3.2 2.2 1.2 0.2 experience )egaw(gol 0 5 10 15 20 25 30 35 40 (b)RegressionwithConfidenceBands Figure19.5:ConfidenceBandConstruction 19.20 ClusteredObservations Clusteredobservationsare(Y ,X )forindividualsi =1,...,n inclusterg =1,...,G.Themodelis ig ig g Y =m (cid:161) X (cid:162)+e ig ig ig (cid:69)(cid:163) e |X (cid:164)=0 ig g whereX isthestackedX .Theassumptionisthattheclustersaremutuallyindependent.Dependence g ig withineachclusterisunstructured. Write (cid:181) (cid:182) 1 Z (x)= . ig X âx ig (cid:189) (cid:181)X âx(cid:182)(cid:190) StackY ,e and Z (x)intocluster-levelvariablesY ,e and Z (x). LetK (x)=diag K ig . ig ig ig g g g g h",
    "page": 707,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 688 Thelocallinearestimatorcanbewrittenas Î² (cid:98)(x)= (cid:195) (cid:88) G (cid:88) ng K (cid:181)X ig âx(cid:182) Z ig (x)Z ig (x) (cid:48) (cid:33)â1(cid:195) (cid:88) G (cid:88) ng K (cid:181)X ig âx(cid:182) Z ig (x)Y ig (cid:33) g=1i=1 h g=1i=1 h (cid:195) (cid:33)â1(cid:195) (cid:33) G G = (cid:88) Z (x) (cid:48) K (x)Z (x) (cid:88) Z (x) (cid:48) K (x)Y . (19.21) g g g g g g g=1 g=1 Thelocallinearestimatorm (cid:98) (x)=Î² (cid:98)1 (x)istheinterceptin(19.21). The natural method to obtain prediction errors is by delete-cluster regression. The delete-cluster estimatorofÎ²is (cid:195) (cid:33)â1(cid:195) (cid:33) Î² (cid:101)(âg) (x)= (cid:88) Z j (x) (cid:48) K j (x)Z j (x) (cid:88) Z j (x) (cid:48) K j (x)Y j . (19.22) j(cid:54)=g j(cid:54)=g Thedelete-clusterestimatorofm(x)istheinterceptm (cid:101)1 (x)=Î² (cid:101)1(âg) (x)from(19.22). Thedelete-cluster predictionerrorforobservationig is e (cid:101)ig =Y ig âÎ² (cid:101)1(âg) (X ig ). (19.23) Lete bethestackede forclusterg. (cid:101)g (cid:101)ig Thevarianceof(19.21),conditionalontheregressorsX,is (cid:195) (cid:33)â1(cid:195) (cid:33)(cid:195) (cid:33)â1 G G G V (x)= (cid:88) Z (x) (cid:48) K (x)Z (x) (cid:88) Z (x) (cid:48) K (x)S (x)K (x)Z (x) (cid:88) Z (x) (cid:48) K (x)Z (x) Î²(cid:98) g g g g g g g g g g g g=1 g=1 g=1 (19.24) (cid:104) (cid:105) where S =(cid:69) e e (cid:48) |X . The covariance matrix (19.24) can be estimated by replacing S with an es- g g g g g (cid:48) timatorofe e . Basedonanalogywithregressionestimationwesuggestthedelete-clusterprediction g g errorse astheyarenotsubjecttoover-fitting.Thiscovariancematrixestimatorusingthischoiceis (cid:101)g (cid:195) (cid:33)â1(cid:195) (cid:33)(cid:195) (cid:33)â1 G G G V(cid:98)Î²(cid:98) (x)= (cid:88) Z g (x) (cid:48) K g (x)Z g (x) (cid:88) Z g (x)K g (x) (cid:101) e g(cid:101) e (cid:48) g K g (x)Z g (x) (cid:88) Z g (x)K g (x)Z g (x) . g=1 g=1 g=1 (19.25) Thestandarderrorform (cid:98) (x)isthesquarerootofthefirstdiagonalelementofV(cid:98)Î²(cid:98) (x). There is no current theory on how to select the bandwidth h for nonparametric regression using clusteredobservations.TheFan-GhybelsROTbandwidthh isdesignedforindependentobservations rot so is likely to be a crude choice in the case of clustered observations. Standard cross-validation has similarlimitations.Apracticalalternativeistoselectthebandwidthhtominimizeadelete-clustercross- valiationcriterion.Whilethereisnoformaltheorytojustifythischoice,itseemslikeareasonableoption. Thedelete-clusterCVcriterionis CV(h)= 1 (cid:88) G (cid:88) ng e2 n g=1i=1 (cid:101)ig wheree arethedelete-clusterpredictionerrors(19.23). Thedelete-clusterCVbandwidthisthevalue (cid:101)ig whichminimizesthisfunction: h =argminCV(h). cv hâ¥h(cid:96) Asforthecaseofconventionalcross-validation,itmaybevaluabletoplotCV(h)againsthtoverifythat theminimumhasbeenobtainedandtoassesssensitivity.",
    "page": 708,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 689 19.21 ApplicationtoTestscores We illustrate kernel regression with clustered observations by using the Duflo, Dupas and Kremer (2011)investigationoftheeffectofstudenttrackingontestscores.Recallthatthecorequestionwaseffect of the dummy variable tracking on the continuous variable testscore. A set of controls were included includingacontinuousvariablepercentilewhichrecordedthestudentâsinitialtestscore(asapercentile). Weinvestigatetheauthorsâspecificationofthiscontrolusinglocallinearregression. Wetookthesubsampleof1487girlswhoexperiencedtrackingandestimatedtheregressionoftestscores onpercentile. Forthisapplicationweusedunstandardized7testscoreswhichrangefrom0toabout40. WeusedlocallinearregressionwithaGaussiankernel. Firstconsiderbandwidthselection. TheFan-GhybelsROTandconventionalcross-validationband- widthsareh =6.7andh =12.3.Wethencalculatedtheclusteredcross-validationcriterionwhichhas rot cv minimizerh =6.2. Tounderstandthedifferencesweplotthestandardandclusteredcross-validation cv functionsinFigure19.6(a). Inordertoplotonthesamegraphwenormalizeeachbysubtractingtheir minimizedvalue(soeachisminimizedatzero). WhatwecanseefromFigure19.6(a)isthatwhilethe conventionalCVcriterionissharplyminimizedath=12.3,theclusteredCVcriterionisessentiallyflat between5and11.ThismeansthattheclusteredCVcriterionhasdifficultydiscriminatingbetweenthese bandwidthchoices 01.0 80.0 60.0 40.0 20.0 00.0 h )h(VC CV(h) Clustered CV(h) 4 6 8 10 12 14 16 18 20 (a)Cross-ValidationCriterion 03 52 02 51 01 5 0 Initial Percentile erocstseT LL Using Conventional CV Bandwith LL Using Clustered CV Bandwith 0 10 20 30 40 50 60 70 80 90 100 (b)LocalLinearRegression 03 52 02 51 01 5 0 Initial Percentile erocstseT m(x) Linear Model 0 10 20 30 40 50 60 70 80 90 100 (c)ConfidenceBands Figure19.6:TestScoreasaFunctionofInitialPercentile To compare the estimated regression functions, in Figure 19.6(b) we plot the estimated regression functionswhichusethebandwidthsselectedbyconventionalandclusteredcross-validation. Inspect- ing the plots, the estimator using the conventional CV bandwidth is smoother than the estimator us- ingthesmallerclusteredCVbandwidth. Themostnoticeabledifferencesarisesattherightendofthe plotwhichshowstheexpectedtestscoreforthestudentswhohadtheverybestpreliminarytestscores. The estimator using the clustered CV bandwidth shows a meaningful upturn for students with initial testscorepercentileabove90%.Basedonthisevidenceweselectthelocallinearestimatorm (x)using (cid:98)LL theclusteredcross-validationbandwidthh =6.2. cv Usingthisbandwidthweestimatethedelete-clusterpredictionerrorse andusethesetocalculate (cid:101)g thestandarderrorsforthelocallinearestimatorm (x)usingformula(19.25).Thesestandarderrorsare (cid:98)LL 7InSection4.23,followingDuflo,DupasandKremer(2011)thedependentvariablewasstandardizedtestscores(normalized tohavemeanzeroandvarianceone).",
    "page": 709,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 690 roughlytwiceaslargeasthosecalculatedusingthenon-clusteredformula. Weusethestandarderrors tocalculate95%asymptoticpointwiseconfidencebandsasin(19.20). Figure19.6(c)showsourestimatedregressionfunctionandpointwise95%confidencebands. Also plottedforcomparisonisanestimatedlinearregressionline. Thelocallinearestimatorissimilartothe globallinearregressionestimatorforinitialpercentilesbelow80%. Butforinitialpercentilesabove80% thetwolinesdiverge. Theconfidencebandssuggestthatthesedifferencesarestatisticallymeaningful. Studentswithinitialtestscoresatthetopoftheinitialdistributionhavehigherfinaltestscoresonaverage thanpredictedbyalinearspecification. 19.22 MultipleRegressors Ouranalysishasfocusonthecaseofreal-valuedX forsimplicity,butthemethodsofkernelregres- sionextendtothemultipleregressorcaseatthecostofareducedrateofconvergence.Inthissectionwe considerthecaseofestimationoftheconditionalexpectationfunction(cid:69)[Y |X =x]=m(x)where ï£« ï£¶ X 1 X =ï£¬ ï£­ . . . ï£· ï£¸ â(cid:82)d. X d Foranyevaluationpointxandobservationi definethekernelweights (cid:181) X âx (cid:182) (cid:181) X âx (cid:182) (cid:181) X âx (cid:182) K (x)=K 1i 1 K 2i 2 Â·Â·Â·K di d , i h h h 1 2 d ad-foldproductkernel. ThekernelweightsK (x)assessiftheregressorvectorX isclosetotheevalua- i i tionpointxintheEuclideanspace(cid:82)d. Theseweightsdependonasetofd bandwidths,h ,oneforeachregressor. Giventheseweights,the j Nadaraya-Watsonestimatortakestheform n (cid:88) K (x)Y i i m(x)= i=1 . (cid:98) n (cid:88) K (x) i i=1 Forthelocal-linearestimator,define (cid:181) (cid:182) 1 Z (x)= i X âx i andthenthelocal-linearestimatorcanbewrittenasm(x)=Î±(x)where (cid:98) (cid:98) (cid:181) Î± (cid:98) (x) (cid:182) = (cid:195) (cid:88) n K (x)Z (x)Z (x) (cid:48) (cid:33)â1 (cid:88) n K (x)Z (x)Y Î² (cid:98)(x) i=1 i i i i=1 i i i =(cid:161) Z (cid:48) KZ (cid:162)â1 Z (cid:48) KY whereK =diag{K (x),...,K (x)}. 1 n Inmultipleregressorkernelregressioncross-validationremainsarecommendedmethodforband- widthselection. Theleave-one-outresidualse andcross-validationcriterionCV(h ,...,h )aredefined (cid:101)i 1 d identicallyasinthesingleregressorcase. TheonlydifferenceisthatnowtheCVcriterionisafunction overthed bandwidthsh ,...,h . Thismeansthatnumericalminimizationneedstobedonemoreeffi- 1 d cientlythanbyasimplegridsearch.",
    "page": 710,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 691 The asymptotic distribution of the estimators in the multiple regressor case is an extension of the singleregressorcase. Let f(x)denotethemarginaldensityofX,Ï2(x)=(cid:69)(cid:163) e2|X =x (cid:164) denotethecondi- tionalvarianceofe=Y âm(X),andset|h|=h h Â·Â·Â·h . 1 2 d Proposition19.1 Letm(x)denoteeithertheNadarya-WatsonorLocalLinear (cid:98) estimatorofm(x).Asnââandh â0suchthatn|h|ââ, j (cid:112) n|h| (cid:195) m(x)âm(x)â (cid:88) d h2B (x) (cid:33) ââN (cid:195) 0, R K dÏ2(x) (cid:33) . (cid:98) j=1 j j d f(x) FortheNadaraya-Watsonestimator B (x)= 1 â2 m(x)+f(x) â1 â f(x) â m(x) j 2âx2 j âx j âx j andfortheLocalLinearestimator 1 â2 B (x)= m(x). j 2âx2 j WedonotprovideregularityconditionsoraformalproofbutinsteadreferinterestedreaderstoFan andGijbels(1996). 19.23 CurseofDimensionality Thetermâcurseofdimensionalityâisusedtodescribethephenomenonthattheconvergencerateof nonparametricestimatorsslowsasthedimensionincreases. WhenX isvector-valuedwedefinetheAIMSEastheintegralofthesquaredbiasplusvariance,inte- gratingwithrespectto f(x)w(x)wherew(x)isanintegrableweightfunction. Fornotationalsimplicity considerthecasethatthereisasinglecommonbandwidthh.InthiscasetheAIMSEofm(x)equals (cid:98) (cid:90) (cid:195) d (cid:33)2 Rd (cid:90) AIMSE=h4 (cid:88) B (x) f(x)w(x)dx+ K Ï2(x)w(x)dx. j S j=1 nhd S Weseethatthesquaredbiasisoforderh4,thesameasinthesingleregressorcase. Thevariance,how- ever,isoflargerorder(nhd) â1. IfpickthebandwithtominimizingtheAIMSEwefindthatitequalsh=cn â1/(4+d)forsomeconstant c. Thisgeneralizestheformulafortheone-dimensionalcase. Theraten â1/(4+d)isslowerthanthen â1/5 rate.Thiseffectivelymeansthatwithmultipleregressorsalargerbandwidthisrequired. Whenthebandwidthissetash=cn â1/(4+d)thentheAIMSEisoforderO (cid:161) n â4/(4+d)(cid:162) .Thisisaslower rateofconvergencethanintheone-dimensionalcase. Theorem19.11 For vector-valued X the bandwidth which minimizes the AIMSEisoforderhâ¼n â1/(4+d).Withhâ¼n â1/(4+d)thenAIMSE=O (cid:161) n â4/(4+d)(cid:162) .",
    "page": 711,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 692 SeeExercise19.6. WeseethattheoptimalAIMSErateO (cid:161) n â4/(4+d)(cid:162) dependsonthedimensiond. Asd increasesthis rate slows. Thus the precision of kernel regression estimators worsens with multiple regressors. The reasonistheestimatorm(x)isalocalaverageofY forobservationssuchthatX isclosetox,andwhen (cid:98) therearemultipleregressorsthenumberofsuchobservationsisinherentlysmaller. Thisphenomenonâthattherateofconvergenceofnonparametricestimationdecreasesasthedi- mension increases â is called the curse of dimensionality. It is common across most nonparametric estimationproblemsandisnotspecifictokernelregression. The curse of dimensionality has led to the practical rule thatmost applications of non-parametric regressionhaveasingleregressor.Somehavetworegressors;onoccassion,three.Moreisuncommon. 19.24 PartiallyLinearRegression Tohandlediscreteregressorsand/orreducethedimensionalitywecanseparatetheregressionfunc- tionintoanonparametricandaparametricpart.Lettheregressorsbepartitionedas(X,Z)whereX and Z ared-andk-dimensional,respectively.Apartiallylinearregressionmodelis Y =m(X)+Z (cid:48)Î²+e (19.26) (cid:69)[e|X,Z]=0. Thismodelcombinestwoelements. One,itspecifiesthattheconditionalmeanisseparablebetweenX andZ (therearenononparametricinteractions). Two,itspecifiesthattheconditionalmeanislinearin theregressorsZ.Theseareassumptionswhichmaybetrueormaybefalse.Inpracticeitisbesttothink oftheassumptionsasapproximations. When some regressors are discrete (as is common in econometric applications) they belong in Z. The regressors X must be continuously distributed. In typical applications X is either scalar or two- dimensional. This may not be a restriction in practice as many econometric applications only have a smallnumberofcontinuouslydistributedregressors. The seminal contribution for estimation of(19.26) isRobinson (1988) who proposed a nonparma- metricversionofresidualregression. Hiskeyinsightwastoseethatthenonparametriccomponentcan beeliminatedbytransformation.Taketheexpectationofequation(19.26)conditionalonX.Thisis (cid:69)[Y |X]=m(X)+(cid:69)[Z |X] (cid:48)Î². Subtractthisfrom(19.26),obtaining Y â(cid:69)[Y |X]=(Zâ(cid:69)[Z |X]) (cid:48)Î²+e. ThemodelisnowalinearregressionofthenonparametricregressionerrorY â(cid:69)[Y |X]onthevectorof nonparametricregressionerrorsZâ(cid:69)[Z |X]. Robinsonâsestimatorreplacestheinfeasibleregressionerrorsbynonparametriccounterparts. The resultisathree-stepestimator. 1. Usingnonparametricregression(NWorLL),regressY onX ,Z onX ,Z onX ,...,andZ on i i 1i i 2i i ki X ,obtainingthefittedvaluesg ,g ,...,andg . i (cid:98)0i (cid:98)1i (cid:98)ki 2. RegressY i âg (cid:98)0i onZ 1i âg (cid:98)1i ,...,Z ki âg (cid:98)ki toobtainthecoefficientestimateÎ² (cid:98)andstandarderrors. 3. Use nonparametric regression to regress Y i âZ i (cid:48)Î² (cid:98)on X i to obtain the nonparametric estimator m(x)andconfidenceintervals. (cid:98)",
    "page": 712,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 693 Theresultingestimatorsandstandarderrorshaveconventionalasymptoticdistributionsunderspe- cificassumptionsonthebandwidths. AfullproofisprovidedbyRobinson(1988). Andrews(2004)pro- videsamoregeneraltreatmentwithinsighttothegeneralstructureofsemiparametricestimators. ThemostdifficultchallengeistoshowthattheasymptoticdistributionÎ² (cid:98)isunaffectedbythefirst stepestimation.Briefly,thesearethestepsoftheargument.First,thefirst-steperrorZâ(cid:69)[Z |X]haszero covariancewiththeregressionerrore.Second,theasymptoticdistributionwillbeunaffectedbythefirst- stepestimationifreplacing(inthiscovariance)theexpectation(cid:69)[Z |X]withitsfirst-stepnonparametric estimatorinducesanerrorofordero (cid:161) n â1/2(cid:162) . Third,sincethecovarianceisaproduct,thisholdswhen p thefirst-stepestimatorhasaconvergencerateofo (cid:161) n â1/4(cid:162) . Fourth,thisholdsunderTheorem19.11if p hâ¼n â1/(4+d)andd<4. Thereasonwhythethirdstepestimatorhasaconventionalasymptoticdistributionisabitsimpler toexplain. TheestimatorÎ² (cid:98)convergesataconventionalO p (cid:161) n â1/2(cid:162) rate. Thenonparametricestimator m (cid:98) (x)convergesatarateslowerthanO p (cid:161) n â1/2(cid:162) .ThusthesamplingerrorforÎ² (cid:98)isoflowerorderanddoes notaffectthefirst-orderasymptoticdistributionofm(x). (cid:98) Onceagain,thetheoryisadvancedsotheabovetwoparagraphsshouldnotbetakenasanexplana- tion.Thegoodnewsisthattheestimationmethodisstraightforward. 19.25 Computation Statahastwocommandswhichimplementkernelregression: lpolyandnpregress. lpolyimple- mentslocalpolynomialestimationforanyp,includingNadaraya-Watson(thedefault)andlocallinear estimation, and selects the bandwidth using the Fan-Gijbels ROT method. It uses the Epanechnikov kernel by default but the Gaussian can be selected as an option. The lpoly command automatically displaystheestimatedmeanfunctionalongwith95%confidencebandswithstandarderrorscomputed using(19.18). TheStatacommandnpregressestimateslocallinear(thedefault)orNadaraya-Watsonregression. Bydefaultitselectsthebandwidthbycross-validation. ItusestheEpanechnikovkernelbydefaultbut theGaussiancanbeselectedasanoption. Confidenceintervalsmaybecalculatedusingthepercentile bootstrap. A display of the estimated mean and 95% confidence bands at specific points (computed usingthepercentilebootstrap)maybeobtainedwiththepostestimationcommandmargins. ThereareseveralRpackageswhichimplementkernelregression.Oneflexiblechoiceisnpregavail- ableinthenppackage. ItsdefaultmethodisNadaraya-WatsonestimationusingaGaussiankernelwith bandwidthselectedbycross-validation.Thereareoptionswhichallowlocallinearandlocalpolynomial estimation,alternativekernels,andalternativebandwidthselectionmethods. 19.26 TechnicalProofs* ForalltechnicalproofswemakethesimplifyingassumptionthatthekernelfunctionK(u)hasbounded support,thusK(u)=0for|u|>a. TheresultsextendtotheGaussiankernelbutwithadditiontechnical arguments",
    "page": 713,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". ItsdefaultmethodisNadaraya-WatsonestimationusingaGaussiankernelwith bandwidthselectedbycross-validation.Thereareoptionswhichallowlocallinearandlocalpolynomial estimation,alternativekernels,andalternativebandwidthselectionmethods. 19.26 TechnicalProofs* ForalltechnicalproofswemakethesimplifyingassumptionthatthekernelfunctionK(u)hasbounded support,thusK(u)=0for|u|>a. TheresultsextendtotheGaussiankernelbutwithadditiontechnical arguments. ProofofTheorem19.1.1.Equation(19.3)showsthat b(cid:98)(x) (cid:69)[m (x)|X]=m(x)+ (19.27) (cid:98)nw f(cid:98)(x)",
    "page": 713,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 694 where f(cid:98)(x)isthekerneldensityestimator(19.19)of f(x)withb=hand b(cid:98)(x)= 1 (cid:88) n K (cid:181) X i âx (cid:182) (m(x i )âm(x)). (19.28) nh h i=1 Theorem17.6ofIntroductiontoEconometricsestablishedthat f(cid:98)(x)ââ f(x). Theproofiscompletedby (cid:112) p (cid:179) (cid:180) showingthatb(cid:98)(x)=h2f(x)B nw (x)+o p h2+1/ nh . Sinceb(cid:98)(x)isasampleaverageithastheexpectation (cid:69)(cid:163) b(cid:98)(x) (cid:164)= 1 (cid:69) (cid:183) K (cid:181) Xâx (cid:182) (m(X)âm(x)) (cid:184) h h (cid:90) â 1 (cid:179)vâx(cid:180) = K (m(v)âm(x))f(v)dv ââh h (cid:90) â = K(u)(m(x+hu)âm(x))f(x+hu)du. (19.29) ââ ThesecondequalitywritestheexpectationasanintegralwithrespecttothedensityofX.Thethirduses thechange-of-variablesv=x+hu.WenextusethetwoTaylorseriesexpansions m(x+hu)âm(x)=m (cid:48) (x)hu+ 1 m (cid:48)(cid:48) (x)h2u2+o(h2) (19.30) 2 f(x+hu)=f(x)+f (cid:48) (x)hu+o(h). Insertedinto(19.29)wefindthat(19.29)equals (cid:90) â K(u) (cid:181) m (cid:48) (x)hu+ 1 m (cid:48)(cid:48) (x)h2u2+o(h2) (cid:182) (cid:161) f(x)+f (cid:48) (x)hu+o(h) (cid:162) du (19.31) ââ 2 (cid:181)(cid:90) â (cid:182) =h uK(u)du m (cid:48) (x) (cid:161) f(x)+o(h) (cid:162) (19.32) ââ +h2 (cid:181)(cid:90) â u2K(u)du (cid:182)(cid:181) 1 m (cid:48)(cid:48) (x)f(x)+m (cid:48) (x)f (cid:48) (x) (cid:182) ââ 2 +h3 (cid:181)(cid:90) â u3K(u)du (cid:182) 1 m (cid:48)(cid:48) (x)f (cid:48) (x)+o(h2) ââ 2 (cid:181) (cid:182) =h2 1 m (cid:48)(cid:48) (x)f(x)+m (cid:48) (x)f (cid:48) (x) +o(h2) 2 =h2B (x)f(x)+o(h2). nw ThesecondequalityusesthefactthatthekernelK(x)integratestoone,itsoddmomentsarezero,and thekernelvarianceisone.Wehaveshownthat(cid:69)(cid:163) b(cid:98)(x) (cid:164)=B nw (x)f(x)h2+o(h2). Nowconsiderthevarianceofb(cid:98)(x). Sinceb(cid:98)(x)isasampleaverageofindependentcomponentsand thevarianceissmallerthanthesecondmoment var (cid:163) b(cid:98)(x) (cid:164)= 1 var (cid:183) K (cid:181) Xâx (cid:182) (m(X)âm(x)) (cid:184) nh2 h 1 (cid:183) (cid:181) Xâx (cid:182)2 (cid:184) â¤ (cid:69) K (m(X)âm(x))2 nh2 h 1 (cid:90) â = K(u)2(m(x+hu)âm(x))2f(x+hu)du (19.33) nh ââ = 1 (cid:90) â u2K(u)2du (cid:161) m (cid:48) (x) (cid:162)2 f(x) (cid:161) h2+o(1) (cid:162) nh ââ (cid:181) (cid:182) â¤ h K (cid:161) m (cid:48) (x) (cid:162)2 f(x)+o h . n n",
    "page": 714,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 695 The second equality writes the expectation as an integral. The third uses (19.30). The final inequality usesK(u)â¤K fromDefinition19.1.1andthefactthatthekernelvarianceisone.Thisshowsthat (cid:181) (cid:182) var (cid:163) b(cid:98)(x) (cid:164)â¤O h . n Togetherweconcludethat ï£«(cid:115) ï£¶ b(cid:98)(x)=h2f(x)B nw (x)+o (cid:161) h2(cid:162)+O pï£­ h ï£¸ n and ï£«(cid:115) ï£¶ b(cid:98)(x) =h2B nw (x)+o p (cid:161) h2(cid:162)+O pï£­ h ï£¸. (19.34) f(cid:98)(x) n Togetherwith(19.27)thisimpliesTheorem19.1.1. â  ProofofTheorem19.2.1.Equation(19.4)statesthat v(x) nhvar[m (x)|X]= (cid:98) (cid:98)nw f(cid:98)(x)2 where v(x)= 1 (cid:88) n K (cid:181) X i âx (cid:182)2 Ï2(X ) (cid:98) i nh h i=1 and f(cid:98)(x) is the estimator (19.19) of f(x). Theorem 17.6 of Introduction to Econometrics established f(cid:98)(x)ââf(x).Theproofiscompletedbyshowingv (cid:98) (x)ââR K Ï2(x)f(x). p p First, writing the expectation as an integral with respect to f(x), making the change-of-variables v=x+hu,andappealingtothecontinuityofÏ2(x)and f(x)atx, (cid:90) â 1 (cid:179)vâx(cid:180)2 (cid:69)[v(x)]= K Ï2(v)f(v)dv (cid:98) ââh h (cid:90) â = K(u)2Ï2(x+hu)f(x+hu)du ââ (cid:90) â = K(u)2Ï2(x)f(x)+o(1) ââ =R Ï2(x)f(x). K Second,sincev(x)isanaverageofindependentrandomvariablesandthevarianceissmallerthan (cid:98) thesecondmoment 1 (cid:183) (cid:181) Xâx (cid:182)2 (cid:184) nhvar[v(x)]= var K Ï2(X) (cid:98) h h 1(cid:90) â (cid:179)vâx(cid:180)4 â¤ K Ï4(v)f(v)dv h ââ h (cid:90) â = K(u)4Ï4(x+hu)f(x+hu)du ââ â¤K 2 R Ï4(x)f(x)+o(1) k sovar[v(x)]â0. (cid:98)",
    "page": 715,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 696 WededucefromMarkovâsinequalitythatv(x)ââR Ï2(x)f(x),completingtheproof. â  (cid:98) K p ProofofTheorem19.7. Observethatm(X i )âm (cid:101)âi (X i ,h)isafunctiononlyof(X 1 ,...,X n )and(e 1 ,...,e n ) excludinge i ,andisthusuncorrelatedwithe i .Sincee (cid:101)i (h)=m(X i )âm (cid:101)âi (X i ,h)+e i ,then (cid:69)[CV(h)]=(cid:69)(cid:161) e (h)2w(X ) (cid:162) (cid:101)i i =(cid:69)(cid:163) e i 2w(X i ) (cid:164)+(cid:69)(cid:163) (m (cid:101)âi (X i ,h)âm(X i ))2w(X i ) (cid:164) +2(cid:69)[(m (cid:101)âi (X i ,h)âm(X i ))w(X i )e i ] =Ï2+(cid:69)(cid:163) (m (cid:101)âi (X i ,h)âm(X i ))2w(X i ) (cid:164) . (19.35) Thesecondtermisanexpectationovertherandomvariables X i andm (cid:101)âi (x,h),whichareindependent as the second is not a function of the ith observation. Thus taking the conditional expectation given thesampleexcludingtheith observation,thisistheexpectationoverX only,whichistheintegralwith i respecttoitsdensity (cid:90) (cid:69) âi (cid:163) (m (cid:101)âi (X i ,h)âm(X i ))2w(X i ) (cid:164)= (m (cid:101)âi (x,h)âm(x))2f(x)w(x)dx. Takingtheunconditionalexpecationyields (cid:183)(cid:90) (cid:184) (cid:69)(cid:163) (m (cid:101)âi (X i ,h)âm(X i ))2w(X i ) (cid:164)=(cid:69) (m (cid:101)âi (x,h)âm(x))2f(x)w(x)dx =IMSE nâ1 (h) wherethisistheIMSEofasampleofsizenâ1astheestimatorm (cid:101)âi usesnâ1observations. Combined with(19.35)weobtain(19.12),asdesired. â  ProofofTheorem19.8.WecanwritetheNadaraya-Watsonestimatoras m (x)=m(x)+ b(cid:98)(x) + g (cid:98) (x) (19.36) (cid:98)nw f(cid:98)(x) f(cid:98)(x) where f(cid:98)(x)istheestimator(19.19),b(cid:98)(x)isdefinedin(19.28),and g(x)= 1 (cid:88) n K (cid:181) X i âx (cid:182) e . (19.37) (cid:98) i nh h i=1 Sincef(cid:98)(x)ââf(x)>0byTheorem17.6ofIntroductiontoEconometrics,theproofiscompletedbyshow- p ingb(cid:98)(x)ââ0andg (cid:98) (x)ââ0. p p Takeb(cid:98)(x).From(19.29)andthecontinuityofm(x)and f(x) (cid:90) â (cid:69)(cid:163) b(cid:98)(x) (cid:164)= K(u)(m(x+hu)âm(x))f(x+hu)du=o(1) ââ ashââ.From(19.33), (cid:90) â nhvar (cid:163) b(cid:98)(x) (cid:164)â¤ K(u)2(m(x+hu)âm(x))2f(x+hu)du=o(1) ââ ashââ.Thusvar (cid:163) b(cid:98)(x) (cid:164)ââ0.ByMarkovâsinequalityweconcludeb(cid:98)(x)âpâ0.",
    "page": 716,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 697 Take g(x). Since g(x) is linear in e and (cid:69)[e|X]=0, we find (cid:69)(cid:163) g(x) (cid:164)=0. Since g(x) is an average (cid:98) (cid:98) i (cid:98) (cid:98) ofindependentrandomvariables,thevarianceissmallerthanthesecondmoment,andthedefinition Ï2(X)=(cid:69)(cid:163) e2|X (cid:164) nhvar (cid:163) g(x) (cid:164)= 1 var (cid:183) K (cid:181) Xâx (cid:182) e (cid:184) (cid:98) h h 1 (cid:183) (cid:181) Xâx (cid:182)2 (cid:184) â¤ (cid:69) K e2 h h 1 (cid:183) (cid:181) Xâx (cid:182)2 (cid:184) = (cid:69) K Ï2(X) h h (cid:90) â = K(u)2Ï2(x+hu)f(x+hu)du ââ =R Ï2(x)f(x)+o(1) (19.38) K since Ï2(x) and f(x) are continuous in x. Thus var (cid:163) g(x) (cid:164)ââ0. By Markovâs inequality we conclude (cid:98) g(x)ââ0,completingtheproof. â  (cid:98) p ProofofTheorem19.9.From(19.36),Theorem17.6ofIntroductiontoEconometrics,and(19.34)wehave (cid:112) (cid:112) (cid:195) (cid:33) (cid:112) (cid:195) (cid:33) nh (cid:161) m (x)âm(x)âh2B (x) (cid:162)= nh g (cid:98) (x) + nh b(cid:98)(x) âh2B (x) (cid:98)nw nw nw f(cid:98)(x) f(cid:98)(x) ï£« ï£«(cid:115) ï£¶ï£¶ (cid:112) (cid:181) (cid:182) (cid:112) = nh g (cid:98) (x) (cid:161) 1+o p (1) (cid:162)+ nhï£­o p (cid:161) h2(cid:162)+O pï£­ h ï£¸ï£¸ f(x) n (cid:112) (cid:181) (cid:182) = nh g (cid:98) (x) (cid:161) 1+o (1) (cid:162)+ (cid:179) o (cid:179)(cid:112) nh5 (cid:180) +O (h) (cid:180) p p p f(x) (cid:112) (cid:181) (cid:182) g(x) = nh (cid:98) +o (1) p f(x) (cid:112) where the final equality holds s(cid:112)ince nhg (cid:98) (x)=O p (1) by (19.38) and the assumption nh5 =O(1). The proofiscompletedbyshowing nhg(x)ââN (cid:161) 0,R Ï2(x)f(x) (cid:162) . (cid:98) K DefineY =h â1/2K (cid:179) Xi âx (cid:180) e whichar d eindependentandmeanzero. Wecanwrite (cid:112) nhg(x)= (cid:112) nY ni h i (cid:98) asastandardizedsampleaverage. WeverifytheconditionsfortheLindebergCLT(Theorem6.4). Inthe (cid:104)(cid:112) (cid:105) notationofTheorem6.4,setÏ2 =var nY âR f(x)Ï2(x)ashâ0.TheCLTholdsifwecanverifythe n K Lindebergcondition. Thisisanadvancedcalculationandwillnotinterestmostreaders. Itisprovidedforthoseinterested inacompletederivation. Fix(cid:178)>0andÎ´>0.SinceK(u)isboundedwecanwriteK(u)â¤K.Letnhbesufficientlylargesothat (cid:181)(cid:178)nh (cid:182)(râ2)/2 Ï â¥ . 2 Î´ K",
    "page": 717,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 698 Theconditionalmomentbound(19.14)impliesthatforxâN , (cid:69) (cid:183) e21 (cid:189) e2> (cid:178)nh (cid:190)(cid:175) (cid:175) (cid:175)X =x (cid:184) =(cid:69) (cid:183) |e|r 1 (cid:189) e2> (cid:178)nh (cid:190)(cid:175) (cid:175) (cid:175)X =x (cid:184) K 2 (cid:175) |e|râ2 K 2 (cid:175) ï£® (cid:175) ï£¹ (cid:175) |e|r (cid:175) â¤(cid:69)ï£¯ (cid:175)X =xï£º ï£°(cid:179) (cid:178)nh/K 2(cid:180)(râ2)/2(cid:175) (cid:175) (cid:175) ï£» â¤Î´. SinceY2 â¤h â1K 2 e2wefind ni i (cid:69)(cid:163) Y21(cid:169) Y2 >(cid:178)n (cid:170)(cid:164)â¤ 1 (cid:69) (cid:183) K (cid:181) Xâx (cid:182)2 e21 (cid:189) e2> (cid:178)nh (cid:190)(cid:184) ni ni h h 2 K = 1 (cid:69) (cid:183) K (cid:181) Xâx (cid:182)2 (cid:69) (cid:181) e21 (cid:189) e2> (cid:178)nh (cid:190)(cid:175) (cid:175) (cid:175)X (cid:182)(cid:184) h h 2 (cid:175) K = (cid:90) â K(u)2(cid:69) (cid:104) e21 (cid:110) e2>(cid:178)nh/K 2(cid:111)(cid:175) (cid:175)X =x+hu (cid:105) f(x+hu)du (cid:175) ââ (cid:90) â â¤Î´ K(u)2f(x+hu)du ââ =Î´R f(x)+o(1) K =o(1) sinceÎ´isarbitrary.ThisistheLindebergcondition(6.2).TheLindebergCLT(Theorem6.4)showsthat (cid:112) (cid:112) nhg(x)= nY ââN (cid:161) 0,R Ï2(x)f(x) (cid:162) . (cid:98) K d Thiscompletestheproof. â  _____________________________________________________________________________________________ 19.27 Exercises Exercise19.1 For kernel regression suppose you rescale Y, for example replace Y with 100Y. How shouldthebandwidthhchange? Toanswerthis,firstaddresshowthefunctionsm(x)andÏ2(x)change underrescaling,andthencalculatehowB andÏ2 change. Deducehowtheoptimalh changesdueto 0 rescalingY.Doesyouranswermakeintuitivesense? Exercise19.2 Showthat(19.6)minimizestheAIMSE(19.5). Exercise19.3 Describeinwordshowthebiasofthelocallinearestimatorchangesoverregionsofcon- vexityandconcavityinm(x).Doesthismakeintuitivesense? Exercise19.4 Supposethetrueregressionfunctionislinearm(x)=Î±+Î²xandweestimatethefunction usingtheNadaraya-Watsonestimator. CalculatethebiasfunctionB(x). SupposeÎ²>0. Forwhichre- gionsisB(x)>0andforwhichregionsisB(x)<0? NowsupposethatÎ²<0andre-answerthequestion. CanyouintuitivelyexplainwhytheNWestimatorispositivelyandnegativelybiasedfortheseregions? Exercise19.5 Supposem(x)=Î±isaconstantfunction.FindtheAIMSE-optimalbandwith(19.6)forNW estimation?Explain.",
    "page": 718,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER19. NONPARAMETRICREGRESSION 699 Exercise19.6 ProveTheorem19.11:Showthatwhendâ¥1theAIMSEoptimalbandwidthtakestheform h =cn â1/(4+d)andAIMSEisO (cid:161) n â4/(4+d)(cid:162) . 0 Exercise19.7 Take the DDK2011 dataset and the subsample of boys who experienced tracking. As in Section 19.21 use the Local Linear estimator to estimate the regression of testscores on percentile but nowwiththesubsampleofboys. Plotwith95%confidenceintervals. Commentonthesimilaritiesand differenceswiththeestimateforthesubsampleofgirls. Exercise19.8 Takethecps09mardatasetandthesubsampleofindividualswitheducation=20(profes- sionaldegreeordoctorate),withexperiencebetween0and40years. (a) Use Nadaraya-Watson to estimate the regression of log(wage) on experience, separately for men andwomen. Plotwith95%confidenceintervals. Commentonhowtheestimatedwageprofiles varywithexperience.Inparticular,doyouthinktheevidencesuggeststhatexpectedwagesfallfor experiencelevelsabove20forthiseducationgroup? (b) RepeatusingtheLocalLinearestimator.Howdotheestimatesandconfidenceintervalschange? Exercise19.9 Take the Invest1993 dataset and the subsample of observations with Q â¤ 5. (In the datasetQ isthevariablevala.) (a) UseNadaraya-WatsontoestimatetheregressionofI onQ. (InthedatasetI isthevariableinva.) Plotwith95%confidenceintervals. (b) RepeatusingtheLocalLinearestimator. (c) Isthereevidencetosuggestthattheregressionfunctionisnonlinear? Exercise19.10 TheRR2010datasetisfromReinhartandRogoff(2010). Itcontainsobservationsonan- nual U.S. GDP growth rates, inflation rates, and the debt/gdp ratio for the long time span 1791-2009. Thepapermadethestrongclaimthatgdpgrowthslowsasdebt/gdpincreasesandinparticularthatthis relationshipisnonlinearwithdebtnegativelyaffectinggrowthfordebtratiosexceeding90%. Theirfull datasetincludes44countries.OurextractonlyincludestheUnitedStates. (a) UseNadaraya-Watsontoestimatetheregressionofgdpgrowthonthedebtratio. Plotwith95% confidenceintervals. (b) RepeatusingtheLocalLinearestimator. (c) Doyouseeevidenceofnonlinearityand/orachangeintherelationshipat90%? (d) Nowestimatearegressionofgdpgrowthontheinflationrate.Commentonwhatyoufind. Exercise19.11 WewillconsideranonlinearAR(1)modelforgdpgrowthrates Y t =m(Y tâ1 )+e t (cid:181)(cid:181) GDP (cid:182)4 (cid:182) Y =100 t â1 t GDP tâ1 (a) CreateGDPgrowthratesY .ExtractthelevelofrealU.S.GDP(gdpc1)fromFRED-QDandmakethe t abovetransformationtogrowthrates. (b) UseNadaraya-Watsontoestimatem(x).Plotwith95%confidenceintervals. (c) RepeatusingtheLocalLinearestimator. (d) Doyouseeevidenceofnonlinearity?",
    "page": 719,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Chapter 20 Series Regression 20.1 Introduction Chapter19studiednonparametricregressionbykernelsmoothingmethods.Inthischapterwestudy analternativeclassofnonparametricregressionmethodsknownasseriesregression. The basic model is identical to that examined in Chapter 19. We assume that there are random variables(Y,X)suchthat(cid:69)(cid:163) Y2(cid:164)<âandsatisfytheregressionmodel Y =m(X)+e (20.1) (cid:69)[e|X]=0 (cid:69)(cid:163) e2|X (cid:164)=Ï2(X). Thegoalistoestimatetheconditionalmeanfunctionm(x). WestartwiththesimplesettingwhereX is scalarandconsidermoregeneralcaseslater. AseriesregressionmodelisasequenceK =1,2,...,ofapproximatingmodelsm (x)withK param- K eters. In this chapter we exclusively focus on linear series models, and in particular polynomials and splines. This is because these are simple, convenient, and cover most applications of series methods in applied economics. Other series models include trigonometric polynomials, wavelets, orthogonal wavelets,B-splines,andneuralnetworks.ForadetailedreviewseeChen(2007). Linearseriesregressionmodelstaketheform Y =X (cid:48) Î² +e (20.2) K K K whereX =X (X)isavectorofregressorsobtainedbymakingtransformationsofX andÎ² isacoeffi- K K K cientvector.TherearemultiplepossibledefinitionsofthecoefficientÎ² .Wedefine1itbyprojection K Î² =(cid:69)(cid:163) X X (cid:48) (cid:164)â1(cid:69)[X Y]=(cid:69)(cid:163) X X (cid:48) (cid:164)â1(cid:69)[X m(X)]. (20.3) K K K K K K K The series regression error e is defined by (20.2) and (20.3), is distinct from the regression error e in K (20.1),andisindexedbyK sinceitdependsontheregressorsX .Theseriesapproximationtom(x)is K m (x)=X (x) (cid:48)Î² . (20.4) K K K Thecoefficientistypically2estimatedbyleastsquares (cid:195) (cid:33)â1(cid:195) (cid:33) n n Î² (cid:98)K = (cid:88) X Ki X K (cid:48) i (cid:88) X Ki Y i =(cid:161) X (cid:48) K X K (cid:162)â1(cid:161) X (cid:48) K Y (cid:162) . (20.5) i=1 i=1 1AnalternativeistodefineÎ² K asthebestuniformapproximationasin(20.8).Itisnotcriticalsolongaswearecarefultobe consistentwithournotation. 2Penalizedestimatorshavealsobeenrecommended.Wedonotreviewthesemethodshere. 700",
    "page": 720,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 701 Theestimatorform(x)is m (cid:98)K (x)=X K (x) (cid:48)Î² (cid:98)K . (20.6) ThedifferencebetweenspecificmodelsarisesduetothedifferentchoicesoftransformationsX (x). K Thetheoreticalissueswewillexploreinthischapterare:(1)Approximationpropertiesofpolynomi- alsandsplines;(2)Consistentestimationofm(x);(3)Asymptoticnormalapproximations;(4)Selection ofK;(5)Extensions. ForatextbooktreatmentofseriesregressionseeLiandRacine(2007).Foranadvancedtreatmentsee Chen(2007). TwoseminalcontributionsareAndrews(1991a)andNewey(1997). Tworecentimportant papersareBelloni,Chernozhukov,Chetverikov,andKato(2015)andChenandChristensen(2015). 20.2 PolynomialRegression Theprototypicalseriesregressionmodelform(x)isapth orderpolynomial m (x)=Î² +Î² x+Î² x2+Â·Â·Â·+Î² xp. K 0 1 2 p Wecanwriteitinvectornotationas(20.4)where ï£« ï£¶ 1 ï£¬ x ï£· X K (x)=ï£¬ ï£¬ ï£¬ . . . ï£· ï£· ï£· . ï£­ ï£¸ xp ThenumberofparametersisK =p+1.NoticethatweindexX (x)andÎ² byK astheirdimensionsand K K valuesvarywithK. Theimpliedpolynomialregressionmodelfortherandompair(Y,X)is(20.2)with ï£« ï£¶ 1 ï£¬ X ï£· X K =X K (X)=ï£¬ ï£¬ ï£¬ . . . ï£· ï£· ï£· . ï£­ ï£¸ Xp Thedegreeofflexibilityofapolynomialregressioniscontrolledbythepolynomialorderp. Alarger p yieldsamoreflexiblemodelwhileasmallerp typicallyresultsinaestimatorwithasmallervariance. Ingeneral,alinearseriesregressionmodeltakestheform m (x)=Î² Ï (x)+Î² Ï (x)+Â·Â·Â·+Î² Ï (x) K 1 1 2 2 K K wherethefunctionsÏ (x)arecalledthebasistransformations. Thepolynomialregressionmodeluses j the power basis Ï (x)=xjâ1. The model m (x) is called a series regression because it is obtained by j K sequentiallyaddingtheseriesofvariablesÏ (x). j 20.3 IllustratingPolynomialRegression Considerthecps09mardatasetandaregressionoflogwagesonexperienceforwomenwithacollege education(education=16),separatelyforwhitewomenandBlackwomen. TheclassicalMincermodel usesaquadraticinexperience. Giventhelargesamplesizes(4682forwhitewomenand517forBlack",
    "page": 721,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 702 women)wecanconsiderhigherorderpolynomials. InFigure20.1weplotleastsquaresestimatesofthe conditionalmeanfunctionsusingpolynomialsoforder2,4,8,and12. Examine panel (a) which shows the estimates for the sub-sample of white women. The quadratic specificationappearsmis-specifiedwithashapenoticablydifferentfromtheotherestimates. Thedif- ferencebetweenthepolynomialsoforder4,8,and12isrelativelyminor,especiallyforexperiencelevels below20. Nowexaminepanel(b)whichshowstheestimatesforthesub-sampleofBlackwomen. Thispanel isquitedifferentfrompanel(a). Theestimatesareerraticandincreasinglysoasthepolynomialorder increases.Assumingweareexpectingaconcave(ornearlyconcave)experienceprofiletheonlyestimate whichsatisfiesthisisthequadratic. Whythedifferencebetweenpanels(a)and(b)? Themostlikelyexplanationisthedifferentsample sizes. Thesub-sampleofBlackwomenhasmuchfewerobservationssothemeanfunctionismuchless preciselyestimated,givingrisetotheerraticplots. Thissuggests(informally)thatitmaybepreferredto useasmallerpolynomialorderp inthesecondsub-sample,orequivalentlytousealargerp whenthe samplesizenislarger.TheideathatmodelcomplexityâthenumberofcoefficientsK âshouldvarywith samplesizenisanimportantfeatureofseriesregression. TheerraticnatureoftheestimatedpolynomialregressionsinFigure20.1(b)isacommonfeatureof higher-order estimated polynomial regressions. Better results can sometimes be obtained by a spline regressionwhichisdescribedinSection20.5. 0 10 20 30 40 2.3 0.3 8.2 6.2 4.2 Labor Market Experience (Years) ruoH rep sralloD goL polynomial (2) polynomial (4) polynomial (8) polynomial (12) 0 10 20 30 40 (a)WhiteWomen 2.3 0.3 8.2 6.2 4.2 Labor Market Experience (Years) ruoH rep sralloD goL polynomial (2) polynomial (4) polynomial (8) polynomial (12) (b)BlackWomen Figure20.1:PolynomialEstimatesofExperienceProfile,College-EducatedWomen 20.4 OrthogonalPolynomials Standardimplementationoftheleastsquaresestimator(20.5)ofapolynomialregressionmayreturn acomputationalerrormessagewhenp islarge. (SeeSection3.24.) Thisisbecausethemomentsof Xj canbehighlyheterogeneousacross j andbecausethevariablesXj canbehighlycorrelated. Thesetwo (cid:48) factorsimplyinpracticethatthematrixX X canbeill-conditioned(theratioofthelargesttosmallest K K eigenvaluecanbequitelarge)andsomepackageswillreturnerrormessagesratherthancomputeÎ² (cid:98)K .",
    "page": 722,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 703 (cid:48) Inmostcasestheconditionof X X canbedramaticallyimprovedbyrescalingtheobservations. K K AsdiscussedinSection3.24asimplemethodfornon-negativeregressorsistorescaleeachbyitssam- (cid:179) (cid:180) ple mean, e.g. replace X j with X j / n â1(cid:80)n X j . Even better conditioning can often be obtained by i i i=1 i rescaling X toliein[â1,1]beforeapplyingpowers. Inmostapplicationsoneofthesemethodswillbe i sufficientforawell-conditionedregression. Acomputationallymorerobustimplementationcanbeobtainedbyusingorthogonalpolynomials. Thesearelinearcombinationsofthepolynomialbasisfunctionsandproduceidenticalregressionesti- mators(20.6). Thegoaloforthogonalpolynomialsistoproduceregressorswhichareeitherorthogonal (cid:48) orclosetoorthogonalandhavesimilarvariancessothat X X isclosetodiagonalwithsimilardiago- K K nalelements. TheseorthogonalizedregressorsX â=A X canbewrittenaslinearcombinationsofthe K K K originalvariables X . Iftheregressorsareorthogonalizedthentheregressionestimator(20.6)ismodi- K fiedbyreplacingX (x)withX â (x)=A X (x). K K K K j Oneapproachistousesampleorthogonalization.ThisisdonebyasequenceofregressionsofX on i thepreviouslyorthogonalizedvariablesandthenrescaling. Thiswillresultinperfectlyorthogonalized variables. This is what is implemented in many statistical packages under the label âorthogonal poly- nomialsâ,forexample,thefunctionpolyinR.Ifthisisdonethentheleastsquarescoefficientshaveno meaning outside this specific sample and it is not convenient for calculation of m (x) for values of x (cid:98)K otherthansamplevalues.Thisistheapproachusedfortheexamplespresentedintheprevioussection. Another approach is to use an algebraic orthogonal polynomial. This is a polynomial which is or- thogonalwithrespecttoaknownweightfunctionw(x). Specifically,itisasequencep (x), j =0,1,2,..., j withthepropertythat (cid:82) p j (x)p(cid:96)(x)w(x)dx =0for j (cid:54)=(cid:96). Thismeansthatif w(x)= f(x), themarginal densityof X,thenthebasistransformations p (X)willbemutuallyorthogonal(inexpectation). Since j wedonowknowthedensityof X thisisnotfeasibleinpractice,butifw(x)isclosetothedensityof X thenwecanexpectthatthebasistransformationswillbeclosetomutuallyorthogonal.Toimplementan algebraicorthogonalpolynomialyoufirstshouldrescaleyour X variablesothatitsatisfiesthesupport fortheweightfunctionw(x). Thefollowingthreechoicesaremostrelevantforeconomicapplications. Legendre Polynomial. These are orthogonal with respect to the uniform density on [â1,1]. (So shouldbeappliedtoregressorsscaledtohavesupportin[â1,1].) p (x)= 1 (cid:88) j (cid:195) j (cid:33)2 (xâ1)jâ(cid:96) (x+1) (cid:96) . j 2j (cid:96) (cid:96)=0 For example, the first four are p (x)=1, p (x)=x, p (x)=(cid:161) 3x2â1 (cid:162) /2, and p (x)=(cid:161) 5x3â3x (cid:162) /2. The 0 1 2 3 bestcomputationalmethodistherecurrencerelationship p j+1 (x)= (cid:161) 2j+1 (cid:162) xp j j ( + x) 1 âjp jâ1 (x) . LaguerrePolynomial",
    "page": 723,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". (So shouldbeappliedtoregressorsscaledtohavesupportin[â1,1].) p (x)= 1 (cid:88) j (cid:195) j (cid:33)2 (xâ1)jâ(cid:96) (x+1) (cid:96) . j 2j (cid:96) (cid:96)=0 For example, the first four are p (x)=1, p (x)=x, p (x)=(cid:161) 3x2â1 (cid:162) /2, and p (x)=(cid:161) 5x3â3x (cid:162) /2. The 0 1 2 3 bestcomputationalmethodistherecurrencerelationship p j+1 (x)= (cid:161) 2j+1 (cid:162) xp j j ( + x) 1 âjp jâ1 (x) . LaguerrePolynomial. Theseareorthogonalwithrespecttotheexponentialdensitye âx on[0,â). (So should be applied to non-negative regressors scaled if possible to have approximately unit mean and/orvariance.) (cid:88) j (cid:195) j (cid:33) (âx) (cid:96) p (x)= . j (cid:96) (cid:96)! (cid:96)=0 Forexample,thefirstfourarep (x)=1,p (x)=1âx,p (x)=(cid:161) x2â4x+2 (cid:162) /2,andp (x)=(cid:161)âx3+9x2â18x+6 (cid:162) /6. 0 1 2 3 Thebestcomputationalmethodistherecurrencerelationship p j+1 (x)= (cid:161) 2j+1âx (cid:162) p j j + (x 1 )âjp jâ1 (x) .",
    "page": 723,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 704 HermitePolynomial.Theseareorthogonalwithrespecttothestandardnormaldensityon(ââ,â). (Soshouldbeappliedtoregressorsscaledtohavemeanzeroandvarianceone.) (cid:98) (cid:88) j/2 (cid:99) (â1/2) (cid:96) x (cid:96)â2j p (x)=j! . j (cid:96)=0 (cid:96)! (cid:161) jâ2(cid:96)! (cid:162) Forexample,thefirstfourarep (x)=1,p (x)=x,p (x)=x2â1,andp (x)=x3â3x. Thebestcompu- 0 1 2 3 tationalmethodistherecurrencerelationship p j+1 (x)=xp j (x)âjp jâ1 (x). TheRpackageorthopolynomprovidesaconvenientsetofcommandstocomputemanyorthogonal polynomialsincludingtheabove. 20.5 Splines Asplineisapiecewisepolynomial.Typicallytheorderofthepolynomialispre-selectedtobelinear, quadratic,orcubic. Theflexibilityofthemodelisdeterminedbythenumberofpolynomialsegments. Thejoinpointsbetweenthesegmentsarecalledknots. To impose smoothness and parsimony it is common to constrain the spline function to have con- tinuousderivativesuptotheorderofthespline. Thusalinearsplineisconstrainedtobecontinuous,a quadraticsplineisconstrainedtohaveacontinuousfirstderivative,andacubicsplineisconstrainedto havecontinuousfirstandsecondderivatives. Asimplewaytoconstructaregressionsplineisasfollows.AlinearsplinewithoneknotÏis m (x)=Î² +Î² x+Î² (xâÏ) 1 {xâ¥Ï}. K 0 1 2 Toseethatthisisalinearspline,observethatforxâ¤Ïthefunctionm (x)=Î² +Î² xislinearwithslope K 0 1 Î² ;forxâ¥Ïthefunctionm (x)islinearwithslopeÎ² +Î² ;andthefunctioniscontinuousatx=Ï.Note 1 K 1 2 thatÎ² isthechangeintheslopeatÏ.AlinearsplinewithtwoknotsÏ <Ï is 2 1 2 m (x)=Î² +Î² x+Î² (xâÏ ) 1 {xâ¥Ï }+Î² (xâÏ ) 1 {xâ¥Ï }. K 0 1 2 1 2 3 2 2 Aquadraticsplinewithoneknotis m (x)=Î² +Î² x+Î² x2+Î² (xâÏ)21 {xâ¥Ï}. K 0 1 2 3 Toseethatthisisaquadraticspline,observethatforxâ¤ÏthefunctionisthequadraticÎ² +Î² x+Î² x2 0 1 2 withsecondderivativem (cid:48)(cid:48) (Ï)=2Î² ;forxâ¥Ïthesecondderivativeism (cid:48)(cid:48) (Ï)=2 (cid:161)Î² +Î² (cid:162) ;so2Î² isthe K 2 K 2 3 3 change in the second derivative at Ï. The first derivative at x =Ï is the continuous function m (cid:48) (Ï)= K Î² +2Î² Ï. 1 2 Ingeneral,apth-ordersplinewithN knotsÏ <Ï <Â·Â·Â·<Ï is 1 2 N p N m K (x)= (cid:88) Î² j xj+ (cid:88) Î² p+k (xâÏ k )p1 {xâ¥Ï k } j=0 k=1 whichhasK =N+p+1coefficients.",
    "page": 724,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 705 Theimpliedsplineregressionmodelfortherandompair(Y,X)is(20.2)where ï£« ï£¶ 1 ï£¬ X ï£· ï£¬ ï£· ï£¬ . . ï£· ï£¬ . ï£· ï£¬ ï£· X =X (X)=ï£¬ Xp ï£·. K K ï£¬ ï£· ï£¬ ï£¬ (XâÏ 1 )p1 {X â¥Ï 1 } ï£· ï£· ï£¬ . ï£· ï£¬ . ï£· ï£­ . ï£¸ (XâÏ )p1 {X â¥Ï } N N In practice a spline will depend critically on the choice of the knots Ï . When X is bounded with k anapproximatelyuniformdistributionitiscommontospacetheknotsevenlysoallsegmentshavethe samelength.WhenthedistributionofX isnotuniformanalternativeistosettheknotsatthequantiles j/(N+1)sothattheprobabilitymassisequalizedacrosssegments.Athirdalternativeistosettheknots atthepointswherem(x)hasthegreatestchangeincurvature(seeSchumaker(2007),Chapter7). Inall casesthesetofknotsÏ canchangewithK. Thereforeasplineisaspecialcaseofanapproximationof j theform m (x)=Î² Ï (x)+Î² Ï (x)+Â·Â·Â·+Î² Ï (x) K 1 1K 2 2K K KK wherethebasistransformationsÏ (x)dependonboth j andK. Manyauthorscallsuchapproxima- jK tionsasieveratherthanaseriesbecausethebasistransformationschangewithK. Thisdistinctionis notcriticaltoourtreatmentsoforsimplicitywerefertosplinesasseriesregressionmodels. 20.6 IllustratingSplineRegression InSection20.3weillustratedregressionsoflogwagesonexperienceforwhiteandBlackwomenwith a college education. Now we consider a similar regression for Black men with a college education, a sub-samplewith394observations. Weuseaquadraticsplinewithfourknotsatexperiencelevelsof10,20,30,and40. Thisisaregres- sionmodelwithsevencoefficients. TheestimatedregressionfunctionisdisplayedinFigure20.2(a). An estimated6thorderpolynomialregressionisalsodisplayedforcomparison(a6thorderpolynomialisan appropriatecomparisonbecauseitalsohassevencoefficients). While the spline is a quadratic over each segment, what you can see is that the first two segments (experiencelevelsbetween0-10and10-20years)areessentiallylinear. Mostofthecurvatureoccursin the third and fourth segments (20-30 and 30-40 years) where the estimated regression function peaks andtwistsintoanegativeslope.Theestimatedregressionfunctionissmooth. AquadraticorcubicsplineisusefulwhenitisdesiredtoimposesmoothnessasinFigure20.2(a). In contrast,alinearsplineisusefulwhenitisdesiredtoallowforsharpchangesinslope. To illustrate we consider the data set CHJ2004 which is a sample of 8684 urban Phillipino house- holds from Cox, B. E. Hansen, and Jimenez (2004). This paper studied the crowding-out impact of a familyâs income on non-governmental (e.g., extended family) income transfers3. A model of altruistic transfers predicts that extended families will make gifts (transfers) whenthe recipientfamilyâs income is sufficiently low, but will not make transfers if the recipient familyâs income exceeds a threshold. A purealtruisticmodelpredictsthattheregressionoftransfersreceivedonfamilyincomeshouldhavea slopeofâ1uptothisthresholdandbeflatabovethisthreshold.Weestimatedthisregression(including 3Definedasthesumoftransfersreceiveddomestically,fromabroad,andin-kind,lessgifts.",
    "page": 725,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 706 l l l l 0 10 20 30 40 4.3 2.3 0.3 8.2 6.2 4.2 Labor Market Experience (Years) ruoH rep sralloD goL l knots quadratic spline polynomial (6) 0 50000 100000 150000 200000 (a)ExperienceProfile 00041 00021 00001 0008 0006 0004 Total Income (Pesos) )soseP( srefsnarT knots linear spline (b)EffectofIncomeonTransfers Figure20.2:SplineRegressionEstimates thesamecontrolsastheauthors4)usingalinearsplinewithknotsat10000,20000,50000,100000,and 150000pesos. Theseknotswereselectedtogiveflexibilityforlowincomelevelswheretherearemore observations.Thismodelhasatotalof22coefficients. Theestimatedregressionfunction(asafunctionofhouseholdincome)isdisplayedinFigure20.2(b). Forthefirsttwosegments(incomeslevelsbelow20000pesos)theregressionfunctionisnegativelysloped aspredictedwithaslopeaboutâ0.7from0to10000pesos,andâ0.3from10000to20000pesos.Theesti- matedregressionfunctioniseffectivelyflatforincomelevelsabove20000pesos.Thisshapeisconsistent withthepurealtruismmodel. Alinearsplinemodelisparticularlywellsuitedforthisapplicationasit allowsfordiscontinuouschangesinslope. LinearsplinemodelswithasingleknothavebeenrecentlypopularizedbyCard,Lee,Pei,andWeber (2015)withthelabelregressionkinkdesign. 20.7 TheGlobal/LocalNatureofSeriesRegression Recall from Section 19.18 that we described kernel regression as inherently local in nature. The Nadaraya-Watson, Local Linear, and Local Polynomial estimators of the conditional mean m(x) are weightedaveragesofY forobservationsforwhichX isclosetox. i i Incontrast,seriesregressionistypicallydescribedasglobalinnature.Theestimatorm (cid:98)K (x)=X K (x) (cid:48)Î² (cid:98)K isafunctionoftheentiresample. Thecoefficientsofafittedpolynomial(orspline)areaffectedbythe globalshapeofthefunctionm(x)andthusaffecttheestimatorm (x)atanypointx. (cid:98)K Whilethisdescriptionhassomemerititisnotacompletedescription.Aswenowshow,seriesregres- sionestimatorssharethelocalsmoothingpropertyofkernelregression. Asthenumberofseriesterms K increaseaseriesestimatorm (cid:98)K (x)=X K (x) (cid:48)Î² (cid:98)K alsobecomesalocalweightedaverageestimator. 4Thecontrolsare: ageofhouseholdhead,education(5dummycategories),married,female,marriedfemale,numberof children(3dummies),sizeofhousehold,employmentstatus(2dummies).",
    "page": 726,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 707 Toseethis,observethatwecanwritetheestimatoras m (x)=X (x) (cid:48)(cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48) Y (cid:162) (cid:98)K K K K K = n 1 (cid:88) n X K (x) (cid:48) Q(cid:98) â K 1 X K (X i )Y i i=1 1 (cid:88) n = w (x,X )Y (cid:98)K i i n i=1 whereQ(cid:98)K =n â1X (cid:48) K X K and w (cid:98)K (x,u)=x K (x) (cid:48) Q(cid:98) â K 1 x K (u). Thusm (cid:98)K (x)isaweightedaverageofY i using theweightsw (x,X ). Theweightfunctionw (x,X )appearstobemaximizedat X =x,som(x)puts (cid:98)K i (cid:98)K i i (cid:98) moreweightonobservationsforwhichX isclosetox,similarlytokernelregression. i 0.0 0.2 0.4 0.6 0.8 1.0 8 6 4 2 0 2â u )u,x(w p=4 p=8 p=12 x 0.0 0.2 0.4 0.6 0.8 1.0 (a)x=0.5 8 6 4 2 0 2â 4â u )u,x(w p=4 p=12 x (b)x=0.25 Figure20.3:KernelRepresentationofPolynomialWeightFunction To see this more precisely, observe that sinceQ(cid:98)K will be close in large samples toQ K =(cid:69)(cid:163) X K X K (cid:48) (cid:164) , w (x,u)willbeclosetothedeterministicweightfunction (cid:98)K w (x,u)=X (x) (cid:48) Q â1X (u). K K K K Takethecase X â¼U[0,1]. InFigure20.3weplottheweightfunction w (x,u)asafuntionofu for x = K 0.5 (panel (a)) and x = 0.25 (panel (b)) for p = 4, 8, 12 in panel (a) and p = 4, 12 in panel (b). First, examine panel (a). Here you can see that the weight function w(x,u) is symmetric in u about x. For p =4 the weight function appears similar to a quadratic in u, and as p increases the weight function concentrates its main weight around x. However, the weight function is not non-negative. It is quite similarinshapetowhatareknownashigher-order(orbias-reducing)kernels,whichwerenotreviewed inthepreviouschapterbutarepartofthekernelestimationtoolkit. Second,examinepanel(b). Again theweightfunctionismaximizedatx,butnowitisasymmetricinuaboutthepointx. Still,thegeneral features from panel (a) carry over to panel (b). Namely, as p increases the polynomial estimator puts mostweightonobservationsforwhichX isclosetox(justasforkernelregression),butisdifferentfrom conventionalkernelregressioninthattheweightfunctionisnotnon-negative.Qualitativelysimilarplots areobtainedforsplineregression.",
    "page": 727,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 708 Thereislittleformaltheory(ofwhichIamaware)whichmakesaformallinkbetweenseriesregres- sionandkernelregressionsothecommentspresentedhereareillustrative5. However,thepointisthat statements of the form âSeries regession is a global method; Kernel regression is a local methodâ may notbecomplete. Bothareglobalinnaturewhenh islarge(kernels)orK issmall(series),andarelocal innaturewhenhissmall(kernels)orK islarge(series). 20.8 Stone-WeierstrassandJacksonApproximationTheory A good series approximation m (x) has the property that it gets close to the true CEF m(x) as the K complexityK increases.Formalstatementscanbederivedfromthemathematicaltheoryoftheapprox- imationoffunctions. AnelegantandfamoustheoremistheStone-WeierstrassTheorem(Weierstrass,1885,Stone,1948) whichstatesthatanycontinuousfunctioncanbeuniformlywellapproximatedbyapolynomialofsuffi- cientlyhighorder.Specifically,thetheoremstatesthatifm(x)iscontinuousonacompactsetSthenfor any(cid:178)>0thereissomeK sufficientlylargesuchthat infsup (cid:175) (cid:175)m(x)âX K (x) (cid:48)Î²(cid:175) (cid:175) â¤(cid:178). (20.7) Î² xâS Thusthetrueunknownm(x)canbearbitrarilywellapproximatedbyselectingasuitablepolynomial. Jackson(1912)strengthenedthisresulttogiveconvergencerateswhichdependonthesmoothness ofm(x). Thebasicresulthasbeenextendedtosplinefunctions. Thefollowingnotationwillbeuseful. DefinetheÎ²whichminimizestheleft-sideof(20.7)as Î²â K =argminsup (cid:175) (cid:175)m(x)âX K (x) (cid:48)Î²(cid:175) (cid:175), (20.8) Î² xâS definetheapproximationerror r â (x)=m(x)âX (x) (cid:48)Î²â , (20.9) K K K anddefinetheminimizedvalueof(20.7) Î´â K d=ef in Î² fs x u â p S (cid:175) (cid:175)m(x)X K (x) (cid:48)Î²(cid:175) (cid:175) =s x u â p S (cid:175) (cid:175)m(x)âX K (x) (cid:48)Î²â K (cid:175) (cid:175) =s x u â p S (cid:175) (cid:175)r K â (x) (cid:175) (cid:175). (20.10) Theorem20.1 IfforsomeÎ±â¥0,m(Î±)(x)isuniformlycontinuousonacompact setSandX (x)iseitherapolynomialbasisorasplinebasis(withuniformknot K spacing)ofordersâ¥Î±,thenasK ââ Î´â â¤o (cid:161) K âÎ±(cid:162) . (20.11) K Furthermore, if m(2)(x) is uniformly continuous on S and X (x) is a linear K splinebasis,thenÎ´â â¤O (cid:161) K â2(cid:162) . K ForaproofforthepolynomialcaseseeTheorem4.3ofLorentz(1986)orTheorem3.12ofSchumaker (2007)plushisequations(2.119)and(2.121). ForthesplinecaseseeTheorem6.27ofSchumaker(2007) 5SimilarconnectionsaremadeintheappendixofChen,Liao,andSun(2012).",
    "page": 728,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 709 plushisequations(2.119)and(2.121). ForthelinearsplinecaseseeTheorem6.15ofSchumaker,equa- tion(6.28). Theorem20.1ismoreusefulthantheclassicStone-WeierstrassTheoremasitgivesanapproximation ratewhichdependsonthesmoothnessorderÎ±. Therateo(K âÎ± )in(20.11)meansthattheapproxima- tionerror(20.10)decreasesasK increasesanddecreasesatafasterratewhenÎ±islarge. Thestandard interpretationisthatwhenm(x)issmootheritispossibletoapproximateitwithfewerterms. It will turn out that for our distribution theory it is sufficient to consider the case that m(2)(x) is uniformlycontinuous. ForthiscaseTheorem20.1showsthatpolynomialsandquadratic/cubicsplines achievetherateo(K â2)andlinearsplinesachievetherateO(K â2). Formostofofourresultsthelatter boundwillbesufficient. Moregenerally,Theorem20.1makesadistinctionbetweenpolynomialsandsplinesaspolynomials âÎ± âÎ± achievetherateo(K )adaptively(withoutinputfromtheuser)whilesplinesachievetherateo(K ) only if the spline order s is appropriately chosen. This is an advantage for polynomials. However, as emphasized by Schumaker (2007), splines simultaneously approximate the derivatives m(q)(x) for q < Î±. Thus, for example, a quadratic spline simultaneously approximates the function m(x) and its first (cid:48) derivativem (x). Thereisnocomparableresultforpolynomials. Thisisanadvantageforquadraticand cubicsplines. Sinceeconomistsareoftenmoreinterestedinmarginaleffects(derivatives)thaninlevels thismaybeagoodreasontoprefersplinesoverpolynomials. Theorem20.1isaboundonthebestuniformapproximationerror. ThecoefficientÎ²â whichmini- K mizes(20.11)isnot,however,theprojectioncoefficientÎ² asdefinedin(20.3).ThusTheorem20.1does K not directly inform us concerning the approximation error obtained by series regression. It turns out, however,thattheprojectionerrorcanbeeasilydeducedfrom(20.11). Definition20.1 Theprojectionapproximationerroris r (x)=m(x)âX (x) (cid:48)Î² (20.12) K K K wherethecoefficientÎ² istheprojectioncoefficient(20.3). Therealizedpro- K jection approximation error is r =r (X). The expected squared projection K K erroris Î´2 =(cid:69)(cid:163) r2(cid:164) . (20.13) K K Theprojectionapproximationerrorissimilarto(20.9)butevaluatedusingtheprojectioncoefficient ratherthantheminimizingcoefficientÎ²â (20.8). AssumingthatX hascompactsupportS theexpected K squaredprojectionerrorsatisfies (cid:181)(cid:90) (cid:182)1/2 Î´ = (cid:161) m(x)âX (x) (cid:48)Î² (cid:162)2 dF(x) K K K S (cid:181)(cid:90) (cid:182)1/2 â¤ (cid:161) m(x)âX (x) (cid:48)Î²â(cid:162)2 dF(x) K K S (cid:181)(cid:90) (cid:182)1/2 â¤ Î´â2dF(x) K S =Î´â . (20.14) K ThefirstinequalityholdssincetheprojectioncoefficientÎ² minimizestheexpectedsquaredprojection K error(seeSection2.25). ThesecondinequalityisthedefinitionofÎ´â . CombinedwithTheorem20.1we K haveestablishedthefollowingresult.",
    "page": 729,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 710 Theorem20.2 IfX hascompactsupportS,forsomeÎ±â¥0m(Î±)(x)isuniformly continuous on S, and X (x) is either a polynomial basis or a spline basis of K ordersâ¥Î±,thenasK ââ Î´ â¤Î´â â¤o (cid:161) K âÎ±(cid:162) . K K Furthermore, if m(2)(x) is uniformly continuous on S and X (x) is a linear K splinebasis,thenÎ´ â¤O (cid:161) K â2(cid:162) . K Theavailabletheoryoftheapproximationoffunctionsgoesbeyondtheresultsdescribedhere. For example, there is a theory of weighted polynomial approximation (Mhaskar, 1996) which provides an analogofTheorem20.2fortheunboundedreallinewhenX hasadensitywithexponentialtails. 20.9 RegressorBounds TheapproximationresultinTheorem20.2assumesthattheregressors X haveboundedsupportS. This is conventional in series regression theory as it greatly simplifies the analysis. Bounded support impliesthattheregressorfunctionX (x)isbounded.Define K Î¶ (x)=(cid:161) X (x) (cid:48) Q â1X (x) (cid:162)1/2 (20.15) K K K K Î¶ =supÎ¶ (x) (20.16) K K x whereQ =(cid:69)(cid:163) X X (cid:48) (cid:164) isthepopulationdesignmatrixgiventheregressors X . Thisimpliesthatforall K K K K realizationsofX K (cid:161) X (cid:48) Q â1X (cid:162)1/2â¤Î¶ . (20.17) K K K K The constant Î¶ (x) is the normalized length of the regressor vector X (x). The constant Î¶ is the K K K maximum normalized length. Their values are determined by the basis function transformations and thedistributionofX.TheyareinvarianttorescalingX orlinearrotations. K ForpolynomialsandsplineswehaveexplicitexpressionsfortherateatwhichÎ¶ growswithK. K Theorem20.3 IfX hascompactsupportSwithastrictlypositivedensity f(x) onSthen 1. Î¶ â¤O(K)forpolynomials K 2. Î¶ â¤O (cid:161) K1/2(cid:162) forsplines K ForaproofofTheorem20.3seeNewey(1997,Theorem4). Furthermore,when X isuniformlydistributedthenwecanexplicitlycalculateforpolynomialsthat Î¶ =K,sothepolynomialboundÎ¶ â¤O(K)cannotbeimproved. K K Toillustrate, weplotinFigure20.4(a)thevaluesÎ¶ (x)forthecase X â¼U[0,1]. WeplotÎ¶ (x)fora K K polynomialofdegreep=9andaquadraticsplinewithN =7knots(bothsatisfyK =10).Youcanseethat",
    "page": 730,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 711 thevaluesofÎ¶ (x)arecloseto3forbothbasistransformationsandmostvaluesofx,butÎ¶ (x)increases K K sharplyforxneartheboundary.ThemaximumvaluesareÎ¶ =10forthepolynomialandÎ¶ =7.4forthe K K quadraticspline.WhileTheorem20.3showsthetwohavedifferentratesforlargeK,weseeformoderate K thatthedifferencesarerelativelyminor. 0.0 0.2 0.4 0.6 0.8 1.0 01 8 6 4 2 0 x rotceV rossergeR fo htgneL Polynomial Quadratic Spline 1 2 3 4 5 6 7 8 (a)Î¶ (x),K =10 K 0.1 8.0 6.0 4.0 2.0 0.0 K )K(ESI K-4+K10 K-4+K30 K-4+K150 l l l (b)IntegratedSquaredError Figure20.4:NormalizedRegressorLengthsandIntegratedSquaredError 20.10 MatrixConvergence Oneofthechallengeswhicharisewhendevelopingatheoryfortheleastsquaresestimatorishowto describethelarge-samplebehaviorofthesampledesignmatrix Q(cid:98)K = n 1 (cid:88) n X Ki X K (cid:48) i i=1 asK ââ.ThedifficultyisthatitsdimensionchangeswithK sowecannotapplyastandardWLLN. Itturnsouttobeconvenientifwefirstrotatetheregressorvectorsothattheelementsareorthogonal inexpectation.Thuswedefinethestandardizedregressorsanddesignmatrixas X(cid:101)Ki =Q â K 1/2X Ki (20.18) Q(cid:101)K = n 1 (cid:88) n X(cid:101)Ki X(cid:101) K (cid:48) i . i=1 Notethat(cid:69)(cid:163) X(cid:101)K X(cid:101) K (cid:48) (cid:164)=I K .Thestandardizedregressorsarenotusedinpractice;theyareintroducedonly tosimplifythetheoreticalderivations. Ourconvergencetheorywillrequirethefollowingfundamentalrateboundonthenumberofcoeffi- cientsK.",
    "page": 731,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 712 Assumption20.1 1. Î» (cid:161) Q (cid:162)â¥Î»>0 min K 2. Î¶2 log(K)/nâ0asn,K ââ. K Assumption20.1.1ensuresthatthetransformation(20.18)iswelldefined6.Assumption20.1.2states thatthesquaredmaximumregressorlengthÎ¶2 growsslowerthann. SinceÎ¶ increaseswithK thisisa K K boundontherateatwhichK canincreasewithn. ByTheorem20.2therateinAssumption20.1.2holds forpolynomialsifK2log(K)/nâ0andforsplinesifKlog(K)/nâ0. Ineithercase,thismeansthatthe numberofcoefficientsK isgrowingatarateslowerthann. Wearenowinapositiontodescribeaconvergenceresultforthestandardizeddesignmatrix. The followingisLemma6.2ofBelloni,Chernozhukov,Chetverikov,andKato(2015). Theorem20.4 IfAssumption20.1holdsthen (cid:176) (cid:176)Q(cid:101)K âI K (cid:176) (cid:176) âpâ0. (20.19) AproofofTheorem20.4usingastrongerconditionthanAssumption20.1canbefoundinSection 20.31.Thenormin(20.19)isthespectralnorm (cid:107)A(cid:107)=(cid:161)Î» (cid:161) A (cid:48) A (cid:162)(cid:162)1/2 max whereÎ» (B)denotesthelargesteigenvalueofthematrixB.ForafulldescriptionseeSectionA.23. max For the least squares estimator what is particularly important is the inverse of the sample design matrix. Fortunately we can easily deduce consistency of its inverse from (20.19) when the regressors havebeenorthogonalizedasdescribed. Theorem20.5 IfAssumption20.1holdsthen (cid:176) (cid:176) (cid:176) Q(cid:101) â K 1âI K (cid:176) (cid:176) (cid:176) âpâ0 (20.20) and Î» max (cid:179) Q(cid:101) â K 1 (cid:180) =1/Î» min (cid:161) Q(cid:101)K (cid:162)âpâ1. (20.21) TheproofofTheorem20.5canbefoundinSection20.31. 6Technically,whatisrequiredisthatÎ» min (cid:179) BKQKB (cid:48) K (cid:180) â¥Î»>0forsomeKÃK sequenceofmatricesBK,orequivalentlythat Assumption20.1.1holdsafterreplacingXK withBKXK.",
    "page": 732,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 713 20.11 ConsistentEstimation Inthissectionwegiveconditionsforconsistentestimationofm(x)bytheseriesestimatorm (x)= (cid:98)K X K (x) (cid:48)Î² (cid:98)K . WeknowfromstandardregressiontheorythatforanyfixedK,Î² (cid:98)K âpâÎ² K andthusm (cid:98)K (x)=X K (x) (cid:48)Î² (cid:98)K âpâ X (x) (cid:48)Î² asnââ.Furthermore,fromtheStone-WeierstrassTheoremweknowthatX (x) (cid:48)Î² âm(x) K K K K as K ââ. It therefore seems reasonable to expect that m (x)âpâm(x) as both n ââ and K ââ (cid:98)K together. Making this argument rigorous, however, is technically challenging, in part because the di- mensionsofÎ² (cid:98)K anditscomponentsarechangingwithK. Sincem (x)andm(x)arefunctions,convergenceshouldbedefinedwithrespecttoanappropriate (cid:98)K metric. Forkernelregressionwefocusedonpointwiseconvergence(foreachvalueof x separately)as thatisthesimplesttoanalyze. Forseriesregressionitturnsouttobesimplesttodescribeconvergence withrespecttointegratedsquarederror(ISE).Wedefinethelatteras (cid:90) ISE(K)= (m (x)âm(x))2dF(x) (20.22) (cid:98)K whereF isthemarginaldistributionof X. ISE(K)istheaveragesquareddistancebetweenm (x)and (cid:98)K m(x), weightedbythemarginaldistributionof X. TheISEisrandom, dependsonbothsamplesizen andmodelcomplexityK, anditsdistributionisdeterminedbythejointdistributionof(Y,X). Wecan establishthefollowing. Theorem20.6 UnderAssumption20.1andÎ´ =o(1),thenasn,K ââ, K ISE(K)=o (1). (20.23) p TheproofofTheorem20.6canbefoundinSection20.31. Theorem20.6showsthattheseriesestimatorm (x)isconsistentintheISEnormundermildcon- (cid:98)K ditions. TheassumptionÎ´ =o(1)holdsforpolynomialsandsplinesifK ââandm(x)isuniformly K continuous. ThisresultisanalogoustoTheorem19.8whichshowedthatkernelregressionestimatoris consistentifm(x)iscontinuous. 20.12 ConvergenceRate Wenowgivearateofconvergence. Theorem20.7 UnderAssumption20.1andÏ2(x)â¤Ï2<â,thenasn,K ââ, (cid:181) (cid:182) K ISE(K)â¤O Î´2 + (20.24) p K n where Î´2 is the expected squared prediction error (20.13). Furthermore, if K (cid:48)(cid:48) m (x)isuniformlycontinuousthenforpolynomialorsplinebasisfunctions (cid:181) (cid:182) ISE(K)â¤O K â4+ K . (20.25) p n",
    "page": 733,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 714 TheproofofTheorem20.7canbefoundinSection20.31.ItisbasedonNewey(1997). The bound (20.25) is particularly useful as it gives an explicit rate in terms of K and n. The result shows that the integrated squared error is bounded in probability by two terms. The first K â4 is the squared bias. The second K/n is the estimation variance. This is analogous to the AIMSE for kernel regression(19.5).WecanseethatincreasingthenumberofseriestermsK affectstheintegratedsquared errorbydecreasingthebiasbutincreasingthevariance.Thefactthattheestimationvarianceisoforder K/ncanbeintuitivelyexplainedbythefactthattheregressionmodelisestimatingK coefficients. Forpolynomialsandquadraticsplinesthebound(20.25)canbewrittenaso (cid:161) K â4(cid:162)+O (K/n). p p WeareinterestedinthesequenceK whichminimizesthetrade-offin(20.25).Byexaminingthefirst- orderconditionwefindthatthesequencewhichminimizesthisboundisK â¼n1/5. Withthischoicewe obtaintheoptimalintegratedsquarederrorISE(K)â¤O (cid:161) n â4/5(cid:162) . Thisisthesameconvergencerateas p obtainedbykernelregressionundersimilarassumptions. ItisinterestingtocontrasttheoptimalrateK â¼n1/5forseriesregressionwithhâ¼n â1/5forkernelre- gression.Essentially,onecanviewK â1inseriesregressionasaâbandwidthâsimilartokernelregression, oronecanview1/hinkernelregressionastheeffectivenumberofcoefficients. TherateK â¼n1/5meansthattheoptimalK increasesveryslowlywiththesamplesize.Forexample, doublingyoursamplesizeimpliesa15%increaseintheoptimalnumberofcoefficientsK. Toobtaina doublingintheoptimalnumberofcoefficientsyouneedtomultiplythesamplesizeby32. To illustrate, Figure 20.4(b) displays the ISE rate bounds K â4+K/n as a function of K for n =10, 30,150. ThefilledcirclesmarktheISE-minimizingK,whichareK =2,3,and4forthethreefunctions. NoticethattheISEfunctionsaresteeplydownwardslopingforsmallK andnearlyflatforlargeK (when n islarge). ThisisbecausethebiastermK â4 dominatesforsmallvaluesofK whilethevarianceterm K/ndominatesforlargevaluesofK andthelatterflattensasnincreases. 20.13 AsymptoticNormality Take a parameter Î¸ =a(m) which is a real-valued linear function of the regression function. This includes the regression function m(x) at a given point x, derivatives of m(x), and integrals over m(x). Givenm (cid:98)K (x)=X K (x) (cid:48)Î² (cid:98)K asanestimatorform(x),theestimatorforÎ¸ isÎ¸ (cid:98)K =a(m (cid:98)K )=a K (cid:48) Î² (cid:98)K forsome KÃ1vectorofconstantsa K (cid:54)=0.(Therelationshipa(m (cid:98)K )=a K (cid:48) Î² (cid:98)K followssinceaislinearinmandm (cid:98)K islinearinÎ² (cid:98)K .) IfK werefixedasnââthenbystandardasymptotictheorywewouldexpectÎ¸ (cid:98)K tobeasymptot- icallynormalwithvarianceV =a (cid:48) Q â1â¦ Q â1a whereâ¦ =(cid:69)(cid:163) X X (cid:48) e2(cid:164) . Thestandardjustification, K K K K K K K K K however,isnotvalidinthenonparametriccase. ThisisinpartbecauseV maydivergeasK ââ,and K inpartduetothefinitesamplebiasduetotheapproximationerror. Thereforeanewtheoryisrequired",
    "page": 734,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Thestandardjustification, K K K K K K K K K however,isnotvalidinthenonparametriccase. ThisisinpartbecauseV maydivergeasK ââ,and K inpartduetothefinitesamplebiasduetotheapproximationerror. Thereforeanewtheoryisrequired. Interestingly,itturnsoutthatinthenonparametriccaseÎ¸ (cid:98)K isstillasymptoticallynormalandV K isstill the appropriate variance for Î¸ (cid:98)K . The proof is different than the parametric case as the dimensions of thematricesareincreasingwithK andweneedtobeattentivetotheestimatorâsbiasduetotheseries approximation.",
    "page": 734,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 715 Assumption20.2 InadditiontoAssumption20.1 1. lim sup(cid:69)(cid:163) e21(cid:169) e2>B (cid:170)|X =x (cid:164)=0 Bââ x 2. (cid:69)(cid:163) e2|X (cid:164)â¥Ï2>0 3. Î¶ Î´ =o(1)asK ââ K K Assumption20.2.1isconditionalsquareintegrability.Itimpliesthattheconditionalvariance(cid:69)(cid:163) e2|X (cid:164) isbounded.ItisusedtoverifytheLindebergconditionfortheCLT. Assumption20.2.2statesthattheconditionalvarianceisnowheredegenerate.ThusthereisnoX for whichY isperfectlypredictable.ThisisatechnicalconditionusedtoboundV frombelow. K Assumption 20.2.3 states that approximation error Î´ declines faster than the maximal regressor K lengthÎ¶ . Forpolynomialsasufficientconditionforthisassumptionisthatm (cid:48)(cid:48) (x)isuniformlycontin- K (cid:48) uous.Forsplinesasufficientconditionisthatm (x)isuniformlycontinuous. Theorem20.8 UnderAssumption20.2,asnââ, (cid:112) n (cid:161)Î¸ (cid:98)K âÎ¸+a(r K ) (cid:162) ââN(0,1). (20.26) V1/2 d K TheproofofTheorem20.8canbefoundinSection20.31. Theorem20.8showsthattheestimatorÎ¸ (cid:98)K isapproximatelynormalwithbiasâa(r K )andvariance V /n.Thevarianceisthesameasintheparametriccase.Theasymptoticbiasissimilartothatfoundin K kernelregression. One useful message from Theorem 20.8 is that the classical variance formulaV K for Î¸ (cid:98)K applies to seriesregression.ThisjustifiesconventionalestimatorsforV aswillbediscussedinSection20.18. K Theorem20.8showsthattheestimatorÎ¸ (cid:98)K hasabiasa(r K ).Whatisthis? Itisthesametransforma- tionofthefunctionr (x)asÎ¸=a(m)isoftheregressionfunctionm(x). Forexample,ifÎ¸=m(x)isthe K regressionatafixedpointx thena(r )=r (x),theapproximationerroratthesamepoint. IfÎ¸=m (cid:48) (x) K K istheregressionderivativethena(r )=r (cid:48) (x)isthederivativeoftheapproximationerror. K K ThismeansthatthebiasintheestimatorÎ¸ (cid:98)K forÎ¸showninTheorem20.8issimplytheapproximation errortransformedbythefunctionalofinterest.Ifweareestimatingtheregressionfunctionthenthebias istheerrorinapproximatingtheregressionfunction;ifweareestimatingtheregressionderivativethen thebiasistheerrorinthederivativeintheapproximationerrorfortheregressionfunction. 20.14 RegressionEstimation A special yet important example of a linear estimator is the regression function at a fixed point x. In the notation of the previous section, a(m)=m(x) and a = X (x). The series estimator of m(x) is K K Î¸ (cid:98)K =m (cid:98)K (x)=X K (x) (cid:48)Î² (cid:98)K .AsthisisakeyproblemofinterestwerestatetheasymptoticresultofTheorems 20.8forthisestimator.",
    "page": 735,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 716 Theorem20.9 UnderAssumption20.2,asnââ, (cid:112) n(m (x)âm(x)+r (x)) (cid:98)K K ââN(0,1) (20.27) V1/2(x) d K whereV (x)=X (x) (cid:48) Q â1â¦ Q â1X (x). K K K K K K Thereareseveralimportantfeaturesabouttheasymptoticdistribution(20.27). First,asmentionedintheprevioussectionitshowsthattheclassicalvarianceformulaV (x)applies K fortheseriesestimatorm (x). Second,(20.27)showsthattheestimatorhastheasymptoticbiasr (x). (cid:98)K K Thisisduetothefactthatthefiniteorderseriesisanapproximationtotheunknownregressionfunction m(x)andthisresultsinfinitesamplebias. ThereisanotherfascinatingconnectionbetweentheasymptoticvarianceofTheorem20.9andthe regression lengths Î¶ (x) of (20.15). Under conditional homoskedasticity we have the simplification K V (x)=Ï2Î¶ (x)2.Thustheasymptoticvarianceoftheregressionestimatorisproportionaltothesquared K K regressionlengths.FromFigure20.4(a)welearnedthattheregressionlengthÎ¶ (x)ismuchhigheratthe K edgeofthesupportoftheregressors, especiallyforpolynomials. Thismeansthattheprecisionofthe seriesregressionestimatorisconsiderablydegradedattheedgeofthesupport. 20.15 Undersmoothing AnunpleasantaspectaboutTheorem20.9isthebiasterm. Aninterestingtrickisthatthisbiasterm canbemadeasymptoticallynegligibleifweassumethatK increaseswithnatasufficientlyfastrate. Theorem20.10 UnderAssumption20.2,ifinadditionnÎ´â2â0then K (cid:112) n(m (x)âm(x)) (cid:98)K ââN(0,1). (20.28) V1/2(x) d K TheconditionnÎ´â2â0impliesthatthesquaredbiasconvergesfasterthantheestimationvariance K (cid:48)(cid:48) sotheformerisasymptoticallynegligible. Ifm (x)isuniformlycontinuousthenasufficientcondition for polynomials and quadratic splines is K â¼n1/4. For linear splines a sufficient condition is for K to divergefasterthanK1/4.TherateK â¼n1/4issomewhatfasterthantheISE-optimalrateK â¼n1/5. The assumption nÎ´â2 â0 is often stated by authors as an innocuous technical condition. This is K misleadingasitisatechnicaltrickandshouldbediscussedexplicitly. Thereasonwhytheassumption eliminatesthebiasfrom(20.28)isthattheassumptionforcestheestimationvariancetodominatethe squaredbiassothatthelattercanbeignored.Thismeansthattheestimatoritselfisinefficient. BecausenÎ´â2â0meansthatK islargerthanoptimalwesaythatm (x)isundersmoothedrelative K (cid:98)K totheoptimalseriesestimator. Many authors like to focus their asymptotic theory on the assumptions in Theorem 20.10 as the distribution (20.28) appears cleaner. However, it is a poor use of asymptotic theory. There are three problems with the assumption nÎ´â2 â0 and the approximation (20.28). First, the estimator m (x) is K (cid:98)K inefficient. Second, whiletheassumptionnÎ´â2â0makesthebiasoflowerorderthanthevarianceit K",
    "page": 736,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 717 onlymakesthebiasofslightlylowerorder,meaningthattheaccuracyoftheasymptoticapproximation ispoor. Effectively,theestimatorisstillbiasedinfinitesamples.Third,nÎ´â2â0isanassumptionnot K aruleforempiricalpractice. ItisunclearwhatthestatementâAssumenÎ´â2â0âmeansinapractical K application. Fromthisviewpointthedifferencebetween(20.26)and(20.28)isintheassumptionsnot in the actual reality nor in the actual empirical practice. Eliminating a nuisance (the asymptotic bias) throughanassumptionisatricknotasubstantiveuseoftheory.Mystrongviewisthattheresult(20.26) ismoreinformativethan(20.28).Itshowsthattheasymptoticdistributionisnormalbuthasanon-trivial finitesamplebias. 20.16 ResidualsandRegressionFit Thefittedregressionat x =X i ism (cid:98)K (X i )=X K (cid:48) i Î² (cid:98)K andthefittedresidualise (cid:98)Ki =Y i âm (cid:98)K (X i ). The leave-one-outpredictionerrorsare e (cid:101)Ki =Y i âm (cid:98)K,âi (X i )=Y i âX K (cid:48) i Î² (cid:98)K,âi whereÎ² (cid:98)K,âi istheleastsquarescoefficientwiththeith observationomitted. Using(3.44)wehavethe simplecomputationalformula e =e (1âX (cid:48) (cid:161) X (cid:48) X (cid:162)â1 X ) â1. (20.29) (cid:101)Ki (cid:98)Ki Ki K K Ki As for kernel regression the prediction errors e are better estimators of the errors than the fitted (cid:101)Ki residualse astheformerdonothavethetendencytoover-fitwhenthenumberofseriestermsislarge. (cid:98)Ki 20.17 Cross-ValidationModelSelection A common method for selection of the number of series terms K is cross-validation. The cross- validationcriterionissum7ofsquaredpredictionerrors n n CV(K)= (cid:88) e2 = (cid:88) e2 (1âX (cid:48) (cid:161) X (cid:48) X (cid:162)â1 X ) â2. (20.30) (cid:101)Ki (cid:98)Ki Ki K K Ki i=1 i=1 TheCV-selectedvalueofK istheintegerwhichminimizesCV(K). As shown in Theorem 19.7 CV(K) is an approximately unbiased estimator of the integrated mean- squared error (IMSE), which is the expected integrated squared error (ISE). The proof of the result is the same for all nonparametric estimators (series as well as kernels) so does not need to be repeated here. Therefore,findingtheK whichproducesthesmallestvalueofCV(K)isagoodindicatorthatthe estimatorm (x)hassmallIMSE. (cid:98)K For practical implementation we first designate a set of models (sets of basis transformations and number of variables K) over which to search. (For example, polynomials of order 1 through K for max somepre-selectedK .)Foreach,thereisasetofregressorsX whichareobtainedbytransformations max K oftheoriginalvariablesX. Foreachsetweestimatetheregressionbyleastsquares,calculatetheleave- one-outpredictionerrors, andtheCVcriterion. Sincetheerrorsarealinearoperationthisisasimple calculation.TheCV-selectedK istheintegerwhichproducesthesmallestvalueofCV(K).PlotsofCV(K) againstK canaidassessmentandinterpretation. SincethemodelorderK isanintegertheCVcriterion forseriesregressionisadiscretefunction,unlikethecaseofkernelregression",
    "page": 737,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Foreachsetweestimatetheregressionbyleastsquares,calculatetheleave- one-outpredictionerrors, andtheCVcriterion. Sincetheerrorsarealinearoperationthisisasimple calculation.TheCV-selectedK istheintegerwhichproducesthesmallestvalueofCV(K).PlotsofCV(K) againstK canaidassessmentandinterpretation. SincethemodelorderK isanintegertheCVcriterion forseriesregressionisadiscretefunction,unlikethecaseofkernelregression. Ifitisdesiredtoproduceanestimatorm (x)withreducedbiasitmaybepreferredtoselectavalue (cid:98)K ofK slightlyhigherthanthatselectedbyCValone. 7SomeauthorsdefineCV(K)astheaverageratherthanthesum.",
    "page": 737,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 718 1 2 3 4 5 6 7 8 0621 5521 0521 5421 0421 5321 0321 5221 Polynomial Order )K(VC 1 2 3 4 5 6 7 8 (a)WhiteWomen 921 821 721 621 521 Polynomial Order )K(VC (b)BlackWomen Figure 20.5: Cross-Validation Functions for Polynomial Estimates of Experience Profile, College- EducatedWomen Toillustrate,inFigure20.5weplotthecross-validationfunctionsforthepolynomialregressiones- timates from Figure 20.1. The lowest point marks the polynomial order which minimizes the cross- validationfunction. Inpanel(a)weplottheCVfunctionforthesub-sampleofwhitewomen. Herewe seethattheCV-selectedorderisp=3,acubicpolynomial. Inpanel(b)weplottheCVfunctionforthe sub-sampleofBlackwomen,andfindthattheCV-selectedorderisp=2,aquadratic. Asexpectedfrom visualexaminationofFigure20.1,theselectedmodelismoreparsimoniousforpanel(b),mostlikelybe- causeithasasubstantiallysmallersamplesize.Whatmaybesurprisingisthatevenforpanel(a),which hasalargesampleandsmoothestimates,theCV-selectedmodelisstillrelativelyparsimonious. Auserwhodesiresareducedbiasestimatormightincreasethepolynomialordersto p =4oreven p=5forthesubsampleofwhitewomenandtop=3orp=4forthesubsampleofBlackwomen. Both CVfunctionsarerelativelysimilaracrossthesevalues. 20.18 VarianceandStandardErrorEstimation TheexactconditionalvarianceoftheleastsquaresestimatorÎ² (cid:98)K underindependentsamplingis (cid:195) (cid:33) n V =(cid:161) X (cid:48) X (cid:162)â1 (cid:88) X X (cid:48) Ï2(X ) (cid:161) X (cid:48) X (cid:162)â1 . (20.31) Î²(cid:98) K K Ki Ki i K K i=1 Theexactconditionalvariancefortheconditionalmeanestimatorm (cid:98)K (x)=X K (x) (cid:48)Î² (cid:98)K is (cid:195) (cid:33) n V (x)=X (x) (cid:48)(cid:161) X (cid:48) X (cid:162)â1 (cid:88) X X (cid:48) Ï2(X ) (cid:161) X (cid:48) X (cid:162)â1 X (x). K K K K Ki Ki i K K K i=1 UsingthenotationofSection20.7thisequals 1 (cid:88) n w (x,X )2Ï2(X ). n2 (cid:98)K i i i=1",
    "page": 738,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 719 Inthecaseofconditionalhomoskedasticitythelattersimplifiesto 1 1 w (x,x)Ï2(cid:39) Î¶ (x)2Ï2. (cid:98)K K n n whereÎ¶ (x)isthenormalizedregressorlengthdefinedin(20.15). Underconditionalheteroskedasticty, K largesamples,andK large(sothatw (x,X )isalocalkernel)itapproximatelyequals (cid:98)K i 1 1 w (x,x)Ï2(x)= Î¶ (x)2Ï2(x). K K n n Ineithercasewefindthatthevarianceisapproximately 1 V (x)(cid:39) Î¶ (x)2Ï2(x). K K n This shows thatthevariance oftheseries regressionestimator isa scale of Î¶ (x)2 and theconditional K variance.FromtheplotofÎ¶ (x)showninFigure20.4wecandeducethattheseriesregressionestimator K willberelativelyimpreciseattheboundaryofthesupportofX. Theestimatorof(20.31)recommendedbyAndrews(1991a)istheHC3estimator (cid:195) (cid:33) n V(cid:98)Î²(cid:98) =(cid:161) X (cid:48) K X K (cid:162)â1 (cid:88) X Ki X K (cid:48) i e (cid:101)K 2 i (cid:161) X (cid:48) K X K (cid:162)â1 (20.32) i=1 wheree istheleave-one-outpredictionerror(20.29).AlternativesincludetheHC1orHC2estimators. (cid:101)Ki Given(20.32)avarianceestimatorform (cid:98)K (x)=X K (x) (cid:48)Î² (cid:98)K is (cid:195) (cid:33) n V(cid:98)K (x)=X K (x) (cid:48)(cid:161) X (cid:48) K X K (cid:162)â1 (cid:88) X Ki X K (cid:48) i e (cid:101)K 2 i (cid:161) X (cid:48) K X K (cid:162)â1 X K (x). (20.33) i=1 Astandarderrorform(x)isitssquareroot. (cid:98) 20.19 ClusteredObservations Clusteredobservationsare(Y ,X )forindividualsi =1,...,n inclusterg =1,...,G.Themodelis ig ig g Y =m (cid:161) X (cid:162)+e ig ig ig (cid:69)(cid:163) e |X (cid:164)=0 ig g whereX isthestackedX .StackY ande intocluster-levelvariablesY ande . g ig ig ig g g Theseriesregressionmodelusingcluster-levelnotationisY =X Î² +e .Wecanwritetheseries g g K Kg estimatoras (cid:195) (cid:33)â1(cid:195) (cid:33) G G Î² (cid:98)K = (cid:88) X (cid:48) g X g (cid:88) X (cid:48) g Y g . g=1 g=1 Thecluster-levelresidualvectoris (cid:98) e g =Y g âX g Î² (cid:98)K . Asforparametricregressionwithclusteredobservationsthestandardassumptionisthattheclusters are mutually independent but dependence within each cluster is unstructured. We therefore use the samevarianceformulaeasusedforparametricregression.Thestandardestimatoris (cid:195) (cid:33) V(cid:98) C Î²(cid:98) R1= (cid:181) G G â1 (cid:182) (cid:161) X (cid:48) K X K (cid:162)â1 g (cid:88) G =1 X (cid:48) g(cid:98) e g(cid:98) e (cid:48) g X g (cid:161) X (cid:48) K X K (cid:162)â1 .",
    "page": 739,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 720 Analternativeistousethedelete-clusterpredictionerror (cid:101) e g =Y g âX g Î² (cid:101)K,âg where (cid:195) (cid:33)â1(cid:195) (cid:33) Î² (cid:101)K,âg = (cid:88) X (cid:48) j X j (cid:88) X (cid:48) j Y j j(cid:54)=g j(cid:54)=g leadingtotheestimator (cid:195) (cid:33) G V(cid:98) C Î²(cid:98) R3=(cid:161) X (cid:48) K X K (cid:162)â1 (cid:88) X (cid:48) g(cid:101) e g(cid:101) e (cid:48) g X g (cid:161) X (cid:48) K X K (cid:162)â1 . g=1 ThereisnocurrenttheoryonhowtoselectthenumberofseriestermsK forclusteredobservations. Areasonablechoiceistominimizethedelete-clustercross-validationcriterionCV(K)=(cid:80)G e (cid:48) e . g=1(cid:101)g(cid:101)g 20.20 ConfidenceBands Whendisplayingnonparametricestimatorssuchasm (x)itiscustomarytodisplayconfidencein- (cid:98)K tervals. Anasymptoticpointwise95%confidenceintervalform(x)ism (cid:98)K (x)Â±1.96V(cid:98) K 1/2(x). Theseconfi- denceintervalscanbeplottedalongwithm (x). (cid:98)K Toillustrate,Figure20.6plotspolynomialestimatesoftheregressionoflog(wage)onexperienceusing theselectedestimatesfromFigure20.1,plus95%confidencebands. Panel(a)plotstheestimateforthe subsampleofwhitewomenusingp=5. Panel(b)plotstheestimateforthesubsampleofBlackwomen usingp=3.Thestandarderrorsarecalculatedusingtheformula(20.33).Youcanseethattheconfidence bandswidenattheboundaries.Theconfidencebandsaretightforthelargersubsampleofwhitewomen, andsignificantlywiderforthesmallersubsampleofBlackwomen. Regardless,bothplotsindicatethat theaveragewagerisesforexperiencelevelsuptoabout20yearsandthenflattensforexperiencelevels above20years. 0 10 20 30 40 2.3 0.3 8.2 6.2 4.2 Labor Market Experience (Years) ruoH rep sralloD goL 0 10 20 30 40 (a)WhiteWomen 2.3 0.3 8.2 6.2 4.2 Labor Market Experience (Years) ruoH rep sralloD goL (b)BlackWomen Figure20.6:PolynomialEstimateswith95%ConfidenceBands,College-EducatedWomen There are two deficiencies with these confidence bands. First, they do not take into account the biasr (x)oftheseriesestimator. Consequently,weshouldinterprettheconfidenceboundsasvalidfor K the pseudo-true regression (the best finite K approximation) rather than the true regression function",
    "page": 740,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 721 m(x). Second, the above confidence intervals are based on a pointwise (in x) asymptotic distribution theory. Consequently we should interpret their coverage as having pointwise validity and be cautious aboutinterpretingglobalshapesfromtheconfidencebands. 20.21 UniformApproximations Since m (x) is a function it is desirable to have a distribution theory which applies to the entire (cid:98)K function,notjusttheestimatoratapoint.Thiscanbeused,forexample,toconstructconfidencebands withuniform(inx)coverageproperties. Forthosefamiliarwithempiricalprocesstheory,itmightbehopedthatthestochasticprocess (cid:112) n(m (x)âm(x)) Î· (x)= (cid:98)K K V1/2(x) K mightconvergetoastochastic(Gaussian)process,butthisisnotthecase.Effectively,theprocessÎ· (x) K isnotstochasticallyequicontinuoussoconventionalempiricalprocesstheorydoesnotapply. Todevelopauniformtheory,Belloni,Chernozhukov,Chetverikov,andKato(2015)haveintroduced whatareknownasstrongapproximations. TheirmethodshowsthatÎ· (x)isequalindistributiontoa K sequenceofGaussianprocessesplusanegligibleerror. Theirtheory(Theorem4.4)takesthefollowing form.UnderstrongerconditionsthanAssumption20.2 X (x) (cid:48)(cid:161) Q â1â¦ Q â1(cid:162)1/2 Î· (x)= K K K K G +o (1) K d V1/2(x) K p K uniformlyinx,whereâ= âmeansâequalityindistributionâandG â¼N(0,I ). d K K ThisshowsthedistributionalresultinTheorem20.10canbeinterpretedasholdinguniformlyinx. It can also be used to develop confidence bands (different from those from the previous section) with asymptoticuniformcoverage. 20.22 PartiallyLinearModel Acommonuseofaseriesregressionistoallowm(x)tobenonparametricwithrespecttoonevariable yetlinearintheothervariables.Thisallowsflexibilityinaparticularvariableofinterest.Apartiallylinear modelwithvector-valuedregressorX andreal-valuedcontinuousX takestheform 1 2 m(x ,x )=x (cid:48)Î² +m (x ). 1 2 1 1 2 2 ThismodeliscommonwhenX arediscrete(e.g.binary)andX iscontinuouslydistributed. 1 2 Seriesmethodsareconvenientforpartiallylinearmodelsaswecanreplacetheunknownfunction m (x )withaseriesexpansiontoobtain 2 2 m(X)(cid:39)m (X)=X (cid:48)Î² +X (X ) (cid:48)Î² =X (cid:48) Î² K 1 1 2K 2 2K K K where X =X (x )arebasistransformationsof x (typicallypolynomialsorsplines). Aftertransfor- 2K 2K 2 2 mationtheregressorsareX =(X (cid:48) ,X (cid:48) )withcoefficientsÎ² =(Î²(cid:48) ,Î²(cid:48) ) (cid:48) . K 1 2K K 1 2K",
    "page": 741,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 722 20.23 PanelFixedEffects Theone-wayerrorcomponentsnonparametricregressionmodelis Y =m(X )+u +Îµ it it i it fori =1,...,N andt =1,...,T. Itisstandardtotreattheindividualeffectu asafixedeffect. Thismodel i canbeinterpretedasaspecialcaseofthepartiallylinearmodelfromtheprevioussectionthoughthe dimensionofu isincreasingwithN. i Aseriesestimatorapproximatesthefunctionm(x)withm (x)=X (x) (cid:48)Î² asin(20.4). Thisleadsto K K K theseriesregressionmodelY =X (cid:48) Î² +u +Îµ whereX =X (X ). it Kit K i Kit Kit K it Thefixedeffectsestimatoristhesameasinlinearpaneldataregression. First,thewithintransfor- mationisappliedtoY andtotheelementsofthebasistransformations X . TheseareYË =Y âY it Kit it it i andXË =X âX . ThetransformedregressionequationisYË =XË(cid:48) Î² +ÎµË . Whatisimportant Kit Kit Kit it Kit K Kit aboutthewithintransformationfortheregressorsisthatitisappliedtothetransformedvariablesXË Kit nottheoriginalregressorX . Forexample,inapolynomialregressionthewithintransformationisap- it j pliedtothepowersX . ItisinappropriatetoapplythewithintransformationtoX andthenconstruct it it thebasistransformations. Thecoefficientisestimatedbyleastsquaresonthewithintransformedvariables (cid:195) (cid:33)â1(cid:195) (cid:33) n T n T Î² (cid:98)K = (cid:88)(cid:88) XË Kit XË K (cid:48) it (cid:88)(cid:88) XË Kit YË it . i=1t=1 i=1t=1 Varianceestimatorsshouldbecalculatedusingtheclusteredvarianceformulas,clusteredatthelevelof theindividuali,asdescribedinSection20.19. ForselectionofthenumberofseriestermsK thereisnocurrenttheory. Areasonablemethodisto usedelete-clustercross-validationasdescribedinSection20.19. 20.24 MultipleRegressors SupposeX â(cid:82)d isvector-valuedandcontinuouslydistributed. Amultivariateseriesapproximation canbeobtainedasfollows. Constructasetofbasistransformationsforeachvariableseparately. Take theirtensorcross-products.Usetheseasregressors.Forexample,apth-orderpolynomialis p p m (x)=Î² + (cid:88) Â·Â·Â· (cid:88) x j1Â·Â·Â·x jdÎ² . K 0 1 d j1,...,jdK j1 =1 jd =1 Thisincludesallpowersandcross-products.ThecoefficientvectorhasdimensionK =1+pd. Theinclusionofcross-productsgreatlyincreasesthenumberofcoefficientsrelativetotheunivariate case.Consequentlyseriesapplicationswithmultipleregressorstypicallyrequirelargesamplesizes. 20.25 AdditivelySeparableModels Asdiscussedintheprevioussection,whenX â(cid:82)d afullseriesexpansionrequiresalargenumberof coefficients,whichmeansthatestimationprecisionwillbelowunlessthesamplesizeisquitelarge. A commonsimplificationistotreattheregressionfunctionm(x)asadditivelyseparableintheindividual regressors.Thismeansthat m(x)=m (x )+m (x )+Â·Â·Â·+m (x ). 1 1 2 2 d d",
    "page": 742,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 723 (cid:161) (cid:162) We then apply series expansions (polynomials or splines) separately for each component m x . j j Essentially,thisisthesameastheexpansionsdiscussedintheprevioussectionbutomittingtheinterac- tionterms. The advantage of additive separability is the reduction in dimensionality. While an unconstrained pth orderpolynomialhas1+pd coefficients,anadditivelyseparablepolynomialmodelhasonly1+dp coefficients.Thisisamajorreduction. Thedisadvantageofadditiveseparabilityisthattheinteractioneffectshavebeeneliminated.Thisis asubstantiverestrictiononm(x). Thedecisiontoimposeadditiveseparabilitycanbebasedonaneconomicmodelwhichsuggeststhe absenceofinteractioneffects,orcanbeamodelselectiondecisionsimilartotheselectionofthenumber ofseriesterms. 20.26 NonparametricInstrumentalVariablesRegression Thebasicnonparametricinstrumentalvariables(NPIV)modeltakestheform Y =m(X)+e (20.34) (cid:69)[e|Z]=0 whereY,X andZ arerealvalued.Here,Z isaninstrumentalvariableandX anendogenousregressor. In recent years there have been many papers in the econometrics literature examining the NPIV model,exploringidentification,estimation,andinference.Manyofthesepapersaremathematicallyad- vanced. TwoimportantandaccessiblecontributionsareNeweyandPowell(2003)andHorowitz(2011). Herewedescribesomeoftheprimaryresults. Aseriesestimatorapproximatesthefunctionm(x)withm (x)=X (x) (cid:48)Î² asin(20.4). Thisleadsto K K K theseriesstructuralequation Y =X (cid:48) Î² +e (20.35) K K K whereX =X (X).Forexample,ifapolynomialbasisisusedthenX =(1,X,...,XKâ1). K K K SinceX isendogenoussoistheentirevectorX .ThusweneedatleastK instrumentalvaribles.Itis K usefultoconsiderthereducedformequationforX.Anonparametricspecificationis X =g(Z)+u (cid:69)[u|Z]=0. Wecanappropriateg(z)bytheseriesexpansion g(z)(cid:39)g (z)=Z (z) (cid:48)Î³ L L L whereZ (z)isanLÃ1vectorofbasistransformationsandÎ³ isanLÃ1coefficientvector.Forexample, L L ifapolynomialbasisisusedthen Z (z)=(1,z,...,zLâ1). Mostoftheliteratureforsimplicityfocuseson L thecaseL=K,butthisisnotessentialtothemethod. IfLâ¥K wecanthenuseZ L =Z L (Z)asinstrumentsforX K .The2SLSestimatorÎ² (cid:98)K,L ofÎ² K is Î² (cid:98)K,L = (cid:179) X (cid:48) K Z L (cid:161) Z (cid:48) L Z L (cid:162)â1 Z (cid:48) L X K (cid:180)â1(cid:179) X (cid:48) K Z L (cid:161) Z (cid:48) L Z L (cid:162)â1 Z (cid:48) L Y (cid:180) . Theestimatorofm(x)ism (cid:98)K (x)=X K (x) (cid:48)Î² (cid:98)K,L .IfL>K thelinearGMMestimatorcanbesimilarlydefined.",
    "page": 743,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 724 Onewaytothinkaboutthechoiceofinstrumentsistorealizethatweareactuallyestimatingreduced formequationsforeachelementofX .Thereducedformsystemis K X =Î(cid:48) Z +u K K L K Î =(cid:69)(cid:163) Z Z (cid:48)(cid:164)â1(cid:69)(cid:163) Z X (cid:48) (cid:164) . K L L L K Forexample,supposeweuseapolynomialbasiswithK =L=3.Thenthereducedformsystem(ignoring intercepts)is ï£® X ï£¹ ï£® Î Î Î ï£¹ï£® Z ï£¹ ï£® u ï£¹ 11 21 31 1 ï£° X2 ï£» = ï£° Î 12 Î 22 Î 32 ï£»ï£° Z2 ï£» + ï£° u 2 ï£». (20.36) X3 Î Î Î Z3 u 13 13 23 3 ThisismodelingtheconditionalmeanofX,X2andX3aslinearfunctionsofZ,Z2andZ3. TounderstandifthecoefficientÎ² isidentifieditisusefultoconsiderthesimplereducedformequa- K tionX =Î³ +Î³ Z+u.AssumethatÎ³ (cid:54)=0sothattheequationisstronglyidentifiedandassumeforsim- 0 1 1 plicitythatu isindependentof Z withmeanzeroandvarianceÏ2. Theidentificationpropertiesofthe u reducedformareinvarianttorescalingandrecenteringX andZ sowithoutlossofgeneralitywecanset Î³ =0andÎ³ =1.Thenwecancalculatethatthecoefficientmatrixin(20.36)is 0 1 ï£® Î Î Î ï£¹ ï£® 1 0 0 ï£¹ 11 21 31 ï£° Î 12 Î 22 Î 32 ï£» = ï£° 0 1 0 ï£». Î Î Î 3Ï2 0 1 13 13 23 u Noticethatthisislowertriangularandfullrank. Itturnsoutthatthispropertyholdsforanyvaluesof K =L so the coefficient matrix in (20.36) is full rank for any choice of K =L. This means that identi- ficationofthecoefficientÎ² isstrongifthereducedformequationfor X isstrong. Thustocheckthe K identificationconditionforÎ² itissufficienttocheckthereducedformequationforX. Acriticallyim- K portantcaveat,however,asdiscussedinthefollowingsection,isthatidentificationofÎ² doesnotmean K thatthestructuralfunctionm(x)isidentified. AsimplemethodforpointwiseinferenceistouseconventionalmethodstoestimateV K,L =var (cid:163)Î² (cid:98)K,L (cid:164) (cid:48) andthenestimatevar[m (cid:98)K (x)]by X K (x)V(cid:98)K,L X K (x)asinseriesregression. Bootstrapmethodsaretypi- callyadvocatedtoachievebettercoverage.SeeHorowitz(2011)fordetails.Forstate-of-the-artinference methodsseeChenandPouzo(2015)andChenandChristensen(2018). 20.27 NPIVIdentification Intheprevioussectionwediscussedidenticationofthepseudo-truecoefficientÎ² . Inthissection K wediscussidentificationofthestructuralfunctionm(x).Thisisconsiderablymorechallenging. To understand how the function m(x) is determined, apply the expectation operator (cid:69)[Â·|Z =z] to (20.34).Wefind (cid:69)[Y |Z =z]=(cid:69)[m(X)|Z =z] withtheremainderequaltozerobecause(cid:69)[e|Z]=0.Wecanwritethisequationas (cid:90) Âµ(z)= m(x)f (x|z)dx (20.37) whereÂµ(z)=(cid:69)[Y |Z =z]istheconditionalmeanofY givenZ =zand f (x|z)istheconditionaldensity of X given Z. These two functions are identified8 from the joint distribution of (Y,X,Z). This means 8Technically,if(cid:69)|Y|<â,thejointdensityof(Z,X)exists,andthemarginaldensityofZispositive.",
    "page": 744,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 725 thattheunknownfunctionm(x)isthesolutiontotheintegralequation(20.37). Conceptually,youcan imagineestimatingÂµ(z)and f (x|z)usingstandardtechniquesandthenfindingthesolutionm(x). In essence,thisishowm(x)isdefinedandisthenonparametricanalogoftheclassicalrelationshipbetween thestructuralandreducedforms. Unfortunately the solution m(x) may not be unique even in situations where a linear IV model is strongly identified. It is related to what is known as the ill-posedinverseproblem. The latter means thatthesolutionm(x)isnotnecessarilyacontinuousfunctionofÂµ(z).Identificationrequiresrestricting theclassofallowablefunctions f (x|z).ThisisanalogoustothelinearIVmodelwhereidentificationre- quiresrestrictionsonthereducedformequations.Specifyingandunderstandingtheneededrestrictions ismoresubtlethaninthelinearcase. Thefunctionm(x)isidentifiedifitistheuniquesolutionto(20.37).Equivalently,m(x)isnotidenti- fiedifwecanreplacem(x)in(20.37)withm(x)+Î´(x)forsomenon-trivialfunctionÎ´(x)yetthesolution doesnotchange.Thelatteroccurswhen (cid:90) Î´(x)f (x|z)dx=0 (20.38) forallz.Equivalently,m(x)isidentifiedif(andonlyif)(20.38)holdsonlyforthetrivialfunctionÎ´(x)=0. NeweyandPowell(2003)definedthisfundamentalconditionascompleteness. Proposition20.1 Completeness. m(x) is identified if (and only if) the com- pletenessconditionholds:(20.38)forallzimpliesÎ´(x)=0. Completenessisapropertyofthereducedformconditionaldensity f (x|z). Itisunaffectedbythe structuralequationm(x). ThisisanalogoustothelinearIVmodelwhereidentificationisapropertyof thereducedformequations,notapropertyofthestructuralequation. Aswestatedabove,completenessmaynotbesatisfiedevenifthereducedformrelationshipisstrong. This may be easiest to see by a constructed example9. Suppose that the reduced form is X = Z +u, var[Z]=1,u isindependentof Z,andu isdistributedU[â1,1]. ThisreducedformequationhasR2= 0.75 so is strong. The reduced form conditional density is f (x|z) = 1/2 on [â1+z,1+z]. Consider Î´(x)=sin(x/Ï).Wecalculatethat (cid:90) (cid:90) 1+z Î´(x)f (x|z)dx= sin(x/Ï)dx=0 â1+z for every z, since sin(x/Ï) is periodic on intervals of length 2 and integrates to zero over [â1,1]. This meansthatequation(20.37)holds10form(x)+sin(x/Ï). Thusm(x)isnotidentified. Thisisdespitethe factthatthereducedformequationisstrong. Whileidentificationfailsforsomeconditionaldistributions f (x|z), itdoesnotfailforalldistribu- tions. Andrews (2017) provides classes of distributions which satisfy the completeness condition and showsthatthesedistributionclassesarequitegeneral. Whatdoesthismeaninpractice? Ifcompletenessfailsthenthestructuralequationisnotidentified and cannot be consistently estimated. Furthermore, by analogy with the weak instruments literature, weexpectthatiftheconditionaldistributionisclosetoincompletethenthestructuralequationwillbe 9ThisexamplewassuggestedbyJoachimFreyberger. 10Infact,(20.38)holdsform(x)+Î´(x)foranyfunctionÎ´(x)whichisperiodiconintervalsoflength2andintegratestozero on[â1,1].",
    "page": 745,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 726 poorlyidentifiedandourestimatorswillbeimprecise.Sincewhetherornottheconditionaldistribution iscompleteisunknown(andmoredifficulttoassessthaninthelinearmodel)thisistroublingforempir- icalresearch.Effectively,inanygivenapplicationwedonotknowwhetherornotthestructuralfunction m(x)isidentified. ApartialanswerisprovidedbyFreyberger(2017).Heshowsthatthejointhypothesisofincomplete- nessandsmallasymptoticbiascanbetested. ByapplyingthetestproposedinFreyberger(2017)auser can obtain evidence that their NPIV estimator is well-behaved in the sense of having low bias. Unlike StockandYogo(2005),however,Freybergerâsresultdoesnotaddressinference. 20.28 NPIVConvergenceRate AsdescribedinHorowitz(2011)theconvergencerateofm (x)form(x)is (cid:98)K (cid:195) (cid:33) |m (x)âm(x)|=O K âs+Kr (cid:181) K (cid:182)1/2 (20.39) (cid:98)K p n wheresisthesmoothness11ofm(x)andr isthesmoothnessofthejointdensity f (x,z)of(X,Z).The XZ firsttermK âsisthebiasduetotheapproximationofm(x)bym (x)andtakesthesameformasforseries K regression.ThesecondtermKr(K/n)1/2isthestandarddeviationofm (x).Thecomponent(K/n)1/2is (cid:98)K thesameasforseriesregression. TheextracomponentKr isduetotheill-posedinverseproblem(see theprevioussection). From the rate (20.39) we can calculate that the optimal number of series terms is K â¼n1/(2r+2s+1). Given this rate the best possible convergence rate in (20.39) isO (cid:161) n âs/(2r+2s+1)(cid:162) . For r >0 these rates p are slower than for series regression. If we consider the case s = 2 these rates are K â¼ n1/(2r+5) and O (cid:161) n â2/(2r+5)(cid:162) ,whichareslowerthantheK â¼n1/5andO (cid:161) n â2/5(cid:162) ratesobtainedbyseriesregression. p p Averyunusualaspectoftherate(20.39)isthatsmoothnessof f (x,z)adverselyaffectstheconver- XZ gencerate. Largerr meansaslowerrateofconvergence. Thelimitingcaseasr ââ(forexample,joint normalityof X and Z)resultsinalogarithmicconvergencerate. Thisseemsverystrange. Thereason isthatwhenthedensity f (x,z)isverysmooththedatacontainlittleinformationaboutthefunction XZ m(x).Thisisnotintuitiveandrequiresadeepermathematicaltreatment. Apracticalimplicationoftheconvergencerate(20.39)isthatthenumberofseriestermsK should bemuchsmallerthanforregressionestimation. EstimationvarianceincreasesquicklyasK increases. ThereforeK shouldnotbetakentobetoolarge.Inpractice,however,itisunclearhowtoselecttheseries orderK asstandardcross-validationmethodsdonotapply. 20.29 NonparametricvsParametricIdentification Oneoftheinsightsfromthenonparametricidentificationliteratureisthatitisimportanttounder- stand which features of a model are nonparametrically identified, meaning which are identified with- outfunctionalformassumptions,andwhichareonlyidentifiedbasedonfunctionalformassumptions. Sincefunctionalformassumptionsaredubiousinmosteconomicapplicationsthestrongimplicationis thatresearchersshouldstrivetoworkonlywithmodelswhicharenonparametricallyidentified",
    "page": 746,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". 20.29 NonparametricvsParametricIdentification Oneoftheinsightsfromthenonparametricidentificationliteratureisthatitisimportanttounder- stand which features of a model are nonparametrically identified, meaning which are identified with- outfunctionalformassumptions,andwhichareonlyidentifiedbasedonfunctionalformassumptions. Sincefunctionalformassumptionsaredubiousinmosteconomicapplicationsthestrongimplicationis thatresearchersshouldstrivetoworkonlywithmodelswhicharenonparametricallyidentified. Evenifamodelisdeterminedtobenonparametricallyidentifiedaresearchermayestimatealinear (oranothersimpleparametric)model.Thisisvalidbecauseitcanbeviewedasanapproximationtothe nonparametricstructure. If,however,themodelisidentifiedonlyunderaparametricassumption,then itcannotbeviewedasanapproximationanditisunclearhowtointerpretthemodelmorebroadly. 11Thenumberofboundedderivatives.",
    "page": 746,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 727 For example, in the regression model Y =m(X)+e with (cid:69)[e|X]=0 the conditional mean is non- parametricallyidentifiedbyTheorem2.14. Thismeansthatresearcherswhoestimatelinearregressions (orotherlow-dimensionalregressions)caninterprettheirestimatedmodelasanapproximationtothe underlyingconditionalmeanfunction. Asanotherexample,intheNPIVmodelwhere(cid:69)[e|Z]=0thestructuralfunctionm(x)isidentified under the completeness condition. This means that researchers who estimate linear 2SLS regressions caninterprettheirestimatedmodelasanapproximationtom(x)(subjecttothecaveatthatitisdifficult toknowifcompletenessholds). Buttheanalysiscanalsopointoutsimpleyetsubtlemistakes. TakethesimpleIVmodelwithone exogenousregressorX andoneendogenousregressorX 1 2 Y =Î² +Î² X +Î² X +e (20.40) 0 1 1 2 2 (cid:69)[e|X ]=0 1 withnoadditionalinstruments. Supposethatanenterprisingresearchersuggestsusingtheinstrument X2 for X ,usingthereasoningthattheassumptionsimplythat(cid:69)(cid:163) X2e (cid:164)=0so X2 isavalidinstrument. 1 2 1 1 Thetroubleisthatthebasicmodelisnotnonparametricallyidentified. Ifwewrite(20.40)asapartially linearnonparametricIVproblem Y =m(X )+Î² X +e (20.41) 1 2 2 (cid:69)[e|X ]=0 1 thenwecanseethatthismodelisnotidentified. WeneedavalidexcludedinstrumentZ. Since(20.41) isnotidentified,then(20.40)cannotbeviewedasavalidapproximation. Theapparentidentificationof (20.40)criticallyrestsontheunknowntruthofthelinearityin(20.40). Thepointofthisexampleisthat(20.40)shouldneverbeestimatedby2SLSusingtheinstrumentX2 1 forX ,fundamentallybecausethenonparametricmodel(20.41)isnotidentified. 2 Anotherwaytodescribethemistakeistoobservethat X2 isavalidinstrumentin(20.40)onlyifit 1 isavalidexclusionrestrictionfromthestructuralequation(20.40). Viewedinthecontextof(20.41)we canseethatthisisafunctionalformrestriction.Asstatedabove,identificationbasedonfunctionalform restrictionsaloneishighlyundesirablesincefunctionalformassumptionsaredubious. 20.30 Example: AngristandLavy(1999) ToillustratenonparametricinstrumentalvariablesinpracticewefollowHorowitz(2011)byextend- ingtheempiricalworkreportedinAngristandLavy(1999).Theirpaperisconcernedwithmeasuringthe causaleffectofthenumberofstudentsinanelementaryschoolclassroomonacademicachievement. Theyaddressthisusingasampleof4067Israeli4th and5th gradeclassrooms. Thedependentvariable istheclassroomaveragescoreonanachievementtest.Hereweconsiderthereadingscoreavgverb.The explanatory variables are the number of students in the classroom (classize), the number of students inthegradeattheschool(enrollment),andaschool-levelindexofstudentsâsocioeconomicstatusthat theauthorscallpercentdisadvantaged. Thevariablesenrollment anddisadvantaged aretreatedasex- ogenousbutclassizeistreatedasendogenoussincewealthierschoolsmaybeabletooffersmallerclass sizes. Theauthorssuggestthefollowinginstrumentalvariableforclasssize. Israeliregulationsspecifythat classsizesmustbecappedat40",
    "page": 747,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Thevariablesenrollment anddisadvantaged aretreatedasex- ogenousbutclassizeistreatedasendogenoussincewealthierschoolsmaybeabletooffersmallerclass sizes. Theauthorssuggestthefollowinginstrumentalvariableforclasssize. Israeliregulationsspecifythat classsizesmustbecappedat40. Thismeansthatclassize shouldbeperfectlypredictablefromenroll- ment. Iftheregulationisfollowedaschoolwithupto40studentswillhaveoneclassroominthegrade",
    "page": 747,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 728 andschoolswith41-80studentswillhavetwoclassrooms.Theprecisepredictionisthatclasssizeequals enrollment p= (20.42) 1+(cid:98)1âenrollment/40(cid:99) where(cid:98)a(cid:99)istheintegerpartofa.AngristandLavyusep asaninstrumentalvariableforclassize. Theyestimateseveralspecifications. Wefocusonequation(6)fromtheirTableVIIwhichspecifies avgverbasalinearfunctionofclassize,disadvantaged,enrollment,grade4,andtheinteractionofclassize anddisadvantaged,wheregrade4isadummyindicatorfor4th gradeclassrooms. Theequationisesti- mated by instrumental variables, using p and pÃdisadvantaged as instruments. The observations are treatedasclusteredattheleveloftheschool.Theirestimatesshowanegativeandstatisticallysignificant impactofclasssizeonreadingtestscores. Weareinterestedinanonparametricversionoftheirequation.Tokeepthespecificationreasonably parsimoniousyetflexibleweusethefollowingequation. (cid:181) classize (cid:182) (cid:181) classize (cid:182)2 (cid:181) classize (cid:182)3 avgverb=Î² +Î² +Î² 1 2 3 40 40 40 (cid:181) disadvantaged (cid:182) (cid:181) disadvantaged (cid:182)2 (cid:181) disadvantaged (cid:182)3 +Î² +Î² +Î² 4 5 6 14 14 14 (cid:181) (cid:182)(cid:181) (cid:182) classize disadvantaged +Î² +Î² enrollment+Î² grade4+Î² +e. 7 8 9 10 40 14 Thisisacubicequationinclassizeanddisadvantaged,withasingleinteractionterm,andlinearinenroll- mentandgrade4.Thecubicindisadvantagedwasselectedbyadelete-clustercross-validationregression withoutclassize.Thecubicinclassizewasselectedtoallowforaminimaldegreeofnonparametricflexi- bilitywithoutoverparameterization.Thevariablesclassizeanddisadvantagedwerescaledby40and14, respectively,sothattheregressioniswellconditioned. Thescalingforclassizewasselectedsothatthe variableessentiallyfallsin[0,1]andthescalingfordisadvantagedwasselectedsothatitsmeanis1. The equation is estimated by 2SLS using (p/40), (p/40)2, (p/40)3 and (p/40)Ã(disadvantaged/14) asinstrumentsforthefourvariablesinvolvingclassize. TheparameterestimatesarereportedinTable 20.1. Thestandarderrorsareclusteredattheleveloftheschool. Mostoftheindividualcoefficientsdo nothaveinterpretablemeaningexceptthepositivecoefficientonenrollment showsthatlargerschools achieveslightlyhighertestscores,andthenegativecoefficientongrade4showsthat4th gradestudents havesomewhatlowertestscoresthan5th gradestudents. ToobtainabetterinterpretationoftheresultswedisplaytheestimatedregressionfunctionsinFigure 20.7. Panel (a) displays the estimated effect of classize on reading test scores. Panel (b) displays the estimatedeffectofdisadvantaged.Inbothfigurestheothervariablesaresetattheirsamplemeans12. Inpanel(a)wecanseethatincreasingclasssizedecreasestheaveragetestscore. Thisisconsistent with the results from the linear model estimated by Angrist and Lavy (1999). The estimated effect is remarkablyclosetolinear. Inpanel(b)wecanseethatincreasingthepercentageofdisadvantagedstudentsgreatlydecreases theaveragetestscore",
    "page": 748,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Panel (b) displays the estimatedeffectofdisadvantaged.Inbothfigurestheothervariablesaresetattheirsamplemeans12. Inpanel(a)wecanseethatincreasingclasssizedecreasestheaveragetestscore. Thisisconsistent with the results from the linear model estimated by Angrist and Lavy (1999). The estimated effect is remarkablyclosetolinear. Inpanel(b)wecanseethatincreasingthepercentageofdisadvantagedstudentsgreatlydecreases theaveragetestscore. Thiseffectissubstantiallygreaterinmagnitudethantheeffectofclasssize. The effect also appears to be nonlinear. The effect is precisely estimated with tight pointwise confidence bands. Wecanalsousetheestimatedmodelforhypothesistesting. ThequestionaddressedbyAngristand Lavy was whether or not classsize has an effect on test scores. Within the nonparametric model esti- matedherethishypothesisholdsunderthelinearrestriction(cid:72) :Î² =Î² =Î² =Î² =0. Examiningthe 0 1 2 3 7 individualcoefficientestimatesandstandarderrorsitisunclearifthisisasignificanteffectasnoneof 12Iftheyaresetatothervaluesitdoesnotchangethequalitativenatureoftheplots.",
    "page": 748,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 729 Table20.1:NonparametricInstrumentalVariableRegressionforReadingTestScore classize/40 34.2 (33.4) (classize/40)2 â61.2 (53.0) (classize/40)3 29.0 (26.8) disadvantaged/14 â12.4 (1.7) (disadvantaged/14)2 3.33 (0.54) (disadvantaged/14)3 â0.377 (0.078) (classize/40)(disadvantaged/14) 0.81 (1.77) enrollment 0.015 (0.007) grade4 â1.96 (0.16) Intercept 77.0 (6.9) thesefourcoefficientestimatesisstatisticallydifferentfromzero. Thishypothesisisbettertestedbya Waldtest(usingcluster-robustvarianceestimates).Thisstatisticis12.7whichhasanasymptoticp-value of0.013.Thissuppportsthehypothesisthatclasssizehasanegativeeffectonstudentperformance. Wecanalsousethemodeltoquantifytheimpactofclasssizeontestscores. Considertheimpactof increasingaclassfrom20to40students.Intheabovemodelthepredictedimpactontestscoresis 1 3 7 1 Î¸= Î² + Î² + Î² + Î² . 1 2 3 4 2 4 8 2 Thisisalinearfunctionofthecoefficients. ThepointestimateisÎ¸ (cid:98) =â2.96withastandarderrorof1.21. (Thepointestimateisidenticaltothedifferencebetweentheendpointsoftheestimatedfunctionshown inpanel(a).)Thisisasmallbutsubstantiveimpact. 20.31 TechnicalProofs* Proof of Theorem 20.4. We provide a proof under the stronger assumption Î¶2K/n â 0. (The proof K presentedbyBelloni,Chernozhukov,Chetverikov,andKato(2015)requiresamoreadvancedtreatment.) Let(cid:107)A(cid:107) F denotetheFrobeniusnorm(seeSectionA.23),andwritethe jth elementofX(cid:101)Ki asX(cid:101)jKi .Using (A.18), (cid:195) (cid:33)2 (cid:176) (cid:176)Q(cid:101)K âI K (cid:176) (cid:176) 2â¤(cid:176) (cid:176)Q(cid:101)K âI K (cid:176) (cid:176) 2 F = (cid:88) K (cid:88) K n 1 (cid:88) n (cid:161) X(cid:101)jKi X(cid:101)(cid:96)Ki â(cid:69)(cid:163) X(cid:101)jKi X(cid:101)(cid:96)Ki (cid:164)(cid:162) . j=1(cid:96)=1 i=1",
    "page": 749,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 730 20 25 30 35 40 08 57 07 56 Class Size erocS tseT gnidaeR 0 10 20 30 40 50 (a)EffectofClassize 08 57 07 56 Percentage Disadvantaged erocS tseT gnidaeR (b)EffectofPercentDisadvantaged Figure20.7:NonparametricInstrumentalVariablesEstimatesoftheEffectofClassizeandDisadvantaged onReadingTestScores Then (cid:34) (cid:35) (cid:69) (cid:104)(cid:176) (cid:176)Q(cid:101)K âI K (cid:176) (cid:176) 2 (cid:105) â¤ (cid:88) K (cid:88) K var 1 (cid:88) n X(cid:101)jKi X(cid:101)(cid:96)Ki n j=1(cid:96)=1 i=1 = 1 (cid:88) K (cid:88) K var (cid:163) X(cid:101)jKi X(cid:101)(cid:96)Ki (cid:164) n j=1(cid:96)=1 (cid:34) (cid:35) â¤ n 1 (cid:69) (cid:88) K X(cid:101) j 2 Ki (cid:88) K X(cid:101)(cid:96) 2 Ki j=1 (cid:96)=1 = n 1 (cid:69) (cid:104) (cid:161) X(cid:101) K (cid:48) i X(cid:101)Ki (cid:162)2 (cid:105) Î¶2 Î¶2K â¤ n K(cid:69)(cid:163) X(cid:101) K (cid:48) i X(cid:101)Ki (cid:164)= K n â0 wherefinallinesuse(20.17),(cid:69)(cid:163) X(cid:101) K (cid:48) i X(cid:101)Ki (cid:164)=K,andÎ¶2 K K/nâ0.Markovâsinequalityimplies(20.19). â  ProofofTheorem20.5. BythespectraldecompositionwecanwriteQ(cid:101)K =H (cid:48)ÎH where H (cid:48) H =I K and Î=diag(Î» ,...,Î» )aretheeigenvalues.Then 1 K (cid:176) (cid:176)Q(cid:101)K âI K (cid:176) (cid:176) =(cid:176) (cid:176)H (cid:48) (ÎâI K )H (cid:176) (cid:176) =(cid:107)ÎâI K (cid:107)=max (cid:175) (cid:175) Î» j â1 (cid:175) (cid:175) ââ0 jâ¤K p",
    "page": 750,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 731 byTheorem20.4.Thisimpliesmin (cid:175) (cid:175) Î» j (cid:175) (cid:175) ââ1whichis(20.21).Similarly jâ¤K p (cid:176) (cid:176) (cid:176) Q(cid:101) â K 1âI K (cid:176) (cid:176) (cid:176) =(cid:176) (cid:176)H (cid:48)(cid:161)Îâ1âI K (cid:162) H (cid:176) (cid:176) =(cid:176) (cid:176) Îâ1âI K (cid:176) (cid:176) (cid:175) (cid:175) =max(cid:175)Î»â1â1(cid:175) jâ¤K (cid:175) j (cid:175) â¤ m m ax i j n â¤ j K â¤K (cid:175) (cid:175)1 (cid:175) (cid:175) â Î» j Î» (cid:175) (cid:175) j (cid:175) (cid:175) â p â0. â  ProofofTheorem20.6.Using(20.12)wecanwrite m (cid:98)K (x)âm(x)=X K (x) (cid:48)(cid:161)Î² (cid:98)K âÎ² K (cid:162)âr K (x). (20.43) Since e =r +e is a projection error it satisfies (cid:69)[X e ]=0. Since e is a regression error it satisfies K K K K (cid:69)[X e] = 0. We deduce (cid:69)[X r ] = 0. Hence (cid:82) X (x)r (x)f(x)dx = (cid:69)[X r ] = 0. Also observe that K K K K K K K (cid:82) X (x)X (x) (cid:48) dF(x)=Q and (cid:82) r (x)2dF(x)=(cid:69)(cid:163) r2(cid:164)=Î´2.Then K K K K K K (cid:90) ISE(K)= (cid:161) X K (x) (cid:48)(cid:161)Î² (cid:98)K âÎ² K (cid:162)âr K (x) (cid:162)2 dF(x) (cid:181)(cid:90) (cid:182) =(cid:161)Î² (cid:98)K âÎ² K (cid:162)(cid:48) X K (x)X K (x) (cid:48) dF(x) (cid:161)Î² (cid:98)K âÎ² K (cid:162) (cid:181)(cid:90) (cid:182) (cid:90) â2 (cid:161)Î² (cid:98)K âÎ² K (cid:162)(cid:48) X K (x)r K (x)dF(x) + r K (x)2dF(x) =(cid:161)Î² (cid:98)K âÎ² K (cid:162)(cid:48) Q K (cid:161)Î² (cid:98)K âÎ² K (cid:162)+Î´2 K . (20.44) Wecalculatethat (cid:161)Î² (cid:98)K âÎ² K (cid:162)(cid:48) Q K (cid:161)Î² (cid:98)K âÎ² K (cid:162)=(cid:161) e (cid:48) K X K (cid:162)(cid:161) X (cid:48) K X K (cid:162)â1 Q K (cid:161) X (cid:48) K X K (cid:162)â1(cid:161) X (cid:48) K e K (cid:162) =(cid:161) e (cid:48) K X(cid:101)K (cid:162) (cid:179) X(cid:101) (cid:48) K X(cid:101)K (cid:180)â1(cid:179) X(cid:101) (cid:48) K X(cid:101)K (cid:180)â1(cid:179) X(cid:101) (cid:48) K e K (cid:180) =n â2(cid:161) e (cid:48) K X(cid:101)K (cid:162) Q(cid:101) â K 1 Q(cid:101) â K 1 (cid:179) X(cid:101) (cid:48) K e K (cid:180) â¤ (cid:179) Î» max (cid:179) Q(cid:101) â K 1 (cid:180)(cid:180)2(cid:179) n â2e (cid:48) K X(cid:101)K X(cid:101) (cid:48) K e K (cid:180) â¤O (1) (cid:161) n â2e (cid:48) X Q â1X (cid:48) e (cid:162) (20.45) p K K K K K where X(cid:101)K and Q(cid:101)K are the orthogonalized regressors as defined in (20.18). The first inequality is the QuadraticInequality(B.18),thesecondis(20.21). UsingthefactthatX e aremeanzeroanduncorrelated,(20.17),(cid:69)(cid:163) e2(cid:164)â¤(cid:69)(cid:163) Y2(cid:164)<â,andAssump- K K K tion20.1.2, (cid:69)(cid:163) n â2e (cid:48) X Q â1X (cid:48) e (cid:164)=n â1(cid:69)(cid:163) X (cid:48) Q â1X e2(cid:164) (20.46) K K K K K K K K K Î¶2 â¤ K(cid:69)(cid:163) e2(cid:164)â¤o(1). n K Thisshowsthat(20.45)iso (1).Combinedwith(20.44)wefindISE(K)=o (1)asclaimed. â  p p",
    "page": 751,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 732 ProofofTheorem20.7.TheassumptionÏ2(x)â¤Ï2impliesthat (cid:69)(cid:163) e2 |X (cid:164)=(cid:69)(cid:163) (r +e)2|X (cid:164)=r2 +Ï2(X)â¤r2 +Ï2. K K K K Thus(20.46)isboundedby Î¶2 n â1(cid:69)(cid:163) X (cid:48) Q â1X r2(cid:164)+n â1(cid:69)(cid:163) X (cid:48) Q â1X (cid:164)Ï2â¤ K(cid:69)(cid:163) r2(cid:164)+n â1(cid:69)(cid:163) tr (cid:161) Q â1X X (cid:48) (cid:162)(cid:164)Ï2 K K K K K K K n K K K K Î¶2 = KÎ´2 +n â1tr(I )Ï2 n K K â¤o (cid:161)Î´2(cid:162)+ K Ï2 K n where the inequality is Assumption 20.1.2. This implies (20.45) is o (cid:161)Î´2(cid:162)+O (K/n). Combined with p K p (20.44)wefindISE(K)=O (cid:161)Î´2 +K/n (cid:162) asclaimed. â  p K ProofofTheorem20.8.Using(20.12)andlinearity Î¸=a(m)=a (cid:161) Z (x) (cid:48)Î² (cid:162)+a(r )=a (cid:48) Î² +a(r ). K K K K K K Thus (cid:114) (cid:114) V n (cid:161)Î¸ (cid:98)K âÎ¸+a(r K ) (cid:162)= V n a K (cid:48) (cid:161)Î² (cid:98)K âÎ² K (cid:162) K K (cid:115) = nV 1 a K (cid:48) Q(cid:98) â K 1 X (cid:48) K e K K = (cid:112) 1 a (cid:48) Q â1X (cid:48) e (20.47) K K K nV K +(cid:112) 1 a (cid:48) (cid:179) Q(cid:98) â1âQ â1 (cid:180) X (cid:48) e (20.48) K K K K nV K +(cid:112) 1 a K (cid:48) Q(cid:98) â K 1 X (cid:48) K r K (20.49) nV K wherewehaveusede =e+r .Wetakethetermsin(20.47)-(20.49)separately. Weshowthat(20.47)is K K asymptoticallynormaland(20.48)-(20.49)areasymptoticallynegligible. First,take(20.47).Wecanwrite (cid:112) 1 a (cid:48) Q â1X (cid:48) e= (cid:112) 1 (cid:88) n (cid:112) 1 a (cid:48) Q â1X e . (20.50) nV K K K K n i=1 V K K K Ki i (cid:112) Observethata (cid:48) Q â1X e / V areindependentacrossi,meanzero,andhavevariance1.Wewillapply K K Ki i K Theorem6.4,forwhichitissufficienttoverifyLindebergâscondition:Forall(cid:178)>0 (cid:34)(cid:161) a (cid:48) Q â1X e (cid:162)2 (cid:40)(cid:161) a (cid:48) Q â1X e (cid:162)2 (cid:41)(cid:35) (cid:69) K K K 1 K K K â¥n(cid:178) â0. (20.51) V V K K PickÎ·>0. SetB sufficientlylargesothat(cid:69)(cid:163) e21(cid:169) e2>B (cid:170)|X (cid:164)â¤Ï2Î·whichisfeasiblebyAssumption 20.2.1.PicknsufficientlylargesothatÎ¶2/nâ¤(cid:178)Ï2/B,whichisfeasibleunderAssumption20.1.2. K",
    "page": 752,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 733 ByAssumption20.2.2 (cid:104) (cid:105) V =(cid:69) (cid:161) a (cid:48) Q â1X (cid:162)2 e2 K K K K (cid:104) (cid:105) =(cid:69) (cid:161) a (cid:48) Q â1X (cid:162)2Ï(X2) K K K (cid:104) (cid:105) â¥(cid:69) (cid:161) a (cid:48) Q â1X (cid:162)2Ï2 K K K =a (cid:48) Q â1(cid:69)(cid:163) X X (cid:48) (cid:164) Q â1a Ï2 K K K K K K =a (cid:48) Q â1a Ï2. (20.52) K K K ThenbytheSchwarzInequality,(20.17),(20.52),andÎ¶2/nâ¤(cid:178)Ï2/B K (cid:161) a (cid:48) Q â1X (cid:162)2 (cid:161) a (cid:48) Q â1a (cid:162)(cid:161) X (cid:48) Q â1X (cid:162) Î¶2 (cid:178) K K K â¤ K K K K K K â¤ K â¤ n. V V Ï2 B K K Thentheleft-sideof(20.51)issmallerthan (cid:34)(cid:161) a (cid:48) Q â1X (cid:162)2 (cid:35) (cid:34)(cid:161) a (cid:48) Q â1X (cid:162)2 (cid:35) (cid:69) K K K e21(cid:169) e2â¥B (cid:170) =(cid:69) K K K (cid:69)(cid:163) e21(cid:169) e2â¥B (cid:170)|X (cid:164) V V K K (cid:34)(cid:161) a (cid:48) Q â1X (cid:162)2(cid:35) â¤(cid:69) K K K Ï2Î· V K a (cid:48) Q â1a â¤ K K KÏ2Î·â¤Î· V K thefinalinequalityby(20.52).SinceÎ·isarbitrarythisverifies(20.51)andweconclude (cid:112) 1 a (cid:48) Q â1X (cid:48) eââN(0,1). (20.53) K K K nV K d Second, take (20.48). Assumption 20.2 implies (cid:69)(cid:163) e2|X (cid:164) â¤ Ï2 < â. Since (cid:69)[e|X] = 0, applying (cid:69)(cid:163) e2|X (cid:164)â¤Ï2,theSchwarzandNormInequalities,(20.52),andTheorems20.4and20.5, (cid:69) (cid:183)(cid:181) (cid:112) 1 a (cid:48) (cid:179) Q(cid:98) â1âQ â1 (cid:180) X (cid:48) e (cid:182)2(cid:175) (cid:175) (cid:175)X (cid:184) nV K K K K (cid:175) K = nV 1 a K (cid:48) (cid:179) Q(cid:98) â K 1âQ â K 1 (cid:180) X (cid:48) K (cid:69)(cid:163) ee (cid:48)|X (cid:164) X K (cid:179) Q(cid:98) â K 1âQ â K 1 (cid:180) a K K â¤ V Ï2 a K (cid:48) (cid:179) Q(cid:98) â K 1âQ â K 1 (cid:180) Q(cid:98)K (cid:179) Q(cid:98) â K 1âQ â K 1 (cid:180) a K K â¤ Ï2a K (cid:48) V Q â K 1a K (cid:176) (cid:176) (cid:176) (cid:179) Q(cid:98) â K 1âQ â K 1 (cid:180) Q(cid:98)K (cid:179) Q(cid:98) â K 1âQ â K 1 (cid:180)(cid:176) (cid:176) (cid:176) K = Ï2a K (cid:48) V Q â K 1a K (cid:176) (cid:176) (cid:176) (cid:161) I K âQ(cid:101)K (cid:162) (cid:179) Q(cid:101) â K 1âI K (cid:180)(cid:176) (cid:176) (cid:176) K â¤ Ï Ï 2 2 (cid:176) (cid:176)I K âQ(cid:101)K (cid:176) (cid:176) (cid:176) (cid:176) (cid:176) Q(cid:101) â K 1âI K (cid:176) (cid:176) (cid:176) Ï2 â¤ o (1). Ï2 p Thisestablishesthat(20.48)iso (1). p",
    "page": 753,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 734 Third,take(20.49).BytheCauchy-Schwarzinequality,theQuadraticInequality,(20.52),and(20.21), (cid:181) (cid:112) n 1 v a K (cid:48) Q(cid:98) â K 1 X (cid:48) K r K (cid:182)2 â¤ a K (cid:48) n Q v â K 1a K r (cid:48) K X K Q(cid:98) â K 1 Q K Q(cid:98) â K 1 X (cid:48) K r K K K â¤ Ï 1 2 (cid:179) Î» max Q(cid:101) â K 1 (cid:180)2 n 1 r (cid:48) K X K Q â K 1X (cid:48) K r K â¤O (1) 1 r (cid:48) X Q â1X (cid:48) r . (20.54) p n K K K K K Observethatsincetheobservationsareindependent,(cid:69)[X r ]=0,X (cid:48) Q â1X â¤Î¶2,and(cid:69)(cid:163) r2(cid:164)=Î´2, K K Ki K Ki K K K (cid:34) (cid:35) (cid:69) (cid:183) 1 r (cid:48) X Q â1X (cid:48) r (cid:184) =(cid:69) 1 (cid:88) n r X (cid:48) Q â1 (cid:88) n X r n K K K K K n Ki Ki K Kj Kj i=1 ij=1 =(cid:69)(cid:163) X (cid:48) Q â1X r2(cid:164) K K K K â¤Î¶2(cid:69)(cid:163) r2(cid:164)=Î¶2Î´2 =o(1) K K K K 1 underAssumption20.2.3.Thus r (cid:48) X Q â1X (cid:48) r =o (1),(20.54)iso (1),and(20.49)iso (1). n K K K K K p p p Together,wehaveshownthat (cid:114) n (cid:161)Î¸ (cid:98)K âÎ¸ K +a(r K ) (cid:162)ââN(0,1) V K d asclaimed. â  ProofofTheorem20.10.Itissufficienttoshowthat (cid:112) n r (x)=o(1). (20.55) V1/2(x) K K NoticethatbyAssumption20.2.2 V (x)=X (x) (cid:48) Q â1â¦ Q â1X (x) K K K K K K (cid:104) (cid:105) =(cid:69) (cid:161) X (x) (cid:48) Q â1X (cid:162)2 e2 K K K (cid:104) (cid:105) =(cid:69) (cid:161) X (x) (cid:48) Q â1X (cid:162)2Ï2(X) K K K (cid:104) (cid:105) â¥(cid:69) (cid:161) X (x) (cid:48) Q â1X (cid:162)2 Ï2 K K K =X (x) (cid:48) Q â1(cid:69)(cid:163) X X (cid:48) (cid:164) Q â1X (x)Ï2 K K K K K K =X (x) (cid:48) Q â1X (x)Ï2 K K K =Î¶ (x)2Ï2. (20.56) K UsingthedefinitionsforÎ²â ,r â (x),andÎ´â fromSection20.8,notethat K K K r (x)=m(x)âX (cid:48) (x)Î² =r â (x)+X (cid:48) (x) (cid:161)Î²â âÎ² (cid:162) . K K K K K K K BytheTriangleInequality,thedefinition(20.10),theSchwarzInequality,anddefinition(20.15) |r K (x)|â¤(cid:175) (cid:175)r K â (x) (cid:175) (cid:175) +(cid:175) (cid:175)X K (cid:48) (x) (cid:161)Î²â K âÎ² K (cid:162)(cid:175) (cid:175) â¤Î´â K +(cid:175) (cid:175)X K (cid:48) (x)Q â K 1X K (cid:48) (x) (cid:175) (cid:175) 1/2 (cid:175) (cid:175) (cid:175) (cid:161)Î²â K âÎ² K (cid:162)(cid:48) Q K (cid:161)Î²â K âÎ² K (cid:162) (cid:175) (cid:175) (cid:175) 1/2 =Î´â +Î¶ (x) (cid:175) (cid:175)(cid:161)Î²â âÎ² (cid:162)(cid:48) Q (cid:161)Î²â âÎ² (cid:162) (cid:175) (cid:175) 1/2 . K K (cid:175) K K K K K (cid:175)",
    "page": 754,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 735 Thecoefficientssatisfytherelationship Î² =(cid:69)(cid:163) X X (cid:48) (cid:164)â1(cid:69)[X m(X)]=Î²â +(cid:69)(cid:163) X X (cid:48) (cid:164)â1(cid:69)(cid:163) X r â(cid:164) . K K K K K K K K K Thus (cid:161)Î²â âÎ² (cid:162)(cid:48) Q (cid:161)Î²â âÎ² (cid:162)=(cid:69)(cid:163) r â X (cid:48) (cid:164)(cid:69)(cid:163) X X (cid:48) (cid:164)â1(cid:69)(cid:163) X r â(cid:164)â¤(cid:69)(cid:163) r2â(cid:164)â¤Î´â2. K K K K K K K K K K K K K Thefirstinequalityisbecause(cid:69)(cid:163) r â X (cid:48) (cid:164)(cid:69)(cid:163) X X (cid:48) (cid:164)â1(cid:69)(cid:163) X r â(cid:164) isaprojection. Thesecondinequalityfol- K K K K K K lowsfromthedefinition(20.10).Wededucethat |r (x)|â¤(1+Î¶ (x))Î´â â¤2Î¶ (x)Î´â . (20.57) K K K K K Equations(20.56),(20.57),andnÎ´â2=o(1)togetherimplythat K n r2(x)â¤ 4 nÎ´â2=o(1) V (x) K Ï2 K K whichis(20.55),asrequired. â  _____________________________________________________________________________________________ 20.32 Exercises Exercise20.1 Taketheestimatedmodel Y =â1+2X+5(Xâ1) 1 {X â¥1}â3(Xâ2) 1 {X â¥2}+e. WhatistheestimatedmarginaleffectofX onY forX =3? Exercise20.2 Takethelinearsplinewiththreeknots m (x)=Î² +Î² x+Î² (xâÏ ) 1 {xâ¥Ï }+Î² (xâÏ ) 1 {xâ¥Ï }+Î² (xâÏ ) 1 {xâ¥Ï }. K 0 1 2 1 1 3 2 2 4 3 3 FindtheinequalityrestrictionsonthecoefficientsÎ² sothatm (x)isnon-decreasing. j K Exercise20.3 Takethelinearsplinefromthepreviousquestion. Findtheinequalityrestrictionsonthe coefficientsÎ² sothatm (x)isconcave. j K Exercise20.4 Takethequadraticsplinewiththreeknots m (x)=Î² +Î² x+Î² x3+Î² (xâÏ )21 {xâ¥Ï }+Î² (xâÏ )21 {xâ¥Ï }+Î² (xâÏ )21 {xâ¥Ï }. K 0 1 2 3 1 1 4 2 2 5 3 3 FindtheinequalityrestrictionsonthecoefficientsÎ² sothatm (x)isconcave. j K Exercise20.5 ConsidersplineestimationwithoneknotÏ. ExplainwhytheknotÏmustbewithinthe samplesupportofX.[Explainwhathappensifyouestimatetheregressionwiththeknotplacedoutside thesupportofX]. Exercise20.6 Youestimatethepolynomialregressionmodel: m (cid:98)K (x)=Î² (cid:98)0 +Î² (cid:98)1 x+Î² (cid:98)2 x2+Â·Â·Â·+Î² (cid:98)p xp. (cid:48) Youareinterestedintheregressionderivativem (x)atx.",
    "page": 755,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 736 (cid:48) (cid:48) (a) Writeouttheestimatorm (x)of m (x). (cid:98)K (cid:48) (b) Ism (x)isalinearfunctionofthecoefficientestimates? (cid:98)K (cid:48) (c) UseTheorem20.8toobtaintheasymptoticdistributionofm (x). (cid:98)K (cid:48) (d) Showhowtoconstructstandarderrorsandconfidenceintervalsform (x). (cid:98)K Exercise20.7 DoesrescalingY orX (multiplyingbyaconstant)affecttheCV(K)function?TheK which minimizesit? Exercise20.8 TaketheNPIVapproximatingequation(20.35)anderrore . K (a) Doesitsatisfy(cid:69)[e |Z]=0? K (b) IfL=K canyoudefineÎ² sothat(cid:69)[Z e ]=0? K K K (c) IfL>K does(cid:69)[Z e ]=0? K K Exercise20.9 Takethecps09mardataset(fullsample). (a) Estimatea6thorderpolynomialregressionoflog(wage)onexperience.Toreducetheill-conditioned problemfirstrescaleexperiencetolieintheinterval[0,1]beforeestimatingtheregression. (b) Plottheestimatedregressionfunctionalongwith95%pointwiseconfidenceintervals. (c) Interpretthefindings.Howdoyouinterprettheestimatedfunctionforexperiencelevelsabove65? Exercise20.10 Continuing the previous exercise, compute the cross-validation function (or alterna- tivelytheAIC)forpolynomialorders1through8. (a) Whichorderminimizesthefunction? (b) Plottheestimatedregressionfunctionalongwith95%pointwiseconfidenceintervals. Exercise20.11 Takethecps09mardataset(fullsample). (a) Estimatea6thorderpolynomialregressionoflog(wage)oneducation.Toreducetheill-conditioned problemfirstrescaleeducationtolieintheinterval[0,1]. (b) Plottheestimatedregressionfunctionalongwith95%pointwiseconfidenceintervals. Exercise20.12 Continuing the previous exercise, compute the cross-validation function (or alterna- tivelytheAIC)forpolynomialorders1through8. (a) Whichorderminimizesthefunction? (b) Plottheestimatedregressionfunctionalongwith95%pointwiseconfidenceintervals. Exercise20.13 Takethecps09mardataset(fullsample). (a) Estimate quadratic spline regressions of log(wage) on experience. Estimate four models: (1) no knots(aquadratic);(2)oneknotat20years;(3)twoknotsat20and40;(4)fourknotsat10,20,30, &40.Plotthefourestimates.Intrepretyourfindings.",
    "page": 756,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 737 (b) Comparethefoursplinesmodelsusingeithercross-validationorAIC.Whichisthepreferredspec- ification? (c) For your selected specification plot the estimated regression function along with 95% pointwise confidenceintervals.Intrepretyourfindings. (d) If you also estimated a polynomial specification do you prefer the polynomial or the quadratic splineestimates? Exercise20.14 Takethecps09mardataset(fullsample). (a) Estimate quadratic spline regressions of log(wage) on education. Estimate four models: (1) no knots(aquadratic);(2)oneknotat10years;(3)threeknotsat5,10,and15;(4)fourknotsat4,8, 12,&16.Plotthefourestimates.Intrepretyourfindings. (b) Comparethefoursplinesmodelsusingeithercross-validationorAIC.Whichisthepreferredspec- ification? (c) For your selected specification plot the estimated regression function along with 95% pointwise confidenceintervals.Intrepretyourfindings. (d) If you also estimated a polynomial specification do you prefer the polynomial or the quadratic splineestimates? Exercise20.15 TheRR2010datasetisfromReinhartandRogoff(2010). Itcontainsobservationsonan- nualU.S.GDPgrowthrates,inflationrates,andthedebt/gdpratioforthelongtimespan1791-2009.The papermadethestrongclaimthatGDPgrowthslowsasdebt/gdpincreases, andinparticularthatthis relationshipisnonlinearwithdebtnegativelyaffectinggrowthfordebtratiosexceeding90%. Theirfull datasetincludes44countries,ourextractonlyincludestheUnitedStates.LetY denoteGDPgrowthand t letD denotedebt/gdp.Wewillestimatethepartiallylinearspecification t Y t =Î±Y tâ1 +m(D tâ1 )+e t usingalinearsplineform(D). (a) Estimate(1)linearmodel; (2)linearsplinewithoneknotatD tâ1 =60; (3)linearsplinewithtwo knotsat40and80.Plotthethreeestimates. (b) Forthemodelwithoneknotplotwith95%confidenceintervals. (c) Compare the three splines models using either cross-validation or AIC. Which is the preferred specification? (d) Interpretthefindings. Exercise20.16 TaketheDDK2011dataset(fullsample).Useaquadraticsplinetoestimatetheregression oftestscoreonpercentile. (a) Estimate five models: (1) no knots (a quadratic); (2) one knot at 50; (3) two knots at 33 and 66; (4)threeknotsat25, 50&75; (5)knotsat20, 40, 60, &80. Plotthefiveestimates. Intrepretyour findings.",
    "page": 757,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER20. SERIESREGRESSION 738 (b) Selectamodel.Considerusingleave-cluster-oneCV. (c) For your selected specification plot the estimated regression function along with 95% pointwise confidenceintervals.[Usecluster-robuststandarderrors.]Intrepretyourfindings. Exercise20.17 TheCHJ2004datasetisfromCox,HansenandJimenez(2004). AsdescribedinSection 20.6 it contains a sample of 8684 urban Phillipino households. This paper studied the crowding-out impactofafamilyâsincomeonnon-governmentaltransfers. EstimateananalogofFigure20.2(b)using polynomial regression. Regress transfers on a high-order polynomial in income, and possibly a set of regressioncontrols. Ideally,selectthepolynomialorderbycross-validation. Youwillneedtorescalethe variableincomebeforetakingpolynomialpowers.Plottheestimatedfunctionalongwith95%pointwise confidenceintervals.CommentonthesimilaritiesanddifferenceswithFigure20.2(b).Fortheregression controlsconsiderthefollowingoptions:(a)Includenoadditionalcontrols;(b)Followtheoriginalpaper andFigure20.2(b)byincludingthevariables12-26listedinthedatadescriptionfile;(c)Makeadifferent selection,possiblybasedoncross-validation. Exercise20.18 The AL1999 dataset is from Angrist and Lavy (1999). It contains 4067 observations on classroomtestscoresandexplanatoryvariablesincludingthosedescribedinSection20.30. InSection 20.30 we report a nonparametric instrumental variables regression of reading test scores (avgverb) on classize, disadvantaged, enrollment, and a dummy for grade=4, using the Angrist-Levy variable (20.42) asaninstrument. Repeattheanalysisbutinsteadofreadingtestscoresusemathtestscores(avgmath) asthedependentvariable. Commentonthesimilaritiesanddifferenceswiththeresultsforreadingtest scores.",
    "page": 758,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Chapter 21 Regression Discontinuity 21.1 Introduction Oneofthecoregoalsinappliedeconometricsisestimationoftreatmenteffects. Amajorbarrieris that in observational data treatment is rarely exogenous. Techniques discussed so far in this textbook to deal with potential endogeneity include instrumental variables, fixed effects, and difference in dif- ferences. Anotherimportantmethodarisesinthecontextoftheregressiondiscontinuitydesign. This is a rather special situation (not at the control of the econometrician) where treatment is determined byathresholdcrossingrule. Forexample: (1)Dopoliticalincumbantshaveanadvantageinelections? Anincumbantisthewinnerofthepreviouselection,whichmeanstheirvoteshareexceededathresh- old. (2)Whatistheeffectofcollegeattendence? Collegestudentsareadmittedbasedonanadmission exam,whichmeanstheirexamscoreexceededaspecificthreshold. Inthesecontextsthetreatment(in- cumbancy,collegeattendence)canbeviewedasrandomlyassignedforindividualsnearthecut-off. (In theexamples,forcandidateswhohadvotesharesnearthewinningthresholdandforstudentswhohad admissionexamscoresnearthecut-offthreshold.) ThissettingiscalledtheRegressionDiscontinuity Design(RDD).Whenitappliestherearesimpletechniquesforestimationofthecausaleffectoftreat- ment. ThefirstuseofregressiondiscontinuityisattributedtoThistlethwaiteandCampbell(1960). Itwas popularizedineconomicsbyBlack(1999),LudwigandMiller(2007),andLee(2008). Importantreviews includeImbensandLeimieux(2008),LeeandLeimieux(2010),andCattaneo,Idrobo,andTitiunik(2020, 2021). Thecoremodelissharpregressiondiscontinuitywheretreatmentisadiscontinuousdeterminis- tic rule of an observable. Most applications, however, concern fuzzy regression discontinuity where theprobabilityoftreatmentisdiscontinuousinanobservable. Westartbyreviewingsharpregression discontinuityandthencoverfuzzyregressiondiscontinuity. 21.2 SharpRegressionDiscontinuity Takethepotentialoutcomesframework. AnindividualisuntreatedifD =0andistreatedifD =1. TheindividualhasoutcomeY ifuntreatedandY iftreated. Thetreatmenteffectforanindividualis 0 1 Î¸=Y âY ,whichisrandom. Anobservablecovariateis X. TheconditionalAverageTreatmentEffect 1 0 (ATE)forthesubpopulationwithX =xisÎ¸(x)=(cid:69)[Î¸|X =x]. Thesharpregressiondiscontinuitydesignoccurswhentreatmentisdeterminedbyathresholdfunc- tionof X,e.g. D =1 {X â¥c}. Inmostapplicationsthethresholdc isdeterminedbypolicyorrule. The covariateX whichdeterminestreatmentistypicallycalledtherunningvariable.Thethresholdcisoften 739",
    "page": 759,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER21. REGRESSIONDISCONTINUITY 740 calledtheâcut-offâ. Itmaybehelpfultodiscussaspecificexample.LudwigandMiller(2007)usedasharpregressiondis- continuitydesigntoevaluateaU.S.federalanti-povertyprogramcalledHeadStart. HeadStartwases- tablishedin1965toprovidepreschool,health,andothersocialservicestopoorchildrenagethreetofive andtheirfamilies. HeadStartfundingwasawardedtolocalmunicipalitiesthroughacompetitivegrant application. Due to a worry that poor regions may not apply at the same rate as well-funded regions, duringthespringof1965thefederalgovernmentprovidedgrant-writingassistancetothe300poorest countiesintheUnitedStates. The300countieswereselectedbasedonthepovertyrateasmeasuredby the1960U.S.census. As Ludwig and Miller document, the result was a surge in applications from the assisted counties witharesultingsurgeinprogramfunding. 80%ofthe300treatedcountiesreceivedHeadStartsupport whileonly43%oftheremainingcountiesreceivedsupport. Thusitseemsreasonabletoconcludethat thesecountiesreceivedasubstantialexogenousincreaseinfunding. Ludwig and Miller were interested to see if this increase in Head Start funding led to measurable changes in outcomes. Their paper examined both mortality and education. We will focus exclusively onmortality. Specifically,theywereinterestedintheimpactonmortalityforchildrenintheagerange 5-9,fordeathstheycodedasâHeadStartRelatedâ(forexample,tuberculosis)meaningthatagoalofthe HeadStartprogramwastoreducetheseevents.Theywerealsointerestedinthelong-termeffectsofthis interventionsofocusedonmortalityratesinthe1973-1983periodwhichiseighttoeighteenyearsafter the grant-writing intervention. A subset of their data (assembled by Cattaneo, Titiunik, and Vazquez- Bare(2017))ispostedonthetextbookwebsiteasLM2007. Tosummarize,thequestionaddressedbyLudwigandMillerwaswhethergrant-writingassistancein 1965tothe300U.S.countiesselectedonapovertyindexhadameasurableeffectonchildhoodmortality eighttoeighteenyearslaterinthesamecounties,relativetocountieswhichdidnotreceivethegrant- writingassistance. InthisapplicationtheunitofmeasurementisaU.S.county. TheoutcomevariableY isthecounty mortalityratein1973-1983. TherunningvariableX isthecountypovertyrate(percentageofthepopu- lationbelowthepovertyline)in1960. Thecut-offc is59.1984. (Thelaterissimplyduetothefactthat therewere300countieswithpovertyratesequalorabovethiscut-off.) 21.3 Identification In this section we present the core identification theorem for the regression discontinuity model. RecallthatÎ¸istherandomindividualtreatmenteffectandÎ¸(x)=(cid:69)[Î¸|X =x]istheconditionalATE.Set Î¸=Î¸(c),theconditionalATEforthesubpopulationatthecut-off. Thisisthesubpopulationaffectedat themarginbythedecisiontosetthecut-offatc.ThecoreidentificationtheoremstatesthatÎ¸isidentified bytheregressiondiscontinuitydesignundermildassumptions. Let m(x)=(cid:69)[Y |X =x], m (x)=(cid:69)[Y |X =x], and m (x)=(cid:69)[Y |X =x]. Note that Î¸(x)=m (x)â 0 0 1 1 1 m (x).Setm(x+)=limm(z)andm(xâ)=limm(z)",
    "page": 760,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". RecallthatÎ¸istherandomindividualtreatmenteffectandÎ¸(x)=(cid:69)[Î¸|X =x]istheconditionalATE.Set Î¸=Î¸(c),theconditionalATEforthesubpopulationatthecut-off. Thisisthesubpopulationaffectedat themarginbythedecisiontosetthecut-offatc.ThecoreidentificationtheoremstatesthatÎ¸isidentified bytheregressiondiscontinuitydesignundermildassumptions. Let m(x)=(cid:69)[Y |X =x], m (x)=(cid:69)[Y |X =x], and m (x)=(cid:69)[Y |X =x]. Note that Î¸(x)=m (x)â 0 0 1 1 1 m (x).Setm(x+)=limm(z)andm(xâ)=limm(z). 0 zâx zâx Thefollowingisthecoreidentificationtheoremfortheregressiondiscontinuitydesign. Itisdueto Hahn,Todd,andVanderKlaauw(2001). Theorem21.1 Assume that treatment is assigned as D =1 {X â¥c}. Suppose thatm (x)andm (x)arecontinuousatx=c.ThenÎ¸=m(c+)âm(câ). 0 1",
    "page": 760,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER21. REGRESSIONDISCONTINUITY 741 The conditions for Theorem 21.1 are minimal. The continuity of m (x) and m (x) means that the 0 1 conditionalexpectationoftheuntreatedandtreatedoutcomearecontinuouslyaffectedbytherunning variable. Take the Head Start example. m (x) is the average mortality rate given the poverty rate for 0 counties which received no grant-writing assistance. m (x) is the average mortality rate for counties 1 whichreceivedgrant-writingassistance.Thereisnoreasontoexpectadiscontinuityineitherfunction. TheintuitionforthetheoremcanbeseeninFigure21.1(a).Thetwocontinuousfunctionsplottedare theconditionalmeansm (x)andm (x).Theverticaldistancebetweenthesefunctionsistheconditional 0 1 ATEfunctionÎ¸(x). SincethetreatmentruleassignsallcountieswithX â¥c totreatmentandallcounties withX <ctonon-treatmenttheconditionalmeanoftheobservedoutcomem(x)isthesolidline,which equalsm (x)forx<c andm (x)forxâ¥0. Thediscontinuityinm(x)atx=c equalstheRDDtreatment 0 1 effectÎ¸. TheplotinFigure21.1(a)wasdesignedtomimicwhatwemightexpectintheHeadStartapplication. Wehaveplottedbothm (x)andm (x)asincreasingfunctionsof x, meaningthatthemortalityrateis 0 1 increasinginthepovertyrate. Wealsohaveplottedthefunctionssothatm (x)liesbelowm (x)aswe 1 0 expectthatgrant-writingassistanceshouldreducemortality. Weknowfromregressiontheorythattheconditionalmeanm(x)isgenericallyidentified. Thussois theRDDtreatmenteffectÎ¸=m(c+)âm(câ).Thisisthekeytake-awayfromtheidentificationtheorem. TheregressiondiscontinuitydesignidentifiestheconditionalATEatthetreatmentcut-off. IntheHead StartexamplethisistheATEforacountywithapovertyrateof59%. UseofÎ¸toinfertheATEforother countiesisextrapolation. AsdisplayedinFigure21.1(a)allthatisidentifiedisthesolidline,thedashed linesarenotidentified. ThusalimitationoftheRDDapproachisthatitestimatesanarrowly-defined treatmenteffectthoughbroadereffectsaretypicallyofinterest. IdentificationoftheRDDtreatmenteffectisintertwinedwithnonparametrictreatmentofthefunc- tions m (x) and m (x). If parametric (e.g. linear) forms are imposed, then the best-fitting approxi- 0 1 mations for x <c and x â¥c will generically have a discontinuity even if the true conditional mean is continuous. Thusanonparametrictreatmentisessentialtoprecludefalselylabelingnonlinearityasa discontinuity. AformalproofofTheorem21.1issimple. WecanwritetheobservedoutcomeasY =Y 1 {X <c}+ 0 Y 1 {X â¥c}.TakingexpectationsconditionalonX =xwefind 1 m(x)=m (x) 1 {x<c}+m (x) 1 {xâ¥c}. (21.1) 0 1 Since m (x) and m (x) are continuous at x =c, we deduce m(c+)=m (c) and m(câ)=m (c). Thus 0 1 1 0 m(c+)âm(câ)=m (c)âm (c)=Î¸(c),asclaimed. 1 0 21.4 Estimation OurgoalisestimationoftheconditionalATEÎ¸givenobservations{Y ,X }andknowncut-offc. The i i conditionalATEcanbecalculatedfromtheconditionalmeanm(x).Estimationoftheconditionalmean nonparametricallyallowingforadiscontinuityisthesameasseparatelyestimatingtheconditionalmean for the untreated observations X <c and the treated observations X â¥c",
    "page": 761,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Thus 0 1 1 0 m(c+)âm(câ)=m (c)âm (c)=Î¸(c),asclaimed. 1 0 21.4 Estimation OurgoalisestimationoftheconditionalATEÎ¸givenobservations{Y ,X }andknowncut-offc. The i i conditionalATEcanbecalculatedfromtheconditionalmeanm(x).Estimationoftheconditionalmean nonparametricallyallowingforadiscontinuityisthesameasseparatelyestimatingtheconditionalmean for the untreated observations X <c and the treated observations X â¥c. The estimator for Î¸ is the i i differencebetweentheadjoiningestimatedendpoints. Theprevioustwochaptershavestudiednonparametrickernelandseriesregression.Oneofthefind- ingsisthatforboundaryestimationthepreferredmethodislocallinear(LL)regression(Section19.4). In contrast, the Nadaraya-Watson estimator is biased at a boundary point (see Section 19.10), and se- riesestimatorshavehighvarianceattheboundary(seeSection20.14andGelmanandImbens(2019)).",
    "page": 761,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER21. REGRESSIONDISCONTINUITY 742 ( ) ( ) m0x q m1x X<c c X>c 20 30 40 50 60 70 80 (a)SharpRegressionDiscontinuity 5 4 3 2 1 Poverty Rate etaR ytilatroM Untreated Treated (b)EffectofHeadStartonChildhoodMortality Figure21.1:SharpRegressionDiscontinuityDesign Consequently,locallinearestimationispreferredandisthemostwidelyusedtechnique1forregression discontinuitydesigns. Todescribetheestimatorset (cid:181) (cid:182) 1 Z (x)= . i X âx i LetK(u)beakernelfunctionandhabandwidth.TheLLcoefficientestimatorforx<c is Î² (cid:98)0 (x)= (cid:195) (cid:88) n K (cid:181) X i âx (cid:182) Z i (x)Z i (x) (cid:48)1 {X i <c} (cid:33)â1(cid:195) (cid:88) n K (cid:181) X i âx (cid:182) Z i (x)Y i 1 {X i <c} (cid:33) h h i=1 i=1 andforxâ¥c is Î² (cid:98)1 (x)= (cid:195) (cid:88) n K (cid:181) X i âx (cid:182) Z i (x)Z i (x) (cid:48)1 {X i â¥c} (cid:33)â1(cid:195) (cid:88) n K (cid:181) X i âx (cid:182) Z i (x)Y i 1 {X i â¥c} (cid:33) . h h i=1 i=1 Theestimatoroftheconditionalmeanisthefirstelementofthecoefficientvectors m (cid:98) (x)=(cid:163)Î² (cid:98)0 (x) (cid:164) 1 1 {x<c}+(cid:163)Î² (cid:98)1 (x) (cid:164) 1 1 {xâ¥c}. TheestimatorofÎ¸isthedifferenceatx=c Î¸ (cid:98) =(cid:163)Î² (cid:98)1 (c) (cid:164) 1 â(cid:163)Î² (cid:98)0 (c) (cid:164) 1 =m (cid:98) (c+)âm (cid:98) (câ). (21.2) For efficient estimation at boundary points the Triangular kernel is recommended. However, the Epanechnikov and Gaussian have similar efficiencies (see Section 19.10). Some authors have made a case for the Rectangular kernel as this permits standard regression software to be used. There is an efficiencyloss(3%inrootAMSE)inreturnforthisconvenience. 1Someauthorsusepolynomialsinadditiontolocallinearestimationasanappealtoârobustnessâ.Thisshouldbediscour- agedasarguedinGelmanandImbens(2019).",
    "page": 762,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER21. REGRESSIONDISCONTINUITY 743 Theconditionalmeanestimatem(x)shouldbeplottedtogiveavisualinspectionoftheregression (cid:98) functionanddiscontinuity. Manyauthorsplottheconditionalmeanonlyoverthesupportnear x =c toemphasizethelocalnatureoftheestimation. Confidencebandsshouldbecalculatedandplottedas described in Section 19.17. These are calculated separately for the non-treatment and treatment sub- samplesbutotherwiseareidenticaltothosedescribedinSection19.17. Toillustrate,Figure21.1(b)displaysourestimatesoftheLudwig-Miller(2007)HeadStartRDDmodel forchildhoodmortalityduetoHS-relatedcauses. WeuseanormalizedTriangularkernelandaband- widthofh=8. ThisbandwidthchoiceisdescribedinSection21.6. Thex-axisisthe1960povertyrate. Thecut-offis59.2%.Countiesbelowthecut-offdidnotreceivegrant-writingassistance,countiesabove thecut-offreceivedassistance. Themortalityrateisonthey-axis(deathsper100,000). Theestimates showthatthemortalityrateisincreasinginthepovertyrate(nearlylinear)withasubstantialdownward discontinuityatthe59%cut-off.Thediscontinuityisabout1.5deathsper100,000.Theconfidencebands indicatethattheestimatedconditionalmeanshaveafairamountofuncertaintyattheboundaries. The conditionalmeaninthetreatedsampleappearsnonlinearandtheconfidencebandsareverywide. ThereisacustomintheappliedeconomicsliteraturetodisplayFigure21.1(b)somewhatdifferently. Rather than displaying confidence intervals along with the local linear estimates they display binned means.Thebinnedmeansaredisplayedbysquaresortrianglesandaremeanttoindicatearawestimate of the nonparametric shape of the conditional mean. This custom is a poor choice, a bad habit, and shouldbeavoided. Binnedmeansaresimplyaninaccuratenonparametricestimator. Binnedmeansis thesameastheNadaraya-WatsonestimatorusingaRectangularkernelandonlyevalutatedatagridof pointsratherthancontinuously. LocallinearestimationissuperiortotheNadaraya-Watson,anykernel issuperiortotheRectangular,andthereisnoreasontoevaluateonlyonanarbitrarygrid.Plotsofbinned meansgivesthefalsevisualimpressionofascatterplotofrawdata. Theyarenotrawdata,however,so this visual impression is misleading. These plots are not âbest practiceâ; rather, they are a bad habit. Thebestpracticeistoplotthebestpossiblenonparametricestimatorandtoplotconfidenceintervalsto conveyuncertainty. 21.5 Inference AsdescribedinTheorems19.6and19.9,theLLestimatorm(x)isasymptoticallynormalunderstan- (cid:98) dardregularityconditions.ThisextendstotheRDDestimatorÎ¸ (cid:98).Ithasasymptoticbias h2Ï2 bias (cid:163)Î¸ (cid:98) (cid:164)= Kâ (cid:161) m (cid:48)(cid:48) (c+)âm (cid:48)(cid:48) (câ) (cid:162) 2 andvariance var (cid:163)Î¸ (cid:98) (cid:164)= R K â (cid:181)Ï2(c+) + Ï2(câ) (cid:182) . nh f(c+) f(câ) Theasymptoticvariancecanbeestimatedbythesumoftheasymptoticvarianceestimatorsofthe twoboundaryregressionestimatorsasdescribedinSection19.16.Lete betheleave-one-outprediction (cid:101)i errorandset (cid:181) (cid:182) 1 Z = i X âc i (cid:181) X âc (cid:182) K =K i . i h",
    "page": 763,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER21. REGRESSIONDISCONTINUITY 744 Thecovariancematrixestimatorsare (cid:195) (cid:33)â1(cid:195) (cid:33)(cid:195) (cid:33)â1 n n n V(cid:98)0 = (cid:88) K i Z i Z i (cid:48)1 {X i <c} (cid:88) K i 2Z i Z i (cid:48) e (cid:101)i 21 {X i <c} (cid:88) K i Z i Z i (cid:48)1 {X i <c} i=1 i=1 i=1 (cid:195) (cid:33)â1(cid:195) (cid:33)(cid:195) (cid:33)â1 n n n V(cid:98)1 = (cid:88) K i Z i Z i (cid:48)1 {X i â¥c} (cid:88) K i 2Z i Z i (cid:48) e (cid:101)i 21 {X i â¥c} (cid:88) K i Z i Z i (cid:48)1 {X i â¥c} . i=1 i=1 i=1 The asymptotic variance estimator for Î¸ (cid:98) is the sum of the first diagonal element from these two co- variancematrixestimators, (cid:163) V(cid:98)0 (cid:164) 11 +(cid:163) V(cid:98)0 (cid:164) 11 . ThestandarderrorforÎ¸ (cid:98)isthesquarerootofthevariance estimator. InferentialstatementsaboutthetreatmenteffectÎ¸areaffectedbybiasjustasinanynonparametric estimationcontext. Ingeneralthedegreeofbiasisuncertain. Therearetworecommendationswhich may help to reduce the finite sample bias. First, use a common bandwidth for estimation of the LL regressiononeachsub-sample. Whenm(x)hasacontinuoussecondderivativeatx=c thiswillresult inazerofirst-orderasymptoticbias. Second,useabandwidthwhichissmallerthantheAMSE-optimal bandwidth.Thisreducesthebiasatthecostofincreasedvarianceandstandarderrors.Overallthisleads tomorehonestinferencestatements. Table21.1:RDDEstimatesoftheEffectofHeadStartAssistanceonChildhoodMortality Baseline Covariates Î¸ (cid:98) â1.51 â1.56 s(Î¸ (cid:98)) (0.71) (0.71) %Black 0.027 s(Î² (cid:98)1 ) (0.007) %Urban â0.0094 s(Î² (cid:98)2 ) (0.0046) To illustrate, Table 21.1 presents the RDD estimate of the Head Start treatment effect (the effect of grant-writing assistance on a county with poverty rate at the policy cut-off). This equals the vertical distance between the estimated conditional means from Figure 21.1(b). The point estimate is â1.51 withastandarderrorof0.71. Thet-statisticforatestofnoeffecthasap-valueof3%, consistentwith statisticalsignificanceatconventionallevels. Theestimatedpolicyimpactislarge. Itstatesthatfederal grant-writingassistance,andtheresultingsurgeinspendingontheHeadStartprogram,ledtoalong- termdecreaseintargetedmortalitybyabout1.5childrenper100,000.Giventhattheestimateduntreated mortalityrateis3.3childrenper100,000atthecut-offthisisanear50%decreaseinthemortalityrate. 21.6 BandwidthSelection Innonparametricestimationthemostcriticalchoiceisthebandwidth. Thisisespeciallyimportant inRDDestimationasthereisnotbroadagreementonthebestbandwidthselectionmethod.Ittherefore isprudenttocalculateseveraldata-basedbandwidthrulesbeforeestimation.Iwilldescribetwosimple approachesbasedontheglobalfitoftheRDDestimator. OurfirstsuggestionistheRule-of-Thumb(ROT)bandwidth(19.9)ofFanandGijbels(1996)modified toallowforadiscontinuityatx=c.Themethodrequiresareferencemodel.AmodestextensionofFan- Gijbelsâapproachisaqth orderpolynomialplusalevelshiftdiscontinuity.Thismodelis m(x)=Î² 0 +Î² 1 x+Î² 2 x2+Â·Â·Â·+Î² q xq+Î² q+1 D",
    "page": 764,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER21. REGRESSIONDISCONTINUITY 745 whereD=1 {xâ¥c}. Estimatethismodelbyleastsquares,obtaincoefficientestimatesandthevariance estimateÏ2.Fromthecoefficientestimatescalculatetheestimatedsecondderivative (cid:98) m (cid:98) (cid:48)(cid:48) (x)=2Î² (cid:98)2 +6Î² (cid:98)3 x+12Î² (cid:98)4 x2+Â·Â·Â·+q(qâ1)Î² (cid:98)q xqâ2. TheconstantB in(19.9)isestimatedby B(cid:98) = 1 (cid:88) n (cid:181) 1 m (cid:98) (cid:48)(cid:48) (X i ) (cid:182)2 1 {Î¾ 1 â¤X i â¤Î¾ 2 } n 2 i=1 where[Î¾ ,Î¾ ]istheregionofevaluation(andcanbesettoequaltothesupportof X whenthelatteris 1 2 bounded).Thereferencebandwidth(19.9)isthen h =0.58 (cid:181)Ï (cid:98) 2(Î¾ 2 âÎ¾ 1 ) (cid:182)1/5 n â1/5. (21.3) rot B(cid:98) Fan-Gijbelsrecommendq =4butotherchoicescanbeusedforthepolynomialorder. TheROTband- width(21.3)isappropriateforanynormalized(varianceone)kernel. Fortheunnormalizedrectangular kernelK(u)=1/2for|u|â¤1replacetheconstant0.58with1.00.FortheunnormalizedTriangularkernel K(u)=1â|u|for|u|â¤1replacetheconstant0.58with1.42. Anotherusefulmethodiscross-validation. CVfortheRDDestimatorisessentiallythesameasfor anyothernonparametricestimator. Foreachbandwidththeleave-one-outresidualsarecalculatedand theirsumofsquaresrecorded.ThebandwidthwhichminimizesthiscriterionistheCV-selectedchoice. PlotsoftheCVcriterionasafunctionofhcanaidindetermininingthesensitivityofthefitwithrespect tothebandwidth. Thesetwoproposalsaimtoproduceabandwidthh withglobalaccuracy. Analternativeisaband- widthselectionrulewhichaimsataccuracyatornearthecut-off.Theadvantageoftheglobalapproach isthatitisasimplerestimationproblemandthusmoreaccurateandlessvariable. Bandwidthestima- tionisahardproblem. Noiseinestimationofthebandwidthwilltranslateintoestimationnoiseforthe RDDestimate. Ontheotherhand,methodswhichaimataccuracyatthecut-offaretargetedattheob- jectofinterest.ThisisachallengingestimationissuesoIwillnotreviewitfurther.Forspecificproposals seeImbensandKalyanaraman(2012),AraiandIchimura(2018),andCattaneo,Idrobo,Titiunik(2020). AcompromiseiscalculatetheCVcriteriawiththeregionofevaluation[Î¾ ,Î¾ ]asubsetofthefullsup- 1 2 portof X centeredclosetothecut-off. Severaloftheearlyreviewpapersrecommendedthisapproach. ThechallengewiththisapproachisthattheCVcriteriaisanoisyestimatorandbyrestrictingtheregion ofevaluationweareincreasingitsestimationvariance.Thisincreasesnoise. InapplicationsIrecommendthatyoustartbycalculatingtheFan-GijbelsROTbandwidthforseveral valuesofpolynomialorderq. Whencomparingtheresultspayattentiontotheprecisionofthecoeffi- cientsinthepolynomialregression. Ifthehigh-orderpowersareimpreciselyestimatedthebandwidth estimatesmaybenoisyaswell. Second,findthebandwidthwhichminimizesthecross-validationcri- terion. PlottheCVcriterion. Ifitisrelativelyflatthisinformsyouthatitisdifficulttorankbandwidths. CombinetheaboveinformationtoselectanAMSE-minimizingbandwidth.Thenreducethisbandwidth somewhat(perhaps25%)toreduceestimationbias. Somerobustnesschecking(estimationwithalternativebandwidths)isprudent,butnarrowlyso",
    "page": 765,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Ifthehigh-orderpowersareimpreciselyestimatedthebandwidth estimatesmaybenoisyaswell. Second,findthebandwidthwhichminimizesthecross-validationcri- terion. PlottheCVcriterion. Ifitisrelativelyflatthisinformsyouthatitisdifficulttorankbandwidths. CombinetheaboveinformationtoselectanAMSE-minimizingbandwidth.Thenreducethisbandwidth somewhat(perhaps25%)toreduceestimationbias. Somerobustnesschecking(estimationwithalternativebandwidths)isprudent,butnarrowlyso. A ratheroddimplicationoftherobustnesscrazeistodesireresultswhichdonotchangewithbandwidths. Contrariwise,ifthetrueregressionfunctionisnonlinearthenresultswillchangewithbandwidths.What you should expect is that as you reduce the bandwidth the estimated function will reveal a combina- tion of shape and noise accompanied by wider confidence bands. As you increase the bandwidth the estimateswillstraightenoutandtheconfidencebandswillnarrow.Thenarrownessmeansthattheesti- mateshavereducedvariancebutthiscomesatthecostofincreased(anduncertain)bias.",
    "page": 765,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER21. REGRESSIONDISCONTINUITY 746 5 10 15 20 25 30 00.33 59.23 09.23 58.23 08.23 57.23 07.23 20 30 40 50 60 70 80 (a)Cross-ValidationFunction 5 4 3 2 1 Poverty Rate etaR ytilatroM Untreated Treated 20 30 40 50 60 70 80 (b)RDDwithCovariates 571 051 521 001 57 05 52 0 (c)HistogramofPovertyRate Figure21.2:RDDDiagnostics WeillustrateusingtheLudwig-Miller(2007)HeadStartapplication.WecalculatedthemodifiedFan- Gijbels ROT using q =2, 3, and 4, obtaining bandwidths of h (q =2)=24.6, h (q =3)=11.0, and rot rot h (q =4)=5.2. Theseresultsaresensitivetothechoiceofpolynomial. Examiningthesepolynomial rot regressionsweseethatthethirdandfourthcoefficientestimateshavelargestandarderrorssoarenoisy. Wenextevalulatedthecross-validationcriterionontheregion[1,30],plottedinFigure21.2(a). Wesee that the CV criterion is monotonically decreasing with h, though quite flat for h â¥20. Essentially the CVcriterionrecommendsaninfinitebandwidthwhichmeansusingallobservationsequallyweighted. SincewewantabandwidthwhichissmallerthanAMSE-optimal,weleantowardssmallerbandwidths andtakearoughaverageoftheROTbandwidthswithq=3andq=4toobtainh=8. Thisistheband- widthusedintheempiricalresultsshowninthischapter. Largerbandwidthsresultinflatter(morelinear)estimatedconditionalmeanfunctionsandasmaller estimatedHeadStarteffect. Smallerbandwidthsresultinmorecurvatureintheestimatedconditional meanfunctions,inparticularforthesectionabovethecut-off. 21.7 RDDwithCovariates A powerful implication of Theorem 21.1 is that covariates are not necessary to identify the condi- tionalATE.Thisimpliesthataugmentingtheregressionmodeltoincludecovariatesisnotnecessaryfor estimationandinference. Theprecisionofestimation, however, willbeaffected. Inclusionofrelevant covariates can reduce the equation error. It is therefore prudent to consider the addition of relevant covariateswhenavailable. Denotethevariablesas(Y,X,Z)whereZ isavectorofcovariates. Againconsiderthepotentialout- comesframeworkwhereY andY aretheoutcomewithandwithouttreatment. Assumethatthecon- 0 1 ditionalmeanstakethepartiallylinearform (cid:69)[Y |X =x, Z =z]=m (x)+Î²(cid:48) z 0 0 (cid:69)[Y |X =x, Z =z]=m (x)+Î²(cid:48) z. 1 1 Forsimplicityweassumethatthelinearcoefficientsarethesameinthetwoequations. Thisisnotes- sentialbutsimplifiestheestimationstrategy.ItfollowsthattheconditionalmeanforY equals m(x,z)=m (x) 1 {x,c}+m (x) 1 {xâ¥c}+Î²(cid:48) z. 0 1",
    "page": 766,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER21. REGRESSIONDISCONTINUITY 747 AminorextensionofTheorem21.1showsthattheconditionalATEisÎ¸=m(c+,z)âm(câ,z). DifferentauthorshavesuggesteddifferentmethodsforestimationoftheRDDwithcovariatesmodel. The preferred method is the estimator of Robinson (1988). See Section 19.24. (It is preferred because Robinsondemonstratedthatitissemiparametricallyefficientwhiletheothersuggestionshavenoeffi- ciencyjustification.)Theestimationmethodisasfollows. 1. Usethe RDDlocal linearestimator toregress Y on X to obtainthe first-stepfittedvaluesm = i i (cid:98)i m(X ). (cid:98) i 2. UsingLLregression,regressZ onX ,Z onX ,...,andZ onX ,obtainingthefittedvaluesfor i1 i i2 i ik i thecovariates,sayg ,...,g . (cid:98)1i (cid:98)ki 3. RegressY i âm (cid:98)i onZ 1i âg (cid:98)1i ,...,Z ki âg (cid:98)ki toobtainthecoefficientestimateÎ² (cid:98)andstandarderrors. 4. Constructtheresiduale (cid:98)i =Y i âZ i (cid:48)Î² (cid:98). 5. UsetheRDDlocallinearestimatortoregresse onX toobtainthenonparametricestimatorm(x), (cid:98)i i (cid:98) conditionalATEÎ¸ (cid:98),andassociatedstandarderrors. AsshownbyRobinson(1988)anddiscussedinSection19.24,theaboveestimatorissemiparametri- callyefficient,theconventionalasymptotictheoryvalid,andconventionalinferenceisvalid. Thusthe estimatorscanbeusedtoassesstheconditionalATE. Asmentionedabove, inclusionofcovariatesdoesnotaltertheconditionalATEparameterÎ¸ under correctspecification. Inclusionofcovariatescan, however, affecttheconditionalmeanfunction m(x) at points x away from the discontinuity. Covariates will also affect the precision of the estimator and standarderrors. To illustrate, we augment the Ludwig-Miller Head Start estimates with two covariates: the county- levelBlackpopulationpercentage,andthecounty-levelurbanpopulationpercentage. Thesevariables can be viewed as proxies for income. We estimate the model using the Robinson estimator. The esti- matednonlinearfunctionm(x)isdisplayedinFigure21.2(b),thecoefficientestimatesinTable21.1. ComparingFigure21.2(b)withFigure21.1(b)itappearsthattheestimatedconditionalATE(thetreat- menteffectofthepolicy)isaboutthesamebuttheshapeofm(x)isdifferent. Withthecovariatesin- cludedm(x)isconsiderablyflatter.ExaminingTable21.1wecanseethattheestimatedtreatmenteffect isnearlythesameasinthebaselinemodelwithoutcovariates. Wealsoseethatthecoefficientonthe Blackpercentageispositiveandthatontheurbanpercentageisnegative,consistentwiththeviewthat theseareservingasproxiesforincome. 21.8 ASimpleRDDEstimator AsimpleRDDestimatorcanbeimplementbyastandardregressionusingconventionalsoftware. It isequivalenttoaLLestimatorwithaRectangularbandwidth.Estimatetheregression Y =Î² +Î² X+Î² (Xâc)D+Î¸D+e (21.4) 0 1 3 forthesubsampleofobservationssuchthat|Xâc|â¤h. ThecoefficientestimateÎ¸ (cid:98)istheestimatedcon- ditionalATEandinferencecanproceedconventionallyusingregressionstandarderrors. Themostim- portantchoiceisthebandwidth.TheROTchoiceis(21.3)with1.00replacingtheconstant0.58. Toillustrate,taketheHeadStartsample",
    "page": 767,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". It isequivalenttoaLLestimatorwithaRectangularbandwidth.Estimatetheregression Y =Î² +Î² X+Î² (Xâc)D+Î¸D+e (21.4) 0 1 3 forthesubsampleofobservationssuchthat|Xâc|â¤h. ThecoefficientestimateÎ¸ (cid:98)istheestimatedcon- ditionalATEandinferencecanproceedconventionallyusingregressionstandarderrors. Themostim- portantchoiceisthebandwidth.TheROTchoiceis(21.3)with1.00replacingtheconstant0.58. Toillustrate,taketheHeadStartsample. Forthenorm(cid:112)alizedTriangularkernelwehadusedaband- widthofh=8.Thisisconsistentwithabandwidthofh=8 3(cid:39)13.8fortheRectangularkernel.Wetook",
    "page": 767,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER21. REGRESSIONDISCONTINUITY 748 thesubsampleof482withpovertyratesintheinterval59.2Â±13.8=[45.4,72.0]andestimatedequation (21.4)byleastsquares.Theestimatesare Y(cid:98) = â3.11 + 0.11 X+ 0.18 (Xâ59.2)Dâ 2.20 D. (21.5) (9.13) (0.17) (0.23) (1.06) The point estimate â2.2 of the conditional ATE is larger than those reported in Table 21.1 but within samplingvariation. Thestandarderrorfortheeffectisalsolarger,consistentwithourexpectationthat therectangularkernelestimatorislessaccurate. 21.9 DensityDiscontinuityTest Thecoreidentificationtheoremassumesthattheconditionalmeansm (x)andm (x)arecontinu- 0 1 ousatthecut-off.Theseassumptionsmaybeviolatediftherunningvariableismanipulatedbyindivid- ualsseekingoravoidingtreatment.Manipulationtoobtaintreatmentislikelytoleadtobunchingofthe runningvariablejustaboveorbelowthecut-off. IfthereisnomanipulationweexpectthedensityofX tobecontinuousatx =c,butifthereismanipulationweexpectthattheremightbeadiscontinuityin thedensityofX atx=c. A reasonable specification check is to assess if the density f(x) of X is continuous at x =c. Some careneedstobeexercisedinimplementation,however,asconventionaldensityestimatorssmoothover discontinuitiesandconventionaldensityestimatorsarebiasedatboundarypoints(similarlytothebias oftheNadaraya-Watsonestimatoratboundarypoints). Asimplevisualcheckisthehistogramoftherunningvariablewithnarrowbins,carefullyconstructed so that no bin spans the cut-off. If the histogram bins display no evidence of bunching at one side of the cut-off this is consistent with the hypothesis that the density is continuous at the cut-off; on the otherhandifthereisanoticablespikeoneithersidethisisinconsistentwiththehypothesisofcorrect specification. IntheHeadStartexampleitisnotcrediblethattherunningvariablewasmanipulatedbytheindivid- ualcountiessinceitwasconstructedfromthe1960censusbyafederalagencyin1965.Never-the-lesswe canexaminetheevidence. InFigure21.2(c)wedisplayahistogramoffrequencycountsfortherunning variable (county poverty rate), with bins of width 2, constructed so that one of the bin endpoints falls exactlyatthecut-off(thesolidline). Thehistogramappearstobecontinuouslydecreasingthroughout itssupport.Inparticularthereisnovisualevidenceofbunchingaroundthecut-off. McCrary (2008) implements a formal test for continuity of the density at the cut-off. I only give a briefsummaryhere;seehispaperfordetails. Thefirststepisafinehistogramestimator,similartoFig- ure21.2(c)butwithmorenarrowbinwidths. ThesecondstepistoapplytheRDDlocallinearestimator treatingthehistogramheightsastheoutcomevariableandthebinmidpointsattherunningvariable. Thisisalocallineardensityestimatorandisnotsubjecttotheboundarybiasproblemsoftheconven- tionalkerneldensityestimator. TheRDDconditionalATEisthedifferenceinthedensityatthecut-off. McCraryderivestheasymptoticdistributionoftheestimatorofthedensitydifferenceandproposesan appropriate t-statistic for testing the hypothesis of a continuous density",
    "page": 768,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". ThesecondstepistoapplytheRDDlocallinearestimator treatingthehistogramheightsastheoutcomevariableandthebinmidpointsattherunningvariable. Thisisalocallineardensityestimatorandisnotsubjecttotheboundarybiasproblemsoftheconven- tionalkerneldensityestimator. TheRDDconditionalATEisthedifferenceinthedensityatthecut-off. McCraryderivestheasymptoticdistributionoftheestimatorofthedensitydifferenceandproposesan appropriate t-statistic for testing the hypothesis of a continuous density. If the statistic is large this is evidenceagainsttheassumptionofnomanipulation,suggestingthattheRDDdesignisnotappropriate. 21.10 FuzzyRegressionDiscontinuity Thesharpregressiondiscontinuityrequiresthatthecut-offperfectlyseparatestreatmentfromnon- treatment.Analternativecontextiswherethisseparationisimperfectbuttheconditionalprobabilityof",
    "page": 768,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER21. REGRESSIONDISCONTINUITY 749 treatmentisdiscontinuousatthecut-off.Thisiscalledfuzzyregressiondiscontinuity(FRD). Againconsiderthepotentialoutcomesframework,whereY andY aretheoutcomeswithouttreat- 0 1 mentandwithtreatment,Î¸=Y âY isthetreatmenteffect, X istherunningvariable,theconditional 1 0 averagetreatmenteffectatthecutoffisÎ¸=(cid:69)[Î¸|X =c],andD=1indicatestreatment. Definethecon- ditionalprobabilityoftreatment p(x)=(cid:80)[D=1|X =x]. andtheleftandrightlimitsatthecut-offp(c+) andp(câ).TheFRDapplieswhenp(c+)(cid:54)=p(câ). ThissiutationisillustratedinFigure21.3(a). Thisdisplaystheconditionalprobabilityoftreatment asafunctionoftherunningvariableX withadiscontinuityatX =c. 0.1 8.0 6.0 4.0 2.0 0.0 ( ) px ( ) m0x ( ) mx ( ) m1x c X<c c X>c (a)ConditionalTreatmentProbability (b)FuzzyRegressionDiscontinuity Figure21.3:FuzzyRegressionDiscontinuityDesign Thefollowingisthecoreidentificationtheoremfortheregressiondiscontinuitydesign. Itisdueto Hahn,Todd,andVanderKlaauw(2001). Theorem21.2 Supposethatm (x)andm (x)arecontinuousatx=c,p(x)is 0 1 discontinuousatx=c,andD isindependentofÎ¸forX nearc.Then m(c+)âm(câ) Î¸= . (21.6) p(c+)âp(câ) Theorem21.2isamoresubstantialidentificationresultthanTheorem21.1asitisinherentlysurpris- ing. ItstatesthattheconditionalATEisidentifiedbytheratioofthediscontinuitiesintheconditional meanandconditionalprobabilityfunctionsunderthestatedassumptions. Thisbroadensthescopefor potentialapplicationoftheregressiondiscontinuityframeworkbeyondthesharpRDD. In addition to the discontinuity of p(x), the key additional assumption relative to Theorem 21.1 is thattreatmentDisindependentofthetreatmenteffectÎ¸atX =x.Thisisastrongassumption.Itmeans thattreatmentassignmentisrandomlyassignedforindividualswith X nearc. Thisdoesnotallow,for",
    "page": 769,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER21. REGRESSIONDISCONTINUITY 750 example,forindividualstoselectintotreatment,forthenindividualswithhightreatmenteffectsÎ¸ are morelikelytoseektreatmentthanindividualswithlowtreatmenteffectsÎ¸. A display of the outcomes is given in Figure 21.3(b). The two dashed lines are the mean potential outcomesm (x)andm (x). Therealizedconditionalmeanm(x)istheprobabilityweightedaverageof 0 1 thesetwofunctionsusingtheprobabilityfunctiondisplayedinpanel(a). Sincetheprobabilityfunction is discontinuous at x =c the conditional mean m(x) also is discontinuous at x =c. The discontinu- ity, however, isnotthefullconditionalATEÎ¸. TheimportantcontributionofTheorem21.2isthatthe conditionalATEequalstheratioofthediscontinuitiesinpanels(b)and(a). ToprovetheTheorem,firstobservethattheobservedconditionalmeanis m(x)=(cid:161) 1âp(x) (cid:162) m (x)+p(x)m (x)=m (x)+p(x)(m (x)âm (x)). 0 1 0 1 0 Itâsleftandrightlimitsatc are m(c+)=m (c)+p(c+)Î¸ 0 m(câ)=m (c)+p(câ)Î¸. 0 Takingthedifferenceandre-arrangingweestablishthetheorem. 21.11 EstimationofFRD Asdisplayedin(21.2)theLLestimatorofthediscontinuitym(c+)âm(câ)isobtainedbylocallinear regressionofY onX onthetwosidesofthecut-off,leadingto m (cid:98) (c+)âm (cid:98) (câ)=(cid:163)Î² (cid:98)1 (c) (cid:164) 1 â(cid:163)Î² (cid:98)0 (c) (cid:164) 1 . Similarly,aLLestimatorp(c+)âp(câ)ofthediscontinuityp(c+)âp(câ)canobtainedbylocallinear (cid:98) (cid:98) regressionofY onD onthetwosidesofthecut-off.Dividingweobtaintheestimatoroftheconditional ATE m(c+)âm(câ) Î¸ (cid:98) = (cid:98) (cid:98) . (21.7) p(c+)âp(câ) (cid:98) (cid:98) ThisgeneralizesthesharpRDDestimator,forinthatcasep(c+)âp(câ)=1. ThisestimatorbearsastrikingresemblancetotheWaldexpression(12.27)forthestructuralcoeffi- cientandestimator(12.28)inanIVregressionwithabinaryinstrument.Infact,Î¸ (cid:98)canbethoughtofasa locallyweightedIVestimatorofaregressionofY on X withinstrumentD. However,theeasiestwayto implementestimationisusingtheexpressionforÎ¸ (cid:98)above. Theestimator(21.7)requiresfourLLregressions.Itisunclearifcommonbandwidthsshouldbeused forthenumeratoranddenominatororifdifferentbandwidthsisabetterchoice.Bandwidthselectionis criticallyimportant. InadditiontoassessingthefitoftheregressionofY onX,itisimportanttocheck thefitoftheregressionofD onX fortheestimatorp(x). ThelatteristhereducedformoftheIVmodel. (cid:98) Identificationrestsonitsprecision. TheidentificationoftheFRDconditionalATEdependsonthemagnitudeofthediscontinuityinthe conditionalprobabilityp(x)atx=c.Asmalldiscontinuitywillleadtoaweakinstrumentsproblem. Standard errors can be calculated similar to IV regression. Let s (cid:161)Î¸ (cid:98) (cid:162) be a standard error m (cid:98) (c+)â m (cid:98) (câ).ThenastandarderrorforÎ¸ (cid:98)iss (cid:161)Î¸ (cid:98) (cid:162) / (cid:175) (cid:175)p (cid:98) (c+)âp (cid:98) (câ) (cid:175) (cid:175). In FRD applications it is recommended to plot the estimated functions m(x) and p(x) along with (cid:98) (cid:98) confidence bands to assess precision",
    "page": 770,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Standard errors can be calculated similar to IV regression. Let s (cid:161)Î¸ (cid:98) (cid:162) be a standard error m (cid:98) (c+)â m (cid:98) (câ).ThenastandarderrorforÎ¸ (cid:98)iss (cid:161)Î¸ (cid:98) (cid:162) / (cid:175) (cid:175)p (cid:98) (c+)âp (cid:98) (câ) (cid:175) (cid:175). In FRD applications it is recommended to plot the estimated functions m(x) and p(x) along with (cid:98) (cid:98) confidence bands to assess precision. You are looking for evidence that the discontinuity in p(x) is real and meaningful so that the conditional ATE Î¸ is identified. A discontinuity in m(x) is an indica- torwhetherornottheconditionalATEisnon-zero. Ifthereisnodiscontinuityinm(x)thenÎ¸=0. The estimateoftheconditionalATEistheratioofthesetwoestimateddiscontinuities. _____________________________________________________________________________________________",
    "page": 770,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER21. REGRESSIONDISCONTINUITY 751 21.12 Exercises Exercise21.1 We have described the RDD when treatment occurs for D =1 {X â¥c}. Suppose instead thattreatmentoccursforD=1 {X â¤c}. Describethedifferences(ifany)involvedinestimatingthecon- ditionalATEÎ¸. Exercise21.2 SupposetreatmentoccursforD =1 {c â¤X â¤c }wherebothc andc areintheinterior 1 2 1 2 ofthesupportofX.Whattreatmentaffectsareidentified? Exercise21.3 Showthat(21.1)isobtainedbytakingtheconditionalexpectationasdescribed. Exercise21.4 Explainwhyequation(21.4)estimatedonthesubsampleforwhich|Xâc|â¤hisidentical toalocallinearregressionwithaRectangularbandwidth. Exercise21.5 UsethedatafileLM2007onthetextbookwebpage. Replicatetheregresssion(21.5)using aRectangularkernelandabandwidthof13.8(asdescribedinthetext). Repeatwithabandwidthof7 and 20. Report your estimates of the conditional ATE and standard error. The dependent variable is mort_age59_related_postHS.(Therunningvariableispovrate60.) Exercise21.6 UsethedatafileLM2007onthetextbookwebpage. ReplicatethebaselineRDDestimate as reported in Table 21.1. Repeat with a bandwidth of h =4 and h =12. Report your estimates of the conditionalATEandstandarderror. Exercise21.7 UsethedatafileLM2007onthetextbookwebpage. LudwigandMiller(2007)showsthat similar RDD estimates for other forms of mortality do not display similar discontinuities. Perform a similar check. Estimate the conditional ATE using the dependent variable mort_age59_injury_postHS (mortalityduetoinjuriesinthe5-9agegroup). Exercise21.8 Do a similar estimation as in the previous exercise, but using the dependent variable mort_age25plus_related_postHS(mortalityduetoHS-relatedcausesinthe25+agegroup). Exercise21.9 Do a similar estimation as in the previous exercise, but using the dependent variable mort_age59_related_preHS(mortalityduetoHS-relatedcausesinthe5-9agegroupduring1959-1964, beforetheHeadStartprogramwasstarted).",
    "page": 771,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Part VI NonLinear Methods 752",
    "page": 772,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Chapter 22 M-Estimators 22.1 Introduction So far in this textbook we have primarily focused on estimators which have explicit algebraic ex- pressions. However,manyeconometricestimatorsneedtobecalculatedbynumericalmethods. These estimatorsarecollectivelydescribedasnonlinear.Manyfallinabroadclassknownasm-estimators.In thispartofthetextbookwedescribeanumberofm-estimatorsinwideuseineconometrics. Theyhave acommonstructurewhichallowsforaunifiedtreatmentofestimationandinference. Anm-estimatorisdefinedasaminimizerofasampleaverage Î¸ (cid:98) =argminS n (Î¸) Î¸âÎ 1 (cid:88) n S (Î¸)= Ï (Î¸) n i n i=1 whereÏ (Î¸)=Ï(Y ,X ,Î¸)issomefunctionoftheith observationandaparameterÎ¸âÎ. Thefunction i i i S (Î¸)iscalledthecriterionfunctionorobjectivefunction. n ThisincludesmaximumlikelihoodwhenÏ (Î¸)isthenegativelog-densityfunction. âm-estimatorsâ i areabroaderclass;theprefixâmâstandsforâmaximumlikelihood-typeâ. The issues we focus on in this chaper are: (1) identification; (2) estimation; (3) consistency; (4) asymptoticdistribution;and(5)covariancematrixestimation. 22.2 Examples Therearemanym-estimatorsincommoneconometricusage.Someexamplesincludethefollowing. 1. OrdinaryLeastSquares:Ï (Î¸)=(cid:161) Y âX (cid:48)Î¸(cid:162)2 . i i i 2. NonlinearLeastSquares:Ï (Î¸)=(Y âm(X ,Î¸))2(Chapter23). i i i 3. LeastAbsoluteDeviations:Ï i (Î¸)=(cid:175) (cid:175)Y i âX i (cid:48)Î¸(cid:175) (cid:175)(Chapter24). 4. QuantileRegression:Ï (Î¸)=(cid:161) Y âX (cid:48)Î¸(cid:162)(cid:161)Ïâ1(cid:169)(cid:161) Y âX (cid:48)Î¸(cid:162)<0 (cid:170)(cid:162) (Chapter24). i i i i i 5. MaximumLikelihood:Ï (Î¸)=âlogf (Y |X ,Î¸). i i i 753",
    "page": 773,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER22. M-ESTIMATORS 754 The final category âMaximumLikelihoodEstimationâ includesmany estimators asspecial cases. Thisincludesmanystandardestimatorsoflimited-dependent-variablemodels(Chapters25-27). Toil- lustrate,theprobitmodelforabinarydependentvariableis (cid:80)[Y =1|X]=Î¦(cid:161) X (cid:48)Î¸(cid:162) whereÎ¦(u)isthenormalcumulativedistributionfunction. Wewillstudyprobitestimationindetailin Chapter25.Thenegativelog-densityfunctionis Ï (Î¸)=âY log (cid:161)Î¦(cid:161) X (cid:48)Î¸(cid:162)(cid:162)â(1âY )log (cid:161) 1âÎ¦(cid:161) X (cid:48)Î¸(cid:162)(cid:162) . i i i i i Notall nonlinearestimatorsarem-estimators. Examplesincludemethodofmoments, GMM, and minimumdistance. 22.3 IdentificationandEstimation AparametervectorÎ¸ isidentifiedifitisuniquelydeterminedbytheprobabilitydistributionofthe observations.Thisisapropertyoftheprobabilitydistribution,notoftheestimator. However,whendiscussingaspecificestimatoritiscommontodescribeidentificationintermsofthe criterionfunction.Assume(cid:69)(cid:175) (cid:175) Ï(Y,X,Î¸) (cid:175) (cid:175) <â.Define S(Î¸)=(cid:69)[S (Î¸)]=(cid:69)(cid:163)Ï(Y,X,Î¸) (cid:164) n anditspopulationminimizer Î¸ =argminS(Î¸). 0 Î¸âÎ WesaythatÎ¸isidentified(orpointidentified)byS(Î¸)iftheminimizerÎ¸ isunique. 0 Innonlinearmodelsitisdifficulttoprovidegeneralconditionsunderwhichaparameterisidentified. Identificationneedstobeexaminedonamodel-by-modelbasis. Anm-estimatorÎ¸ (cid:98)bydefinitionminimizesS n (Î¸). Whenthereisnoexplicitalgebraicexpressionfor thesolutiontheminimizationisdonenumerically.SuchnumericalmethodsarereviewedinChapter12 ofIntroductiontoEconometrics. Weillustrateusingtheprobitmodeloftheprevioussection.WeusetheCPSdatasetforY equaltoan indicatorthattheindividualismarried1,andsettheregressorsequaltoyearsofeducation,age,andage squared.Weobtainthefollowingestimates ï£« ï£¶ (cid:179)age(cid:180) (cid:179)age(cid:180)2 (cid:80)[married=1]=Î¦ ï£­ 0.031 education+ 16.4 â 16.7 + 3.73 ï£¸. 100 100 (.002) (0.3) (0.4) (0.07) StandarderrorcalculationwillbediscussedinSection22.7.Inthisapplicationweseethattheprobability ofmarriageisincreasinginyearsofeducationandisanincreasingyetconcavefunctionofage. 22.4 Consistency Itseemsreasonabletoexpectthatifaparameterisidentifiedthenweshouldbeabletoestimatethe parameterconsistently.ForlinearestimatorswedemonstratedconsistencybyapplyingtheWLLNtothe 1Wedefinemarried=1ifmaritalequals1,2,or3.",
    "page": 774,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER22. M-ESTIMATORS 755 explicitalgebraicexpressionsfortheestimators. Thisisnotpossiblefornonlinearestimatorssincethey donothaveexplicitalgebraicexpressions. Instead,whatisavailabletousisthatanm-estimatorminimizesthecriterionfunctionS (Î¸)whichis n itselfasampleaverage. ForanygivenÎ¸theWLLNshowsthatS (Î¸)ââS(Î¸). Itisintuitivethatthemin- n p imizerofS n (Î¸)(them-estimatorÎ¸ (cid:98))willconvergeinprobabilitytotheminimizerofS(Î¸)(theparameter Î¸ ).However,theWLLNbyitselfisnotsufficienttomakethisextension. 0 S(q)+e lSn (q 0 ) lS(q^) S(q) S S S S ( n n n 1 2 3 q ( ( ( ) q q q ) ) ) S(q)-e Sn (q) S( l q 0 ) Sn l (q^) q 0 q^ n1 q^ n2 q^ n3 (a)Non-UniformConvergence (b)UniformConvergence (c)Consistency Figure22.1:Consistencyofm-Estimators ToseetheproblemexamineFigure22.1(a). ThisdisplaysasequenceoffunctionsS (Î¸)(thedashed n lines)forthreevaluesofn.WhatisillustratedisthatforeachÎ¸thefunctionS (Î¸)convergestowardsthe n limitfunctionS(Î¸).HoweverforeachnthefunctionS (Î¸)hasaseveredipintheright-handregion.The n resultisthatthesampleminimizerÎ¸ (cid:98)n convergestotheright-limitoftheparameterspace. Incontrast, theminimizerÎ¸ ofthelimitcriterionS(Î¸)isintheinterioroftheparameterspace. Whatweobserveis 0 thatS n (Î¸)convergestoS(Î¸)foreachÎ¸buttheminimizerÎ¸ (cid:98)n doesnotconvergetoÎ¸ 0 . Asufficientconditiontoexcludethispathologicalbehaviorisuniformconvergenceâuniformityover theparameterspaceÎ.AsweshowinTheorem22.1,uniformconvergenceinprobabilityofS (Î¸)toS(Î¸) n issufficienttoestablishthatthem-estimatorÎ¸ (cid:98)isconsistentforÎ¸ 0 . Definition22.1 S (Î¸)convergesinprobabilitytoS(Î¸)uniformlyoverÎ¸âÎif n sup|S (Î¸)âS(Î¸)|ââ0 n Î¸âÎ p asnââ. UniformconvergenceexcludeserraticwigglesinS (Î¸)uniformlyacrossÎ¸ andn (e.g.,whatoccurs n in Figure 22.1(a)). The idea is illustrated in Figure 22.1(b). The heavy solid line is the function S(Î¸). ThedashedlinesareS(Î¸)+ÎµandS(Î¸)âÎµ. ThethinsolidlineisthesamplecriterionS (Î¸). Thefigure n illustratesasituationwherethesamplecriterionsatisifessupÎ¸âÎ |S n (Î¸)âS(Î¸)|<Îµ.Thesamplecriterion as displayed weaves up and down but stays within Îµ of S(Î¸). Uniform convergence holds if the event showninFigure22.1(b)holdswithhighprobabilityfornsufficientlylarge,foranyarbitrarilysmallÎµ.",
    "page": 775,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER22. M-ESTIMATORS 756 Theorem22.1 Assume 1. S (Î¸)convergesinprobabilitytoS(Î¸)uniformlyoverÎ¸âÎ. n 2. Î¸ uniquelyminimizesS(Î¸)inthesensethatforall(cid:178)>0, 0 inf S(Î¸)>S(Î¸ ). 0 Î¸:(cid:107)Î¸âÎ¸ 0 (cid:107)â¥(cid:178) ThenÎ¸ (cid:98) ââÎ¸ 0 asnââ. p Theorem22.1showsthatanm-estimatorisconsistentforitspopulationparameter. Thereareonly twoconditions.First,thecriterionfunctionconvergesuniformlyinprobabilitytoitsexpectedvalue,and second,theminimizerÎ¸ isunique. Theassumptionexcludesthepossibilitythatlim S(Î¸ )=S(Î¸ )for 0 j j 0 somesequenceÎ¸ âÎnotconvergingtoÎ¸ . j 0 TheproofofTheorem22.1isprovidedinSection22.8. 22.5 UniformLawofLargeNumbers TheuniformconvergenceofDefinition22.1isahigh-levelassumption. Inthissectionweprovide lowerlevelsufficientconditions. Theorem22.2 UniformLawofLargeNumbers(ULLN)Assume 1. (Y ,X )arei.i.d. i i 2. (cid:69)(cid:175) (cid:175) Ï(Y,X,Î¸) (cid:175) (cid:175) <âforallÎ¸âÎ. 3. Îisbounded. 4. Forsome A<âandÎ±>0,(cid:69)(cid:175) (cid:175) Ï(Y,X,Î¸ 1 )âÏ(Y,X,Î¸ 2 ) (cid:175) (cid:175) â¤A(cid:107)Î¸ 1 âÎ¸ 2 (cid:107)Î± for allÎ¸ ,Î¸ âÎ. 1 2 ThensupÎ¸âÎ |S n (Î¸)âS(Î¸)|ââ0. p Theorem22.2isestablishedinTheorem18.7ofIntroductiontoEconometrics. Thecriticalassumptionisthefourth. Itisacontinuityrequirementontheexpectationofthesum- mands Ï(Y,X,Î¸). This holds if the summands themselves are HÃ¶lder continuous but is considerably broaderasitallowsfordiscontinuousfunctions.ExpectationisasmoothingoperationsowhileÏ(Y,X,Î¸) maybediscontinuousinÎ¸itsexpectationistypicallycontinuous. TheULLNextendstotimeseriesandclusteredsamples. SeeB.E.HansenandS.Lee(2019)forclus- teredsamples.",
    "page": 776,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER22. M-ESTIMATORS 757 22.6 AsymptoticDistribution Wenowestablishanasymptoticdistributiontheory.Westartbyaninformaldemonstration,present ageneralresultunderhigh-levelconditions,andthendiscusstheassumptionsandconditions.Define â Ï (Î¸)= Ï (Î¸) i âÎ¸ i â Ï (Î¸)= S (Î¸) n âÎ¸ n â Ï(Î¸)= S(Î¸) âÎ¸ Ï =Ï (Î¸ ). i i 0 Sincethem-estimatorÎ¸ (cid:98)minimizesS n (Î¸)itsatisfies2thefirst-ordercondition0=Ï n (cid:161)Î¸ (cid:98) (cid:162) .Expandthe right-handsideasafirstorderTaylorexpansionaboutÎ¸ 0 . ThisisvalidwhenÎ¸ (cid:98)isinaneighborhoodof Î¸ ,whichholdsfornsufficientlylargebyTheorem22.1.Thisyields 0 â2 0=Ï n (cid:161)Î¸ (cid:98) (cid:162)(cid:39)Ï n (Î¸ 0 )+ âÎ¸âÎ¸(cid:48) S n (Î¸ 0 ) (cid:161)Î¸ (cid:98) âÎ¸ 0 (cid:162) . (22.1) Rewriting,weobtain (cid:112) (cid:181) â2 (cid:182)â1 (cid:112) n (cid:161)Î¸ (cid:98) âÎ¸ 0 (cid:162)(cid:39)â âÎ¸âÎ¸(cid:48) S n (Î¸ 0 ) (cid:161) nÏ n (Î¸ 0 ) (cid:162) . Considerthetwocomponents.First,bytheWLLN â2 S (Î¸ )= 1 (cid:88) n â2 Ï (Î¸ )ââ(cid:69) (cid:183) â2 Ï (Î¸ ) (cid:184) d=ef Q. âÎ¸âÎ¸(cid:48) n 0 n i=1 âÎ¸âÎ¸(cid:48) i 0 p âÎ¸âÎ¸(cid:48) i 0 Second, (cid:112) 1 (cid:88) n nÏ (Î¸ )= (cid:112) Ï . (22.2) n 0 i n i=1 SinceÎ¸ minimizesS(Î¸)=(cid:69)(cid:163)Ï (Î¸) (cid:164) itsatisfiesthefirst-ordercondition 0 i 0=Ï(Î¸ )=(cid:69)(cid:163)Ï (cid:164) . (22.3) 0 i Thusthesummandsin(22.2)aremeanzero.ApplyingaCLTthissumconvergesindistributiontoN(0,â¦) whereâ¦=(cid:69)(cid:163)Ï Ï(cid:48)(cid:164) .Wededucethat i i (cid:112) n (cid:161)Î¸ (cid:98) âÎ¸ 0 (cid:162)ââQ â1N(0,â¦)=N (cid:161) 0,Q â1â¦Q â1(cid:162) . d ThetechnicalhurdletomakethisderivationrigorousisjustifyingtheTaylorexpansion(22.1). This can be done through smoothness of the second derivative of Ï (Î¸ ). An alternative (more advanced) i 0 argumentbasedonempiricalprocesstheoryusesweakerassumptions.Set â2 Q(Î¸)= S(Î¸) âÎ¸âÎ¸(cid:48) Q=Q(Î¸ ). 0 LetN besomeneighborhoodofÎ¸ . 0 2IfÎ¸(cid:98)isaninteriorsolution. SinceÎ¸(cid:98)isconsistentthisoccurswithprobabilityapproachingoneifÎ¸ 0isintheinteriorofthe parameterspaceÎ.",
    "page": 777,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER22. M-ESTIMATORS 758 Theorem22.3 AssumetheconditionsofTheorem22.1hold,plus 1. (cid:69)(cid:176) (cid:176) Ï i (cid:176) (cid:176) 2<â. 2. Q(Î¸)iscontinuousinÎ¸âN . 3. For some A < â and Î± > 0, (cid:69)(cid:176) (cid:176) Ï i (Î¸ 1 )âÏ i (Î¸ 2 ) (cid:176) (cid:176) 2 â¤ A(cid:107)Î¸ 1 âÎ¸ 2 (cid:107)Î± for all Î¸ ,Î¸ âN . 1 2 4. Q>0. 5. Î¸ isintheinteriorofÎ. 0 (cid:112) Thenasnââ, n (cid:161)Î¸ (cid:98) âÎ¸ 0 (cid:162)ââN(0,V)whereV =Q â1â¦Q â1. d TheproofofTheorem22.3ispresentedinSection22.8. In some cases the asymptotic covariance matrix simplifies. The leading case is correctly specified maximumlikelihoodestimation,whereQ=â¦soV =Q â1=â¦â1. Assumption1statesthatthescoresÏ haveafinitesecondmoment. Thisisnecessaryinorderto i applytheCLT.AsufficientconditionforAssumption2isthatthescoresÏ (Î¸)arecontinuouslydiffer- i entiablebutthisisnotnecessary. Assumption2isbroader,allowingfordiscontinuousÏ (Î¸),solongas i itsexpectationiscontinuousanddifferentiable.Assumption3issimilar.ItholdsifÏ (Î¸)hasabounded i derivativebutthisisnotnecessary.Assumption4isafull-rankconditionandisrelatedtoidentification. Assumption5isrequiredinordertojustifytheapplicationofthemean-valueexpansion. 22.7 CovarianceMatrixEstimation ThestandardestimatorforV takesthesandwichform.Weestimateâ¦by â¦ (cid:98) = n 1 (cid:88) n Ï (cid:98)i Ï (cid:98) (cid:48) i . i=1 whereÏ (cid:98)i = â â Î¸ Ï i (cid:161)Î¸ (cid:98) (cid:162) .WhenÏ i (Î¸)istwicedifferentiableanestimatorofQ is Q(cid:98) = n 1 (cid:88) n âÎ¸ â â 2 Î¸(cid:48) Ï i (cid:161)Î¸ (cid:98) (cid:162) . i=1 WhenÏ (Î¸)isnotseconddifferentiablethenestimatorsofQ areconstructedonacase-by-casebasis. i Givenâ¦ (cid:98) andQ(cid:98) anestimatorforV is V(cid:98) =Q(cid:98) â1â¦ (cid:98)Q(cid:98) â1 . (22.4) It is possible to adjust V(cid:98) by multiplying by a degree-of-freedom scaling such as n/(nâk) where k = dim(Î¸).Thereisnoformalguidance. FormaximumlikelihoodestimatorsthestandardcovariancematrixestimatorisV(cid:98) =Q(cid:98) â1 .Thischoice isnotrobusttomisspecification. Thereforeitisrecommendedtousetherobustversion(22.4),forex- amplebyusingtheâ,râoptioninStata.Thisisunfortunatelynotuniformlydoneinpractice. Forclusteredandtime-seriesobservationstheestimatorQ(cid:98) isunalteredbuttheestimatorâ¦ (cid:98) changes. Forclusteredsamplesitis 1 (cid:88) G (cid:195) (cid:88) ng (cid:33)(cid:195) (cid:88) ng (cid:33)(cid:48) â¦ (cid:98) = Ï (cid:98)(cid:96)g Ï (cid:98)(cid:96)g . n g=1 (cid:96)=1 (cid:96)=1",
    "page": 778,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER22. M-ESTIMATORS 759 Fortime-seriesdatatheestimatorâ¦ (cid:98) isunalteredifthescoresÏ i areseriallyuncorrelated(whichoccurs whenamodelisdynamicallycorrectlyspecified).OtherwiseaNewey-Westcovariancematrixestimator canbeusedandequals â¦ (cid:98) = (cid:88) M (cid:181) 1â M |(cid:96) + | 1 (cid:182) n 1 (cid:88) Ï (cid:98)tâ(cid:96) Ï (cid:98) (cid:48) t . (cid:96)=âM 1â¤tâ(cid:96)â¤n Standard errors for the parameter estimates are formed by taking the square roots of the diagonal elementsofn â1V(cid:98). 22.8 TechnicalProofs* ProofofTheorem22.1Theproofproceedsintwosteps. First, weshowthatS(Î¸ (cid:98))ââS(Î¸). Secondwe p showthatthisimpliesÎ¸ (cid:98) ââÎ¸. p SinceÎ¸ 0 minimizesS(Î¸),S(Î¸ 0 )â¤S (cid:161)Î¸ (cid:98) (cid:162) .Hence 0â¤S (cid:161)Î¸ (cid:98) (cid:162)âS(Î¸ 0 ) =S (cid:161)Î¸ (cid:98) (cid:162)âS n (cid:161)Î¸ (cid:98) (cid:162)+S n (Î¸ 0 )âS(Î¸ 0 )+S n (cid:161)Î¸ (cid:98) (cid:162)âS n (Î¸ 0 ) â¤2sup(cid:107)S (Î¸)âS(Î¸)(cid:107)ââ0. n Î¸âÎ p ThesecondinequalityusesthefactthatÎ¸ (cid:98)minimizesS n (Î¸)soS n (cid:161)Î¸ (cid:98) (cid:162)â¤S n (Î¸ 0 )andreplacestheothertwo pairwisecomparisonsbythesupremum.Thefinalconvergenceistheassumeduniformconvergencein probability. ThepreceedingargumentisillustratedinFigure22.1(c). Thefiguredisplaystheexpectedcriterion S(Î¸)withthesolidline,andthesamplecriterionS (Î¸)isdisplayedwiththedashedline. Thedistances n betweenthetwofunctionsatthetruevalueÎ¸ 0 andtheestimatorÎ¸ (cid:98)aremarkedbythetwodash-dotted lines.ThesumofthesetwolengthsisgreaterthantheverticaldistancebetweenS (cid:161)Î¸ (cid:98) (cid:162) andS(Î¸ 0 )because thelatterdistanceequalsthesumofthetwodash-dottedlinesplustheverticalheightofthethicksection ofthedashedline(betweenS n (Î¸ 0 )andS n (cid:161)Î¸ (cid:98) (cid:162) )whichispositivesinceS n (cid:161)Î¸ (cid:98) (cid:162)â¤S n (Î¸ 0 ).Thelengthsofthe dottedlinesconvergetozeroundertheassumptionofuniformconvergence. HenceS (cid:161)Î¸ (cid:98) (cid:162) convergesto S(Î¸ ).Thiscompletesthefirststep. 0 InthesecondstepoftheproofweshowÎ¸ (cid:98) ââÎ¸.Fix(cid:178)>0.Theuniqueminimumassumptionimplies p there is a Î´ > 0 such that (cid:107)Î¸ 0 âÎ¸(cid:107) > (cid:178) implies S(Î¸)âS(Î¸ 0 ) â¥ Î´. This means that (cid:176) (cid:176) Î¸ 0 âÎ¸ (cid:98) (cid:176) (cid:176) > (cid:178) implies S (cid:161)Î¸ (cid:98) (cid:162)âS(Î¸ 0 )â¥Î´.Hence (cid:80)(cid:163)(cid:176) (cid:176) Î¸ 0 âÎ¸ (cid:98) (cid:176) (cid:176) >(cid:178)(cid:164)â¤(cid:80)(cid:163) S (cid:161)Î¸ (cid:98) (cid:162)âS(Î¸ 0 )â¥Î´(cid:164) . Theright-hand-sideconvergestozerosinceS (cid:161)Î¸ (cid:98) (cid:162)ââS(Î¸). Thustheleft-hand-sideconvergestozeroas p well.Since(cid:178)isarbitrarythisimpliesthatÎ¸ (cid:98) ââÎ¸asstated. p To illustrate, again examine Figure 22.1(c). We see S (cid:161)Î¸ (cid:98) (cid:162) marked on the graph of S(Î¸). Since S (cid:161)Î¸ (cid:98) (cid:162) convergestoS(Î¸ 0 )thismeansthatS (cid:161)Î¸ (cid:98) (cid:162) slidesdownthegraphofS(Î¸)towardstheminimum. Theonly wayforÎ¸ (cid:98)tonotconvergetoÎ¸ 0 wouldbeifthefunctionS(Î¸)wereflatattheminimum",
    "page": 779,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Thustheleft-hand-sideconvergestozeroas p well.Since(cid:178)isarbitrarythisimpliesthatÎ¸ (cid:98) ââÎ¸asstated. p To illustrate, again examine Figure 22.1(c). We see S (cid:161)Î¸ (cid:98) (cid:162) marked on the graph of S(Î¸). Since S (cid:161)Î¸ (cid:98) (cid:162) convergestoS(Î¸ 0 )thismeansthatS (cid:161)Î¸ (cid:98) (cid:162) slidesdownthegraphofS(Î¸)towardstheminimum. Theonly wayforÎ¸ (cid:98)tonotconvergetoÎ¸ 0 wouldbeifthefunctionS(Î¸)wereflatattheminimum. Thisisexcluded bytheassumptionofauniqueminimum. â  ProofofTheorem22.3Expandingthepopulationfirst-ordercondition0=Ï(Î¸ 0 )aroundÎ¸=Î¸ (cid:98)usingthe meanvaluetheoremwefind 0=Ï(cid:161)Î¸ (cid:98) (cid:162)+Q(Î¸ n â ) (cid:161)Î¸ 0 âÎ¸ (cid:98) (cid:162)",
    "page": 779,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER22. M-ESTIMATORS 760 whereÎ¸ n â isintermediate3betweenÎ¸ 0 andÎ¸ (cid:98).Solving,wefind (cid:112) (cid:112) n (cid:161)Î¸ (cid:98) âÎ¸ 0 (cid:162)=Q(Î¸ n â ) â1 nÏ(cid:161)Î¸ (cid:98) (cid:162) . TheassumptionthatÏ(Î¸)iscontinuouslydifferentiablemeansthatQ(Î¸)iscontinuousinN .SinceÎ¸â is n intermediatebetweenÎ¸ 0 andÎ¸ (cid:98)andthelatterconvergesinprobabilitytoÎ¸ 0 ,itfollowsthatÎ¸ n â converges inprobabilitytoÎ¸ aswell.ThusbythecontinuousmappingtheoremQ (cid:161)Î¸â(cid:162)ââQ(Î¸ )=Q. 0 n 0 (cid:112) p Wenextexaminetheasymptoticdistributionof nÏ(cid:161)Î¸ (cid:98) (cid:162) .Define (cid:112) v (Î¸)= n (cid:161)Ï (Î¸)âÏ(Î¸) (cid:162) . n n Animplicationofthesamplefirst-orderconditionÏ n (cid:161)Î¸ (cid:98) (cid:162)=0is (cid:112) (cid:112) nÏ(cid:161)Î¸ (cid:98) (cid:162)= n (cid:161)Ï(cid:161)Î¸ (cid:98) (cid:162)âÏ n (cid:161)Î¸ (cid:98) (cid:162)(cid:162)=âv n (cid:161)Î¸ (cid:98) (cid:162)=âv n (Î¸ 0 )+r n wherer n =v n (Î¸ 0 )âv n (cid:161)Î¸ (cid:98) (cid:162) . SinceÏ ismeanzero(see(22.3))andhasafinitecovariancematrixâ¦byassumptionitsatisfiesthe i multivariatecentrallimittheorem.Thus (cid:112) 1 (cid:88) n nÏ (Î¸)= (cid:112) Ï ââN(0,â¦). n i n i=1 d The final step is to show that r =o (1). Pick any Î·>0 and (cid:178)>0. As shown by Theorem 18.6 of n p IntroductiontoEconometrics,Assumption3impliesthatv (Î¸)isstochasticallyequicontinuous,which n meansthat(seeDefinition18.4inIntroductiontoEconometrics)given(cid:178)andÎ·thereisaÎ´>0suchthat (cid:34) (cid:35) limsup(cid:80) sup (cid:107)v (Î¸ )âv (Î¸)(cid:107)>Î· â¤(cid:178). (22.5) n 0 n nââ (cid:107)Î¸âÎ¸ (cid:107)â¤Î´ 0 Theorem22.1impliesthatÎ¸ (cid:98) ââÎ¸ 0 or p limsup(cid:80)(cid:163)(cid:176) (cid:176) Î¸ (cid:98) âÎ¸ 0 (cid:176) (cid:176) >Î´(cid:164)â¤(cid:178). (22.6) nââ Wecalculatethat limsup(cid:80)(cid:163) r n >Î·(cid:164)â¤limsup(cid:80)(cid:163)(cid:176) (cid:176)v n (Î¸ 0 )âv n (cid:161)Î¸ (cid:98) (cid:162)(cid:176) (cid:176) >Î·, (cid:176) (cid:176) Î¸ (cid:98) âÎ¸ 0 (cid:176) (cid:176) â¤Î´(cid:164)+limsup(cid:80)(cid:163)(cid:176) (cid:176) Î¸ (cid:98) âÎ¸ 0 (cid:176) (cid:176) >Î´(cid:164) nââ nââ nââ (cid:34) (cid:35) â¤limsup(cid:80) sup (cid:107)v (Î¸ )âv (Î¸)(cid:107)>Î· +(cid:178)â¤2(cid:178). n 0 n nââ (cid:107)Î¸âÎ¸ (cid:107)â¤Î´ 0 Thesecondinequalityis(22.6)andthefinalinequalityis(22.5). SinceÎ·and(cid:178)arearbitrarywededuce thatr =o (1).Weconcludethat n p (cid:112) nÏ(cid:161)Î¸ (cid:98) (cid:162)=âv n (Î¸ 0 )+r n ââN(0,â¦). d Together,wehaveshownthat (cid:112) (cid:112) n (cid:161)Î¸ (cid:98) âÎ¸ 0 (cid:162)=Q(Î¸ n â ) â1 nÏ(cid:161)Î¸ (cid:98) (cid:162)ââQ â1N(0,â¦)â¼N (cid:161) 0,Q â1â¦Q â1(cid:162) d asclaimed. â  _____________________________________________________________________________________________ 3Technically,sinceÏ(cid:161)Î¸(cid:98) (cid:162) isavector,theexpansionisdoneseparatelyforeachelementofthevectorsotheintermediatevalue variesbytherowsofQ(Î¸â ).Thisdoesnâtaffecttheconclusion. n",
    "page": 780,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER22. M-ESTIMATORS 761 22.9 Exercises Exercise22.1 TakethemodelY =X (cid:48)Î¸+e wheree isindependentofX andhasknowndensityfunction f(e)whichiscontinuouslydifferentiable. (a) ShowthattheconditionaldensityofY givenX =xis f (cid:161) yâx (cid:48)Î¸(cid:162) . (b) FindthefunctionsÏ (Î¸)andÏ (Î¸). i i (c) Calculatetheasymptoticcovariancematrix. Exercise22.2 TakethemodelY =X (cid:48)Î¸+e.Considerthem-estimatorofÎ¸withÏ (Î¸)=g (cid:161) Y âX (cid:48)Î¸(cid:162) where i i i g(u)isaknownfunction. (a) FindthefunctionsÏ (Î¸)andÏ (Î¸). i i (b) Calculatetheasymptoticcovariancematrix. Exercise22.3 FortheestimatordescribedinExercise22.2setg(u)= 1u4. 4 (a) Sketchg(u).Isg(u)continuous?Differentiable?Seconddifferentiable? (b) FindthefunctionsÏ (Î¸)andÏ (Î¸). i i (c) Calculatetheasymptoticcovariancematrix. Exercise22.4 FortheestimatordescribedinExercise22.2setg(u)=1âcos(u). (a) Sketchg(u).Isg(u)continuous?Differentiable?Seconddifferentiable? (b) FindthefunctionsÏ (Î¸)andÏ (Î¸). i i (c) Calculatetheasymptoticcovariancematrix.",
    "page": 781,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Chapter 23 Nonlinear Least Squares 23.1 Introduction Anonlinearregressionmodelisaparametericregressionfunctionm(x,Î¸)=(cid:69)[Y |X =x]whichis nonlinearintheparametersÎ¸âÎ.Wewritethemodelas Y =m(X,Î¸)+e (cid:69)[e|X]=0. Innonlinearregressiontheordinaryleastsquaresestimatordoesnotapply. Insteadtheparametersare typicallyestimatedbynonlinearleastsquares(NLLS).NLLSisanm-estimatorwhichrequiresnumerical optimization. Weillustratenonlinearregressionwiththreeexamples. OurfirstexampleistheBox-Coxregressionmodel.TheBox-Coxtransformation(BoxandCox,1964) forastrictlypositivevariablex>0is ï£± x Î»â1 ï£´ ï£´ , ifÎ»(cid:54)=0 x(Î») = ï£² Î» (23.1) ï£´ ï£´ ï£³ log(x), ifÎ»=0. TheBox-Coxtransformationcontinuouslynestslinear(Î»=1)andlogarithmic(Î»=0)functions. Figure 23.1(a) displays the Box-Cox transformation (23.1) over x â (0,2] for Î» = 2, 1, 0, 0.5, 0, and â1. The parameterÎ»controlsthecurvatureofthefunction. TheBox-Coxregressionmodelis Y =Î² +Î² X(Î»)+e 0 1 whichhasparametersÎ¸=(Î² ,Î² ,Î»).Theregressionfunctionislinearin(Î² ,Î² )butnonlinearinÎ». 0 1 0 1 (cid:161) (cid:162) Toillustratewerevisitthereducedformregression(12.87)ofriskonlog mortality fromAcemoglu, Johnson and Robinson (2001). A reasonable question is why the authors specified the equation as a (cid:161) (cid:162) regression on log mortality rather than on mortality. The Box-Cox regression model allows both as specialcases,andequals risk=Î² +Î² mortality(Î»)+e. (23.2) 0 1 OursecondexampleisaConstantElasticityofSubstitution(CES)productionfunction,whichwas introducedbyArrow,Chenery,Minhas,andSolow(1961)asageneralizationofthepopularCobb-Douglass 762",
    "page": 782,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER23. NONLINEARLEASTSQUARES 763 productionfunction.TheCESfunctionfortwoinputsis ï£± ï£´ A (cid:161)Î±X Ï +(1âÎ±)X Ï(cid:162)Î½/Ï , ifÏ(cid:54)=0 ï£´ ï£² 1 2 Y = ï£´ ï£´ ï£³ A (cid:179) X Î± X(1âÎ±) (cid:180)Î½ , ifÏ=0. 1 2 where A isheterogeneous(random)productivity, Î½>0, Î±â(0,1), andÏâ(ââ,1]. ThecoefficientÎ½is theelasticityofscale. ThecoefficientÎ±istheshareparameter. ThecoefficientÏ isare-writing1 ofthe elasticityofsubstitutionÏbetweentheinputsandsatisfiesÏ=1/(1âÏ). TheelasticitysatisfiesÏ>1if Ï>0,andÏ<1ifÏ<0. AtÏ=0weobtaintheunitelasticCobb-Douglasfunction. SettingÏ=1and Î½=1weobtainalinearproductionfunction.TakingthelimitÏâââweobtaintheLeontiefproduction function. Toillustrate,Figure23.1(b)displaystheisoquants(levelsetsoftheproductionfunction)fortheCES productionfunctionwithparameterssettotheestimatesfromtheexamplediscussedbelow. SetlogA=Î²+e.Theframeworkimpliestheregressionmodel Î½ logY =Î²+ log (cid:161)Î±X Ï +(1âÎ±)X Ï(cid:162)+e (23.3) Ï 1 2 withparametersÎ¸=(Ï,Î½,Î±,Î²). We illustrate CES production function estimation with a modification of Papageorgiou, Saam, and Schulte(2017). TheseauthorsestimateaCESproductionfunctionforelectricityproductionwhere X 1 isgenerationcapacityusingâcleanâtechnologyand X isgenerationcapacityusingâdirtyâtechnology. 2 Theyestimatethemodelusingapanelof26countriesfortheyears1995to2009.Theirgoalwastomea- suretheelasticityofsubstitutionbetweencleananddirtyelectricalgeneration. ThedatafilePPS2017is anextractoftheauthorsâdataset. Our third example is the regressionkinkmodel. This is essentially a piecewise continuous linear splinewheretheknotistreatedasafreeparameter. Themodelusedinourapplicationisthenonlinear AR(1)model Y t =Î² 1 (X tâ1 âc)â +Î² 2 (X tâ1 âc)+ +Î² 3 Y tâ1 +Î² 4 +e t (23.4) where(a)âand(a)+arethenegative-partandpositive-partfunctions,cisthekinkpoint,andtheslopes are Î² and Î² on the two sides of the kink. The parameters are Î¸ = (Î² ,Î² ,Î² ,Î² ,c). The regression 1 2 1 2 3 4 functionislinearin(Î² ,Î² ,Î² ,Î² )andnonlinearinc. 1 2 3 4 Toillustrate,Figure23.1(c)displaysaregressionkinkfunctionfromtheapplicationdiscussedbelow. Thekinkc =44ismarkedbythesquare. Youcanseethatthefunctionisupwardslopedfor X <c and downwardslopedforX >c. WeillustratetheregressionkinkmodelwithanapplicationfromB.E.Hansen(2017)whichisafor- malizationofReinhartandRogoff(2010). Thedataareatime-seriesofannualobservationsonU.S.real GDPgrowthY andtheratiooffederaldebttoGDPX fortheyears1791-2009.Reinhart-Rogoffwerein- t t terestedinthehypothesisthatthegrowthrateofGDPslowswhenthelevelofdebtexceedsathreshold. 23.2 Identification The regression model m(x,Î¸) is correctly specified if there exists a parameter value Î¸ such that 0 m(x,Î¸ )=(cid:69)[Y |X =x].TheparameterispointidentifiedifÎ¸ isunique.Incorrectly-specifiednonlinear 0 0 regressionmodelstheparameterispointidentifiedifthereisauniquetrueparameter",
    "page": 783,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". 23.2 Identification The regression model m(x,Î¸) is correctly specified if there exists a parameter value Î¸ such that 0 m(x,Î¸ )=(cid:69)[Y |X =x].TheparameterispointidentifiedifÎ¸ isunique.Incorrectly-specifiednonlinear 0 0 regressionmodelstheparameterispointidentifiedifthereisauniquetrueparameter. 1ItistemptingtowritethemodelasafunctionoftheelasticityofsubstitutionÏratherthanitstransformationÏ.However thisisunadvisedasitrenderstheregressionfunctionmorenonlinearanddifficulttooptimize.",
    "page": 783,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER23. NONLINEARLEASTSQUARES 764 x )(lx 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 0.1 8.0 6.0 4.0 2.0 0.0 2.0â 4.0â 6.0â 8.0â 0.1â l= 2 l= 1 l= 0.5 l= 0 l=- 1 X1 (a)Box-CoxTransformation 2X 0 25 50 75 100 125 150 175 200 0001 008 006 004 002 0 Y=4000 Y=3000 Y=2000 Y=1000 Y=500 0 20 40 60 80 100 120 Debt/GDP (b)CESProductionFunctionIsoquants etaR htworG PDG 8 6 4 2 0 2â 4â 6â 8â (c)RegressionKinkModel Figure23.1:NonlinearRegressionModels Assume(cid:69)(cid:163) Y2(cid:164)<â. Sincetheconditionalmeanisthebestmean-squaredpredictoritfollowsthat thetrueparameterÎ¸ satisfiestheoptimizationexpression 0 Î¸ =argminS(Î¸) (23.5) 0 Î¸âÎ where S(Î¸)=(cid:69)(cid:163) (Y âm(X,Î¸))2(cid:164) istheexpectedsquarederror.Thisexpressestheparameterasafunctionofthedistributionof(Y,X). Theregressionmodelismis-specifiedifthereisnoÎ¸ suchthatm(x,Î¸)=(cid:69)[Y |X =x]. Inthiscase we define the pseudo-true value Î¸ as the best-fitting parameter (23.5). It is difficult to give general 0 conditions under which the solution is unique. Hence identification of the pseudo-true value under mis-specificationistypicallyassumedratherthandeduced. 23.3 Estimation TheanalogestimatoroftheexpectedsquarederrorS(Î¸)isthesampleaverageofsquarederrors S (Î¸)= 1 (cid:88) n (Y âm(X ,Î¸))2. n i i n i=1 SinceÎ¸ minimizesS(Î¸)itsanalogestimatorminimizesS (Î¸) 0 n Î¸ (cid:98)nlls =argminS n (Î¸). Î¸âÎ ThisiscalledtheNonlinearLeastSquares(NLLS)estimator. ItincludesOLSasthespecialcasewhen m(X ,Î¸)islinearinÎ¸.Itisanm-estimatorwithÏ (Î¸)=(Y âm(X ,Î¸))2. i i i i AsS (Î¸)isanonlinearfunctionofÎ¸ingeneralthereisnoexplicitalgebraicexpressionforthesolution n Î¸ (cid:98)nlls .Insteaditisfoundbynumericalminimization.Chapter12ofIntroductiontoEconometricsprovides anoverview.TheNLLSresidualsaree (cid:98)i =Y i âm (cid:161) X i ,Î¸ (cid:98)nlls (cid:162) . Insomecases,includingourfirstandthirdexamplesinSection23.1,themodelm(x,Î¸)islinearin most of the parameters. In these cases a computational shortcut is to use nestedminimization (also",
    "page": 784,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER23. NONLINEARLEASTSQUARES 765 knownasconcentrationorprofiling).TakeExample1(Box-CoxRegression).GiventheBox-Coxparam- eterÎ»theregressionislinear. Thecoefficients (cid:161)Î² ,Î² (cid:162) canbeestimatedbyleastsquares,obtainingthe 0 1 residualsandsampleconcentratedaverageofsquarederrorsS â (Î»). Thelattercanbeminimizedusing n one-dimensionalmethods.TheminimizerÎ» (cid:98)istheNLLSestimatorofÎ».GivenÎ» (cid:98)nlls ,theNLLScoefficient estimators (cid:161)Î² (cid:98)0 ,Î² (cid:98)1 (cid:162) arefoundbyOLSregressionofY i onaconstantandX i (Î»(cid:98)). â2.0 â1.5 â1.0 â0.5 0.0 0.5 1.0 l 0.2 9.1 8.1 7.1 6.1 5.1 4.1 3.1 r (a)Box-CoxRegression a â3 â2 â1 0 1 7.0 6.0 5.0 4.0 3.0 2.0 1.0 0.0 l q^ 10 20 30 40 50 60 70 (b)CESProductionFunction 5.71 4.71 3.71 2.71 Threshold Parameter c (c)RegressionKinkApplication Figure23.2:SumofSquaredErrorsFunctions Weillustratewithourthreeexamples. Figure23.2(a)displaystheconcentratedaverageofsquarederrorsS â (Î»)fortheBox-Coxregression n modelappliedto(23.2),displayedasafunctionoftheBox-CoxparameterÎ». YoucanseethatS â (Î»)is n neitherquadraticnorgloballyconvex,buthasawell-definedminimumatÎ» (cid:98) =â0.77.Thisisaparameter value which produces a regression model considerably more curved than the logarithm specification usedbyAcemoglouet.al. Figure 23.2(b) displays the average of squared errors for the CES production function application, displayed as a function of (Ï,Î±) with the other parameters set at the minimizer. You can see that the minimum is obtained at (Ï,Î±)=(.36,.39). We have displayed the function S (Ï,Î±) by its contour sur- (cid:98) (cid:98) n faces. Aquadraticfunctionhasellipticalcontoursurfaces. Youcanseethatthefunctionappearstobe closetoquadraticneartheminimumbutbecomesincreasinglynon-quadraticawayfromtheminimum. â Figure23.2(c)displaystheconcentratedaverageofsquarederrorsS (c)fortheregressionkinkap- n plication. Youcanseethatthefunctionappearssimilartoaquadraticonlylocaltotheminimum. Away fromtheminimumitisclosetolinear,andfordistantvaluesisconcave. TheparameterestimatesandstandarderrorsforthethreemodelsarepresentedinTable23.1.Stan- darderrorcalculationwillbediscussedinSection23.5.ThestandarderrorsfortheBox-CoxandRegres- sion Kink models were calculated using the heteroskedasticity-robust formula, and those for the CES productionfunctionwerecalculatedbythecluster-robustformula,clusteringbycountry. TaketheBox-Coxregression. TheestimateÎ» (cid:98) =â0.77showsthattheestimatedrelationshipbetween risk and mortality has stronger curvature than the logarithm function, and the estimate Î² (cid:98)1 = â17 is negativeaspredicted.ThelargestandarderrorforÎ² (cid:98)1 ,however,indicatesthattheslopecoefficientisnot preciselyestimated. Take the CES productionfunction. The estimate Ï =0.36 ispositive, indicating that thecleanand (cid:98) dirtytechnologiesaresubstitutes",
    "page": 785,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". TaketheBox-Coxregression. TheestimateÎ» (cid:98) =â0.77showsthattheestimatedrelationshipbetween risk and mortality has stronger curvature than the logarithm function, and the estimate Î² (cid:98)1 = â17 is negativeaspredicted.ThelargestandarderrorforÎ² (cid:98)1 ,however,indicatesthattheslopecoefficientisnot preciselyestimated. Take the CES productionfunction. The estimate Ï =0.36 ispositive, indicating that thecleanand (cid:98) dirtytechnologiesaresubstitutes. TheimpliedelasticityofsubstitutionÏ=1/(1âÏ)isÏ=1.57,withits (cid:98) standarderrorcalculatedbythedeltamethod.TheestimatedelasticityofscaleÎ½=1.05isslightlyabove (cid:98) one, consistent with increasing returns to scale. The share parameter for clean technology Î±=0.39 is (cid:98)",
    "page": 785,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER23. NONLINEARLEASTSQUARES 766 Table23.1:NLLSEstimatesofExampleModels Parameter Estimate StandardError Box-CoxRegression Î² 27.5 12.4 0 Î² â17.0 15.3 1 Î» â0.77 0.28 CESProductionFunction Ï 0.36 0.29 Î½ 1.05 0.03 Î± 0.39 0.06 Î² 1.66 0.31 Ï 1.57 0.46 RegressionKinkRegression Î² 0.033 0.026 1 Î² â0.067 0.046 2 Î² 0.28 0.09 3 Î² 3.78 0.68 4 c 43.9 11.8 somewhatlessthanone-half,indicatingthatdirtytechnologyisthedominatinginput. Taketheregressionkinkfunction. TheestimatedslopeofGDPgrowthforlowdebtlevelsÎ² (cid:98)1 =0.03 is positive, and the estimated slope for high debt levels Î² (cid:98)2 =â0.07 is negative. This is consistent with theReinhart-Rogoffhypothesisthathighdebtlevelsleadtoaslowdownineconomicgrowth. Theesti- matedkinkpointisc =44%whichisconsiderablylowerthanthepostulated90%kinkpointsuggested (cid:98) byReinhart-Rogoffbasedontheirinformalanalysis. Interpreting conventional t-ratios and p-values in nonlinear models should be done thoughtfully. Thisisacontextwheretheannoyingempiricalcustomofappendingasteriskstoallâsignificantâcoef- ficientestimatesisparticularlyinappropriate. Take, forexample, theCESestimatesinTable23.1. The ât-ratioâ for Î½ is for the test of the hypothesis that Î½=0, which is a meaningless hypothesis. Similarly thet-ratioforÎ±isforanuninterestinghypothesis. Itdoesnotmakesensetoappendasteriskstothese estimates and describe them as âsignificantâ as there is no reason to take 0 as an interesting value for theparameter. SimilarlyintheBox-CoxregressionthereisnoreasontotakeÎ»=0asanimportanthy- pothesis.IntheRegressionKinkmodelthehypothesisc=0isgenerallymeaninglessandcouldeasilylie outsidetheparameterspace. 23.4 AsymptoticDistribution WefirstconsidertheconsistencyoftheNLLSestimator. WeappealtoTheorems22.1,22.2,and22.3 form-estimators.",
    "page": 786,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER23. NONLINEARLEASTSQUARES 767 Assumption23.1 1. (Y ,X )arei.i.d. i i 2. (cid:69)(cid:163) Y2(cid:164)<â. 3. (cid:69)(cid:163) m(X,Î¸)2(cid:164)<âforallÎ¸âÎ. 4. Îisbounded. 5. Forsome A<âandÎ±>0,(cid:69)|m(X,Î¸ )âm(X,Î¸ )|2â¤A(cid:107)Î¸ âÎ¸ (cid:107)Î± forall 1 2 1 2 Î¸ ,Î¸ âÎ. 1 2 6. Forall(cid:178)>0, inf S(Î¸)>S(Î¸ ). 0 Î¸:(cid:107)Î¸âÎ¸ 0 (cid:107)â¥(cid:178) Theorem23.1 ConsistencyofNLLSEstimator IfAssumption23.1holdsthenÎ¸ (cid:98) ââÎ¸ 0 asnââ. p Thefirstthreeassumptionsarestandard.Assumption23.1.4isnotessentialbutsimplifiestheproof. Assumption 23.1.5 is technical. It is used to ensure that the criterion function is not too variable so that the uniform law of large numbers can be applied. Assumption 23.1.6 is critical. It states that the minimizerÎ¸ isunique. 0 We next discuss the asymptotic distribution. We first present the main result, then discuss the as- (cid:104) (cid:105) (cid:104) (cid:105) sumptions.SetmÎ¸(x,Î¸)= â â Î¸ m(x,Î¸)andmÎ¸i =mÎ¸(X i ,Î¸ 0 ).DefineQ=(cid:69) mÎ¸i m Î¸ (cid:48) i andâ¦=(cid:69) mÎ¸i m (cid:48) Î¸i e i 2 . LetN besomeneighborhoodofÎ¸ . 0",
    "page": 787,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER23. NONLINEARLEASTSQUARES 768 Assumption23.2 ForsomeneighborhoodN ofÎ¸ , 0 1. (cid:69)[e|X]=0. 2. (cid:69)(cid:163) Y4(cid:164)<âand(cid:69)(cid:107)mÎ¸i (cid:107)4<â. 3. (cid:69)|m(X,Î¸)|4<â. 4. m(x,Î¸) and(cid:69)[mÎ¸(X i ,Î¸)]aredifferentiablefunctionsforÎ¸âN . 5. Forsome A<âandÎ±>0, (cid:69)|m(X,Î¸ )âm(X,Î¸ )|4â¤A(cid:107)Î¸ âÎ¸ (cid:107)Î± 1 2 1 2 (cid:69)(cid:107)mÎ¸(X,Î¸ 1 )âmÎ¸(X,Î¸ 2 )(cid:107)4â¤A(cid:107)Î¸ 1 âÎ¸ 2 (cid:107)Î± forallÎ¸ ,Î¸ âÎ. 1 2 (cid:104) (cid:105) 6. Q=(cid:69) mÎ¸i m Î¸ (cid:48) i >0. 7. Î¸ isintheinteriorofÎ. 0 Theorem23.2 AsymptoticNormalityofNLLSEstimator (cid:112) IfAssumptions23.1and23.2holdthen n (cid:161)Î¸ (cid:98) âÎ¸ 0 (cid:162)ââN(0,V)asnââ,where d V =Q â1â¦Q â1. Assumption 23.2.1 imposes that the model is correctly specified. If we relax this assumption the asymptoticdistributionisstillnormalbutthecovariancematrixchanges. Assumption23.2.2aremomentboundsneededforasymptoticnormality. Assumption23.2.4states thattheregressionfunctionisdifferentiableintheparameterandtheexpectationofthefirstderivative is also differentiable. This holds if m(x,Î¸) is second differentiable but is broader as expectation is a smoothingoperation.Assumption23.2.5isatechnicalassumptionwhichensurethecriterionisnottoo variableintheparameter. Assumption23.2.6iscritical. ItstatesthattheâlinearizedregressorâmÎ¸i hasafullrankpopulation designmatrix.IfthisassumptionfailsthenmÎ¸i willbemulticollinear. Assumption23.2.7requiresthattheparametersarenotontheboundaryoftheparameterspace.This isimportantasotherwisethesamplingdistributionwillbeasymmetric. Theorem23.2showsthatundergeneralconditionstheNLLSestimatorhasanasymptoticdistribu- tionwithsimilarstructuretothatoftheOLSestimator.Theestimatorconvergesataconventionalrateto anormaldistributionwithasandwich-formcovariancematrix.Furthermore,theasymptoticvarianceis identicaltothatinahypotheticalOLSregressionwiththelinearizedregressormÎ¸i .Thus,asymptotically, thedistributionofNLLSisidenticaltoalinearregression. The asymptotic distribution simplifies under conditional homoskedasticity. If (cid:69)(cid:163) e2|X (cid:164)=Ï2 then theasymptoticvarianceisV =Ï2Q â1.",
    "page": 788,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER23. NONLINEARLEASTSQUARES 769 23.5 CovarianceMatrixEstimation TheasymptoticcovariancematrixV isestimatedsimilarlytolinearregressionwiththeadjustment thatweuseanestimateofthelinearizedregressormÎ¸i .Thisestimateis â m (cid:98)Î¸i =mÎ¸ (cid:161) X i ,Î¸ (cid:98) (cid:162)= âÎ¸ m (cid:161) X i ,Î¸ (cid:98) (cid:162) . Itisbestifthederivativeiscalculatedalgebraicallybutanumericalderivative(adiscretederivative)can substitute. Take,forexample,theBox-Coxregressionmodelforwhichm(x,Î² ,Î² ,Î»)=Î² +Î² x(Î»).Wecalculate 0 1 0 1 thatforÎ»(cid:54)=0 ï£« â (cid:161)Î² +Î² x(Î»)(cid:162) ï£¶ ï£« 1 ï£¶ âÎ² 0 1 mÎ¸ (cid:161) x,Î² 0 ,Î² 1 ,Î»(cid:162)=ï£¬ ï£¬ ï£­ â â Î² â0 1 (cid:161) (cid:179) Î² Î² 0 + + Î² Î² 1 x x ( ( Î» Î» ) ) (cid:162) (cid:180) ï£· ï£· ï£¸ =ï£¬ ï£¬ ï£­ x Î» log x (x (Î» ) ) âx(Î») ï£· ï£· ï£¸ . âÎ² Î» 0 1 i Î» ForÎ»=0thethirdentryislog2(x)/2.TheestimateisobtainedbyreplacingÎ»withtheestimatorÎ» (cid:98).Hence forÎ» (cid:98) (cid:54)=0 ï£« ï£¶ 1 ï£¬ x(Î»(cid:98)) ï£· m (cid:98)Î¸i =ï£¬ ï£¬ ï£­ 1âx Î»(cid:98)+Î»x Î»(cid:98)log(x) ï£· ï£· ï£¸ . Î» (cid:98)2 Thecovariancematrixcomponentsareestimatedas Q(cid:98) = n 1 (cid:88) n m (cid:98)Î¸i m (cid:98)Î¸ (cid:48) i i=1 â¦ (cid:98) = n 1 (cid:88) n m (cid:98)Î¸i m (cid:98)Î¸ (cid:48) i e (cid:98)i 2 i=1 V(cid:98) =Q(cid:98) â1â¦ (cid:98)Q(cid:98) â1 (23.6) where e (cid:98)i =Y i âm (cid:161) X i ,Î¸ (cid:98) (cid:162) are the NLLS residuals. Standard errors are calculated conventionally as the squarerootsofthediagonalelementsofn â1V(cid:98). Iftheerrorishomoskedasticthecovariancematrixcanbeestimatedusingtheformula V(cid:98) 0=Q(cid:98) â1Ï (cid:98) 2 Ï2= 1 (cid:88) n e2. (cid:98) n (cid:98)i i=1 Iftheobservationssatisfyclusterdependencethenastandardclustervarianceestimatorcanbeused, againtreatingthelinearizedregressorestimatem (cid:98)Î¸i astheeffectiveregressor. Toillustrate,standarderrorsforourthreeestimatedmodelsaredisplayedinTable23.1.Thestandard errorsforthefirstandthirdmodelswerecalculatedusingtheformula(23.6).Thestandarderrorsforthe CESmodelwereclusteredbycountry. InsmallsamplesthestandarderrorsforNLLSmaynotbereliable. Analternativeistousebootstrap methodsforinference.Thenonparametricbootstrapdrawswithreplacementfromtheobservationpairs (Y ,X )tocreatebootstrapsamples,towhichNLLSisappliedtoobtainbootstrapparameterestimates i i Î¸ (cid:98) â . FromÎ¸ (cid:98) â wecancalculatebootstrapstandarderrorsand/orbootstrapconfidenceintervals, forex- amplebythebias-correctedpercentilemethod.",
    "page": 789,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER23. NONLINEARLEASTSQUARES 770 23.6 PanelData Considerthenonlinearregressionmodelwithanadditiveindividualeffect Y =m(X ,Î¸)+u +Îµ it it i it (cid:69)[Îµ |X ]=0. it it Toeliminatetheindividualeffectwecanapplythewithinorfirst-differencingtransformations. Ap- plyingthewithintransformationweobtain YË =mË (X ,Î¸)+ÎµË (23.7) it it it where 1 (cid:88) mË (X ,Î¸)=m(X ,Î¸)â m(X ,Î¸) it it it T i tâSi using the panel data notation. Thus mË (X ,Î¸) is the within transformation applied to m(X ,Î¸). It is it it notm (cid:161) XË ,Î¸(cid:162) . Equation (23.7) is a nonlinear panel model. The coefficient canbe estimated by NLLS. it TheestimatorisappropriatewhenX isstrictlyexogenous,asmË (X ,Î¸)isafunctionofX foralltime it it is periods. Analternativeistoapplythefirst-differencetransformation.Thusyields âY =âm(X ,Î¸)+âÎµ (23.8) it it it whereâm(X it ,Î¸)=m(X it ,Î¸)âm (cid:161) X i,tâ1 ,Î¸(cid:162) . Equation(23.8)canbeestimatedbyNLLS.Againthisre- quiresthatX isstrictlyexogenousforconsistentestimation. it If the regressors X it contains a lagged dependent variable Y i,tâ1 then NLLS is not an appropriate estimator.GMMcanbeappliedto(23.8)similartolineardynamicpanelregressionmodels. 23.7 ThresholdModels Anextremeexampleofnonlinearregressionistheclassofthresholdregressionmodels. Theseare discontinuousregressionmodelswherethekinkpointsaretreatedasfreeparameters. Theyhavebeen usedsuccesfullyineconomicstomodelthresholdeffectsandtippingpoints. Theyarealsothecoretool for the modern machine learning methods of regression trees and random forests. In this section we provideareview. Athresholdregressionmodeltakestheform Y =Î²(cid:48) X +Î²(cid:48) X 1(cid:169) Qâ¥Î³(cid:170)+e 1 1 2 2 (cid:69)[e|X]=0 whereX andX arek Ã1andk Ã1,respectively,andQisscalar.ThevariableQiscalledthethreshold 1 2 1 2 variableandÎ³iscalledthethreshold. Typically, both X and X contain an intercept, and X andQ are subsets of X . In the latter case 1 2 2 1 Î² isthechangeintheslopeatthethreshold. ThethresholdvariableQ shouldbeeithercontinuously 2 distributedorordinal. InafullthresholdspecificationX =X =X.Inthiscaseallcoefficientsswitchatthethreshold.This 1 2 regressioncanalternativelybewrittenas ï£± Î¸(cid:48) X+e, Q<Î³ ï£² 1 Y = ï£³ Î¸(cid:48) X+e, Qâ¥Î³ 2",
    "page": 790,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER23. NONLINEARLEASTSQUARES 771 whereÎ¸ =Î² andÎ¸ =Î² +Î² . 1 1 2 1 2 Asimpleyetfullthresholdmodelariseswhenthereisonlyasingleregressor X. Theregressioncan bewrittenas Y =Î± +Î² X+Î± 1(cid:169) X â¥Î³(cid:170)+Î² X 1(cid:169) X â¥Î³(cid:170)+e. 1 1 2 2 ThisresemblesaRegressionKinkmodel,butismoregeneralasitallowsforadiscontinuityatX =Î³.The RegressionKinkmodelimposestherestrictionÎ±+Î²Î³=0. Athresholdmodelismostsuitableforacontextwhereaneconomicmodelpredictsadiscontinuityin theconditionalmean.Itcanalsobeusedasaflexibleapproximationforacontextwhereitisbelievedthe conditionalmeanhasasharpnonlinearitywithrespecttoonevariable,orhassharpinteractioneffects. TheRegressionKinkmodel,forexample,doesnotallowforkinkinteractioneffects. The threshold model is critically dependent on the choice of threshold variable Q. This variable controlstheabilityoftheregressionmodeltodisplaynonlinearity. Inprinciplethiscanbegeneralized byincorporatingmultiplethresholdsinpotentiallydifferentvariablesbutthisgeneralizationislimited bysamplesizeandinformation. ThethresholdmodelislinearinthecoefficientsÎ²=(cid:161)Î² ,Î² (cid:162) andnonlinearinÎ³. TheparameterÎ³is 1 2 ofcriticalimportanceasitdeterminesthemodelâsnonlinearityâthesamplesplit. Manyempiricalapplicationsestimatethresholdmodelsusinginformaladhocmethods. Whatyou mayseeisasplittingofthesampleintoâsubgroupsâbasedonregressorcharacteristics. Whenthelatter splitisbasedonacontinuousregressorthesplitpointisexactlyathresholdparameter. Whenyousee suchtablesitisprudenttobeskeptical.Howwasthisthresholdparameterselected?Basedonintuition? Or based on data exploration? If the former do you expect the results to be informative? If the latter shouldyoutrustthereportedtests? ToillustratethresholdregressionwereviewaninfluentialpaperbyCard,MasandRothstein(2008). TheywereinterestedintheprocessofracialsegregationinU.S.cities.Acommonhypothesisconcerning the behavior of white Americans is that they are only comfortable living in a neighborhood if it has a smallpercentageofminorityresidents.Asimplemodelofthisbehavior(exploredintheirpaper)predicts thatthispreferenceleadstoanunstablemixed-raceequilibriuminthefractionofminorities. Theycall thisequilibriumthetippingpoint. Iftheminorityfractionexceedsthistippingpointtheoutcomewill changediscontinuously. Theeconomicmechanismisthatifminoritiesmoveintoaneighborhoodata roughlycontinuousrate,whenthetippingpointisreachedtherewillbeasurgeinexitsbywhiteresidents whoelecttomoveduetotheirdiscomfort. Thispredictsathresholdregressionwithadiscontinuityat thetippingpoint.ThedatafileCMR2008isanabridgedversionoftheauthorsâdataset",
    "page": 791,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Theycall thisequilibriumthetippingpoint. Iftheminorityfractionexceedsthistippingpointtheoutcomewill changediscontinuously. Theeconomicmechanismisthatifminoritiesmoveintoaneighborhoodata roughlycontinuousrate,whenthetippingpointisreachedtherewillbeasurgeinexitsbywhiteresidents whoelecttomoveduetotheirdiscomfort. Thispredictsathresholdregressionwithadiscontinuityat thetippingpoint.ThedatafileCMR2008isanabridgedversionoftheauthorsâdataset. Theauthorsuseaspecificationsimilartothefollowing âW cit =Î´ 0 1(cid:169) M citâ1 â¥Î³(cid:170)+Î´ 1 (cid:161) M citâ1 âÎ³(cid:162)1(cid:169) M citâ1 â¥Î³(cid:170) +Î² 1 M citâ1 +Î² 2 M c 2 itâ1 +Î¸(cid:48) X citâ1 +Î±+u c +e cit (23.9) wherec isthecity(MSA)2,i isacensustractwithinthecity, t isthetimeperiod(decade),âW isthe cit whitepopulationpercentagechangeinthetractoverthedecade,M isthefractionofminortiesinthe cit tract, u is a fixed effect for the city, and X are tract-level regression controls. The sample is based c cit on Census data which is collected at ten-year intervals. They estimate models for three decades; we focuson1970-1980. ThusâW isthechangeinwhitepopulationovertheperiod1970-1980andthe cit remaining variables are for 1970. The controls used in the regression are the unemployment rate, the logmeanfamilyincome,housingvacancyrate,rentershare,fractionofhomesinsingle-unitbuildings, and fraction of workers who commute by public transport. This model has n = 35,656 observations and N =104 cities. This specification allows the relationship between âW and M to be nonlinear (a 2MetropolitanStatisticalArea(MSA).Theauthorsusethe104MSAswithatleast100censustracts.",
    "page": 791,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER23. NONLINEARLEASTSQUARES 772 0.1 0.2 0.3 0.4 0.5 g 2773 0773 8673 6673 Sn (g) l ^g Fraction Minority in 1970 (a)EstimationCriterion 0891â0791 ,noitalupoP etihW ni egnahC 0.1 0.2 0.3 0.4 0.5 02 01 0 01â 02â 0.1 0.2 0.3 0.4 0.5 g (b)ThresholdRegressionEstimates 08 07 06 05 04 03 02 01 0 Multiplier Bootstrap 99% Critical Value Fn (^g) 95% 90% Fn (g) (c)TestforaThreshold Figure23.3:ThresholdRegressionâCard-Mas-Rothstein(2008)Model quadratic) with a discontinuous shift in the intercept and slope at the threshold. The authorsâ major prediction is that Î´ should be large and negative. The threshold parameter Î³ is the minority fraction 0 whichtriggersdiscontinuouswhiteoutwardmigration. As the threshold regression model is an explicit nonlinear regression the appropriate estimation methodisNLLS.SincethemodelislinearinallcoefficientsexceptforÎ³thebestcomputationaltech- nique is concentrated least squares. For each Î³ the model is linear and the coefficients can be esti- mated by least squares. This produces a concentrated average of squared errors S â (Î³) which can be n minimized to find the NLLS estimator Î³. To illustrate, the concentrated least squares criterion for the (cid:98) Card-Mas-Rothsteindataset3 isdisplayedinFigure23.3(a). Asyoucansee,thecriterionS â (Î³)ishighly n non-smooth. This is typical in threshold applications. Consequently, the criterion needs to be mini- mizedbygridsearch.Thecriterionisastepfunctionwithastepateachobservation.Afullsearchwould calculateS n â (Î³)forÎ³equallingeachvalueofM citâ1 inthesample. Asimplification(whichweemploy) istocalculatethecriterionatasmallernumberofgridpoints. Inourillustrationweuse100gridpoints equally-spacedbetweenthe0.1and0.9quantiles4ofM citâ1 .(Thesequantilesaretheboundariesofthe displayedgraph.) WhatyoucanseeisthatthecriterionisgenerallylowerforvaluesofÎ³between0.05 and0.25, andespeciallylowerforvaluesofÎ³near0.2. TheminimumisobtainedatÎ³=0.198. Thisis (cid:98) theNLLSestimator. Inthecontextoftheapplicationthismeansthatthepointestimateofthetipping pointis20%,whichmeansthatwhentheneighborhoodminorityfractionexceeds20%whitehouseholds discontinuouslychangetheirbehavior. TheremainingNLLSestimatesareobtainedbyleastsquaresre- gression(23.9)settingÎ³=Î³. (cid:98) Our estimates are reported in Table 23.2. Following Card, Mas, and Rothstein (2008) the standard errors are clustered5 by city (MSA). Examining Table 23.2 we can see that the estimates suggest that 3Usingthe1970-1980sampleandmodel(23.9). 4ItisimportantthatthesearchbeconstrainedtovaluesofÎ³whichliewellwithinthesupportofthethresholdvariable.Oth- erwisetheregressionmaybeinfeasible.Therequireddegreeoftrimming(awayfromtheboundariesofthesupport)depends ontheindividualapplication",
    "page": 792,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". (cid:98) Our estimates are reported in Table 23.2. Following Card, Mas, and Rothstein (2008) the standard errors are clustered5 by city (MSA). Examining Table 23.2 we can see that the estimates suggest that 3Usingthe1970-1980sampleandmodel(23.9). 4ItisimportantthatthesearchbeconstrainedtovaluesofÎ³whichliewellwithinthesupportofthethresholdvariable.Oth- erwisetheregressionmaybeinfeasible.Therequireddegreeoftrimming(awayfromtheboundariesofthesupport)depends ontheindividualapplication. 5Itisnotcleartomewhetherclusteringisappropriateinthisapplication.Onemotivationforclusteringisinclusionoffixed effectsasthisinducescorrelationacrossobservationswithinacluster.Howeverinthiscasethetypicalnumberofobservations perclusterisseveralhundredsothiscorrelationisnearzero.Anothermotivationforclusteringisthattheregressionerrorecit (theunobservedfactorsforchangesinwhitepopulation)iscorrelatedacrosstractswithinacity.Whileitmaybeexpectedthat attitudestowardsminoritiesamongwhitesmaybecorrelatedwithinacity,itseemslessclearthatweshouldexpectuncondi- tionalcorrelationinpopulationchanges.",
    "page": 792,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER23. NONLINEARLEASTSQUARES 773 neighborhood declines in the white population were increasing in the minority fraction, with a sharp andacceleratingdeclineabovethetippingpointof20%. Theestimateddiscontinuityisâ11.6%. Thisis nearlyidenticaltotheestimateobtainedbyCard,MasandRothstein(2008)usingadifferentestimation method. Thewhitepopulationwasalsodecreasinginresponsetotheunemploymentrate,therentershare, andtheuseofpublictransportation,butincreasinginresponsetothevacancyrate.Anotherinteresting observationisthatdespitethefactthatthesamplehasaverylarge(35,656)numberofobservationsthe standarderrorsfortheparameterestimatesareratherlargeindicatingconsiderableimprecision.Thisis mostlyduetotheclusteredcovariancematrixcalculationasthereareonlyN =104clusters. Table23.2:ThresholdEstimates:Card-Mas-Rothstein(2008)Model Variable Estimate StandardError InterceptChange â11.6 3.7 SlopeChange â74.1 42.6 MinorityFraction â54.4 28.8 MinorityFraction2 142.3 23.9 UnemploymentRate â81.1 38.8 (cid:161) (cid:162) log MeanFamilyIncome 3.4 3.6 HousingVacancyRate 324.9 40.2 RenterShare â62.7 13.6 FractionSingle-Unit â4.8 9.5 FractionPublicTransport â91.6 24.5 Intercept 14.8 na MSAFixedEffects yes Threshold 0.198 99%ConfidenceInterval [0.198,0.209] N =NumberofMSAs 104 n=Numberofobservations 35,656 Theasymptotictheoryofthresholdregressionisnon-standard.Chan(1993)showedthatundercor- rectspecificationthethresholdestimatorÎ³convergesinprobabilitytoÎ³atthefastrateO (n â1)andthat (cid:98) p theotherparameterestimatorshaveconventionalasymptoticdistributions,justifyingthestandarder- rorsasreportedinTable23.2.HealsoshowedthatthethresholdestimatorÎ³hasanon-standardasymp- (cid:98) toticdistributionwhichcannotbeusedforconfidenceintervalconstruction. B. E. Hansen (2000) derived the asymptotic distribution of Î³ and associated test statistics under a (cid:98) âsmallthresholdeffectâasymptoticframeworkforacontinuousthresholdvariableQ. Thisdistribution theorypermitssimpleconstructionofanasymptoticconfidenceintervalforÎ³. Inbrief, heshowsthat undercorrectspecification,independentobservations,andhomoskedasticity,theFstatisticfortesting thehypothesis(cid:72) :Î³=Î³ hastheasymptoticdistribution 0 0 n (cid:161) S â(cid:161)Î³ (cid:162)âS â(cid:161)Î³(cid:162)(cid:162) n 0 n (cid:98) ââÎ¾ S n â(cid:161)Î³ (cid:98) (cid:162) d where(cid:80)[Î¾â¤x]=(cid:161) 1âexp(âx/2) (cid:162)2 .(cid:112)The1âÎ±quantileofÎ¾canbefoundbysolving (cid:161) 1âexp(âc 1âÎ±/2) (cid:162)2= 1âÎ±,andequalsc 1âÎ± =â2log(1â 1âÎ±).Forexample,c .95 =7.35andc .99 =10.6. Basedontestinversionavalid1âÎ±asymptoticconfidenceintervalforÎ³isthesetofFstatisticswhich",
    "page": 793,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER23. NONLINEARLEASTSQUARES 774 arelessthanc 1âÎ±andequals C 1âÎ± = (cid:189) Î³: n (cid:161) S n â(cid:161) S Î³ â (cid:162) (cid:161) â Î³ S (cid:162) n â(cid:161)Î³ (cid:98) (cid:162)(cid:162) â¤c 1âÎ± (cid:190) = (cid:110) Î³:S n â(cid:161)Î³(cid:162)â¤S n â(cid:161)Î³ (cid:98) (cid:162) (cid:179) 1+ c 1 n âÎ±(cid:180)(cid:111) . n (cid:98) Thisisconstructednumericallybygridsearch. InourexampleC =[0.198,0.209]. Thisisanarrow 0.99 confidenceinterval. However,thisintervaldoesnottakeintoaccountclustereddependence. Basedon Hansenâs theory we can expect that under cluster dependence the asymptotic distribution Î¾ needs to be re-scaled. This will result in replacing 1+c 1âÎ±/n in the above formula with 1+Ïc 1âÎ±/n for some adjustmentfactorÏ. Thiswillwidentheconfidenceinterval. BasedontheshapeofFigure23.3(a)the adjusted confidence interval may not be too wide. However this is a conjecture as the theory has not beenworkedoutsowecannotestimatetheadjustmentfactorÏ. Empiricalpracticeandsimulationresultssuggestthatthresholdestimatestendtobequiteimprecise unlessamoderatelylargesample(e.g.,nâ¥500)isused. Thethresholdparameterisidentifiedbyobser- vationsclosetothethreshold, notbyobservationsfarfromthethreshold. Thisrequireslargesamples toensurethatthereareasufficientnumberofobservationsnearthethresholdinordertobeabletopin downitslocation Giventhecoefficientestimatestheregressionfunctioncanbeplottedalongwithconfidenceintervals calculatedconventionally. InFigure23.3(b)weplottheestimatedregressionfunctionwith95%asymp- toticconfidenceintervalscalculatedbasedonthecovariancematrixfortheestimates(Î² (cid:98)1 ,Î² (cid:98)2 ,Î´ (cid:98)1 ,Î´ (cid:98)2 ).The estimateÎ¸ (cid:98)doesnotcontributeiftheregressionfunctionisevaluatedatmeanvalues.Weignoreestima- tionoftheinterceptÎ±asitsvarianceisnotidentifiedunderclusteringdependenceandweareprimarily (cid:98) interestinthemagnitudeofrelativecomparisons. WhatweseeinFigure23.3(b)isthattheregression functionisgenerallydownwardsloped,indicatingthatthechangeinthewhitepopulationisgenerally decreasingastheminorityfractionincreases,asexpected.Thetippingeffectisvisuallystrong.Whenthe fractionminoritycrossesthetippingpointtherearesharpdecreasesinboththelevelandtheslopeof theregressionfunction. Theleveloftheestimatedregressionfunctionalsoindicatesthattheexpected changeinthewhitepopulationswitchesfrompositivetonegativeatthetippingpoint,consistentwith thesegregationhypothesis. Itisinstructivetoobservethattheconfidencebandsarequitewidedespite thelargesample.Thisislargelyduetothedecisiontouseaclusteredcovariancematrixestimator.Con- sequentlythereisconsiderableuncertaintyinthelocationoftheregressionfunction. Theconfidence bandsarewidestattheestimatedtippingpoint. The empirical results presented in this section are distinct from, yet similar to, those reported in Card,Mas,andRothstein(2008).Thisisaninfluentialpaperasitusedtherigorofaneconomicmodelto giveinsightaboutsegregationbehavior,andusedarichdetaileddatasettoinvestigatethestrongtipping pointprediction",
    "page": 794,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Theconfidence bandsarewidestattheestimatedtippingpoint. The empirical results presented in this section are distinct from, yet similar to, those reported in Card,Mas,andRothstein(2008).Thisisaninfluentialpaperasitusedtherigorofaneconomicmodelto giveinsightaboutsegregationbehavior,andusedarichdetaileddatasettoinvestigatethestrongtipping pointprediction. 23.8 TestingforNonlinearComponents Identificationcanbetrickyinnonlinearregressionmodels.Supposethat m(X,Î¸)=X (cid:48)Î²+X(Î³) (cid:48)Î´ where X (cid:161)Î³(cid:162) is a function of X and an unknown parameter Î³. Examples for X (cid:161)Î³(cid:162) include the Box-Cox transformationandX 1(cid:169) X >Î³(cid:170) .ThelatterarisesintheRegressionKinkandthresholdregressionmodels. ThemodelislinearwhenÎ´=0.Thisisoftenausefulhypothesis(sub-model)toconsider.Forexam- ple,intheCard-Mas-Rothstein(2008)applicationthisisthehypothesisofnotippingpointwhichisthe keyissueexploredintheirpaper.",
    "page": 794,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER23. NONLINEARLEASTSQUARES 775 In this section we consider tests of the hypothesis (cid:72) :Î´=0. Under (cid:72) the model is Y = X (cid:48)Î²+e 0 0 andbothÎ´andÎ³havedroppedout. Thismeansthatunder(cid:72) theparameterÎ³isnotidentified. This 0 renders standard distribution theory invalid. When the truth is Î´=0 the NLLS estimator of (cid:161)Î²,Î´,Î³(cid:162) is not asymptotically normally distributed. Classical tests excessively over-reject (cid:72) if applied with con- 0 ventionalcriticalvalues. As an example consider the threshold regression (23.9). The hypothesis of no tipping point corre- spondstothejointhypothesisÎ´ =0andÎ´ =0.UnderthishypothesistheparameterÎ³isnotidentified. 0 1 TotestthehypothesisastandardtestistorejectforlargevaluesoftheFstatistic F= n (cid:161) S(cid:101)n âS n â(cid:161)Î³ (cid:98) (cid:162)(cid:162) S â(cid:161)Î³(cid:162) n (cid:98) whereS(cid:101)n =n â1(cid:80)n i=1 (cid:161) Y i âX i (cid:48)Î² (cid:98) (cid:162)2 andÎ² (cid:98)istheleastsquarescoefficientfromtheregressionofY onX.This isthedifferencebetweentheerrorvarianceestimatorsbasedonestimatescalculatedunderthenull(S(cid:101)n ) andalternative(S â(cid:161)Î³(cid:162) ). n (cid:98) TheFstatisticcanbewrittenas F=maxF (Î³)=F (Î³) Î³ n n (cid:98) where F (Î³)= n (cid:161) S(cid:101)n âS n â(cid:161)Î³(cid:162)(cid:162) . n S â(cid:161)Î³(cid:162) n ThestatisticF (Î³)istheclassicalFstatisticforatestof(cid:72) :Î´=0whenÎ³isknown. Wecanseefromthis n 0 representationthatFisnon-standardasitisthemaximumoverapotentiallylargenumberofstatistics F (Î³). n Toillustrate,Figure23.3(c)plotstheteststatisticF (Î³)asafunctionofÎ³.Youcanseethatthefunction n iserratic,similartotheconcentratedcriterionS â(cid:161)Î³(cid:162) . Thisissensible,sinceF (Î³)isanaffinefunction n n oftheinverseofS â(cid:161)Î³(cid:162) . ThestatisticismaximizedatÎ³becauseofthisduality. Themaximumvalueis n (cid:98) F=F (Î³).InthisapplicationwefindF=62.4.Thisisextremelyhighbyconventionalstandards. n (cid:98) TheasymptotictheoryofthetesthasbeenworkedoutbyAndrewsandPloberger(1994)andB.E. Hansen (1996). In particular, Hansen shows the validity of the multiplier bootstrap for calculation of p-valuesforindependentobservations.Themethodisasfollows. 1. Ontheobservations(Y ,X )calculatetheFteststatisticfor(cid:72) against(cid:72) (oranyotherstandard i i 0 1 statisticsuchasaWaldorlikelihoodratio). 2. Forb=1,...,B: (a) GeneratenrandomvariablesÎ¾â withmeanzeroandvariance1(standardchoicesarenormal i andRademacher). (b) SetY â=e Î¾â wheree aretheNLLSresiduals. i (cid:98)i i (cid:98)i (c) On (cid:161) Y â ,X (cid:162) calculatetheFstatisticF â for(cid:72) against(cid:72) . i i b 0 1 3. Themultiplierbootstrapp-valueisp â= 1 (cid:80)B 1(cid:169) F â>F (cid:170) . n B b=1 b 4. Ifp â<Î±thetestissignificantatlevelÎ±. n â 5. CriticalvaluescanbecalcualtedasempiricalquantilesofthebootstrapstatisticsF . b",
    "page": 795,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER23. NONLINEARLEASTSQUARES 776 Instep2byoucanalternativelysetY i â=Î² (cid:98) (cid:48) Z i +e (cid:98)i Î¾â i . TestsonÎ´areinvarianttothebootstrapvalue ofÎ´.Whatisimportantisthatthebootstrapdatasatisfythenullhypothesis. Forclusteredsamplesweneedtomakeaminormodification.Writetheregressionbyclusteras Y =X Î²+X (Î³)Î´+e . g g g g Thebootstrapmethodismodifiedbyalteringsteps2aand2babove. LetN denotethenumberofclus- ters.Themodifiedalgorithmusesthefollowingsteps. 1. (a) GenerateN randomvariablesÎ¾â withmeanzeroandvariance1. g (b) SetY â=e Î¾â . g (cid:98)g g Toillustrateweapplythistesttothethresholdregression(23.9)estimatedwiththeCard-Mas-Rothstein (2008)data.WeuseB=10,000bootstrapreplications.Applyingthefirstalgorithm(suitableforindepen- dentobservations)thebootstrapp-valueis0%. The99%criticalvalueis16.7,sotheobservedvalueof F=62.4farexceedsthisthreshold. Applyingthesecondalgorithm(suitableunderclusterdependence) thebootstrapp-valueis2.6%.The95%criticalvalueis55.3andthe99%is75.2.Thustheobservedvalue ofF=62.4isâsignificantâatthe5%butnotthe1%level. Forasampleofsizen=35,656thisissurpris- inglymildsignificance. ThesecriticalvaluesareindicatedonFigure23.3(c)bythedashedlines. TheF statisticprocessbreaksthe90%and95%criticalvaluesbutnotthe99%.Thusdespitethevisuallystrong evidenceofatippingeffectfromtheprevioussectionthestatisticalevidenceofthiseffectisstrongbut notoverwhelming. 23.9 Computation Statahasabuilt-incommandnlforNLLSestimation. Youneedtospecifythenonlinearequation and give starting values for the numerical search. It is prudent to try several starting values since the algorithmisnotguaranteedtoconvergetotheglobalminimum. Estimation of NLLS in R or MATLAB requires a bit more programming but is straightforward. You writeafunctionwhichcalculatestheaveragesquarederrorS (Î¸)(orconcentratedaveragesquareder- n ror)asafunctionoftheparameters. Youthencallanumericaloptimizertominimizethisfunction.For example,inRforvector-valuedparametersthestandardoptimizerisoptim. Forscalarparametersuse optimize. 23.10 TechnicalProofs* ProofofTheorem23.1. ThestatedresultholdsbyTheorem22.1underthelatterâsAssumptions1and 2. Assumption 2 of Theorem 22.1 is satisfied by Assumption 23.1.6. Assumption 1 of Theorem 22.1 is satisfiedby Theorem22.2 underthelatterâsAssumptions1-4. Its Assumptions1 and3 are satisfiedby Assumptions23.1.1and23.1.4.ToverifyAssumption2ofTheorem22.2observethatbythec inequality r (B.5) (cid:69)(cid:163) (Y âm(X,Î¸))2(cid:164)â¤2(cid:69)(cid:163) Y2(cid:164)+2(cid:69)(cid:163) m(X,Î¸)2(cid:164)<â underAssumptions23.1.2and23.1.3. ToverifyAssumption4ofTheorem22.2,setC =(cid:161)(cid:69)(cid:163) Y2(cid:164)(cid:162)1/2 ,C = 1 2 (cid:161)(cid:69)(cid:163) m(X,Î¸)2(cid:164)(cid:162)1/2 andC = sup (cid:107)Î¸ âÎ¸ (cid:107)Î±/2whichareallfiniteunderAssumptions23.1.2-23.1.4. Then 3 1 2 Î¸ 1,Î¸ 1 âÎ",
    "page": 796,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER23. NONLINEARLEASTSQUARES 777 usingtheCauchy-Schwarzinequality(B.32), (cid:69)(cid:175) (cid:175)(Y âm(X,Î¸ 1 ))2â(Y âm(X,Î¸ 2 ))2(cid:175) (cid:175) =(cid:69)(cid:175) (cid:175)2Y (m(X,Î¸ 2 )âm(X,Î¸ 1 ))+m(X,Î¸ 1 )2âm(X,Î¸ 2 )2(cid:175) (cid:175) â¤2C (cid:161)(cid:69)|m(X,Î¸ )âm(X,Î¸ )|2(cid:162)1/2+(cid:69)|m(X,Î¸ )âm(X,Î¸ )|2 1 1 2 2 1 +2(cid:69)|m(X,Î¸ )(m(X,Î¸ )âm(X,Î¸ ))| 1 2 1 â¤2C A1/2(cid:107)Î¸ âÎ¸ (cid:107)Î±/2+A(cid:107)Î¸ âÎ¸ (cid:107)Î±+2C (cid:161)(cid:69)|m(X,Î¸ )âm(X,Î¸ )|2(cid:162)1/2 1 1 2 1 2 2 2 1 â¤(cid:161) 2A1/2C +2A1/2C +AC (cid:162)(cid:107)Î¸ âÎ¸ (cid:107)Î±/2. 1 2 3 1 2 ThisisAssumption4ofTheorem22.2. WehaveverifiedtheassumptionsofTheorem22.2asappliedto thecriterionfunctionandthustheassumptionsofTheorem22.1aresatisfied.WeconcludethatÎ¸ (cid:98) ââÎ¸ 0 p asstated. â  ProofofTheorem23.2.WeneedtoshowthattheassumptionsimplythoseofTheorem22.3.Itisconve- nienttorescalethecriterionsothatÏ i (Î¸)= 2 1(Y i âm(X i ,Î¸))2.ThenÏ i =mÎ¸i e i . ForAssumption1ofTheorem22.3,bytheCauchy-Schwarzinequality(B.32)andAssumption23.2.2 (cid:69)(cid:176) (cid:176) Ï i (cid:176) (cid:176) 2=(cid:69)(cid:107)mÎ¸i e i (cid:107)2â¤(cid:161)(cid:69)(cid:107)mÎ¸i (cid:107)4(cid:69)(cid:163) e i 4(cid:164)(cid:162)1/2<â. WenextshowAssumption2ofTheorem22.3.UsingAssumption23.2.1,wecalculatethat S(Î¸)=(cid:69)(cid:163)Ï (Î¸) (cid:164)= 1 (cid:69)(cid:163) e2(cid:164)+ 1 (cid:69)(cid:163) (m(X ,Î¸ )âm(X ,Î¸))2(cid:164) . i 2 i 2 i 0 i Thus â Ï(Î¸)= âÎ¸ S(Î¸)=â(cid:69)[mÎ¸(X i ,Î¸)(m(X i ,Î¸ 0 )âm(X i ,Î¸))] withderivative â Q(Î¸)=â âÎ¸(cid:48) (cid:69)[mÎ¸(X i ,Î¸)(m(X i ,Î¸ 0 )âm(X i ,Î¸))] â =(cid:69)(cid:163) mÎ¸(X i ,Î¸)mÎ¸(X i ,Î¸) (cid:48)(cid:164)â âÎ¸(cid:48) (cid:69)[mÎ¸(X i ,Î¸ 1 )(m(X i ,Î¸ 0 )âm(X i ,Î¸))]| Î¸ 1 =Î¸. (23.10) 1 ThisexistsandiscontinuousforÎ¸âN underAssumption23.2.4. WenextshowAssumption3ofTheorem22.3.Set C =(cid:161)(cid:69)(cid:163) Y i 4(cid:164)(cid:162)1/4+sup (cid:161)(cid:69)(cid:163) m(X i ,Î¸)4(cid:164)(cid:162)1/4+sup (cid:161)(cid:69)(cid:107)mÎ¸(X i ,Î¸)(cid:107)4(cid:162)1/4 . (23.11) Î¸âN Î¸âN Noticethat Ï i (Î¸)=mÎ¸(X i ,Î¸)(Y i âm(X i ,Î¸)) so Ï i (Î¸ 1 )âÏ i (Î¸ 2 )=(mÎ¸(X i ,Î¸ 1 )âmÎ¸(X i ,Î¸ 2 )) (cid:161) y i âm(X i ,Î¸) (cid:162)+mÎ¸(X i ,Î¸ 2 )(m(X i ,Î¸ 2 )âm(X i ,Î¸ 2 )). ThususingtheMinkowskiandCauchy-Schwarzinequalities,(23.11),andAssumption23.2.5, (cid:179) (cid:69)(cid:176) (cid:176) Ï i (Î¸ 1 )âÏ i (Î¸ 2 ) (cid:176) (cid:176) 2 (cid:180)1/2 â¤ (cid:179) (cid:161)(cid:69)(cid:163) Y i 4(cid:164)(cid:162)1/4+(cid:161)(cid:69)(cid:163) m(X i ,Î¸)4(cid:164)(cid:162)1/4 (cid:180) (cid:161)(cid:69)(cid:107)mÎ¸(X i ,Î¸ 1 )âmÎ¸(X i ,Î¸ 2 )(cid:107)4(cid:162)1/4 +(cid:161)(cid:69)(cid:107)mÎ¸(X i ,Î¸)(cid:107)4(cid:162)1/4(cid:161)(cid:69)|m(X i ,Î¸ 1 )âm(X i ,Î¸ 2 )|4(cid:162)1/4 â¤(2C+1)A1/4(cid:107)Î¸ âÎ¸ (cid:107)Î±/4. 1 2",
    "page": 797,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER23. NONLINEARLEASTSQUARES 778 ThisisAssumption3ofTheorem22.3 Assumptions4and5ofTheorem22.3areimplieddirectlybyAssumptions23.2.6and23.2.7.Thusthe assumptionsofTheorem22.3aresatisfied. ItremainstoverifytheexpressionsforQ andâ¦. Expression (23.10)evaluatedatÎ¸ is 0 â âÎ¸(cid:48) Ï(Î¸ 0 )=(cid:69)(cid:163) mÎ¸i m Î¸ (cid:48) i (cid:164) which verifies the expression forQ. We also calculate that â¦=(cid:69)(cid:163)Ï i Ï(cid:48) i (cid:164)= (cid:69)(cid:163) mÎ¸i m Î¸ (cid:48) i e i 2(cid:164) , verifying the statedexpression. ThestatedresultfollowsfromTheorem22.3. â  _____________________________________________________________________________________________ 23.11 Exercises Exercise23.1 TakethemodelY =exp(Î¸)+ewith(cid:69)[e]=0. (a) IstheconditionalmeanlinearornonlinearinÎ¸?Isthisanonlinearregressionmodel? (b) Isthereawaytoestimatethemodelusinglinearmethods?Ifso,explainhowtoobtainanestimator Î¸ (cid:98)forÎ¸. (c) Isyouranswerinpart(b)thesameastheNLLSestimator,ordifferent? Exercise23.2 TakethemodelY(Î»)=Î² +Î² X+ewith(cid:69)[e|X]=0whereY(Î»)istheBox-Coxtransforma- 0 1 tionofY. (a) Isthisanonlinearregressionmodelintheparameters(Î»,Î² ,Î² )?(Careful,thisistricky.) 0 1 Î² Exercise23.3 TakethemodelY = 1 +ewith(cid:69)[e|X]=0. Î² +Î² X 2 3 (a) Aretheparameters(Î² ,Î² ,Î² )identified? 1 2 3 (b) Ifnot,whatparametersareidentified?Howwouldyouestimatethemodel? Exercise23.4 TakethemodelY =Î² exp (cid:161)Î² X (cid:162)+ewith(cid:69)[e|X]=0. 1 2 (a) Aretheparameters(Î² ,Î² )identified? 1 2 (b) FindanexpressiontocalculatethecovariancematrixoftheNLLSestimatiors(Î² (cid:98)1 ,Î² (cid:98)2 ). Exercise23.5 TakethemodelY =m(X,Î¸)+ewithe|X â¼N(0,Ï2).FindtheMLEforÎ¸andÏ2. Exercise23.6 TakethemodelY =exp (cid:161) X (cid:48)Î¸(cid:162)+ewith(cid:69)[Ze]=0,whereX iskÃ1andZ is(cid:96)Ã1. (a) Whatrelationshipbetween(cid:96)andk isnecessaryforidentificationofÎ¸? (b) DescribehowtoestimateÎ¸byGMM. (c) Describeanestimatoroftheasymptoticcovariancematrix. Exercise23.7 SupposethatY =m(X,Î¸)+ewith(cid:69)[e|X]=0,Î¸ (cid:98)istheNLLSestimator,andV(cid:98) theestimator of var (cid:163)Î¸ (cid:98) (cid:164) . You are interested in the conditional mean function (cid:69)[Y |X =x]=m(x) at some x. Find an asymptotic95%confidenceintervalform(x).",
    "page": 798,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER23. NONLINEARLEASTSQUARES 779 Exercise23.8 The file PSS2017 contains a subset of the data from Papageorgiou, Saam, and Schulte (2017). For a robustness check they re-estimated their CES production function using approximated capital stocks rather than capacities as their input measures. Estimate the model (23.3) using this al- ternative measure. The variables for Y, X , and X are EG_total, EC_c_alt, and EC_d_alt, respectively. 1 2 ComparetheestimateswiththosereportedinTable23.1. Exercise23.9 ThefileRR2010containstheU.S.observationsfromtheReinhartandRogoff(2010). The datasethasobservationsonrealGDPgrowth,debt/GDP,andinflationrates. Estimatethemodel(23.4) settingY astheinflationrateandX asthedebtratio. Exercise23.10 InExercise9.26,youestimatedacostfunctiononacross-sectionofelectriccompanies. Considerthenonlinearspecification logTC =Î² +Î² logQ+Î² (cid:161) logPL+logPK+logPF (cid:162)+Î² logQ +e. (23.12) 1 2 3 4 1+exp (cid:161)â(cid:161) logQâÎ³(cid:162)(cid:162) Thismodeliscalledasmooththresholdmodel.ForvaluesoflogQmuchbelowÎ³,thevariablelogQhas aregressionslopeofÎ² .ForvaluesmuchaboveÎ² ,theregressionslopeisÎ² +Î² .Themodelimposesa 2 7 2 4 smoothtransitionbetweentheseregimes. (a) ThemodelworksbestwhenÎ³isselectedsothatseveralvalues(inthisexample,atleast10to15) oflogQ arebothbelowandaboveÎ³.ExaminethedataandpickanappropriaterangeforÎ³. i (b) EstimatethemodelbyNLLSusingaglobalnumericalsearchover(Î² ,Î² ,Î² ,Î² ,Î³). 1 2 3 4 (c) Estimate the model by NLLS using a concentrated numerical search over Î³. Do you obtain the sameresults? (d) Calculatestandarderrorsforalltheparametersestimates(Î² ,Î² ,Î² ,Î² ,Î³). 1 2 3 4",
    "page": 799,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Chapter 24 Quantile Regression 24.1 Introduction This chapter introduces median regression(leastabsolute deviations) and quantile regression. An excellentmonographonthesubjectisKoenker(2005). Aconventionalgoalineconometricsisestimationofimpactofavariable X onanothervariableY. We have discussed projections and conditional means but these are not the only measures of impact. Alternative measures include the conditional median and conditional quantile. We will focus on the caseofcontinuously-distributedY wherequantilesareuniquelydefined. 24.2 MedianRegression Recall that the median of Y is the value m =med[Y] such that (cid:80)[Y â¤m] =(cid:80)[Y â¥m] =0.5. The median can be thought of the âtypical realizationâ. For example, the median wage $19.23 in the CPS datasetcanbeinterpretedasthewageofaâtypicalwage-earnerâ. One-halfofwageearnershavewages lessthan$19andone-halfhavewagesgreaterthan$19. When a distribution is symmetric then the median equals the mean but when the distribution is asymmetrictheydiffer. Throughoutthistextbookwehaveprimarilyfocusedonconditionalrelationships. Forexample,the conditional mean is the expected value within a sub-population. Similarly we define the conditional medianasthemedianofasub-population. Definition24.1 TheconditionalmedianofY givenX =x isthevaluem(x)= med[Y |X =x]suchthat(cid:80)[Y â¤m(x)|X =x]=0.5. Forexample,intheCPSsamplethemedianwageformenis$21.15andthemedianwageforwomen is$16.83.Thesearethewagesofaâtypicalâmanandwoman. WecanwritetherelationshipbetweenY andX asthemedianregressionmodel: Y =m(X)+e med[e|X]=0. 780",
    "page": 800,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER24. QUANTILEREGRESSION 781 Asstatedthisissimplyadefinitionalframework.m(X)istheconditionalmediangiventherandomvari- ableX.Theerrore isthedeviationofY fromitsconditionalmedianandbydefinitionhasaconditional medianofzero. Wecallm(x)themedianregressionfunction.Ingeneralitcantakeanyshape.However,forpractical conveniencewefocusonmodelswhicharelinearinparametersm(x)=x (cid:48)Î². (Thisisnotfundamentally restrictiveasitallowsseriesapproximations.)Thisgivesrisetothelinearmedianregressionmodel: Y =X (cid:48)Î²+e (24.1) med[e|X]=0. Equivalently, themodelstatesthatmed[Y |X]=X (cid:48)Î². Asinthecaseofregressionthetruemedianre- gression function is not necessarily linear, so the assumption of linearity is a meaningful assumption. Themodelresemblesthelinearregressionmodelbutisdifferent. ThecoefficientsÎ²inthemedianand meanregressionmodelsarenotnecessarilyequaltooneanother. To estimate Î² it is useful to characterize Î² as a function of the distribution. Recall that the least squares estimator is derived from the foundational property that the mean minimizes the expected squaredloss,thatis,Âµ=argminÎ¸ (cid:69)(cid:163) (Y âÎ¸)2(cid:164) .Wenowpresentanalogouspropertiesofthemedian. Definethesignfunction d (cid:189) 1 {x>0}â1 {x<0}, x(cid:54)=0 |x|=sgn(x)= dx 0 x=0. Theorem24.1 AssumeY iscontinuouslydistributed.Thenthemedianmsat- isfies (cid:69)(cid:163) sgn(Y âm) (cid:164)=0. (24.2) Ifinaddition(cid:69)|Y|<âitsatisfies m=argmin(cid:69)|Y âÎ¸|. (24.3) Î¸ IftheconditionaldistributionF(y |x)ofY given X =x iscontinuousin y the conditionalmedianerrore=Y âm(X)satisfies (cid:69)(cid:163) sgn(e)|X (cid:164)=0. (24.4) Ifinaddition(cid:69)|Y|<âtheconditionalmediansatisfies m(x)=argmin(cid:69)[|Y âÎ¸||X =x]. (24.5) Î¸ If(Y,X)satisfythelinearmedianregressionmodel(24.1)andE|Y|<âthen thecoefficientÎ²satisfies Î²=argmin(cid:69)(cid:175) (cid:175)Y âX (cid:48) b (cid:175) (cid:175). (24.6) b TheproofisinSection24.16.",
    "page": 801,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER24. QUANTILEREGRESSION 782 Expression(24.6)isfoundational. ItshowsthatthemedianregressioncoefficientÎ²minimizesthe expectedabsolutedifferencebetweenY andthepredictedvalueX (cid:48)Î².Thisisfoundationalasitexpresses thecoefficientasafunctionoftheprobabilitydistribution. Thisresultisadirectanalogoftheproperty thatthemeanregressioncoefficientminimizestheexpectedsquaredloss. Thedifferencebetweenthe twoisthelossfunctionâthemeasureofthemagnitudeofapredictionerror.Tovisualize,Figure24.1(a) displaysthetwolossfunctions. Comparingthetwo,squaredlossputssmallpenaltyonsmallerrorsyet largepenaltyonlargeerrors.Botharesymmetricandsotreatpositiveandnegativeerrorsidentically. InapplicationsthelinearassumptionX (cid:48)Î²isunlikelytobevalidexceptinasaturateddummyvariable regression.Thusinpracticeweshouldviewalinearmodelasausefulapproximationratherthanaliteral truth. ToallowthemodeltobeanapproxiamtionwedefinethecoefficientÎ²asthebestlinearmedian predictor Î²d=ef argmin(cid:69)(cid:175) (cid:175)Y âX (cid:48) b (cid:175) (cid:175). (24.7) b Thisequalsthetrueconditionalmediancoefficientwhentheconditionalmedianislinear,butisdefined forgeneraldistributionssatisfyingE|Y|<â.Thefirstorderconditionforminimizationimpliesthat (cid:69)(cid:163) Xsgn(e) (cid:164)=0. (24.8) The facts that (24.4) holds for median regression and (24.8) for the best linear median predictor are analogs to the relationships (cid:69)[e|X]=0 and (cid:69)[Xe]=0 in the conditional mean and linear projection models. 24.3 LeastAbsoluteDeviations Theorem 24.1 shows that in the linear median regression model the median regression coefficient minimizesM(Î²)=(cid:69)(cid:175) (cid:175)Y âX (cid:48)Î²(cid:175) (cid:175),theexpectedabsoluteerror. Thesampleestimatorofthisfunctionisthe averageofabsoluteerrors M n (Î²)= n 1 (cid:88) n (cid:175) (cid:175)Y i âX i (cid:48)Î²(cid:175) (cid:175). i=1 Thisissimilartotheclassicalaverageofsquarederrorsfunctionbutinsteadistheaverageofabsoluteer- rors.Bynotsquaringtheerrors,M (Î²)putslesspenaltyonlargeerrorsrelativetotheaverageofsquared n errorsfunction. Since Î² minimizes M(Î²) which is estimated by M (Î²) the m-estimator for Î² is the minimizer of n M (Î²): n Î² (cid:98) =argminM n (Î²). Î² ThisiscalledtheLeastAbsoluteDeviations(LAD)estimatorofÎ²asitminimizesthesumofabsolute âdeviationsâofY i fromthefittedvalueX i (cid:48)Î².Thefunctionm (cid:98) (x)=x (cid:48)Î² (cid:98)isthemedianregressionestimator. TheLADestimatorÎ² (cid:98)doesnothasaclosedformsolutionsomustbefoundbynumericalminimization. TheLADresidualsaree (cid:98)i =Y i âX i (cid:48)Î² (cid:98).Theyapproximatelysatisfytheproperty 1 (cid:88) n X sgn(e )(cid:39)0. i (cid:98)i n i=1 Theapproximationholdsexactlyife (cid:54)=0foralli whichcanoccurwhenY iscontinuouslydistributed. (cid:98)i Thisisthesampleversionof(24.8). Thecriterion M (Î²)isgloballycontinuousandconvex. Itssurfaceresemblesthesurfaceofanin- n vertedcutgemstone,asitiscoveredbyanetworkofflatfacets.Thefacetsarejoinedatthenlineswhere",
    "page": 802,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER24. QUANTILEREGRESSION 783 sgn (cid:161) Y âX (cid:48)Î²(cid:162)=0. Toillustrate,Figure24.1(b)displaystheLADcriterionM (Î²)forsevenobservations1 i i n withasingleregressorandnointercept.TheLADestimatoristheminimizer. Sincethecriterionisfacetedtheminimummaybeaset. Furthermore, sincethecriterionhasdis- continuous derivatives classical minimization methods fail. The minimizer can be defined by a set of linear constraints so linear programming methods are appropriate. Fortunately for applications good estimationalgorithmsareavailableandsimpletouse. l x2 l l x Mn (b ) l Mn (b ) l l l x 0.17 0.18 0.19 0.20 0.21 0.22 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.23 0.24 0.25 0.26 b b (a) Quadratic and Absolute Loss Func- tions (b)LADCriterionwithn=7 (c)LADCriterionwithn=50,742 Figure24.1:LADCriterion InlargesampleswhenY hasacontinuousdistributionthecriterionapproachesasmoothfunction. ToillustrateFigure24.1(c)displaystheLADcriterionM (Î²)forthefulln=50,742sample.Thiscriterion n M (Î²)has11,248facets(thenumberislessthanthenumberofobservationssincetherearetiesinthe n wageobservations)butthelargenumbermakesthecriterionvisuallysmoothandclosetoquadratic. InStata,LADisimplementedbyqreg.InR,LADisimplementedbyrqinthequantregpackage. 24.4 QuantileRegression The mean and median are measures of the central tendency of a distribution. A measure of the spreadofthedistributionisitsquantiles. RecallthatforÏâ[0,1]theÏth quantile qÏ ofY isdefinedas thevaluesuchthat(cid:80)(cid:163) Y â¤qÏ (cid:164)=Ï. ThemedianisthespecialcaseÏ=0.5. Itwillbeconvenienttodefine thequantileoperator(cid:81) Ï[Y]asthesolutiontotheequation (cid:80)[Y â¤(cid:81) Ï[Y]]=Ï. Asanexample,takethedistributionofwagesfromtheCPSdataset.Themedianwageis$21.14.This tellsustheâtypicalâwageratebutnottherangeoftypicalvalues. The0.2quantileis$11.65andthe0.8 quantileis$31.25. Thisshowsusthat20%ofwageearnershadwagesof$11.65orbelowand20%had wagesof$31.25andabove. Wearealsointerestedinthequantilesofconditionaldistributions. Continuingtheaboveexample, considerthedistributionofwagesamongmenandwomen.The0.2,0.5,and0.8quantilesaredisplayed inTable24.1.Weseethatthedifferencesbetweenmenâsandwomenâswagesareincreasingbyquantile. 1ThesearesevenofthetwentyobservationsfromTable3.1.",
    "page": 803,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER24. QUANTILEREGRESSION 784 Table24.1:QuantilesofWageDistribution q q q .2 .5 .8 All $11.65 $19.23 $31.25 Men $12.82 $21.14 $35.90 Women $10.58 $16.83 $26.44 Definition24.2 TheconditionalquantileofY given X =x isthevalue qÏ(x) suchthat(cid:80)(cid:163) Y â¤qÏ(x)|X =x (cid:164)=Ï. Giventhisnotationwedefinetheconditionalquantileoperators(cid:81) Ï[Y |X =x]and(cid:81) Ï[Y |X]. The functionqÏ(x)isalsocalledthequantileregressionfunction. The conditional quantile function qÏ(x) can take any shape with respect to x. It is monotonically increasinginÏ,thusifÏ 1 <Ï 2 thenqÏ 1 (x)â¤qÏ 2 (x)forallx. l l l l l l l l l l l l Years of Education ruoH rep sralloD 4 6 8 10 12 14 16 18 20 011 001 09 08 07 06 05 04 03 02 01 90% 70% l50% 30% l10% l l l l l l l l l l l l l l l l l l l l l l l l Years of Education (a)WageQuantileRegression )ruoH rep sralloD(gol 4 6 8 10 12 14 16 18 20 4 3 2 90% 70% l50% 30% l10% l r 0.5 (x) l l l l l l l l l l l r 0.2 (x) x (b)log(Wage)QuantileRegression (c)QuantileLossforÏ=0.5andÏ=0.2 Figure24.2:QuantileRegressions ToillustratewedisplayinFigure24.2(a)theconditionalquantilefunctionofU.S.wages2 asafunc- tion of education, for Ï=0.1, 0.3, 0.5, 0.7, and 0.9. The five lines plotted are the quantile regression functions qÏ(x) with wage on the y-axis and education on the x-axis. For each level of education the conditionalquantiles qÏ(x)arestrictlyrankedinÏ,thoughforlowlevelsofeducationtheyarecloseto oneanother. Thefivequantileregressionfunctionsare(generally)increasingineducation,thoughnot monotonically. Thequantileregressionfunctionsalsospreadoutaseducationincreases; thusthegap betweenthequantilesincreaseswitheducation.Thesequantileregressionfunctionsprovideasummary oftheconditionaldistributionofwagesgiveneducation. Ausefulfeatureofquantileregressionisthatitisequivarianttomonotonetransformations. IfY = 2 Ï(Y 1 ) where Ï(y) is nondecreasing then (cid:81) Ï[Y 2 |X =x]=Ï((cid:81) Ï[Y 1 |X =x]). Alternatively, if qÏ 1(x) and qÏ 2(x)arethequantilefunctionsofY 1 andY 2 thenqÏ 2(x)=Ï(cid:161) qÏ 1(x) (cid:162) .Forexample,thequantileregression oflogwagesoneducationisthelogarithmofthequantileregressionofwagesoneduction. Thisisdis- playedinFigure24.2(b).Interestingly,thequantileregressionfunctionsoflogwagesareroughlyparallel withoneanotherandareroughlylinearineducationforlevelsabove12years. 2Calculatedusingthefullcps90mardataset.",
    "page": 804,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER24. QUANTILEREGRESSION 785 Wedefinethequantileregressionmodelanalogouslytothemedianregressionmodel: Y =qÏ(X)+e (cid:81) Ï[e|X]=0. Animportantfeatureofthequantileregressionmodelisthattheerrore isnotcenteredatzero. Instead itiscenteredsothatitsÏth quantileiszero. Thisisanormalizationbutitpointsoutthatthemeaning of the intercept changes when we move from mean regression to quantile regression and as we move betweenquantiles.Thelinearquantileregressionmodelis Y =X (cid:48)Î² Ï +e (24.9) (cid:81) Ï[e|X]=0. Recallthatthemeanminimizesthesquarederrorlossandthemedianminimizestheabsoluteerror loss.Thereisananalogforthequantile.Definethetiltedabsolutelossfunction: (cid:189) âx(1âÏ) x<0 Ï Ï(x)= xÏ xâ¥0 (24.10) =x(Ïâ1 {x<0}). ForÏ=0.5thisisthescaledabsoluteloss 1|x|. ForÏ<0.5thefunctionistiltedtotheright. ForÏ>0it 2 istiltedtotheleft. Tovisualize,Figure24.2(c)displaysthefunctionsÏ Ï(x)forÏ=0.5andÏ=0.2. The latterfunctionisatiltedversionoftheformer. ThefunctionÏ Ï(x)hascometobeknownasthecheck (cid:88) functionbecauseitresemblesacheckmark( ). LetÏ Ï(x)= d Ï Ï(x)=Ïâ1 {x<0}forx(cid:54)=0.Wenowdescribesomepropertiesofthequantileregres- dx sionfunction. Theorem24.2 Assume Y is continuously distributed. Then the quantile qÏ satisfies (cid:69)(cid:163)Ï Ï (cid:161) Y âqÏ (cid:162)(cid:164)=0. (24.11) Ifinaddition(cid:69)|Y|<âitsatisfies qÏ =argmin(cid:69)(cid:163)Ï Ï(Y âÎ¸) (cid:164) . (24.12) Î¸ IftheconditionaldistributionF(y |x)ofY given X =x iscontinuousin y the conditionalquantileerrore=Y âqÏ(X)satisfies (cid:69)(cid:163)Ï Ï(e)|X (cid:164)=0. (24.13) Ifinaddition(cid:69)|Y|<âtheconditionalquantilefunctionsatisfies qÏ(x)=argmin(cid:69)(cid:163)Ï Ï(Y âÎ¸)|X =x (cid:164) . (24.14) Î¸ If(Y,X)satisfythelinearquantileregressionmodel(24.9)andE|Y|<âthen thecoefficientÎ²satisfies Î²=argmin(cid:69)(cid:163)Ï Ï (cid:161) Y âX (cid:48) b (cid:162)(cid:164) . (24.15) b",
    "page": 805,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER24. QUANTILEREGRESSION 786 TheproofisinSection24.16. Expression (24.15) shows that the quantile regression coefficient Î² minimizes the expected check functiondistancebetweenY andthepredictedvalue X (cid:48)Î². Thisconnectsquantileregressionwithme- dianandmeanregression. AsformeanandmedianregressionweshouldthinkofthelinearmodelX (cid:48)Î²asanapproximation.In generalwethereforedefinethecoefficientÎ²asthebestlinearquantilepredictor Î² Ï d=ef argmin(cid:69)(cid:163)Ï Ï (cid:161) Y âX (cid:48) b (cid:162)(cid:164) . (24.16) b Thisequalsthetrueconditionalquantilecoefficientwhentruefunctionislinear. Thefirstordercondi- tionforminimizationimpliesthat (cid:69)(cid:163) XÏ Ï(e) (cid:164)=0. UnlikethebestlinearpredictorwedonothaveanexplicitexpressionforÎ² Ï.Howeverfromitsdefinition wecanseethatÎ² Ï willproduceanapproximation x (cid:48)Î² Ï tothetrueconditionalquantilefunction qÏ(x) withtheapproximationweightedbytheprobabilitydistributionofX. 24.5 ExampleQuantileShapes q0.9 (x) q0.9 (x) q0.9 (x) q0.7 (x) q0.7 (x) q0.7 (x) q0.5 (x) q0.3 (x) q0.5 (x) q0.5 (x) q0.3 (x) q0.3 (x) q0.1 (x) q0.1 (x) q0.1 (x) x x x (a)Linear (b)Parallel (c)CoefficientHeterogeneity Figure24.3:QuantileShapes LinearQuantileFunctions ThelinearquantileregressionmodelimpliesthatthethequantilefunctionsqÏ(x)arelinearinx.An exampleisshowninFigure24.3(a).HereweplotlinearquantileregressionfunctionsforÏ=0.1,0.3,0.5, 0.7,and0.9.InthisexampletheslopesarepositiveandincreasingwithÏ. Linearquantileregressionsareconvenientastheyaresimpletoestimateandreport.Sometimeslin- earitycanbeinducedbyjudiciouschoiceofvariabletransformation. Comparethequantileregressions inFigure24.2(a)andFigure24.2(b). Thequantileregressionfunctionsforthelevelofwagesappearto beconcave;incontrastthequantileregressionfunctionsforlogwagesareclosetolinearforeducation above12years. ParallelQuantileFunctions",
    "page": 806,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER24. QUANTILEREGRESSION 787 ConsiderthemodelY =m(X)+e withe independentof X. Let zÏ betheÏth quantileofe. Inthis casetheconditionalquantilefunctionforY isqÏ(x)=m(x)+zÏ. ThisimpliesthatthefunctionsqÏ (x) 1 andqÏ (x)areparallelsoallofthequantileregressionfunctionsaremutuallyparallel. 2 AnexampleisshowninFigure24.3(b). Hereweplotasetofquantileregressionfunctionswhichare mutuallyparallel. Inthiscontextâwhene isindependentof X and/orthequantileregressionfunctionsareparallelâ thereislittlegainedbyquantileregressionanalysisrelativetomeanregressionormedianregression.The modelshavethesameslopecoefficientsandonlydifferbytheirintercepts. Furthermore, aregression withe independentofX isahomoskedasticregression. Thusparallelquantilefunctionsisindicativeof conditionalhomoskedasticity. Once again examine the quantile regression functions for log wages displayed in Figure 24.2(b). These functions are visually close to parallel shifts of one another. Thus it appears that the log(wage) regression is close to a homoskedastic regression and slope coefficients should be relatively robust to estimation by least squares, LAD, or quantile regression. This is a strong motivation for applying the logarithmictransformationforawageregression. CoefficientHeterogeneity Consider the process Y =Î·(cid:48) X where Î·â¼N(Î²,Î£) is independent of X. We described this earlier as arandomcoefficientmodel,asthecoefficientsÎ·arespecifictotheindividual. Inthissettingthecon- ditional(cid:112)distributionofY given X =x isN(x (cid:48)Î²,x (cid:48)Î£x)sotheconditionalquantilefunctionsare qÏ(x)= x (cid:48)Î²+zÏ x (cid:48)Î£xwherezÏistheÏthquantileofN(0,1).Thesequantilefunctionsareparabolic.Anexample isshowninFigure24.3(c). 24.6 Estimation Theorem24.2showsthatinthelinearquantileregressionmodelthecoefficientÎ² ÏminimizesM(Î²;Ï)= (cid:69)(cid:163)Ï Ï (cid:161) Y âX (cid:48)Î²(cid:162)(cid:164) ,theexpectedcheckfunctionloss.Theestimatorofthisfunctionisthesampleaverage M n (Î²;Ï)= n 1 (cid:88) n Ï Ï (cid:161) Y i âX i (cid:48)Î²(cid:162) . i=1 SinceÎ² Ï minimizesM(Î²;Ï)whichisestimatedbyM n (Î²;Ï)them-estimatorforÎ² Ï istheminimizer ofM (Î²;Ï): n Î² (cid:98)Ï =argminM n (Î²;Ï). Î² ThisiscalledtheQuantileRegressionestimatorofÎ² Ï. ThecoefficientÎ² (cid:98)Ï doesnothaveaclosedform solution so must be found by numerical minimization. The minimization techniques are identical to thoseusedformedianregression;hencetypicalsoftwarepackagestreatthetwotogether. Thequantileregressionresidualse (cid:98)i (Ï)=Y i âX i (cid:48)Î² (cid:98)Ïsatisfytheapproximateproperty 1 (cid:88) n X i Ï Ï(e (cid:98)i (Ï))(cid:39)0. (24.17) n i=1 AsforLAD,(24.17)holdsexactlyife (Ï)(cid:54)=0foralli,whichoccurswithhighprobabilityifY iscontinu- (cid:98)i ouslydistributed. InStata,quantileregressionisimplementedbyqreg.InR,quantileregressionisimplementedbyrq inthequantregpackage.",
    "page": 807,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER24. QUANTILEREGRESSION 788 24.7 AsymptoticDistribution Wefirstprovideconditionsforconsistentestimation. LetÎ² Ïbedefinedin(24.16),e=Y âX (cid:48)Î² Ï,and fÏ(e|x)denotetheconditionaldensityofegivenX =x. Theorem24.3 ConsistencyofQuantileRegressionEstimator Assumethat(Y i ,X i )arei.i.d.,(cid:69)|Y|<â,(cid:69)(cid:107)X(cid:107)<â, fÏ(e|x)existsandsatisfies fÏ(e|x)â¤D<â,andtheparameterspaceforÎ²isbounded. ForanyÏâ(0,1) suchthat QÏ d=ef(cid:69)(cid:163) XX (cid:48) fÏ(0|X) (cid:164)>0 (24.18) thenÎ² (cid:98)Ï ââÎ² Ïasnââ. p TheproofisprovidedinSection24.16. Theorem24.3showsthatthequantileregressionestimatorisconsistentforthebestlinearquantile predictorcoefficientunderbroadassumptions. A technical condition is (24.18) which is used to establish uniqueness of the coefficient Î² Ï. One sufficientconditionfor(24.18)occurswhentheconditionaldensity fÏ(e|x)doesnâtdependonxate=0, thus fÏ(0|x)=fÏ(e)and QÏ =(cid:69)(cid:163) XX (cid:48)(cid:164) fÏ(0). (24.19) Inthiscontext,(24.18)holdsif(cid:69)(cid:163) XX (cid:48)(cid:164)>0and fÏ(0)>0. Theassumptionthat fÏ(e|x)doesnâtdepend on x at e =0 (we call this quantileindependence) is a traditional assumption in the early median re- gression/quantileregressionliterature,butdoesnotmakesenseoutsidethenarrowcontextwheree is independentof X. Thusweshouldavoid(24.19)wheneverpossible,andifnotviewitasaconvenient simplificationratherthanaliteraltruth.Theassumptionthat fÏ(0)>0meansthatthereareanon-trivial setofobservationsforwhichtheerrore isnearzero,orequivalentlyforwhichY isclosetoX (cid:48)Î² Ï. These aretheobservationswhichprovidethedecisiveinformationtopindownÎ² Ï. Aweakerwaytoobtainasufficientconditionfor(24.18)istoassumethatforsomeboundedsetX inthesupportof X, that(a)(cid:69)(cid:163) XX (cid:48)|X âX(cid:164)>0and(b) fÏ(0|x)â¥c >0for x âX. Thisisthesameas stating that if we truncate the regressor X to a bounded set that the design matrix is full rank and the conditionaldensityoftheerroratzeroisboundedawayfromzero. Theseconditionsareratherabstract butmild. Wenowprovidetheasymptoticdistribution. Theorem24.4 AsymptoticDistributionofQuantileRegressionEstimator In addition to the assumptions of Theorem 24.3, assume that (cid:69)(cid:107)X(cid:107)3 < â, fÏ(e|x) is continuous in e, and Î² Ï is in the interior of the parameter space. Thenasnââ (cid:112) n (cid:161)Î² (cid:98)Ï âÎ² Ï (cid:162)ââN(0,VÏ) d whereVÏ =Q â Ï 1â¦ ÏQ â Ï 1andâ¦ Ï =(cid:69)(cid:163) XX (cid:48)Ï2 Ï (cid:164) forÏ Ï =Ïâ1(cid:169) Y <X (cid:48)Î² Ï (cid:170) .",
    "page": 808,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER24. QUANTILEREGRESSION 789 TheproofisprovidedinSection24.16. Theorem24.4showsthatthequantileregressionestimatorisasymptoticallynormalwithasandwich asymptoticcovariancematrix. Asymptoticnormalitydoesnotrelyoncorrectmodelspecification,and thereforeappliesbroadlyforpracticalapplicationswherelinearmodelsareapproximationsratherthan literal truths. The proof of the asymptotic distribution relies on the theory for general m-estimators (Theorem22.3).Theorem24.4includestheleastabsolutedeviationsestimatorasthespecialcaseÏ=0.5. TheasymptoticcovariancematrixinTheorem24.4simplifiesundercorrectspecification.If(cid:81) Ï[Y |X]= X (cid:48)Î² Ïthen(cid:69)(cid:163)Ï2 Ï |X (cid:164)=Ï(1âÏ).Itfollowsthatâ¦ Ï =Ï(1âÏ)Q whereQ=(cid:69)(cid:163) XX (cid:48)(cid:164) . Combinedwith(24.19)wehavethreelevelsofasymptoticcovariancematrices. 1. General:VÏ =Q â Ï 1â¦ ÏQ â Ï 1 2. CorrectSpecification:Vc Ï =Ï(1âÏ)Q â Ï 1QQ â Ï 1 Ï(1âÏ) 3. QuantileIndependence:V0 Ï = Q â1 fÏ(0)2 ThequantileindependencecaseV0 Ïissimilartothehomoskedasticleastsquarescovariancematrix. WhileVÏ isthegenerallyappropriatecovariancematrixformula, thesimplifiedformulaV0 Ï iseas- ier to interpret to obtain intuition about the precision of the quantile regression estimator. Similarly totheleastsquaresestimatorthecovariancematrixisascalemultipleof (cid:161)(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:162)â1 . Thusitinherits therelatedpropertiesoftheleast-squaresestimator: Î² (cid:98)Ï ismoreefficientwhen X hasgreatervariance andislesscollinear. ThecovariancematrixV0 Ï isinverselyproportionalto fÏ(0)2. ThusÎ² (cid:98)Ï ismoreef- ficientwhenthedensityishighat0whichmeansthattherearemanyobservationsneartheÏth quan- tile of the conditional distribution. If there are few observations near the Ïth quantile then fÏ(0) will be small and V0 Ï large. We can also express this relationship in terms of the standard deviation Ï of e. Let u =e/Ï be the error scaled to have a unit variance, which has density gÏ(x)=ÏfÏ(Ïu). Then V0 Ï = Ï(1âÏ) Ï2(cid:161)(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:162)â1 ,whichisascaleofthehomoskedasticleastsquarescovariancematrix. gÏ(0)2 24.8 CovarianceMatrixEstimation TherearemultiplemethodstoestimatetheasymptoticcovariancematrixVÏ. Theeasiestisbased onthequantileindependenceassumption,leadingto V(cid:98) 0 Ï =Ï(1âÏ)f(cid:98)Ï(0) â2Q(cid:98) â1 Q(cid:98) = n 1 (cid:88) n X i X i (cid:48) . i=1 where f(cid:98)Ï(0) â2isanonparametricestimatorof fÏ(0) â2.Forthelatterthereareseveralproposedmethods. OneusesadifferenceinthedistributionfunctionofY.AsecondusesanonparametricestimatoroffÏ(0). AnestimatorofVc Ïassumingcorrectspecificationis V(cid:98) c Ï =Ï(1âÏ)Q(cid:98) â Ï 1 Q(cid:98)Q(cid:98) â Ï 1 whereQ(cid:98)ÏisanonparametricestimatorofQÏ.Afeasiblechoicegivenabandwidthhis Q(cid:98)Ï = 2n 1 h (cid:88) n X i X i (cid:48)1 {|e (cid:98)i |<h}. i=1",
    "page": 809,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER24. QUANTILEREGRESSION 790 AnestimatorofVÏallowingmisspecificationis V(cid:98)Ï =Q(cid:98) â Ï 1â¦ (cid:98)ÏQ(cid:98) â Ï 1 â¦ (cid:98)Ï = h 1 (cid:88) n X i X i (cid:48)Ï (cid:98) 2 iÏ i=1 Ï (cid:98)iÏ =Ïâ1(cid:169) Y i <X i (cid:48)Î² (cid:98)Ï (cid:170) . 0 c Ofthethreecovariancematrixmethodsintroducedabove(V(cid:98)Ï,V(cid:98)Ï, andV(cid:98)Ï)theclassicalestimator 0 V(cid:98)Ï should be avoided for the same reasons why we avoid classical homoskedastic covariance matrix estimators for least squares estimation. Of the two robust estimators the better choice is V(cid:98)Ï (since it does not require correct specification) but unfortunately it is not programmed in standard packages. c ThismeansthatinpracticetheestimatorV(cid:98)Ïisrecommended. Themostcommonmethodforestimationofquantileregressioncovariancematrices, standarder- rors,andconfidenceintervalsisthebootstrap. Theconventionalnonparametricbootstrapisappropri- ate for thegeneral modelallowingfor misspecification, andthebootstrapvarianceisanestimator for V(cid:98)Ï. As we have learned in our study of bootstrap methods, it is generally advised to use a large num- berB ofbootstrapreplications(atleast1000,with10,000preferred). Thisissomewhatcomputationally costlyinlargesamplesbutthisshouldnotbeabarriertoimplementationasthefullbootstrapcalcula- tiononlyneedstobedoneforthefinalcalculation. Also, aswehavelearned, forconfidenceintervals percentile-based intervals are greatly preferred over the normal-based intervals (which use bootstrap standarderrorsmultipliedbynormalquantiles).IrecommendtheBCpercentileintervals.Thisrequires changingthedefaultsettingsincommonprogramssuchasStata. 0 In Stata, quantile regression is implemented using qreg. The default standard errors areV(cid:98)Ï. Use c vce(robust)forV(cid:98)Ï. ThecovariancematrixestimatorV(cid:98)Ï isnotimplemented. Forbootstrapstandard errorsandconfidenceintervalsusebootstrap, reps(#): qreg y x. Thebootstrapcommandfol- lowedbyestat bootstrapproducesBCpercentileconfidenceintervals. InR,quantileregressionisimplementedbythefunctionrqinthequantregpackage. Thedefault c standarderrorsareV(cid:98)Ï.ThecovariancematrixestimatorV(cid:98)Ïisnotimplemented.Forbootstrapstandard errorsonemethodistousetheoptionse=(cid:16)boot(cid:17)withthesummarycommand.Atpresent,thequantreg packagedoesnotincludebootstrappercentileconfidenceintervals. 24.9 ClusteredDependence Underclustereddependencetheasymptoticcovariancematrixchanges.IntheformulaVÏ =Q â Ï 1â¦ ÏQ â Ï 1 thematrixQÏisunalteredbutâ¦ Ïchangesto â¦c Ï luster= n l â im ân 1 g (cid:88) G =1 (cid:69) (cid:34)(cid:195) (cid:96) (cid:88) n = g 1 X(cid:96)g Ï (cid:96)gÏ (cid:33)(cid:195) (cid:96) (cid:88) n = g 1 X(cid:96)g Ï (cid:96)gÏ (cid:33)(cid:48)(cid:35) . Thiscanbeestimatedas â¦ (cid:98) c Ï luster= 1 (cid:88) G (cid:34)(cid:195) (cid:88) ng X(cid:96)g Ï (cid:98)(cid:96)gÏ (cid:33)(cid:195) (cid:88) ng X(cid:96)g Ï (cid:98)(cid:96)gÏ (cid:33)(cid:48)(cid:35)",
    "page": 810,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Thiscanbeestimatedas â¦ (cid:98) c Ï luster= 1 (cid:88) G (cid:34)(cid:195) (cid:88) ng X(cid:96)g Ï (cid:98)(cid:96)gÏ (cid:33)(cid:195) (cid:88) ng X(cid:96)g Ï (cid:98)(cid:96)gÏ (cid:33)(cid:48)(cid:35) . n g=1 (cid:96)=1 (cid:96)=1 Thisleadstothecluster-robustasymptoticcovariancematrixestimatorV(cid:98) c Ï luster=Q(cid:98) â Ï 1â¦ (cid:98) c Ï lusterQ(cid:98) â Ï 1 . cluster The cluster-robust estimator V(cid:98)Ï is not implemented in Stata nor in the R quantreg package. Instead,theclusteredbootstrap(samplingclusterswithreplacement)isrecommended.",
    "page": 810,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER24. QUANTILEREGRESSION 791 In Stata, the clustered bootstrap can be accomplished by: bootstrap, reps(#) cluster(id): qreg y x,followedbyestat bootstrap. InR,theclusteredbootstrapisincludedasanoptioninthequantregpackageforcalculationofstan- darderrors. We illustrate the application of clustered quantile regression using the Duflo, Dupas and Kremer (2011)schooltrackingapplication. (SeeSection4.23.) Recall,thequestionwaswhetherornottracking (separatingstudentsintoclassroomsbasedonaninitialtest)influencedaverageend-of-yearscores.We repeattheanalysisusingquantileregression.Parameterestimatesandbootstrapstandarderrors(calcu- latedbyclusteredbootstrapusing10,000replications,clusteredbyschool)arereportedinTable24.2. Theresultsaremixed.Thepointestimatessuggestthatthereisastrongereffectoftrackingathigher quantilesthanlowerquantiles.Thisisconsistentwiththepremisethattrackingaffectsstudentshetero- geneously, hasnonegativeeffects, andhasthegreatestimpactontheupperend. Thestandarderrors and confidence intervals, however, are also larger for the higher quantiles, such that the quantile re- gressioncoefficientsathighquantilesareimpreciselyestimated. Usingthettest, twoofthefiveslope coefficientsare(borderline)statisticallysignificantatthe5%levelandoneatthe10%level. Inapparant contradiction, all five of the 95% BC percentile intervals include 0. Overall the evidence that tracking affectsstudentperformanceisweak. Table24.2:QuantileRegressionsofStudentTestscoresonTracking Ï=0.1 Ï=0.3 Ï=0.5 Ï=0.7 Ï=0.9 tracking 0.069 0.136 0.125 0.185 0.151 bootstrapstandarderror (0.045) (0.069) (0.074) (0.127) (0.126) 95%confidenceinterval [â0.02,.15] [â0.01,.27] [â0.01,.28] [â0.06,.44] [â0.11,.40] 24.10 QuantileCrossings ApropertyofthequantileregressionfunctionsqÏ(x)isthattheyaremonotonicallyincreasinginÏ. Thismeansthatquantilefunctionsfordifferentquantiles,e.g.qÏ 1 (x)andqÏ 2 (x)forÏ 1 (cid:54)=Ï 2 ,cannotcross eachother. Howeverapropertyoflinearfunctionsx (cid:48)Î²withdifferingslopesisthattheywillnecessarily crossifthesupportforX issufficientlylarge.Thisisapotentialprobleminapplicationsaspracticaluses ofestimatedquantilefunctionsmayrequiremonotonicityinÏ(forexampleiftheyaretobeinvertedto obtainaconditionaldistributionfunction). Thisisonlyaprobleminpracticalapplicationsifestimatedquantilefunctionsactuallycross.Ifthey donotthisissuecanbeignored. Howeverwhenestimatedquantileregressionfunctionscrossonean- otheritcanbeprudenttoaddresstheissue. To illustrate examine Figure 24.4(a). This shows estimated linear quantile regressions of wage on educationinthefullcps09mardataset.ThesearelinearprojectionapproximationstotheplotsinFigure 24.2(a). Sincetheactualquantileregressionfunctionsareconvextheestimatedlinearmodelscrossone anotheratloweducationlevels.Thisisthequantileregressioncrossingphenomenon. Whenquantileregressionscrossoneanotherthereareseveralpossibleremedies. First,youcouldre-specifythemodel",
    "page": 811,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". To illustrate examine Figure 24.4(a). This shows estimated linear quantile regressions of wage on educationinthefullcps09mardataset.ThesearelinearprojectionapproximationstotheplotsinFigure 24.2(a). Sincetheactualquantileregressionfunctionsareconvextheestimatedlinearmodelscrossone anotheratloweducationlevels.Thisisthequantileregressioncrossingphenomenon. Whenquantileregressionscrossoneanotherthereareseveralpossibleremedies. First,youcouldre-specifythemodel. IntheexampleofFigure24.4(a)theproblemarisesinpartbe- causethetruequantileregressionfunctionsareconvexandpoorlyapproximatedbylinearfunctions.In thisexampleweknowthatanimprovedapproximationisobtainedthroughalogarithmictransformation forwages.Afteralogtransformationthequantileregressionfunctionsaremuchbetterapproximatedby linearity. Indeed,suchestimates(obtainedbyquantileregressionoflogwagesoneducation,andthen",
    "page": 811,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER24. QUANTILEREGRESSION 792 education egaw 0 2 4 6 8 10 12 14 16 18 20 07 06 05 04 03 02 01 0 01â q0.9 q0.7 q0.5 q0.3 q0.1 0 2 4 6 8 10 12 14 16 18 20 education (a)LinearModel 07 06 05 04 03 02 01 0 01â q0.9 q0.7 q0.5 q0.3 q0.1 0 2 4 6 8 10 12 14 16 18 20 education (b)LogarithmicModel 07 06 05 04 03 02 01 0 01â q0.9 q0.7 q0.5 q0.3 q0.1 (c)LinearSpline Figure24.4:QuantileCrossings applyingtheexponentialtransformationtoreturntotheoriginalunits)aredisplayedinFigure24.4(b). ThesefunctionsaresmoothapproximationsandarestrictlymonotonicinÏ.Problemsolved. Whilethelogarithmic/exponentialtransformationworkswellforawageregression,itisnotageneric solution. If the underlying quantile regressions are non-linear in X, an improved approximation (and possibleeliminationofthequantilecrossing)maybeobtainedbyanonlinearorsimpleseriesapproxi- mation. AvisualexaminationofFigure24.2(a)suggeststhatthefunctionsmaybepiecewiselinearwith a kink at 11 years of education. This suggests a linear spline with a single knot at x =11. The results fromfittingthismodelaredisplayedinFigure24.4(c). Thefittedquantileregressionfunctionsexhibita substantialchangeinslopeatthekinkpointandarestrictlymonotonicinÏ.Problemsolved. Asecondapproachistoreassesstheempiricaltask.ExaminingFigure24.4(a)weseethatthecrossing phenomenonoccursatverylowlevelsofeducation(4years)forwhichthereareveryfewobservations. Thismaynotbeviewedasanempiricalinterestingregion.Asolutionistotruncatethedatatoeliminate observationswithloweducationlevels. A third approach is to constrain the estimated functions to satisfy monotonicity. Examine Figure 24.4(a).ThefiveregressionfunctionsareincreasingwithincreasingslopesandthesupportforX is[0,20] soitisnecessaryandsufficienttoconstrainthefiveinterceptstobemonotonicallyranked. Thiscanbe imposedonthisexamplebysequentiallyimposingcross-equationequalityconstraints. TheRfunction rqhasanoptiontoimposeparametercontraints.Thisapproachmaybefeasibleifthequantilecrossing problemismild. Afinalapproachisrearrangement. Foreachx takethefiveestimatedquantileregressionfunctions asdisplayedinFigure24.4(a)andrearrangetheestimatessothattheysatisfythemonotonicityrequire- ment. This does not alter the coefficient estimates, only the estimated quantile regressions. This ap- proachisflexibleandworksingeneralcontextswithouttheneedformodelre-specification. Fordetails seeChernozhukov, Fernandez-Val, andGalichon(2010). TheRpackagequantregincludestheoption rearrangetoimplementtheirprocedure. Ofthesefourapproaches,myrecommendationistostartwithacarefulandthoughtfulre-specification ofthemodel.",
    "page": 812,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER24. QUANTILEREGRESSION 793 24.11 QuantileCausalEffects One question which frequently arises in the study of quantile regression is âCan we interpret the quantile regression causally?â We can partially answer this question in the treatment response frame- workbyprovidingconditionsunderwhichthequantileregressionderivativesequalquantiletreatment effects. RecallthatthetreatmentresponsemodelisY =h(D,X,U)whereY istheoutcome,D isthetreat- mentvariable, X arecontrols,andU isanunobservedstructuralrandomerror. Forsimplicitytakethe casethatD isbinary.ForconcretenessletY bewage,D collegeattendence,andU unobservedability. Inthisframework,thecausaleffectofD onY is C(X,U)=h(1,X,U)âh(0,X,U). Ingeneralthisisheterogeneous. Whiletheaveragecausaleffectistheexpectationofthisrandomvari- able,thequantiletreatmenteffectisitsÏth conditionalquantile QÏ(x)=(cid:81) Ï[C(X,U)|X =x]. InSection2.30wepresentedanexampleofapopulationofJennifersandGeorgeswhohaddifferential wageeffectsfromcollegeattendence.InthisexampletheunobservedeffectU isapersonâstype(Jennifer or George). The quantile treatment effectQÏ traces out the distribution of the causal effect of college attendenceandisthereforemoreinformativethantheaveragetreatmenteffectalone. Fromobservationaldatawecanestimatethequantileregressionfunction qÏ(d,x)=(cid:81) Ï[Y |D=d,X =x]=(cid:81) Ï[h(D,X,U)|D=d,X =x] anditsimpliedeffectofD onY: DÏ(x)=qÏ(1,x)âqÏ(0,x). Thequestionis: UnderwhatconditiondoesDÏ =QÏ? Thatis,whendoesquantileregressionmeasure thecausaleffectofD onY? Assumption24.1 ConditionsforQuantileCausalEffect 1. TheerrorU isrealvalued. 2. ThecausaleffectC(x,u)ismonotonicallyincreasinginu. 3. Thetreatmentresponseh(D,X,u)ismonotonicallyincreasinginu. 4. ConditionalonX therandomvariablesD andU areindependent. Assumption24.1.1excludesmulti-dimensionalunobservedheterogeneity. Assumptions24.1.2and 24.1.3 are known as monotonicity conditions. A single monotonicity assumption is not restrictive (it is similar to a normalization) but the two conditions together are a substantive restriction. Take, for example, the case of the impact of college attendence on wages. Assumption 24.1.2 requires that the wagegainfromattendingcollegeisincreasinginlatentabilityU (given X). Assumption24.1.3further requires that wages are increasing in latent abilityU whether or not an individual attends college. In",
    "page": 813,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER24. QUANTILEREGRESSION 794 our Jennifer and George example these assumptions require that Jennifer receives a higher wage than Georgeiftheybotharehighschoolgraduates,iftheyarebothcollegegraduates,andthatJenniferâsgain fromattendingcollegeexceedsGeorgeâsgain. TheseconditionsweresatisfiedintheexampleofSection 2.30butwithatweakwecanchangethemodelsothatoneofthemonotonicityconditionsisviolated. Assumption24.1.4isthetraditionalconditionalindependenceassumption.Thisassumptioniscrit- icalforthecausaleffectinterpretation. Theideaisthatbyconditioningonasufficientlyrichsetofvari- ablesX anyendogeneitybetweenD andU hasbeeneliminated. Theorem24.5 QuantileCausalEffectIfAssumption24.1holdsthenDÏ(x)= QÏ(x),thequantileregressionderivativeequalsthequantiletreatmenteffect. TheproofisinSection24.16. Theorem 24.5 provides conditions under which quantile regression is a causal model. Under the conditional independence and monotonicity assumptions the quantile regression coefficients are the marginalcausaleffectsofthetreatmentvariableD uponthedistributionofY. Thecoefficientsarenot themarginalcausaleffectsforspecificindividuals,rathertheyarethecausaleffectforthedistribution. Theorem24.5showsthatundersuitableassumptionswecanlearnmorethanjusttheaveragetreatment effectâwecanlearnthedistributionoftreatmenteffects. 24.12 RandomCoefficientRepresentation Forsometheoreticalpurposesitisconvenienttowritethequantileregressionmodelusingarandom coefficientrepresentation.Thisalsoprovidesanalternativeinterpretationofthecoefficients. RecallthatwhenY hasacontinuousandinvertibledistributionfunctionF(y)theprobabilityintegral transformationisU =F(Y)â¼U[0,1].Astheinverseofthedistributionfunctionisthequantilefunction, this implies thatwe canwriteY =q , the quantile functionevaluatedatthe random variableU. The U intuitionisthatU istheârelativerankâofY. SimilarlywhentheconditionaldistributionF (cid:161) y|x (cid:162) ofY givenX isinvertible,theprobabilityintegral transformation isU =F(Y |X)â¼U[0,1] which is independent of X. Here,U is the relative rank of Y withintheconditionaldistribution. InvertingweobtainY =q (X). Thereisnoadditionalerrorterm U e astherandomnessiscapturedbyU. TheequationY =q (X)isarepresentationoftheconditional U distributionofY givenX,notastructuralmodel. Howeveritdoesimplyamechanismbywhichwecan generateY.First,drawU â¼U[0,1].Second,drawX fromitsmarginaldistibution.Third,setY =q (X). U IfweinterpretY =q (X)asastructuralmodel(thatis,takeU asastructuralunobservablevariable, U notmerelyaderivationbasedontheprobabilityintegraltransformation)thenwecanviewU asanin- dividualâslatentrelativerankwhichisinvarianttoX. EachpersonisidentifiedwithaspecificU =Ï. In thisframeworkthequantileslope(thederivativeofthequantileregression)isthequantilecausaleffect ofX onY.ThisrepresentationsatisfiestheconditionsofTheorem24.5sinceU isindependentofX. Inthelinearquantileregressionmodel(cid:81) Ï[Y |X]=X (cid:48)Î² Ï,therandomcoefficient3 representationis Y =X (cid:48)Î²",
    "page": 814,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". EachpersonisidentifiedwithaspecificU =Ï. In thisframeworkthequantileslope(thederivativeofthequantileregression)isthequantilecausaleffect ofX onY.ThisrepresentationsatisfiestheconditionsofTheorem24.5sinceU isindependentofX. Inthelinearquantileregressionmodel(cid:81) Ï[Y |X]=X (cid:48)Î² Ï,therandomcoefficient3 representationis Y =X (cid:48)Î² . U 3ThecoefficientsdependsonU soarerandom,butthemodelisdifferentfromtherandomcoefficientmodelwhereeach individualâscoefficientisarandomvector.",
    "page": 814,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER24. QUANTILEREGRESSION 795 24.13 NonparametricQuantileRegression AsemphasizedinSection24.10, quantileregressionfunctionsareundoubtedlynonlinearwithun- knownfunctionalformandhencenonparametric.Quantileregressionfunctionsmaybeestimatedusing standardnonparametricmethods.Thisisapotentiallylargesubject.Forbrevitywebrieflydiscussseries methodswhichhavetheadvantagethattheyareeasilyimplementedwithconventionalsoftware. Thenonparametricquantileregressionmodelis Y =qÏ(X)+e (cid:81) Ï[e|X]=0. ThefunctionqÏ(x)canbeapproximatedbyaseriesregressionasdescribedinChapter20. Forexample, apolynomialapproximationis Y =Î² +Î² X+Î² X2+Â·Â·Â·+Î² XK +e 0 1 2 K K (cid:81) Ï[e K |X](cid:39)0. Asplineapproximationisdefinedsimilarly. For any K the coefficients and regression function qÏ(x) can be estimated by quantile regression. AsinseriesregressionthemodelorderK shouldbeselectedtotradeoffflexibility(biasreduction)and parsimony(variancereduction).AsymptotictheoryrequiresthatK ââasnââbutataslowerrate. AnimportantpracticalquestionishowtoselectK inagivenapplication. Unfortunately, standard information criterion (such as the AIC) do not apply for quantile regression and it is unclear if cross- validationisanappropriatemodelselectiontechnique. Undoubtedlythesequestionsareanimportant topicforfuturestudy. Toillustratewerevisitthenonparametricpolynomialestimatesoftheexperienceprofileforcollege- educatedwomenearlierdisplayedinFigure20.1. Weestimate4 logwagequantileregressionsona5th- orderpolynomialinexperienceanddisplaytheestimatesinFigure24.5.Therearetwonotablefeatures. First,theÏ=0.1quantilefunctionpeaksatalowlevelofexperience(about10years)andthendeclines substantiallywithexperience. Thisislikelyanindicatorofthewage-pathofwomenonthelowendof thepayscale. Second,eventhoughthisisinalogarithmicscalethegapsbetwenthequantilefunctions substantiallywidenwithexperience. Thismeansthatheterogeneityinwagesincreasesmorethanpro- portionatelyasexperienceincreases. 24.14 PanelData Givenapaneldatastructure{Y ,X }itisnaturaltoconsiderapaneldataquantileregressionesti- it it mator.AlinearmodelwithanindividualeffectÎ± iÏis (cid:81) Ï[Y it |X it ,Î± i ]=X i (cid:48) t Î² Ï +Î± iÏ. Itseemsnaturaltoconsiderestimationbyoneofourstandardmethods: (1)Removetheindividualef- fect by the within transformation; (2) Remove the invidual effect by first differencing; (3) Estimate a full quantile regression model using the dummy variable representation. However, all of these meth- odsfail. Thereasonwhymethods(1)and(2)failarethesame: Thequantileoperator(cid:81) Ï isnotalinear operator. Thewithintransformationof(cid:81) Ï[Y it |X it ,Î± iÏ]doesnotequal(cid:81) Ï (cid:163) YË it |X it ,Î± iÏ (cid:164) ,andsimilarly â(cid:81) Ï[Y it |X it ,Î± iÏ](cid:54)=(cid:81) Ï[âY it |X it ,Î± iÏ]. Thereasonwhy(3)failsistheincidentalparametersproblem. A 4Thesampleisthen=5199observationsofwomenwithacollegedegree(16yearsofeducation).",
    "page": 815,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER24. QUANTILEREGRESSION 796 0 10 20 30 40 0.4 5.3 0.3 5.2 0.2 Labor Market Experience (Years) ruoH rep sralloD goL q 0.9 q 0.7 q 0.5 q 0.3 q 0.1 Figure24.5:log(wage)/experienceProfileQuantileRegressionsforCollege-EducatedWomen dummyvariablemodelhasthenumberofparametersproportionaltosamplesizeandinthiscontext nonlinearestimators(includingquantileregression)areinconsistent. Therehavebeenseveralproposalstodealwiththisissuebutnoneareparticularlysatisfactory. We presenthereamethodduetoCanay(2011)whichhastheadvantageofsimplicityandwideapplicability. Thesubstantiveassumptionisthattheindividualeffectiscommonacrossquantiles: Î± iÏ =Î± i . ThusÎ± i shiftsthequantileregressionsupanddownuniformly.ThisisasensibleassumptionwhenÎ± represents i omittedtime-invariantvariableswithcoefficientswhichdonotvaryacrossquantiles. Giventhisassumptionwecanwritethequantileregressionmodelas Y =X (cid:48) Î²(Ï)+Î± +e . it it i it WecanalsousetherandomcoefficientrepresentationofSection24.12towrite Y it =X i (cid:48) t Î²(U iÏ)+Î± i whereU iÏ â¼U[0,1]isindependentof(X it ,Î± i ).Takingconditionalexpectationsweobtainthemodel Y =X (cid:48) Î¸+Î± +u it it i it whereÎ¸=(cid:69)(cid:163)Î²(U iÏ) (cid:164) andu it isconditionallymeanzero. ThecoefficientÎ¸ isaweightedaverageofthe quantileregressioncoefficientsÎ²(Ï). Canayâsestimatortakesthefollowingsteps. 1. EstimateÎ± i byfixedeffectsÎ± (cid:98)i asin(17.51).[EstimateÎ¸bythewithinestimatorÎ¸ (cid:98)andÎ± i bytaking averagesofY it âX i (cid:48) t Î¸ (cid:98)foreachindividual.] 2. EstimateÎ²(Ï)byquantileregressionofY âÎ± onX . it (cid:98)i it ThekeytoCanayâsestimatoristhattheassumptionthatthefixedeffectÎ± doesnotvaryacrossthe i quantiles Ï, which means that the fixed effects can be estimated by conventional fixed effects. Once eliminatedwecanapplyconventionalquantileregression. Theprimarydisadvantageofthisapproach isthattheassumptionthatÎ± doesnotvaryacrossquantilesisrestrictive. Ingeneralthetopicofpanel i quantileregressionisapotentiallyimportanttopicforfurthereconometricresearch.",
    "page": 816,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER24. QUANTILEREGRESSION 797 24.15 IVQuantileRegression As we studied in Chapter 12, in many structural economic models some regressors are potentially endogenous,meaningjointlydependentwiththeregressionerror.Thissituationequallyarisesinquan- tile regression models. A standard method to handle endogenous regressors is instrumental variables regression which relies on a set of instruments Z which satisfy an uncorrelatedness or independence condition. Similarmethodscanbeappliedinquantileregressionthoughthetechniquesarecomputa- tionallymoredifficult,thetheorylesswelldeveloped,andapplicationslimited. Themodelis Y =X (cid:48)Î² Ï +e (cid:81) Ï[e|Z]=0 whereX andÎ² ÏarekÃ1,Z is(cid:96)Ã1,amd(cid:96)â¥k.Thedifferencewiththeconventionalquantileregression modelisthatthesecondequationisconditionalonZ ratherthanX. Theassumptionontheerrorimpliesthat(cid:69)(cid:163)Ï Ï(e)|Z (cid:164)=0. Thisholdsbythesamederivationasfor the quantile regression model. This is a conditional moment equation. It implies the unconditional momentequation5(cid:69)(cid:163) ZÏ Ï(e) (cid:164)=0.Writtenasafunctionoftheobservationsandparameters (cid:69)(cid:163) ZÏ Ï (cid:161) Y âX (cid:48)Î² Ï (cid:162)(cid:164)=0. Thisisasetof(cid:96)momentequationsforkparameters.AsuitableestimationmethodisGMM.Acom- putational challenge is that the moment condition functions are discontinuous in Î² Ï so conventional minimizationtechniquesfail. ThemethodofIVquantileregressionwasarticulatedbyChernozhukovandC.Hansen(2005),which shouldbeconsultedforfurtherdetails. 24.16 TechnicalProofs* ProofofTheorem24.1:Since(cid:80)[Y =m]=0, (cid:69)(cid:163) sgn(Y âm) (cid:164)=(cid:69)[ 1 {Y >m}]â(cid:69)[ 1 {Y <m}]=(cid:80)[Y >m]â(cid:80)[Y <m]= 1 â 1 =0 2 2 whichis(24.2). Exchangingintegrationanddifferentiation (cid:183) (cid:184) d (cid:69)|Y âÎ¸|=(cid:69) d |Y âÎ¸| =(cid:69)(cid:163) sgn(Y âÎ¸) (cid:164)=0, dÎ¸ dÎ¸ thefinalequalityatÎ¸=mby(24.2).Thisisthefirstorderconditionforanoptimum.Since(cid:69)(cid:163) sgn(Y âÎ¸) (cid:164)= 1â2(cid:80)[Y <Î¸]isgloballydecreasinginÎ¸, thesecondorderconditionshowsthatm istheuniquemini- mizer.Thisis(24.3). (24.4)and(24.5)followbysimilarargumentsusingtheconditionaldistribution. (24.6)followsfrom (24.5)undertheassumptionthatmed[Y |X]=X (cid:48)Î². â  ProofofTheorem24.2:Since(cid:80)(cid:163) Y =qÏ (cid:164)=0, (cid:69)(cid:163)Ï Ï (cid:161) Y âqÏ (cid:162)(cid:164)=Ïâ(cid:80)(cid:163) Y <qÏ (cid:164)=0 5Infact,theassumptionsimply(cid:69)(cid:163)Ï(Z)ÏÏ(e) (cid:164)=0foranyfunctionÏ. Weassumethatthedesiredinstrumentshavebeen selectedandareincorporatedinthevectorZasdenoted.",
    "page": 817,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER24. QUANTILEREGRESSION 798 whichis(24.11). Exchangingintegrationanddifferentiation d (cid:69)(cid:163)Ï Ï(Y âÎ¸) (cid:164)=(cid:69)(cid:163)Ï Ï(Y âÎ¸) (cid:164)=0, dÎ¸ thefinalequalityatÎ¸=qÏby(24.11).Thisisthefirstorderconditionforanoptimum.Since(cid:69)(cid:163)Ï Ï(Y âÎ¸) (cid:164)= Ïâ(cid:80)[Y <Î¸]isgloballydecreasinginÎ¸,thesecondorderconditionshowsthatqÏistheuniqueminimizer. Thisis(24.12). (24.13) and (24.14) follow by similar arguments using the conditional distribution. (24.15) follows from(24.14)undertheassumptionthat(cid:81) Ï[Y |X]=X (cid:48)Î². â  ProofofTheorem24.3: Thequantileregressionestimatorisanm-estimator,soweappealtoTheorem 22.1, which shows that Î² (cid:98)Ï is consisent if M n (Î²;Ï) is uniformly consistent for M(Î²;Ï) and Î² Ï uniquely minimizesM(Î²;Ï). ToshowthatM (Î²;Ï)isuniformlyconsistentforM(Î²;Ï)weappealtoTheorem22.2whichholdsif n (a)(Y i ,X i )arei.i.d; (b)(cid:69)(cid:175) (cid:175) Ï Ï (cid:161) Y âX (cid:48)Î²(cid:162)(cid:175) (cid:175) <âforallÎ²; (c)theparameterspaceforÎ²isbounded; and(d) Forsome A<â,(cid:69)(cid:175) (cid:175) Ï Ï (cid:161) Y âX (cid:48)Î² 1 (cid:162)âÏ Ï (cid:161) Y âX (cid:48)Î² 2 (cid:162)(cid:175) (cid:175) â¤A (cid:176) (cid:176) Î² 1 âÎ² 2 (cid:176) (cid:176)forallÎ² 1 ,Î² 2 .Conditions(a)and(c)hold byassumption.Condition(b)holdssince (cid:69)(cid:175) (cid:175) Ï Ï (cid:161) Y âX (cid:48)Î²(cid:162)(cid:175) (cid:175) â¤(cid:69)(cid:175) (cid:175)Y âX (cid:48)Î²(cid:175) (cid:175) â¤(cid:69)|Y|+(cid:176) (cid:176) Î²(cid:176) (cid:176) (cid:69)(cid:107)X(cid:107)<â. Finally, (cid:69)(cid:175) (cid:175) Ï Ï (cid:161) Y âX (cid:48)Î² 1 (cid:162)âÏ Ï (cid:161) Y âX (cid:48)Î² 2 (cid:162)(cid:175) (cid:175) â¤(cid:69)(cid:175) (cid:175)X (cid:48)(cid:161)Î² 1 âÎ² 2 (cid:162)(cid:175) (cid:175) â¤(cid:69)(cid:107)X(cid:107)(cid:176) (cid:176) Î² 1 âÎ² 2 (cid:176) (cid:176) whichiscondition(d)with A=(cid:69)(cid:107)X(cid:107). WenowshowthatÎ² Ï uniquelyminimizesM(Î²;Ï). Itisaminimizerby(24.16). Itisuniquebecause M (cid:161)Î²;Ï(cid:162) isaconvexfunctionand â2 âÎ²âÎ²(cid:48) M (cid:161)Î² Ï;Ï(cid:162)=(cid:69)(cid:163) XX (cid:48) fÏ (cid:161) X (cid:48)Î² Ï |X (cid:162)(cid:164)>0. (24.20) Theinequalityholdsbyassumption;wenowestablishtheequality. Exchangingintegrationanddifferentiation,usingÏ Ï(x)= d Ï Ï(x)=Ïâ1 {x<0},thelawofiterated dx expectations,andtheconditionaldistributionfunctionFÏ(u|x)=(cid:69)[ 1 {e<u}|X] â M (cid:161)Î²;Ï(cid:162)=â(cid:69)(cid:163) XÏ Ï (cid:161) Y âX (cid:48)Î²(cid:162)(cid:164) âÎ² =âÏ(cid:69)[X]+(cid:69)(cid:163) X(cid:69)(cid:163)1(cid:169) Y <X (cid:48)(cid:161)Î²âÎ² Ï (cid:162)(cid:170)|X (cid:164)(cid:164) =âÏ(cid:69)[X]+(cid:69)(cid:163) XFÏ (cid:161) X (cid:48)(cid:161)Î²âÎ² Ï (cid:162)|X (cid:162)(cid:164)",
    "page": 818,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". (24.21) Hence â2 â âÎ²âÎ²(cid:48) M (cid:161)Î²;Ï(cid:162)= âÎ²(cid:48) (cid:69)(cid:163) XFÏ (cid:161) X (cid:48)(cid:161)Î²âÎ² Ï (cid:162)|X (cid:162)(cid:164)=(cid:69)(cid:163) XX (cid:48) fÏ (cid:161) X (cid:48)(cid:161)Î²âÎ² Ï (cid:162)|X (cid:162)(cid:164) , where the right-side is finite under the assumptions. Evaluated at Î² Ï this is (24.20). This shows that M (cid:161)Î² Ï;Ï(cid:162) is globally convex and is strictly convex at the minimum Î² Ï. Thus the latter is the unique minimizer. â  Proof of Theorem 24.4: Since Î² (cid:98)Ï is an m-estimator we verify the conditions of Theorem 22.3. First, calculatethat (cid:69) (cid:104) (cid:161) XÏ Ï (cid:162)(cid:161) XÏ Ï (cid:162)(cid:48)(cid:105) =(cid:69)(cid:163) XX (cid:48)Ï2 Ï (cid:164)=â¦ Ï.",
    "page": 818,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER24. QUANTILEREGRESSION 799 Theorem22.3statesthatasymptoticnormalityholdsif(a)(cid:69)(cid:163)(cid:107)X(cid:107)2Ï2 Ï (cid:164)<â;(b) âÎ² â â 2 Î²(cid:48) M (cid:161)Î²;Ï(cid:162) iscontinu- ousinaneighborhoodofÎ² Ï;(c)Forsome A<â,(cid:69)(cid:176) (cid:176)XÏ Ï (cid:161) Y âX (cid:48)Î² 1 (cid:162)âXÏ Ï (cid:161) Y âX (cid:48)Î² 2 (cid:162)(cid:176) (cid:176) 2â¤ A (cid:176) (cid:176) Î² 1 âÎ² 2 (cid:176) (cid:176) forallÎ² 1 ,Î² 2 inaneighborhoodofÎ² Ï;(d)QÏ >0;(e)Î² Ïisintheinterioroftheparameterspace. Condition (a) holds since (cid:69)(cid:163)(cid:107)X(cid:107)2Ï2 Ï (cid:164) â¤ (cid:69)(cid:107)X(cid:107)2 < â by assumption. Equation (24.21) shows that âÎ² â â 2 Î²(cid:48) M (cid:161)Î²;Ï(cid:162)=â(cid:69)(cid:163) XX (cid:48) fÏ (cid:161) X (cid:48)(cid:161)Î²âÎ² Ï (cid:162)|X (cid:162)(cid:164) which is continuous under the assumption that fÏ(e | x) is continuousine,implyingCondition(b).Wecalculatethat (cid:69)(cid:176) (cid:176)X (cid:161)Ï Ï (cid:161) Y âX (cid:48)Î² 1 (cid:162)âÏ Ï (cid:161) Y âX (cid:48)Î² 2 (cid:162)(cid:162)(cid:176) (cid:176) 2 =(cid:69)(cid:163)(cid:107)X(cid:107)21(cid:169) X (cid:48)Î² â¤Y <X (cid:48)Î² (cid:170)(cid:164)+(cid:69)(cid:163)(cid:107)X(cid:107)21(cid:169) X (cid:48)Î² â¤Y <X (cid:48)Î² (cid:170)(cid:164) 1 2 2 1 =(cid:69)(cid:163)(cid:107)X(cid:107)2(cid:161) FÏ (cid:161) X (cid:48)(cid:161)Î² 2 âÎ² Ï (cid:162)|X (cid:162)âFÏ (cid:161) X (cid:48)(cid:161)Î² 1 âÎ² Ï (cid:162)|X (cid:162)(cid:162)(cid:164) +(cid:69)(cid:163)(cid:107)X(cid:107)2(cid:161) FÏ (cid:161) X (cid:48)(cid:161)Î² 1 âÎ² Ï (cid:162)|X (cid:162)âFÏ (cid:161) X (cid:48)(cid:161)Î² 2 âÎ² Ï (cid:162)|X (cid:162)(cid:162)(cid:164) â¤2D(cid:69)(cid:107)X(cid:107)3(cid:176) (cid:176) Î² 2 âÎ² 1 (cid:176) (cid:176) whereweusetheassumption fÏ(e|x)â¤D. ThisimpliesCondition(c)with A=2D(cid:69)(cid:107)X(cid:107)3. Condition(d) holdsby(24.18).Condition(e)holdsbyassumption. SinceConditions(a)-(e)holdtheresultfollows. â  ProofofTheorem24.5: Bythedefinitionofthequantiletreatmenteffect,monotonicityofcausaleffect (Assumption24.1.2),definitionofthecausaleffect,monotonicityofthetreatmentresponse(Assumption 24.1.3),andthedefinitionofthequantileregressionfunction,wefindthat QÏ(x)=(cid:81) Ï[C(X,U)|X =x] =C(x,(cid:81) Ï[U |X =x]) =h(1,x,(cid:81) Ï[U |X =x])âh(0,x,(cid:81) Ï[U |X =x]) =(cid:81) Ï[h(1,X,U)|X =x]â(cid:81) Ï[h(0,X,U)|X =x] =qÏ(1,x)âqÏ(0,x) =DÏ(x) asclaimed. â  _____________________________________________________________________________________________ 24.17 Exercises Exercise24.1 Prove(24.4)inTheorem24.1. Exercise24.2 Prove(24.5)inTheorem24.1. Exercise24.3 DefineÏ(x)=Ïâ1 {x<0}.LetÎ¸satisfy(cid:69)(cid:163)Ï(Y âÎ¸) (cid:164)=0.IsÎ¸aquantileofthedistribution ofY? Exercise24.4 TakethemodelY =X (cid:48)Î²+ewherethedistributionofegivenX issymmetricaboutzero. (a) Find(cid:69)[Y |X]andmed[Y |X]. (b) DoOLSandLADestimatethesamecoefficientÎ²ordifferentcoefficients? (c) UnderwhichcircumstanceswouldyoupreferLADoverOLS?Underwhichcircumstanceswould youpreferOLSoverLAD?Explain.",
    "page": 819,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER24. QUANTILEREGRESSION 800 Exercise24.5 YouareinterestedinestimatingtheequationY =X (cid:48)Î²+e. Youbelievetheregressorsare exogenous,butyouareuncertainaboutthepropertiesoftheerror. Youestimatetheequationbothby leastabsolutedeviations(LAD)andOLS.AcolleaguesuggeststhatyoushouldprefertheOLSestimate, becauseitproducesahigherR2thantheLADestimate.Isyourcolleaguecorrect? Exercise24.6 Prove(24.13)inTheorem24.2. Exercise24.7 Prove(24.14)inTheorem24.2. Exercise24.8 SupposeX isbinary.Showthat(cid:81) Ï[Y |X]islinearinX. Exercise24.9 SupposeX 1 andX 2 arebinary.Find(cid:81) Ï[Y |X 1 ,X 2 ]. Exercise24.10 Show(24.19). Exercise24.11 Show under correct specification that â¦ Ï =(cid:69)(cid:163) XX (cid:48)Ï2 Ï (cid:164) satisfies the simplification â¦ Ï = Ï(1âÏ)Q. Exercise24.12 TakethetreatmentresponsesettingofTheorem24.5. Supposeh(0,X ,U)=0,meaning 2 thattheresponsevariableY iszerowheneverthereisnotreatment. ShowthatAssumption24.1.3isnot necessaryforTheorem24.5. Exercise24.13 Usingthecps09mardatasettakethesampleofHispanicmenwitheducation11yearsor higher.Estimatelinearquantileregressionfunctionsforlogwagesoneducation.Interpretyourfindings. Exercise24.14 Usingthecps09mardatasettakethesampleofHispanicwomenwitheducation11years orhigher.Estimatelinearquantileregressionfunctionsforlogwagesoneducation.Interpret. Exercise24.15 Take the Duflo, Dupas and Kremer (2011) dataset DDK2011 and the subsample of stu- dentsforwhichtracking=1. Estimatelinearquantileregressionsoftotalscoreonpercentile(thelatteris thestudentâstestscorebeforetheschoolyear). Calculatestandarderrorsbyclusteredbootstrap. Dothe coefficientschangemeaningfullybyquantile?Howdoyouinterprettheseresults? Exercise24.16 Using the cps09mar dataset estimate similarly to Figure 24.5 the quantile regressions forlogwagesona5th-orderpolynomialinexperienceforcollege-educatedBlackwomen. Repeatfor college-educatedwhitewomen.Interpretyourfindings.",
    "page": 820,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Chapter 25 Binary Choice 25.1 Introduction This and the next two chapters treat what are known as limited dependent variables. These are variableswhichhaverestrictedsupport(asubsetoftherealline)andthisrestrictionhasconsequences foreconometricmodeling. ThischapterconcernsthesimplestcasewhereY isbinary,meaningthatit takestwovalues.Withoutlossofgeneralitythesearetakenaszeroandone,thusY hassupport{0,1}.In econometricswetypicallycallthisclassofmodelsbinarychoice. Examples of binary dependent variables include: Purchase of a single item; Market entry; Partici- pation; Approval of an application/patent/loan. The dependent variable may be recorded as Yes/No, True/False,or1/â1,butcanalwaysbewrittenas1/0. Thegoalinbinarychoiceanalysisisestimationoftheconditionalorresponseprobability(cid:80)[Y =1|X] given a set of regressors X. We may be interested in the response probability or some transformation suchasitsderivativeâthemarginaleffect. Atraditionalapproachtobinarychoicemodeling(andlim- iteddependentvariablemodelsingeneral)isparametricwithestimationbymaximumlikelihood.There isalsoasubstantialliteratureonsemiparametricestimation. Inrecentyears,appliedpracticehastilted towardslinearprobabilitymodelsestimatedbyleastsquares. FormoredetailedtreatmentsseeMaddala(1983),CameronandTrivedi(2005)andWooldridge(2010). 25.2 BinaryChoiceModels Let(Y,X)berandomwithY â{0,1}andX â(cid:82)k.TheresponseprobabilityofY withrespecttoX is P(x)=(cid:80)[Y =1|X =x]=(cid:69)[Y |X =x]. Theresponseprobabilitycompletelydescribestheconditionaldistribution.Themarginaleffectis â â â P(x)= (cid:80)[Y =1|X =x]= (cid:69)[Y |X =x]. âx âx âx Thisequalstheregressionderivative.Economicapplicationsoftenfocusonthemarginaleffect. Toillustrate,considertheprobabilityofmarriagegivenage. Weusethecps09mardataset,takethe subset of men with a college degree (n =6441), set Y =1 if the individual is married or widowed but notseparatedordivorced1 andsetY =0otherwise. Theregressorisagewhichtakesvaluesin[19,80]. In Figure 25.1(a) we plot two estimates of P(x). The filled circles are nonparametric estimates â the 1maritalequals1,2,3,or4. 801",
    "page": 821,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER25. BINARYCHOICE 802 empirical marriage frequency for each age â and the solid line is our preferred specification (a probit spline model, described below). What seems apparant is that the probability of marriage is near zero forage=19,increaseslinearlyto80%aroundage=35,remainsroughlyflatat80%forages40-65,and increasesforhigherages. 20 30 40 50 60 70 80 Age 0.1 8.0 6.0 4.0 2.0 0.0 Pr(Married|Age) 20 30 40 50 60 70 80 Age (a)ResponseProbabilities 0.1 8.0 6.0 4.0 2.0 0.0 20 30 40 50 60 70 80 Age (b)ScatterPlot 0.1 8.0 6.0 4.0 2.0 0.0 2.0â 4.0â Linear Probit Linear Series Probit Series (c)BinaryChoiceModels Figure25.1:ProbabilityofMarriageGivenAgeforCollegeEducatedMen Thevariablessatisfytheregressionframework Y =P(X)+e (cid:69)[e|X]=0. Theerroreisnotâclassicalâ.Ithasthetwo-pointconditionaldistribution (cid:189) 1âP(X), withprobabilityP(X) e= (25.1) P(X), withprobability1âP(X). Itisalsohighlyheteroskedasticwithconditionalvariance var[e|X]=P(X)(1âP(X)). (25.2) Regressionscatterplotsareunusual. Forexample,inFigure25.1(b)weplotarandomsubsampleof250 observations from the CPS sample (marked by the Ãâs) along with the estimated response probability. Sinceallobservationslieontheliney=0orony=1thescatterplothaslittleinterpretivevalue. 25.3 ModelsfortheResponseProbability WenowdescribethemostcommonmodelsusedfortheresponseprobabilityP(x). Linear Probability Model: P(x)= x (cid:48)Î² where Î² is a coefficient vector. In this model the response probabilityisalinearfunctionoftheregressors. Thelinearprobabilitymodelhastheadvantagethatit issimpletointerpret. ThecoefficientsÎ²equalthemarginaleffects(whenX doesnotincludenonlinear transformations).Sincetheresponseprobabilityequalstheconditionalmeanthismodelequalsthelin- earregressionmodel.Linearitymeansthatestimationissimpleasleastsquarescanbeusedtoestimate thecoefficients.Inmorecomplicatedsettings(e.g.paneldatawithfixedeffectsorendogenousvariables withinstruments)standardestimatorscanbeemployed.",
    "page": 822,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER25. BINARYCHOICE 803 A disadvantage of the linear probability model is that it does not respect the [0,1] boundary. Fit- tedandpredictedvaluesfromestimatedlinearprobabilitymodelsfrequentlyviolatetheseboundaries producingnonsenseresults. Toillustrate,inFigure25.1(c)weplotwiththedottedlinethelinearprobabilitymodelfittotheob- servationsinthesampledescribedintheprevioussection.Thefittedvaluesareapoorapproximationto theresponseprobabilities. Theyalsoviolatethe[0,1]boundaryformenabovetheageof67. According tothemodelan80-year-oldismarriedwithprobability113%! Overall,thelinearprobabilitymodelisapoorchoiceforcalculationofprobabilities. IndexModels: P(x)=G(x (cid:48)Î²)whereG(u)isalinkfunctionandÎ²isacoefficientvector. Thisframe- workisalsocalledasingleindexmodelwhere x (cid:48)Î²isalinearindexfunction. Inbinarychoicemodels G(u)isdistributionfunctionwhichrespectstheprobabilitybounds0â¤G(u)â¤1. Ineconomicapplica- tionsG(u)istypically thenormal orlogisticdistributionfunction, bothofwhichare symmetricabout zerosothatG(âu)=1âG(u). Weassumethroughoutthischapterthatthissymmetryconditionholds. Letg(u)= â G(u)denotethedensityfunctionofG(u).Inanindexmodelthemarginaleffectfunctionis âu â P(x)=Î²g (cid:161) x (cid:48)Î²(cid:162) . âx Indexmodelsareonlyslightlymorecomplicatedthanthelinearprobabilitymodelbuthavetheadvan- tageofrespectingthe[0,1]boundary.Thetwomostcommonindexmodelsaretheprobitandlogit. ProbitModel: P(x)=Î¦(x (cid:48)Î²)whereÎ¦(u)isthestandardnormaldistributionfunction. Thisisatra- ditional workhorse model for binary choice analysis. It is simple, easy to use, easy to interpret, and is basedontheclassicalnormaldistribution. LogitModel: P(x)=Î(x (cid:48)Î²)whereÎ(u)=(cid:161) 1+exp(âu) (cid:162)â1 isthelogisticdistributionfunction. This isanalternativeworkhorsemodelforbinarychoiceanalysis. Thelogisticandnormaldistributionfunc- tions(appropriatelyscaled)havesimilarshapessotheprobitandlogitmodelstypicallyproducesimilar estimatesfortheresponseprobabilitiesandmarginaleffects. Oneadvantageofthelogitmodelisthat thedistributionfunctionisavailableinclosedformwhichspeedscomputation. LinearSeriesModel: P(x)=x (cid:48) Î² wherex =x (x)isavectoroftransformationsofx andÎ² isa K K K K K coefficientvector. Aseriesexpansionhastheabilitytoapproximateanycontinuousfunctionincluding theresponseprobabilityP(x). Theadvantageofalinearseriesmodelisthatitslinearformallowsthe applicationoflineareconometricmethods.Itisnotguaranteed,however,tobeboundary-respecting. IndexSeriesModel: P(x)=G (cid:161) x (cid:48) Î² (cid:162) whereG(u)isadistributionfunction(eithernormalorlogis- K K tic in practice), x =x (x) is a vector of transformations of x, and Î² is a coefficient vector. A series K K K expansionhastheabilitytoapproximateanycontinuousfunctionincludingthetransformedresponse probabilityG â1(cid:161) p(x) (cid:162) . Thismeansthattheindexseriesmodelhastheabilitytoapproximateanycon- tinuousresponseprobability.Inaddition,themodelisboundary-respecting",
    "page": 823,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". IndexSeriesModel: P(x)=G (cid:161) x (cid:48) Î² (cid:162) whereG(u)isadistributionfunction(eithernormalorlogis- K K tic in practice), x =x (x) is a vector of transformations of x, and Î² is a coefficient vector. A series K K K expansionhastheabilitytoapproximateanycontinuousfunctionincludingthetransformedresponse probabilityG â1(cid:161) p(x) (cid:162) . Thismeansthattheindexseriesmodelhastheabilitytoapproximateanycon- tinuousresponseprobability.Inaddition,themodelisboundary-respecting. ToillustratetheapproximatingcapabilitiesofthemodelsviewFigure25.1(c)whichplotsfouresti- matedresponseprobabilityfunctions: (1)Linear;(2)Probit;(3)LinearSeries;(4)ProbitSeries. Thefirst twomodelsarespecifiedaslinearinage.Thetwoseriesmodelsuseaquadraticsplineinagewithkinks at40and60. Asdiscussedearlier,inthisapplicationthelinearprobabilitymodelisaparticularlypoorfit. Itover- predictstheprobabilityformenunder30andover50,under-predictstheothers,andviolatesthe[0,1]",
    "page": 823,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER25. BINARYCHOICE 804 boundary. The simple probit model is also quite poor. It produces an estimated response probability functionwhichissimilartothelinearprobabilitymodelforagesupto60. Itsadvantageisthatforages over60itdoesnotviolatethe[0,1]boundary.Incontrast,thetwoseriesmodelsproduceexcellentfitted response probability functions. The two estimates are nearly identical for ages above 25. The main differencebetweentheestimatedfunctionsisthattheprobitseriesmodelprovidesagloballyexcellent fit, while the linear series model fails for ages less than 25, severely violating the [0,1] boundary. The linearseriesmodelestimatesthata19-year-oldismarriedwithnegativeprobability:â27%! To summarize, a probit series model has several excellent features. It is simple, based on a popu- larlinkfunction,globallyapproximatesanycontinuousresponseprobabilityfunction,andrespectsthe [0,1] boundary. A linear series model is also a reasonable candidate, but has the disadvantage of not necessarilyrespectingthe[0,1]boundary. ProbitandLogit The intruiging labels probit and logit have a long history in statisti- cal analysis. The term probit was coined by Chester Bliss in 1934 as a contraction of âprobability unitâ. The logistic function was introduced by Pierre FranÃ¯Â£Â¡ois Verhulst in 1938 as a modified exponential growth model.Itisspeculatedthatheusedthetermlogisticasacontrasttolog- arithmic.In1944JosephBerksonproposedabinarychoicemodelbased onthelogisticdistributionfunction. Hemotivatedthelogisticasacon- venientcomputationalapproximationtothenormal. Ashismodelwas ananalogoftheprobitBerksoncalledhismodelthelogit. 25.4 LatentVariableInterpretation Anindexmodelcanbeinterpretedasalatentvariablemodel.Consider Y â=X (cid:48)Î²+e eâ¼G(e) Y =1(cid:169) Y â>0 (cid:170)= (cid:189) 1 ifY â>0 0 otherwise. â Inthismodeltheobservablesare(Y,X).ThevariableY islatent, linearin X andanerrore, withthe latter drawn from a symmetric distribution G. The observed binary variable Y equals 1 if the latent â variableY exceedszeroandequals0otherwise. TheeventY =1isthesameasY â>0,whichisthesameas X (cid:48)Î²+e>0. (25.3) Thismeansthattheresponseprobabilityis P(x)=(cid:80)(cid:163) e>âx (cid:48)Î²(cid:164)=1âG (cid:161)âx (cid:48)Î²(cid:162)=G (cid:161) x (cid:48)Î²(cid:162) . ThefinalequalityusestheassumptionthatG(u)issymmetricaboutzero. Thisshowsthattheresponse probabilityisP(x)=G(x (cid:48)Î²)whichisanindexmodelwithlinkfunctionG(u).",
    "page": 824,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER25. BINARYCHOICE 805 â ThislatentvariablemodelcorrespondstoachoicemodelwhereY isanindividualâsrelativeutility (orprofit)oftheoptionsY =1andY =0, andtheindividualselectstheoptionwiththehigherutility. Weseethatthisstructuralchoicemodelisidenticaltoanindexmodelwithlinkfunctionequallingthe distributionoftheerror. Itisaprobitmodeliftheerrore isstandardnormalandisalogitmodelife is logisticallydistributed. Youmayhavenoticedthatwehavediscussedcaseswheretheerrore iseitherstandard normalor standard logistic, that is, their scale is fixed. This is because the scale of the error distribution is not identified. Toseethis,supposethate=ÏÎµwhereÎµhasadistributionG(u)withunitvariance. Thenthe responseprobabilityis (cid:80)[Y =1|X =x]=(cid:80)(cid:163)Ïe>âx (cid:48)Î²(cid:164)=G (cid:181) x (cid:48)Î²(cid:182) =G (cid:161) x (cid:48)Î²â(cid:162) Ï whereÎ²â=Î²/Ï. ThisisanindexmodelwithcoefficientÎ²â . ThismeansthatÎ²andÏarenotseparately identified;onlytheratioÎ²â=Î²/Ïisidentified. Thestandardsolutionisto(cid:112)normalizeÏtoaconvenient value.TheprobitandlogitmodelsusethenormalizationsÏ=1andÏ=Ï/ 3(cid:39)1.8,respectively. Twoconsequencesoftheaboveanalysisarethat(1)interpretationofthecoefficientvectorÎ²cannot be separated from the scale of the error; and (2) the coefficients of probit and logit models cannot be comparedwithoutrescaling. Ingeneral, itisbesttointerpretthecoefficientofaprobitmodelasÎ²/Ï, thestructuralcoefficientscaledbythestucturalstandarddeviation,andtointerpretthecoefficien(cid:112)tofa logitmodelasÎ²/v,thestructuralcoefficientscaledbythestructurallogisticscaleparameterv=Ï 3/Ï. Foraroughcomparison2ofprobitandlogitcoefficientsmultiplytheprobitcoefficientsby1.8ordivide thelogitcoefficientsby1.8. WhilethecoefficientÎ²isnotidentifiedthefollowingparametersareidentified: 1. Scaledcoefficients:Î²â=Î²/Ï. 2. Ratiosofcoefficients:Î² /Î² =Î²â /Î²â . 1 2 1 2 3. Marginaleffects: â P(x)= Î² g (cid:179) x(cid:48)Î²(cid:180) =Î²â g (cid:161) x (cid:48)Î²â(cid:162) . âx Ï Ï TheseonlydependonÎ²â soareidentified. Concerningidentification,ifwetakeabroadernonparametricviewtheerrordistributionG(u)isnot identified. Toseethis,writethestructuralequationnonparametricallyasY â=m(X)+e. Theresponse probabilityis P(x)=1âG(âm(x)). (25.4) The joint distribution identifies P(x). If G(e) and m(x) are nonparametric they cannot be separately identifiedfromtheresponseprobability.Onlythecomposite(25.4)isidentified. AnimportantimplicationisthatthereisnolossofgeneralityinsettingG(u)equaltoaspecificpara- metricdistributionsuchasthenormalsolongasthefunctionm(x)istreatednonparametrically. 25.5 Likelihood Probitandlogitmodelsaretypicallyestimatedbymaximumlikelihood. Toconstructthelikelihood weneedthedistributionofanindividualobservation.RecallthatifY isBernoulli,suchthat(cid:80)[Y =1]=p and(cid:80)[Y =0]=1âp,thenY hastheprobabilitymassfunction Ï(y)=py(1âp)1ây, y=0,1. 2Thisproducesonlyaroughcomparisonasthisnormalizationonlyputsthecoefficientsonthesamescale. Theyarenot equalsincethemodelsaredifferent.",
    "page": 825,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER25. BINARYCHOICE 806 In the index model (cid:80)[Y =1|X] =G (cid:161) X (cid:48)Î²(cid:162) , Y is conditionally Bernoulli, so its conditional probability massfunctionis Ï(Y |X)=G (cid:161) X (cid:48)Î²(cid:162)Y (cid:161) 1âG (cid:161) X (cid:48)Î²(cid:162)(cid:162)1âY =G (cid:161) X (cid:48)Î²(cid:162)Y G (cid:161)âX (cid:48)Î²(cid:162)1âY =G (cid:161) Z (cid:48)Î²(cid:162) (25.5) where (cid:189) X ifY =1 Z = âX ifY =0. Takinglogsandsummingacrossobservationsweobtainthelog-likelihoodfunction: n (cid:96) (Î²)= (cid:88) logG (cid:161) Z (cid:48)Î²(cid:162) . n i i=1 Fortheprobitandlogitmodelsthisis n (cid:96)probit (Î²)= (cid:88) logÎ¦(cid:161) Z (cid:48)Î²(cid:162) n i i=1 n (cid:96)logit (Î²)= (cid:88) logÎ(cid:161) Z (cid:48)Î²(cid:162) . n i i=1 Definethefirstand(negative)secondderivativesofthelogdistributionfunction: h(x)= d logG(x) dx andH(x)=â d2 logG(x).Forthelogitmodeltheseequal(SeeExercise25.5) dx2 h (x)=1âÎ(x) logit H (x)=Î(x)(1âÎ(x)) logit andfortheprobitmodel(SeeExercise25.6) Ï(x) h (x)= d=efÎ»(x) probit Î¦(x) H (x)=Î»(x)(x+Î»(x)). probit ThefunctionÎ»(x)=Ï(x)/Î¦(x)isknownastheinverseMillsratio. BoththelogitandprobithavethepropertythatH(x)>0. Thisiseasilyseenforthelogitcasesince itistheproductofthedistributionfunctionanditscomplement,butlesssofortheprobitcase.Herewe makeuseofaconvenientpropertyoflogconcave3 functions: ifadensity f(x)islogconcavethenthe distributionfunctionF(x)islogconcave. ThestandardnormaldensityÏ(x)islogconcave4, implying thatÎ¦(x)islogconcave,implyingH (x)>0asdesired. probit ThelikelihoodscoreandHessianare â n S (Î²)= (cid:96) (Î²)= (cid:88) Z h (cid:161) Z (cid:48)Î²(cid:162) (25.6) n âÎ² n i i i=1 â2 n H (Î²)=â (cid:96) (Î²)= (cid:88) X X (cid:48) H (cid:161) Z (cid:48)Î²(cid:162) . (25.7) n âÎ²âÎ²(cid:48) n i i i i=1 Examining(25.7)wecanseethatH(x)>0impliesthatH (Î²)>0globallyinÎ².Thisinturnimpliesthat n thelog-likelihood(cid:96) (Î²)isgloballyconcave. Sinceboth H (x)>0and H (x)>0wededucethat n logit probit theprobitandlogitloglikelihoodfunctions(cid:96)probit (Î²)and(cid:96)logit (Î²)aregloballyconcaveinÎ². n n 3Afunctionf(x)islogconcaveiflogf(x)isconcave. 4logÏ(x)=âlog(2Ï)âx2/2isconcave.",
    "page": 826,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER25. BINARYCHOICE 807 TheMLEisthevaluewhichmaximizes(cid:96) (Î²).Wewritethisas n Î² (cid:98) probit=argmax(cid:96)probit (Î²) n Î² Î² (cid:98) logit=argmax(cid:96)logit (Î²). n Î² Sincetheprobitandlogitlog-likelihoodsaregloballyconcaveÎ² (cid:98) probit andÎ² (cid:98) logit areunique. Thereisno explicitsolutionsotheyneedtobefoundnumerically. Astheloglikelihoodsaresmooth,concave,with knownfirstandsecondderivatives,numericaloptimizationisstraightforward. InStata,usethecommandsprobitandlogittoobtaintheMLE.InR,usethecommands glm(Y~X,family=binomial(link=\"probit\")) glm(Y~X,family=binomial(link=\"logit\")). 25.6 Pseudo-TrueValues Theexpectedlogmassfunctionis (cid:96)(Î²)=(cid:69)(cid:163) logG (cid:161) Z (cid:48)Î²(cid:162)(cid:164) . ThemodeliscorrectlyspecifiedifthereisacoefficientÎ² suchthat(cid:80)[Y =1|X]=G (cid:161) X (cid:48)Î² (cid:162) . Whenthis 0 0 holdsthenÎ² hasthepropertythatitmaximizes(cid:96)(Î²)andthussatisfies 0 Î² =argmax(cid:96)(Î²). (25.8) 0 Î² WesaythatthemodelismisspecifiedifthereisnoÎ²suchthat(cid:80)[Y =1|X]=G (cid:161) X (cid:48)Î²(cid:162) .Inthiscasewe viewthemodelG (cid:161) X (cid:48)Î²(cid:162) asanapproximationtotheresponseprobabilityanddefinethepseudo-trueco- efficientÎ² asthevaluewhichsatisfies(25.8).Byconstruction,(25.8)equalsthetruecoefficientwhenthe 0 modeliscorrectlyspecifiedandotherwiseproducesthebest-fittingmodelwithrespecttotheexpected logmassfunction. WhenthedistributionfunctionG(x)islogconcave(asitisfortheprobitandlogitmodels)then(cid:96)(Î²) isgloballyconcave.Toseethisdefine â2 Q(Î²)=â (cid:96)(Î²)=(cid:69)(cid:163) XX (cid:48) H (cid:161) Z (cid:48)Î²(cid:162)(cid:164) âÎ²âÎ²(cid:48) andobservethatH(x)>0(bylogconcavity),whichimpliesQ(Î²)â¥0,whichimpliesthat(cid:96)(Î²)isglobally concave.Furthermore,theminimizer(25.8)isuniqueunderthefullrankcondition (cid:69)(cid:163) XX (cid:48) H (cid:161) X (cid:48)Î² (cid:162)(cid:164)>0. (25.9) 0 Itisimportanttonotethattheconcavityof(cid:96)(Î²)anduniquenessofthemaximizerÎ² arepropertiesof 0 themodelG (cid:161) X (cid:48)Î²(cid:162) notthetruedistribution. Forspecificity,fortheprobitandlogitmodelsdefinethepopulationcriterions (cid:96)probit(Î²)=(cid:69)(cid:163) logÎ¦(cid:161) Z (cid:48)Î²(cid:162)(cid:164) (cid:96)logit(Î²)=(cid:69)(cid:163) logÎ(cid:161) Z (cid:48)Î²(cid:162)(cid:164)",
    "page": 827,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER25. BINARYCHOICE 808 andthepseudo-truevalues Î²probit=argmax(cid:96)probit(Î²) Î² Î²logit=argmax(cid:96)logit(Î²). Î² Wenowdescribethefullrankcondition(25.9)forthelogitandprobitmodels. Fortheprobitmodel H (âx)= H (x) is symmetric about zero, so H (cid:161) Z (cid:48)Î²(cid:162)= H (cid:161) X (cid:48)Î²(cid:162)=Î(cid:161) X (cid:48)Î²(cid:162)(cid:161) 1âÎ(cid:161) X (cid:48)Î²(cid:162)(cid:162) . We logit logit logit logit deducethat(25.9)isthesameas (cid:104) (cid:179) (cid:180)(cid:179) (cid:179) (cid:180)(cid:180)(cid:105) Q d=ef(cid:69) XX (cid:48)Î X (cid:48)Î²logit 1âÎ X (cid:48)Î²logit >0. (25.10) logit Fortheprobitmodelthecondition(25.9)is (cid:104) (cid:179) (cid:180)(cid:105) Q d=ef(cid:69) XX (cid:48) H Z (cid:48)Î²probit >0. (25.11) probit probit When(25.10)and/or(25.11)holdthepopulationminimizersÎ²probitand/orÎ²logitareunique. 25.7 AsymptoticDistribution Wefirstprovideconditionsforconsistentestimation.LetB betheparameterspaceforÎ². Theorem25.1 Consistency of Logit Estimation. If (1) (Y ,X ) are i.i.d.; (2) i i (cid:69)(cid:107)X(cid:107)<â;(3)Q >0;and(4)B isbounded;thenÎ² (cid:98) logitââÎ²logitasnââ. logit p Theorem25.2 Consistency of Probit Estimation. If (1) (Y ,X ) are i.i.d.; (2) i i (cid:69)(cid:107)X(cid:107)2 < â; (3) Q > 0; and (4) B is bounded; then Î² (cid:98) probit ââ Î²probit as probit p nââ. TheproofsareinSection25.14. ToderivetheasymptoticdistributionsweappealtoTheorem22.3 form-estimatorswhichshowsthattheasymptoticdistributionisnormalwithacovariancematrixV = Q â1â¦Q â1whereQ isdefinedin(25.10)forthelogitmodelandisdefinedin(25.11)fortheprobitmodel. (cid:104) (cid:105) Thevarianceofthescoreisâ¦=(cid:69) XX (cid:48) h (cid:161) Z (cid:48)Î²(cid:162)2 .Inthelogitmodelwehavethesimplification (cid:183) (cid:184) â¦ =(cid:69) XX (cid:48) (cid:179) Y âÎ (cid:179) X (cid:48)Î²logit (cid:180)(cid:180)2 (25.12) logit (explained below). We do not have a similar simplification for the probit model (exceptunder correct specification,discussedbelow)andthusdefine (cid:183) (cid:184) â¦ =(cid:69) XX (cid:48)Î» (cid:179) Z (cid:48)Î²probit (cid:180)2 . (25.13) probit",
    "page": 828,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER25. BINARYCHOICE 809 Tosee(25.12),withsomealgebrayoucanshowthat g (cid:161) X (cid:48)Î²(cid:162)2 g (cid:161) X (cid:48)Î²(cid:162)2 g (cid:161) X (cid:48)Î²(cid:162)2(cid:161) Y âG (cid:161) X (cid:48)Î²(cid:162)(cid:162)2 h (cid:161) Z (cid:48)Î²(cid:162)2= Y + (1âY)= . (25.14) G (cid:161) X (cid:48)Î²(cid:162)2 (cid:161) 1âG (cid:161) X (cid:48)Î²(cid:162)(cid:162)2 G (cid:161) X (cid:48)Î²(cid:162)2(cid:161) 1âG (cid:161) X (cid:48)Î²(cid:162)(cid:162)2 Inthelogitmodeltherightsidesimplifiesto (cid:161) Y âÎ(cid:161) X (cid:48)Î²(cid:162)(cid:162)2 .Thisimplies (cid:183) (cid:184) â¦ =(cid:69) (cid:104) XX (cid:48) h (cid:161) Z (cid:48)Î²(cid:162)2 (cid:105) =(cid:69) XX (cid:48) (cid:179) Y âÎ (cid:179) X (cid:48)Î²logit (cid:180)(cid:180)2 logit logit asclaimed. Theorem25.3 If the conditions of Theorem 25.1 hold plus (cid:69)(cid:107)X(cid:107)4 < â and Î²logitisintheinteriorofB;thenasnââ (cid:112) (cid:179) (cid:180) n Î² (cid:98) logitâÎ²logit ââN (cid:161) 0,V logit (cid:162) d whereV =Q â1 â¦ Q â1 . logit logit logit logit Theorem25.4 If the conditions of Theorem 25.2 hold plus (cid:69)(cid:107)X(cid:107)4 < â and Î²probitisintheinteriorofB;thenasnââ (cid:112) (cid:179) (cid:180) n Î² (cid:98) probitâÎ²probit ââN (cid:161) 0,V probit (cid:162) d whereV =Q â1 â¦ Q â1 . probit probit probit probit TheproofsareinSection25.14. UndercorrectspecificationtheinformationmatrixequalityimpliesthesimplificationsV =Q â1 logit logit andV =Q â1 .Wealsohavethesimplification probit probit (cid:104) (cid:179) (cid:180) (cid:179) (cid:180)(cid:105) â¦ =Q =(cid:69) XX (cid:48)Î» X (cid:48)Î²probit Î» âX (cid:48)Î²probit . (25.15) probit probit Thisfollowsfrom(25.14),whichfortheprobitmodelcanbewrittenas (cid:161) Y âÎ¦(cid:161) X (cid:48)Î²(cid:162)(cid:162)2 Î»(cid:161) Z (cid:48)Î²(cid:162)2=Î»(cid:161) X (cid:48)Î²(cid:162)Î»(cid:161)âX (cid:48)Î²(cid:162) . Î¦(cid:161) X (cid:48)Î²(cid:162)(cid:161) 1âÎ¦(cid:161) X (cid:48)Î²(cid:162)(cid:162) (cid:104) (cid:105) Under correct specification (cid:69)[Y |X]=Î¦(X (cid:48)Î²) and (cid:69) (cid:161) Y âÎ¦(cid:161) X (cid:48)Î²(cid:162)(cid:162)2|X =Î¦(X (cid:48)Î²) (cid:161) 1âÎ¦(X (cid:48)Î²) (cid:162) . Taking expectations given X the above expression simplifies to Î»(cid:161) X (cid:48)Î²(cid:162)Î»(cid:161)âX (cid:48)Î²(cid:162) . Inserted into (25.13) yields (25.15).",
    "page": 829,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER25. BINARYCHOICE 810 25.8 CovarianceMatrixEstimation ForthelogitmodeldefineÎ (cid:98)i =Î(cid:161) X i (cid:48)Î² (cid:98) logit(cid:162) and Q(cid:98)logit = n 1 (cid:88) n X i X i (cid:48)Î (cid:98)i (cid:161) 1âÎ (cid:98)i (cid:162) i=1 â¦ (cid:98)logit = n 1 (cid:88) n X i X i (cid:48)(cid:161) Y i âÎ (cid:98)i (cid:162)2 . i=1 ThesandwichcovariancematrixestimatorforV logit isV(cid:98)logit =Q(cid:98) â lo 1 git â¦ (cid:98)logit Q(cid:98) â lo 1 git . Undertheassumption ofcorrectspecificationwemayalternativelyuseV(cid:98) 0 logit =Q(cid:98) â lo 1 git . FortheprobitmodeldefineÂµ (cid:98)i =Z i (cid:48)Î² (cid:98) probit,Î» (cid:98)i =Î»(cid:161)Âµ (cid:98)i (cid:162) ,and Q(cid:98)probit = n 1 (cid:88) n X i X i (cid:48)Î» (cid:98)i (cid:161)Âµ (cid:98)i +Î» (cid:98)i (cid:162) i=1 â¦ (cid:98)probit = n 1 (cid:88) n X i X i (cid:48)Î» (cid:98) 2 i . i=1 The sandwich covariance matrix estimator for V probit is V(cid:98)probit =Q(cid:98) â pr 1 obit â¦ (cid:98)probit Q(cid:98) â pr 1 obit . Under the as- sumptionofcorrectspecificationwemayalternativelyuse Q(cid:98) 0 probit = n 1 (cid:88) n X i X i (cid:48)Î» (cid:179) X (cid:48)Î² (cid:98) probit (cid:180) Î» (cid:179) âX (cid:48)Î² (cid:98) probit (cid:180) i=1 andV(cid:98) 0 probit = (cid:179) Q(cid:98) 0 (cid:180)â1 . 0 0 InStataandRthedefaultcovariancematrixandstandarderrorsarecalculatedbyV(cid:98)logit andV(cid:98)probit . FortherobustcovariancematrixestimationandstandarderrorsV(cid:98)logit andV(cid:98)probit ,inStatausetheoption vce(robust).InRusethepackagesandwichwhichhasoptionsfortheHC0,HC1,andHC2(andother) covariancematrixestimators. 25.9 MarginalEffects As we mentioned before it is common to focus on marginal effects rather than coefficients as the latteraredifficulttointerpret. Inthissectionwedescribemarginaleffectsinmoredetailanddescribe commonestimators. Taketheindexmodel(cid:80)[Y =1|X =x]=G (cid:161) x (cid:48)Î²(cid:162) whenxdoesnotincludenonlineartransformations. Inthiscasethemarginaleffectsare â Î´(x)= P(x)=Î²g (cid:161) x (cid:48)Î²(cid:162) . âx Thisvarieswithx. Forexample,inFigure25.1weseethatthemarginaleffectofageonmarriageproba- bilityisabout0.06peryearforagesbetween20and30,butisclosetozeroforagesabove40. Forreportingitistypicaltocalculateanâaverageâvalue. Therearemorethanonewaytodoso. The mostcommonistheaveragemarginaleffect AME=(cid:69)[Î´(X)]=Î²(cid:69)(cid:163) g(X (cid:48)Î²) (cid:164) .",
    "page": 830,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER25. BINARYCHOICE 811 AnestimatorofÎ´(x)isÎ´ (cid:98)(x)=Î² (cid:98)g (cid:161) x (cid:48)Î² (cid:98) (cid:162) .AnestimatoroftheAMEis A(cid:129)ME= n 1 (cid:88) n Î´ (cid:98)(X i )=Î² (cid:98) n 1 (cid:88) n g (cid:161) X i (cid:48)Î² (cid:98) (cid:162) . i=1 i=1 WhenthevectorX includesnonlineartransformationsthemarginaleffectsneedtobecarefullyde- fined.Forexample,inthemodel(cid:80)[Y =1|X =x]=G (cid:161)Î² +Î² x+Â·Â·Â·+Î² xp(cid:162) themarginaleffectis 0 1 p Î´(x)=(cid:161)Î² +Â·Â·Â·+pÎ² xpâ1(cid:162) g (cid:161)Î² +Î² x+Â·Â·Â·+Î² xp(cid:162) . 1 p 0 1 p AnestimatorofÎ´(x)is Î´ (cid:98)(x)=(cid:161)Î² (cid:98)1 +Â·Â·Â·+pÎ² (cid:98)p xpâ1(cid:162) g (cid:161)Î² (cid:98)0 +Î² (cid:98)1 x+Â·Â·Â·+Î² (cid:98)p xp(cid:162) . AnestimatoroftheAMEisA(cid:129)ME= n 1(cid:80)n i=1 Î´ (cid:98)(X i ). InStata,marginaleffectscanbeestimatedwithmargins,dydx(*). 25.10 Application Weillustratewithanapplicationtotheprobabilityofmarriageusingthecps09mardataset. Weuse thesubsampleofmenwithagesupto35(n=9137). Weincludeasregressorsanindividualâsage,edu- cation,andindicatorsforBlack,Asian,Hispanic,andthreeregions. Weestimatelinearlogitandlinear probit models, calculate coefficients and average marginal effects, and report robust standard errors. TheresultsarereportedinTable25.1. Readingthetablesyouwillseethatthelogitandprobitcoefficientestimatesallhavethesamesigns butthelogitcoefficientsarelargerinmagnitude.Weseethattheprobabilityofmarriageisincreasingin age,islowerforBlackindividuals,andvariesacrossgeographicregions.Thecoefficientsthemselvesare difficulttointepretsoitisbettertofocusontheestimatedmarginaleffects. Doingso, weseethatthe logitandprobitestimatesareessentiallyidentical. Thisisacommonfindinginempiricalapplications whenbothprovidegoodapproximationstotheresponseprobability.Examiningthecoefficientsfurther we see that the probability of marriage increases about 4.5% for each year of age. The point estimate of the impact of education is 0.3% per year of education, which is a small magnitude. Black men are married with a reduced probability of 15% relative to the omitted category (men who are not Black, Asian,norHispanic).TheestimatedmarginaleffectsforAsianandHispanicmenaresmall(1%andâ2%, respectively)relativetotheomittedcategory.Comparingregionsweseethatmarriageratesareabout6- 8%higherintheMidwest,South,andWestrelativetotheNorthEast(theomittedcategory).Equivalently, menintheNorthEasthaveareducedprobabilityofabout7%relativetotherestofthecountry. Twomessagesfromthisapplicationarethatthechoiceoflogitversusprobitisunimportantandthat itisbettertofocusonmarginaleffectsthancoefficients.Ahiddenmessageisthat(asforalleconometric applications) model specification is critically important. The estimates in Table 25.1 are for men with ages19-35. Thischoicewasmadesothattheresponseprobabilitywouldbewellmodeledwithasingle linear term in age",
    "page": 831,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Twomessagesfromthisapplicationarethatthechoiceoflogitversusprobitisunimportantandthat itisbettertofocusonmarginaleffectsthancoefficients.Ahiddenmessageisthat(asforalleconometric applications) model specification is critically important. The estimates in Table 25.1 are for men with ages19-35. Thischoicewasmadesothattheresponseprobabilitywouldbewellmodeledwithasingle linear term in age. If instead the estimates were calculated on the full sample (and still using a linear specification)thentheestimatedmarginaleffectofagewouldhavebeen1%peryearratherthan4.5%,a largemis-estimateoftheeffectofageonmarriageprobability. 25.11 SemiparametricBinaryChoice Thesemiparametricbinarychoicemodelis (cid:80)[Y =1|X]=G (cid:161) X (cid:48)Î²(cid:162)",
    "page": 831,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER25. BINARYCHOICE 812 Table25.1:BinaryChoiceRegressionsforMarriage Logit Probit Coefficient AME Coefficient AME age 0.217 0.044 0.132 0.045 (0.006) (0.001) (0.003) (0.001) education 0.014 0.003 0.009 0.003 (0.010) (0.002) (0.006) (0.002) Black â0.767 â0.156 â0.454 â0.153 (0.092) (0.018) (0.054) (0.018) Asian 0.033 0.007 0.025 0.008 (0.103) (0.021) (0.063) (0.021) Hispanic â0.084 â0.017 â0.048 â0.017 (0.063) (0.013) (0.038) (0.013) MidWest 0.272 0.056 0.165 0.056 (0.074) (0.011) (0.045) (0.015) South 0.338 0.069 0.203 0.069 (0.070) (0.014) (0.043) (0.014) West 0.383 0.078 0.228 0.077 (0.072) (0.015) (0.044) (0.015) Intercept â6.45 â3.93 (0.21) (0.12) whereG(x)isunknown.InteresttypicallyfocusesonthecoefficientsÎ². In the latent variable framework G(x) is the distribution function of a latent error e. As it is not credible that the distributionG(x) is known, the semiparametric model treatsG(x) as unknown. The goalistoestimatethecoefficientÎ²whileagnosticaboutG(x). Therearemanycontributionstothisliterature. TwoofthemostinfluentialareManski(1975)and KleinandSpady(1993).BothusethelatentvariableframeworkY â=X (cid:48)Î²+e. Manksi(1975)showsthatÎ²isidentifieduptoscaleifmed[e|X]=0.Heproposesaclevermaximum scoreestimatoriswhichisconsistentforÎ²uptoscaleunderthisweakcondition.Hismethod,however, doesnotpermitestimationoftheresponseprobabilitiesnormarginaleffectssincehisassumptionsare insufficienttoidentifyG(x). KleinandSpady(1993)addtheassumptionthate isindependentof X whichimpliesidentification ofG(x). They propose simultaneous estimation of Î² andG(x) based on the following two properties: (1) IfG(x) is known then Î² can be estimated by maximum likelihood; (2) If Î² is known thenG(x) can beestimatedbynonparametricregressionofY on X (cid:48)Î². Combiningthesetwopropertiesintoanested criterion,theyproduceaconsistent,asymptoticallynormal,andefficientestimatorofÎ². WhiletheideasofManskiandKlein-Spadyarequiteclever, thetroubleisthatmodelG(x (cid:48)Î²)relies on the parametric linear index assumption. Suppose we relax the latter to a nonparametric function m(x) but assume that e is independent of X. Then the response probability is 1âG(âm(x)). In this caseneitherG(x)norm(x)isidentifed;onlythecompositefunctionG(âm(x))isidentified.Thismeans that estimation of m(x) while agnostic aboutG(x) is impossible without a parametric assumption on m(x). Consequently,themodernviewistotakeP(x)=(cid:80)[Y =1|X =x]asnonparametricallyidentified. Consistentestimationcanbeachievedthroughaseriesapproximation,eitherusingalinear,probit,or logit link. From this viewpoint there is no gain from the restriction to the function form G(x (cid:48)Î²), and hencenogainfromthesemiparametricapproach.",
    "page": 832,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER25. BINARYCHOICE 813 25.12 IVProbit Alatentvariablestructuralequationmodelis Y â=X (cid:48)Î² +Y Î² +e 1 1 2 2 1 Y =X (cid:48)Î³ +Z (cid:48)Î³ +e 2 1 2 2 Y =1(cid:169) Y â>0 (cid:170) . 1 1 Inthismodel,Y isscalar,endogenous,andcontinuouslydistributed,X areincludedexogenousregres- 2 sors,andZ areexcludedexogenousinstruments. Thestandardestimationmethodismaximumlikelihoodbasedontheassumptionthattheerrorsare jointlynormal (cid:181) e 1 (cid:182)(cid:175) (cid:175) (cid:175)(X,Z)â¼N (cid:181)(cid:181) 0 (cid:182) , (cid:181) 1 Ï 12 (cid:182)(cid:182) . e (cid:175) 0 Ï Ï2 2 21 2 Thelikelihoodisderivedasfollows.Theregressionofe one equals 1 2 e =Ïe +Îµ 1 2 Ï Ï= 12 Ï2 2 Îµâ¼N (cid:161) 0,Ï2 Îµ (cid:162) Ï2 Ï2 Îµ =1â 12. Ï2 2 Usingtheserelationshipswecanwritethestructuralequationas Y â=Âµ(Î¸)+Îµ (25.16) 1 Âµ(Î¸)=X (cid:48)Î² +Y Î² +Ï(cid:161) Y âX (cid:48)Î³ âZ (cid:48)Î³ (cid:162) . 1 2 2 2 1 2 TheerrorÎµisindependentofe 2 andhenceofY 2 .ThustheconditionaldistributionofY 1 â isN (cid:161)Âµ(Î¸),Ï2 Îµ (cid:162) . Itfollowsthatthejointdensityfor(Y ,Y )is 1 2 (cid:181)Âµ(Î¸) (cid:182)Y1 (cid:181) (cid:181)Âµ(Î¸) (cid:182)(cid:182)1âY1 1 (cid:181) Y âX (cid:48)Î³ âZ (cid:48)Î³ (cid:182) Î¦ 1âÎ¦ Ï 2 1 2 . Ï Ï Ï Ï Îµ Îµ 2 2 TheparametervectorisÎ¸=(cid:161)Î² 1 ,Î² 2 ,Î³ 1 ,Î³ 2 ,Ï,Ï2 Îµ,Ï2 2 (cid:162) . Thejointlog-likelihoodforarandomsample{Y ,Y ,X ,Z }is 1i 2i i i (cid:96) (Î¸)= (cid:88) n (cid:183) Y logÎ¦ (cid:181)Âµ i (Î¸) (cid:182) +(1âY )log (cid:181) 1âÎ¦ (cid:181)Âµ i (Î¸) (cid:182)(cid:182)(cid:184) n 1i Ï 1i Ï i=1 Îµ Îµ â n log(2Ï)â n logÏ2â 1 (cid:88) n (cid:161) Y âX (cid:48)Î³ âZ (cid:48)Î³ (cid:162)2 . 2 2 2 2Ï2 2i=1 2i i 1 i 2 ThemaximumlikelihoodestimatorÎ¸ (cid:98)isfoundbynumericallymaximizing(cid:96) n (Î¸). Theprobitassumption(thetreatmentof(e ,e )asjointlynormal)isimportantforthisderivationasit 1 2 allowsasimplefactorizationofthejointdistributionintotheconditionaldistributionofY givenY and 1 2 themarginaldistributionofY .Asimilarfactorizationisnoteasilyaccomplishedinalogitframework. 2 InStatathisestimatorcanbeimplementedwiththeivprobitcommand.",
    "page": 833,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER25. BINARYCHOICE 814 25.13 BinaryPanelData Abinarychoicepanelmodelistypicallywrittenas Y â=X (cid:48) Î²+u +e it it i it Y =1(cid:169) Y â>0 (cid:170) . it it Here,theobservationsare(Y ,X )fori =1,...,n andt =1,...,T withY binary. Forexample,Y could it it it it denotethedecisionofindividuali tomakeapurchaseinperiodt. Theindividualeffectu isintended i to capture the feature that some individuals i make purchases more (or less) frequently than can be explainedbytheregressors. Asinlinearmodelsthereisadistinctionbetweentreatingtheindividualeffectu asrandom(mean- i ingthatitisindependentoftheregressors)orfixed(meaningthatitiscorrelatedwiththeregressors). Undertherandomeffectsassumptionthemodelcanbeestimatedbymaximumlikelihood. Thisis calledrandomeffectsprobitorrandomeffectslogitdependingontheerrordistribution. Allowingforfixedeffectsismorecomplicated.Theindividualeffectcannotbeeliminatedbyalinear operationsinceY isanonlinearfunctionofu .Forexample,thetransformation it i (cid:110) (cid:111) âY =1(cid:169) Y â>0 (cid:170)â1 Y â >0 it it i,tâ1 doesnoteliminateu . i Consequentlythereisnofixedeffectsestimatorfortheprobitmodel. However,forthelogitmodel Chamberlain (1980, 1984) developed a fixed effects estimator based on a conditional likelihood. He showedthatafeatureofthelogisticdistributionisthattheindividualeffectcanbeeliminatedviaodds ratios,allowingthecalculationofthelikelihoodconditionalonthesumofthedependentvariable. WeillustratetheconstructionofthelikelihoodforthecaseT =2. LetY ,Y denotetheoutcomes i1 i2 andN =Y +Y denotetheirsum.Wecalculatetheconditionaldistributionof(Y ,Y )givenN .This i i1 i2 i1 i2 i isthedistributionofthechoicesgiventhetotalnumberofchoices. WhenN =0orN =2thelikelihoodistrivial.Thatis, i i (cid:80)[Y =0|N =0]=0 it i (cid:80)[Y =1|N =2]=1. it i ThisdoesnotdependonÎ²,sodoesnotaffectestimation.ThuswefocusexclusivelyonthecaseN =1. i Thechoiceprobabilitiesare exp (cid:161)âX (cid:48) Î²âu (cid:162) (cid:80)[Y =0]= it i it 1+exp (cid:161)âX (cid:48) Î²âu (cid:162) it i 1 (cid:80)[Y =1]= . it 1+exp (cid:161)âX (cid:48) Î²âu (cid:162) it i Theirratiois (cid:80)[Y it =0] =exp (cid:161)âX (cid:48) Î²âu (cid:162) . (cid:80)[Y =1] it i it Takingafurtherratiofort=1andt=2weobtain (cid:80)[Y i1 =0](cid:80)[Y i2 =1] = exp (cid:161)âX 1 (cid:48) t Î²âu i (cid:162) =exp (cid:161) (X âX ) (cid:48)Î²(cid:162) . (cid:80)[Y i1 =1](cid:80)[Y i2 =0] exp (cid:161)âX 2 (cid:48) t Î²âu i (cid:162) 2t 1t",
    "page": 834,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER25. BINARYCHOICE 815 Thisdoesnotdependonthefixedeffectu . ItisafunctiononlyofthechangeintheregressorsâX = i i X âX . i2 i1 Thisiseffectivelyanonlinearwithintransformation. Thisoddsratioeliminatesdependenceonthe individualeffectduetotheexponentialstructureofthelogisticdistribution. Thisisspecialtothelogit modelandallowsconstructionofaconditionallikelihoodfunctionwhichisfreeofu . i Considerthechoiceprobabilityforperiod1givenN =1.Itis i (cid:80)[Y =1,Y =0] (cid:80)[Y =1|N =1]= i1 i2 i1 i (cid:80)[Y =1,Y =0]+(cid:80)[Y =0,Y =1] i1 i2 i1 i2 (cid:80)[Y =1](cid:80)[Y =0] = i1 i2 (cid:80)[Y =1](cid:80)[Y =0]+(cid:80)[Y =0](cid:80)[Y =1] i1 i2 i1 i2 1 = 1+(cid:80)[Yi1 =0](cid:80)[Yi2 =1] (cid:80)[Yi1 =1](cid:80)[Yi2 =0] 1 = 1+exp (cid:161)âX (cid:48)Î²(cid:162) i =1âÎ(cid:161)âX (cid:48)Î²(cid:162) . i Similarly(cid:80)[Y =0|N =1]=Î(cid:161)âX (cid:48)Î²(cid:162) .Togetherthelog-likelihoodfunctionis i1 i i n (cid:96) (Î²)= (cid:88)1 {N =1} (cid:163) (1âY )Y logÎ(cid:161)âX (cid:48)Î²(cid:162)+Y (1âY )log (cid:161) 1âÎ(cid:161)âX (cid:48) Î²(cid:162)(cid:162)(cid:164) . n i i1 2i i i1 i2 it i=1 Thisconditionallikelihoodisfreeoftheindividualeffect. Sincetheconditionallikelhoodcanbecom- puteditcanbemaximizedtoobtaintheconditionallikelihoodestimator. Inorderforthelikelihoodtobeanon-degeneratefunctionofÎ²itisnecessaryfortheretobeindivid- ualswhoareswitchers(selectY =0inoneperiodandY =1intheotherperiod)andthatswitchershave time-varyingregressors.Coefficientsfortime-invariantregressorsarenotidentified. OurderivationfocusedonT =2.TheextensiontoT >2issimilarbutisalgebraicallycomplicated. InStata,randomeffectsprobitisimplementedwithxtoprobitandrandomeffectslogitwithxtologit. Fixedeffectsprobitcanbeimplementedwithxtologit,feorclogit. 25.14 TechnicalProofs* ProofofTheorem25.1: TheMLEÎ² (cid:98) logit isanm-estimatorsoweappealtoTheorem22.1, whichshows thatÎ² (cid:98) logit isconsistentifn â1(cid:96)logit (Î²)isuniformlyconsistentfor(cid:96)logit(Î²)andÎ²logit uniquelyminimizes n (cid:96)logit(Î²).ThelatterholdsundertheassumptionQ >0soweneedtoestablishuniformconsistency. logit TodosoweappealtoTheorem22.2whichholdsunderourassumptionsif(a)â(cid:69)(cid:163) logÎ(cid:161) Z (cid:48)Î²(cid:162)(cid:164)<â forallÎ²;and(b)forsome A<â,(cid:69)(cid:175) (cid:175)logÎ(cid:161) Z (cid:48)Î² 1 (cid:162)âlogÎ(cid:161) Z (cid:48)Î² 2 (cid:162)(cid:175) (cid:175) â¤A (cid:176) (cid:176) Î² 1 âÎ² 2 (cid:176) (cid:176)forallÎ² 1 ,Î² 2 . First, âlogÎ(t)=log (cid:161) 1+exp(ât) (cid:162)â¤log (cid:161) 1+exp(|t|) (cid:162)â¤log(2)+|t|. Thus â(cid:69)(cid:163) logÎ(cid:161) Z (cid:48)Î²(cid:162)(cid:164)â¤log(2)+(cid:69)(cid:175) (cid:175)X (cid:48)Î²(cid:175) (cid:175) â¤log(2)+(cid:176) (cid:176) Î²(cid:176) (cid:176) (cid:69)(cid:107)X(cid:107)<â since(cid:69)(cid:107)X(cid:107)<âbyassumption.Thisis(a)",
    "page": 835,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". First, âlogÎ(t)=log (cid:161) 1+exp(ât) (cid:162)â¤log (cid:161) 1+exp(|t|) (cid:162)â¤log(2)+|t|. Thus â(cid:69)(cid:163) logÎ(cid:161) Z (cid:48)Î²(cid:162)(cid:164)â¤log(2)+(cid:69)(cid:175) (cid:175)X (cid:48)Î²(cid:175) (cid:175) â¤log(2)+(cid:176) (cid:176) Î²(cid:176) (cid:176) (cid:69)(cid:107)X(cid:107)<â since(cid:69)(cid:107)X(cid:107)<âbyassumption.Thisis(a). Second,for(b)notethat|h(t)|=1âÎ(t)â¤1.Then (cid:34) (cid:176) â (cid:176)(cid:35) (cid:69) sup (cid:176) (cid:176) logÎ(cid:161) Z (cid:48)Î²(cid:162)(cid:176) (cid:176) â¤(cid:69)(cid:107)X(cid:107)sup|h(t)|â¤(cid:69)(cid:107)X(cid:107)<â. Î² (cid:176)âÎ² (cid:176) t",
    "page": 835,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER25. BINARYCHOICE 816 Part(b)follows.ThisissufficientforTheorem22.2asneeded. â  ProofofTheorem25.2:FollowingtheproofofTheorem25.1,weneedtoshowthat(a)â(cid:69)(cid:163) logÎ¦(cid:161) Z (cid:48)Î²(cid:162)(cid:164)< âforallÎ²;and(b)forsome A<â,(cid:69)(cid:175) (cid:175)logÎ¦(cid:161) Z (cid:48)Î² 1 (cid:162)âlogÎ¦(cid:161) Z (cid:48)Î² 2 (cid:162)(cid:175) (cid:175) â¤A (cid:176) (cid:176) Î² 1 âÎ² 2 (cid:176) (cid:176)forallÎ² 1 ,Î² 2 . Theorem5.7.6ofIntroductiontoEconometricsstatesthat d Ï(t) logÎ¦(t)=Î»(t)= â¤1+|t|. (25.17) dt Î¦(t) Thisimplies (cid:179) (cid:112) (cid:180) t2 âlogÎ¦(t)â¤log 2Ï + +log(1+|t|). 2 UsingtheSchwarz(B.12)andJensen(B.27)inequalities, (cid:112) â(cid:69)(cid:163) logÎ¦(cid:161) Z (cid:48)Î²(cid:162)(cid:164)â¤log (cid:179) 2Ï (cid:180) +(cid:69)(cid:175) (cid:175)X (cid:48)Î²(cid:175) (cid:175) 2+(cid:69)(cid:163) log (cid:161) 1+(cid:175) (cid:175)X (cid:48)Î²(cid:175) (cid:175) (cid:162)(cid:164) (cid:112) â¤2log (cid:179) 2Ï (cid:180) +(cid:176) (cid:176) Î²(cid:176) (cid:176) 2(cid:69)(cid:107)X(cid:107)2+2log (cid:161) 1+(cid:176) (cid:176) Î²(cid:176) (cid:176) (cid:69)(cid:107)X(cid:107)(cid:162)<â. Thuspart(a)holds. For(b),using(25.17) (cid:34) (cid:176) â (cid:176)(cid:35) (cid:34) (cid:35) (cid:69) sup (cid:176) (cid:176) logÎ¦(cid:161) Z (cid:48)Î²(cid:162)(cid:176) (cid:176) â¤(cid:69) (cid:107)X(cid:107)sup (cid:175) (cid:175) Î»(cid:161) Z (cid:48)Î²(cid:162)(cid:175) (cid:175) Î² (cid:176)âÎ² (cid:176) Î² (cid:34) (cid:195) (cid:33)(cid:35) â¤(cid:69) (cid:107)X(cid:107) 1+sup (cid:176) (cid:176)Z (cid:48)Î²(cid:175) (cid:175) Î² â¤(cid:69)(cid:107)X(cid:107)+(cid:176) (cid:176) Î²(cid:176) (cid:176) (cid:69)(cid:107)X(cid:107)2<â. Thisimplies(b)asneeded. â  ProofofTheorem25.3: SinceÎ² (cid:98) logit isanm-estimatorweverifytheconditionsforTheorem22.3,which are (a) (cid:69)(cid:176) (cid:176)Zh logit (cid:161) Z (cid:48)Î²logit(cid:162)(cid:176) (cid:176) 2 <â; (b)Q logit (Î²)=(cid:69)(cid:163) XX (cid:48)Î(cid:161) X (cid:48)Î²(cid:162)(cid:161) 1âÎ(cid:161) X (cid:48)Î²(cid:162)(cid:162)(cid:164) is continuous in a neigh- borhoodofÎ²logit;(c)ForsomeA<âandÎ±>0,(cid:69)(cid:176) (cid:176)Zh logit (cid:161) X (cid:48)Î² 1 (cid:162)âZh logit (cid:161) X (cid:48)Î² 2 (cid:162)(cid:176) (cid:176) 2â¤A (cid:176) (cid:176) Î² 1 âÎ² 2 (cid:176) (cid:176) Î± forall Î² ,Î² inaneighborhoodofÎ²logit;(d)Q >0;(e)Î²logitisintheinterioroftheparameterspace.(d)and 1 2 logit (e)holdbyassumption,soweverify(a),(b)and(c). First, using (cid:175) (cid:175)h logit (t) (cid:175) (cid:175) â¤ 1, we see that (cid:69)(cid:176) (cid:176)Zh logit (cid:161) Z (cid:48)Î²logit(cid:162)(cid:176) (cid:176) 2 â¤ (cid:69)(cid:107)X(cid:107)2 < â and (a) holds. Second, Q (Î²)iscontinuousinÎ²sinceitisboundedandÎ(x)iscontinuous. Thus(b)holds",
    "page": 836,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". First, using (cid:175) (cid:175)h logit (t) (cid:175) (cid:175) â¤ 1, we see that (cid:69)(cid:176) (cid:176)Zh logit (cid:161) Z (cid:48)Î²logit(cid:162)(cid:176) (cid:176) 2 â¤ (cid:69)(cid:107)X(cid:107)2 < â and (a) holds. Second, Q (Î²)iscontinuousinÎ²sinceitisboundedandÎ(x)iscontinuous. Thus(b)holds. Third,asshown logit inExercise25.5(d), (cid:175) (cid:175)H logit (t) (cid:175) (cid:175) â¤1.Then (cid:34) (cid:176) â (cid:176)2 (cid:35) (cid:69) su Î² p (cid:176) (cid:176) (cid:176)âÎ² Zh logit (cid:161) X (cid:48)Î²(cid:162)(cid:176) (cid:176) (cid:176) â¤(cid:69)(cid:107)X(cid:107)4su t p (cid:175) (cid:175)H logit (t) (cid:175) (cid:175) <â whichimplies(c).ThisverifiestheconditionsforTheorem22.3asneeded. â  ProofofTheorem25.4:TheprooffollowsthesamelinesasTheorem25.3.Itissufficienttoveritythat(a) (cid:69)(cid:176) (cid:176)ZÎ»(cid:161) Z (cid:48)Î²probit(cid:162)(cid:176) (cid:176) 2<â;(b)Q probit (cid:161)Î²(cid:162)=(cid:69)(cid:163) XX (cid:48) H probit (cid:161) Z (cid:48)Î²(cid:162)(cid:164) iscontinuousinaneighborhoodofÎ²probit; (c)ForsomeA<âandÎ±>0,(cid:69)(cid:176) (cid:176)ZÎ»(cid:161) X (cid:48)Î² 1 (cid:162)âZÎ»(cid:161) X (cid:48)Î² 2 (cid:162)(cid:176) (cid:176) 2â¤A (cid:176) (cid:176) Î² 1 âÎ² 2 (cid:176) (cid:176) Î± forallÎ² 1 ,Î² 2 inaneighborhood ofÎ²probit. First,using(25.17) (cid:69) (cid:176) (cid:176)ZÎ» (cid:179) Z (cid:48)Î²probit (cid:180)(cid:176) (cid:176) 2 â¤(cid:69) (cid:179) (cid:107)X(cid:107) (cid:179) 1+ (cid:175) (cid:175)X (cid:48)Î²probit (cid:175) (cid:175) (cid:180)(cid:180)2 â¤(cid:69)(cid:107)X(cid:107)2+ (cid:176) (cid:176)Î²probit (cid:176) (cid:176) 2 (cid:69)(cid:107)X(cid:107)4<â (cid:176) (cid:176) (cid:175) (cid:175) (cid:176) (cid:176)",
    "page": 836,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER25. BINARYCHOICE 817 implying(a).Second,Q (Î²)iscontinuousinÎ²sinceitisboundedandH (x)iscontinuous.Thus probit probit (b)holds. Third,Theorem5.7.7ofIntroductiontoEconometricsstatesthat (cid:175) (cid:175)H probit (t) (cid:175) (cid:175) â¤1. Thusbythe same argument as in the proof of Theorem 25.3 we can verify (c). We have verified the conditions for Theorem22.3asneeded. â  _____________________________________________________________________________________________ 25.15 Exercises Exercise25.1 Emily estimates a probit regression setting her dependent variable to equal Y =1 for a purchaseandY =0fornopurchase. Usingthesamedataandregressors, Jacobestimatesaprobitre- gressionsettingthedependentvariabletoequalY =1ifthereisnopurchaseandY =0forapurchase. Whatisthedifferenceintheirestimatedslopecoefficients? Exercise25.2 Jackson estimates a logit regression where the primary regressor is measured in dollars. Julieesitmatesalogitregressionwiththesamesampleanddependentvariable, butmeasuresthepri- maryregressorinthousandsofdollars.Whatisthedifferenceintheestimatedslopecoefficients? Exercise25.3 Show(25.1)and(25.2). Exercise25.4 Verify(25.5),thatÏ(Y |X)=G (cid:161) Z (cid:48)Î²(cid:162) . Exercise25.5 ForthelogisticdistributionÎ(x)=(cid:161) 1+exp(âx) (cid:162)â1 verifythat (a) d Î(x)=Î(x)(1âÎ(x)). dx (b) h (x)= d logÎ(x)=1âÎ(x). logit dx (c) H (x)=â d2 logÎ(x)=Î(x)(1âÎ(x)). logit dx2 (d) (cid:175) (cid:175)H logit (x) (cid:175) (cid:175) â¤1. Exercise25.6 ForthenormaldistributionÎ¦(x)verifythat (a) h (x)= d logÎ¦(x)=Î»(x)whereÎ»(x)=Ï(x)/Î¦(x). probit dx (b) H (x)=â d2 logÎ¦(x)=Î»(x)(x+Î»(x)). probit dx2 Exercise25.7 (a) Verifyequations(25.6)and(25.7). (b) VerifytheassertionthatH(x)>0impliesthatH (Î²)>0globallyinÎ². n (c) Verifytheassertionthatpart(b)impliesthat(cid:96) (Î²)isgloballyconcaveinÎ². n Exercise25.8 Findthefirst-orderconditionforÎ² fromthepopulationmaximizationproblem(25.8). 0 Exercise25.9 Findthefirst-orderconditionforthelogitMLEÎ² (cid:98) logit. Exercise25.10 Findthefirst-orderconditionfortheprobitMLEÎ² (cid:98) probit.",
    "page": 837,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER25. BINARYCHOICE 818 Exercise25.11 Show (25.14). In the logit model show that the right hand side of (25.14) simplies to (cid:161) Y âÎ(cid:161) X (cid:48)Î²(cid:162)(cid:162)2 . Exercise25.12 ShowhowtouseNLLStoestimateaprobitmodel. Exercise25.13 TaketheendogenousprobitmodelofSection25.12. (a) Verifyequation(25.16). (b) ExplainwhyÎµisindependentofe andY . 2 2 (c) VerifythattheconditionaldistributionofY 1 â isN (cid:161)Âµ(Î¸),Ï2 Îµ (cid:162) . Exercise25.14 Taketheheteroskedasticnonparametricbinarychoicemodel Y â=m(X)+e e|X â¼N (cid:161) 0,Ï2(X) (cid:162) Y =Y â1(cid:169) Y â>0 (cid:170) . Theobservablesare{Y ,X :i =1,...,n}.Thefunctionsm(x)andÏ2(x)arenonparametric. i i (a) Findaformulafortheresponseprobability. (b) Arem(x)andÏ2(x)bothidentified?Explain. (c) Findanormalizationwhichachievesidentification. (d) Given your answer to part (c), does it make sense to âallow for heteroskedasticityâ in the binary choicemodel?Explain? Exercise25.15 Usethecps09mardatasetandthesubsetofmen.SetY =1iftheindividualisamember ofalaborunion(union=1)andY =0otherwise.Estimateaprobitmodelasalinearfunctionofage,edu- cation,andindicatorsforBlackindividualsandforHispanicindividuals.Reportthecoefficientestimates andstandarderrors.Interprettheresults. Exercise25.16 Replicatethepreviousexercisebutwiththesubsetofwomen.Interprettheresults. Exercise25.17 Usethecps09mardatasetandthesubsetofwomenwithacollegedegree. SetY =1if marital equals 1, 2, or 3, andset Y =0 otherwise. Estimate a binary choice model for Y as apossibly nonlinearfunctionofage. Describethemotivationforthemodelyouuse. Plottheestimatedresponse probability.HowdotheestimatescomparewiththoseformenfromFigure25.1? Exercise25.18 Use the cps09mar dataset and the subset of men. Set Y as in the previous question. EstimateabinarychoicemodelforY asapossiblynonlinearfunctionofage,alinearfunctionofeduca- tion,andincludingindicatorsforBlackindividualsandforHispanicindividuals. Reportthecoefficient estimatesandstandarderrors.Interprettheresults. Exercise25.19 Replicatethepreviousexercisebutwiththesubsetofwomen.Interprettheresults.",
    "page": 838,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Chapter 26 Multiple Choice 26.1 Introduction Thischaptersurveysmultinomialmodels. Thisincludesmultinomialresponse, multinomiallogit, conditional logit, nested logit, mixed logit, multinomial probit, ordered response, count data, and the BLPdemandmodel. FormoredetailedtreatmentsseeMaddala(1983),CameronandTrivedi(1998),CameronandTrivedi (2005),Train(2009),andWooldridge(2010). 26.2 MultinomialResponse AmultinomialrandomvariableY takesvaluesinafiniteset,typicallywrittenasY â{1,2,...,J}. The elements of the set are often called alternatives. In most applications the alternatives are categorical (car,bicycle,airplane,train)andunordered. Whentherearenoregressorsthemodelisfullydescribed bytheJ probabilitiesP =(cid:80)(cid:163) Y =j (cid:164) . j Wetypicallydescribethepair(Y,X)asmultinomialresponsewhenY ismultinomialandX â(cid:82)k are regressors.TheconditionaldistributionofY givenX issummarizedbytheresponseprobability P (x)=(cid:80)(cid:163) Y =j |X =x (cid:164)=(cid:69)(cid:163)1(cid:169) Y =1 (cid:170)|X =x (cid:164) . j j Theresponseprobabilitiesarenonparametricallyidentifiedandcanbearbitraryfunctionsofx. We illustrate by extending the marriage status example of the previous chapter. The CPS variable maritalrecordssevencategories.Wepartitiontheseintofouralternatives:âmarriedâ1,âdivorcedâ,âsep- aratedâ,andânevermarriedâ. LetX beage. P (x)for j =1,...,4istheprobabilityofeachmarriagestatus j asafunctionofage.Forourillustrationwetakethepopulationofcollege-educatedwomen. SincetheresponseprobabilitiesP (x)arenonparametricallyidentifiedasimpleestimationmethod j isbinaryresponseseparatelyforeachcategory.WeplotinFigure26.1(a)logitestimatesusingaquadratic splineinageandasingleknotatage40. Theestimatesshowthattheprobabilityofânevermarriedâde- creasesmonotonicallywithage,thatforâmarriedâincreasesuntilaround38andthendecreasesslowly, theprobabilityofâdivorcedâincreasesmonotonicallywithage,andtheprobabilityofâseparatedâislow forallagegroups. A defect of the estimates of Figure 26.1(a) is that the sum of the four estimated probabilities (dis- played with the dotted line) does not equal one. This shows that separate estimation of the response probabilitiesneglectssysteminformation. 1marital=1,2,3,4,whichincludeswidowed. 819",
    "page": 839,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER26. MULTIPLECHOICE 820 20 30 40 50 60 70 80 Age 0.1 8.0 6.0 4.0 2.0 0.0 Never Married Married Divorced Separated Total 20 30 40 50 60 70 80 Age (a)BinaryResponseEstimates 0.1 8.0 6.0 4.0 2.0 0.0 Never Married Married Divorced Separated (b)MultinomialLogit Figure26.1:ProbabilityofMaritalStatusGivenAgeforCollegeEducatedWomen Multinomialresponseistypicallymotivatedandderivedfromamodeloflatentutility. Theutilityof alternative j isassumedtoequal U â=X (cid:48)Î² +Îµ (26.1) j j j where Î² are coefficients and Îµ is an alternative-specific error. The coefficients Î² describe how the j j j variableX affectsanindividualâsutilityofalternative j. TheerrorÎµ isindividual-specificandcontains j unobservedfactorsaffectinganindividualâsutility. Inthemarriagestatusexample(where X isage)the coefficientsÎ² describehowtheutilityofeachmarriagestatusvarieswithage,whiletheerrorÎµ contains j j theindividualfactorswhicharenotcapturedbyage. Inthelatentutilitymodelanindividualisassumedtoselectthealternativewiththehighestutility U â . Thus Y = j ifU â â¥U â for all (cid:96). In model (26.1) this choice is unaltered if we add X (cid:48)Î³ to each j j (cid:96) utility. ThismeansthatthecoefficientsÎ² arenotseparatelyidentified,atbestthedifferencesbetween j alternativesÎ² j âÎ² (cid:96)areidentified. Identificationisachievedbyimposinganormalization;thestandard choiceistosetÎ² =0forabasealternative j,oftentakenasthelastcategoryJ.ReportedcoefficientsÎ² j j shouldbeinterpretedasdifferencesrelativetothebasealternative. Thechoiceisalsounchangedifeachutility(26.1)ismultipliedbypositiveconstant.Thismeansthat thescaleofthecoefficientsÎ² isnotidentified.Toachieveidentificationitistypicaltofixthescaleofthe j errorsÎµ .ConsequentlythescaleofthecoefficientsÎ² hasnointerpretivemeaning. j j Twoclassicalmultinomialresponsemodelsarelogitandprobit. Weintroducemultinomiallogitin thenextsectionandmultinomialprobitinSection26.8. 26.3 MultinomialLogit Thesimplemultinomiallogitmodelis exp (cid:161) x (cid:48)Î² (cid:162) P (x)= j . (26.2) j J (cid:88) exp (cid:161) x (cid:48)Î² (cid:96) (cid:162) (cid:96)=1 Themodelincludesbinarylogit(J =2)asaspecialcase. Wecall(26.2)thesimplemultinomiallogitto distinguishitfromtheconditionallogitmodelofthenextsection. Themultinomiallogitarisesfromthelatentutilitymodel(26.1)forthefollowingerrordistributions.",
    "page": 840,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER26. MULTIPLECHOICE 821 Definition26.1 TheTypeIExtremeValuedistributionfunctionis F(Îµ)=exp (cid:161)âexp(âÎµ) (cid:162) . Definition26.2 TheGeneralizedExtremeValue(GEV)jointdistributionis (cid:195) (cid:34) (cid:35)Ï(cid:33) F (cid:161)Îµ ,Îµ ,...,Îµ (cid:162)=exp â (cid:88) J exp (cid:179) â Îµ j(cid:180) (26.3) 1 2 J Ï j=1 for0<Ïâ¤1. For J =1theGEVdistribution(26.3)equalstheTypeIextremevalue. For J >1andÏ=1theGEV distributionequalstheproductofindependentTypeIextremevaluedistributions. For J >1andÏ<1 GEVrandomvariablesaredependentwithcorrelationequalto1âÏ2 (seeKotzandNadarajah(2000)). TheparameterÏisknownasthedissimilarityparameter. Thedistribution(26.3)isaspecialcaseofthe âGEVdistributionâintroducedbyMcFadden(1981).Furthermore,thereisheterogeneityamongauthors regardingthechoiceofnotationandlabeling.ThenotationusedaboveisconsistentwiththeStataman- ual. Incontrast, McFadden(1978, 1981)used1âÏinplaceofÏandcalledÏthesimilarityparameter. CameronandTrivedi(2005)usedÏinsteadofÏandcalledÏthescaleparameter. ThefollowingresultisduetoMcFadden(1978,1981). Theorem26.1 Assumetheutilityofalternative j isU â=X (cid:48)Î² +Îµ andtheer- j j j rorvector(Îµ ,...,Îµ )hasGEVdistribution(26.3). Thentheresponseprobabili- 1 J tiesequal exp (cid:161) X (cid:48)Î² /Ï(cid:162) P (X)= j . j J (cid:88) exp (cid:161) X (cid:48)Î² (cid:96)/Ï(cid:162) (cid:96)=1 TheproofisinSection26.13.TheresponseprobabilitiesinTheorem26.1aremultinomiallogit(26.2) withcoefficientsÎ²â=Î² /Ï.ThedissimilarityparameterÏonlyaffectsthescaleofthecoefficients,which j j isnotidentified.ThusGEVerrorsimplyamultinomiallogitmodelandÏisnotidentified. Asdiscussedabove,whenÏ=1theGEVdistribution(26.3)specializestoi.i.d. TypeIextremevalue. ThusaspecialcaseofTheorem26.1isthefollowing:IftheerrorsÎµ arei.i.d.TypeIextremevaluethenthe j responseprobabilitiesaremultinomiallogit(26.2)withcoefficientsÎ² .Thisisthemostcommonly-used j andcommonly-statedimplicationofTheorem26.1. Incontemporarychoicemodellingacommonly-usedassumptionisthatutilityisextremevaluedis- tributed. ThisisdonesothatTheorem26.1canbeinvokedtodeducethatthechoiceprobabilitiesare multinomial logit. A reasonable deduction is that this assumption is made for algebraic convenience, notbecauseanyonebelievesthatutilityisactuallyextremevalueddistributed.",
    "page": 841,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER26. MULTIPLECHOICE 822 The likelihood function given a random sample {Y ,X } is straightforward to construct. Write the i i response probabilities P (X | Î²) as functions of the parameter vector Î² = (Î² ,...,Î² ). The probability j 1 J massfunctionforY is J Ï(Y |X,Î²)= (cid:89) P (X |Î²) 1{Y=j} . j j=1 Thelog-likelihoodfunctionis n J (cid:96) (cid:161)Î²(cid:162)= (cid:88) (cid:88)1(cid:169) Y =j (cid:170) logP (X |Î²). n i j i i=1j=1 Themaximumlikelihoodestimator(MLE)is: Î² (cid:98) =argmax(cid:96) n (Î²). Î² ThereisnoalgebraicsolutionsoÎ² (cid:98)needstobefoundnumerically.Thelog-likelihoodfunctionisglobally concavesomaximizationisnumericallystraightforward. Toillustrate,weestimatethemarriagestatusexampleoftheprevioussectionusingmultinomiallogit anddisplaytheestimatedresponseprobabilitiesinFigure26.1(b).Theestimatesaresimilartothebinary choiceestimatesinpanel(a)butbyconstructionsumtoone. Thecoefficientsofamultinomialchoicemodelcanbedifficulttointerpret.Thereforeinapplications itmaybeusefultoexamineandreportmarginaleffects.Wecancalculate2thatthemarginaleffectsare (cid:195) (cid:33) â J (cid:88) Î´ j (x)= âx P j (x)=P j (x) Î² j â Î² (cid:96)P(cid:96)(x) . (26.4) (cid:96)=1 Thisisestimatedby (cid:195) (cid:33) J (cid:88) Î´ (cid:98)j (x)=P(cid:98)j (x) Î² (cid:98)j â Î² (cid:98)(cid:96)P(cid:98)(cid:96)(x) . (cid:96)=1 TheaveragemarginaleffectAME =(cid:69)(cid:163)Î´ (X) (cid:164) canbeestimatedby j j 1 (cid:88) n A(cid:129)ME j = Î´ (cid:98)j (X i ). (26.5) n i=1 In Stata, multinomial logit can be implemented using the mlogit command. Probabilities can be calculated by predict and average marginal effects by margins,dydx. In R, multinomial logit can be implementedusingthemlogitcommand. 26.4 ConditionalLogit InthesimplemultinomiallogitmodeloftheprevioussectiontheregressorsX (e.g.,age)arespecific totheindividualbutnotthealternative(theydonothavea j subscript). Inmostapplications,however, thereareregressorswhichvaryacrossalternatives. Atypicalexampleisthepriceorcostofanalterna- tive. In a latent utility model it is reasonable to assume that these alternative-specific regressors only affect an individualâs utility if that specific alternative is selected. A choice model which allows for re- gressorswhichdifferacrossalternativeswasdevelopedbyMcFaddeninthe1970s,whichhecalledthe ConditionalLogitmodel. 2SeeExercise26.3.",
    "page": 842,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER26. MULTIPLECHOICE 823 Anexamplewillhelpillustratethesetting. Supposeyou(astudent)needtoselectamodeoftravel fromyourapartmenttotheuniversity. Travelalternativesmayinclude: walk,bicycle,bus,train,orcar. Whichwillyouselect? Yourchoicewillundoubtedlydependonanumberoffactors, andofparticular importanceisthecost3 ofeachalternative. WecanmodelthisbyspecifyingthattheutilityY â (26.1)of j alternative j isafunctionofitscostX . j AsaconcreteexampleconsiderthedatasetKoppelmanonthetextbookwebpage.Thisisanabridged versionofthedatasetModeCanadadistributedwiththeRpackagemlogit,andusedinthepapersFori- nash and Koppelman (1993), Koppelman and Wen (2000), and Wen and Koppelman (2001). The data areresponsestoasurvey4 ofCanadianbusinesstravelersconcerningtheiractualtravelchoicesinthe Toronto-Montrealcorridor. Eachobservation(n =2779)isaspecificindividualmakingaspecifictrip. Fourtravelalternativeswereconsidered: train,air,bus,andcar. Availableregressorsincludethecost of eachalternative,thein-vehicletraveltime(intime)ofeachalternative,householdincome,andanindi- catorifoneofthetripendpointsisanurbancenter. Theconditionallogitmodelpositsthattheutilityofalternative j isafunctionofregressorsX which j varyacrossalternative j: U â=X (cid:48)Î³+Îµ . (26.6) j j j Here, Î³ are coefficients and Îµ is an alternative-specific error. Notice that in contrast to (26.1) that X j j variesacrossj whilethecoefficientsÎ³arecommon.Forexample,intheKoppelmandatasetthevariables cost andintimearerecordedforeachindividual/alternativepair. (Forexample,thefirstobservationin thesampleisatravelerwhocouldhaveselectedtraintravelfor$58.25andatraveltimeof215minutes, airtravelfor$142.80and56minutes,bustravelfor$27.52and301minutes,orcartravelfor$71.63and 262minutes.Thistravelerselectedtotravelbyair.) Tounderstandthedifferencebetweenthemultinomiallogitandtheconditionallogitmodels,(26.1) describeshowtheutilityofaspecificalternative(e.g. marriedordivorced)isaffectedbyavariablesuch as age. This requires a separate coefficient for each alternative to have an impact. In contrast, (26.6) describeshowtheutilityofanalternative(e.g. trainorcar)isaffectedbyfactorssuchascost andtime. These variables have common meanings across alternatives so the restriction that the coefficients are commonappearsreasonable. More generally the conditional logit model allows some regressors X to vary across alternatives j whileotherregressorsW donotvaryacross j.Thismodelis U â=W (cid:48)Î² +X (cid:48)Î³+Îµ . (26.7) j j j j For example, in the Koppelman dataset the variables cost and intime are components of X while the j variablesincomeandurbanarecomponentsofW. Inmodel(26.7)thecoefficientsÎ³andcoefficientdifferencesÎ² j âÎ² (cid:96)areidentifieduptoscale. Iden- tificationisachievedbynormalizingthescaleofÎµ andsettingÎ² =0forabasealternativeJ",
    "page": 843,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". (26.7) j j j j For example, in the Koppelman dataset the variables cost and intime are components of X while the j variablesincomeandurbanarecomponentsofW. Inmodel(26.7)thecoefficientsÎ³andcoefficientdifferencesÎ² j âÎ² (cid:96)areidentifieduptoscale. Iden- tificationisachievedbynormalizingthescaleofÎµ andsettingÎ² =0forabasealternativeJ. j J Theconditionallogitmodelis(26.6)or(26.7)plustheassumptionthattheerrorsÎµ aredistributed j i.i.d.TypeIextremevalue5.FromTheorem26.1wededucethattheprobabilityresponsefunctionsequal (cid:179) (cid:180) exp w (cid:48)Î² +x (cid:48)Î³ j j P (w,z)= . (26.8) j J (cid:88) exp (cid:161) w (cid:48)Î² (cid:96) +x (cid:96) (cid:48)Î³(cid:162) (cid:96)=1 3Costcanbemulti-dimensional,forexampleincludingmonetarycostandtraveltime. 4ThesurveywasconductedbytheCanadiannationalrailcarriertoassessthedemandforhigh-speedrail. 5ThemodelisunalterediftheerrorsarejointlyGEVwithdissimilarityparameterÏ.However,Ïisnotidentifiedsowithout lossofgeneralityitisassumedthatÏ=1.",
    "page": 843,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER26. MULTIPLECHOICE 824 ThisismultinomiallogitbutwithregressorsandcoefficientsW (cid:48)Î² +X (cid:48)Î³. j j Let Î¸ =(cid:161)Î² ,...Î² ,Î³(cid:162) . Given the observations {Y ,W ,X } where X ={X ,...,X }, the log-likelihood 1 J i i i i 1i Ji functionis n J (cid:96) (Î¸)= (cid:88) (cid:88)1(cid:169) Y =j (cid:170) logP (W ,X |Î¸). n i j i i i=1j=1 Themaximumlikelihoodestimator(MLE)Î¸ (cid:98)maximizes(cid:96) n (Î¸). ThereisnoalgebraicsolutionsoÎ¸ (cid:98)needs tobefoundnumerically. UsingtheKoppelmandatasetweestimateaconditionallogitmodel. EstimatesarereportedinTable 26.1. Includedasregressorsarecost,intime,income andurban. Thebasealternativeistravelbytrain. The firsttwo coefficientestimatesarenegative, meaningthatthe probability ofselectingany mode of transport is decreasing in the monetary and time cost of this mode of travel. The income and urban variablesarenotalternative-specificsohavecoefficientswhichvarybyalternative.Theurbancoefficient for air is positive and that for car is negative, indicating that the probability of air travel is increased relative to train travel if anendpoint is urban, and conversely for car travel. The income coefficientis positive for air travel and negative for bus travel, indicating that transportation choice is affected by a travelerâsincomeintheexpectedway. Asdiscussedpreviously,coefficientestimatescanbedifficulttointerpret. Itmaybeusefultocalcu- latetransformationssuchasaveragemarginaleffects. Theaveragemarginaleffectswithrespecttothe inputW areestimatedasin(26.5)withP(cid:98)(cid:96)(X i )replacedbyP(cid:98)(cid:96)(W i ,X i ). Fortheinputs X j wecalculate6 that â Î´ (w,x)= P (w,x)=Î³P (w,x) (cid:161) 1âP (w,x) (cid:162) (26.9) jj âx j j j j andfor j (cid:54)=(cid:96) â Î´ j(cid:96)(w,x)= âx(cid:96) P j (w,x)=âÎ³P j (w,x)P(cid:96)(w,x). (26.10) Notethatthesearedoubleindexed(j and(cid:96)). Forexamplefor X =cost, j =trainand(cid:96)=air, Î´ j(cid:96) isthe marginaleffectofachangeinthecostofairtravelontheprobabilityoftraintravel. Intheconditional logitmodel, calculation(26.10)impliesthesymmetricresponseÎ´ j(cid:96)(w,x)=Î´ (cid:96)j (w,x). Thismeansthat themarginaleffectof(forexample)aircostontraintravelequalsthemarginaleffectoftraincostonair travel7. TheaveragemarginaleffectsAME j(cid:96) =(cid:69)(cid:163)Î´ j(cid:96)(W,X) (cid:164) canbeestimatedbytheanalogoussample averagesasin(26.5). Oneusefulimplicationof(26.9)and(26.10)isthatthecomponentsofAME have jj thesamesignsasthecomponentsofÎ³andthecomponentsofAME j(cid:96)havetheoppositesigns.Thus,for example,ifthecoefficientÎ³onacostvariableisnegativethentheown-priceeffectisnegativeandthe cross-priceeffectsarepositive. To illustrate, we report a set of estimated AME of cost and time factors on the probability of train travelinTable26.2. Wefocusontraintravelsincethedemandforhigh-speedrailwasthefocusofthe originalstudy.WecalculateandreporttheAMEofthemonetarycostandtraveltimeoftrain,air,andcar travel",
    "page": 844,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". To illustrate, we report a set of estimated AME of cost and time factors on the probability of train travelinTable26.2. Wefocusontraintravelsincethedemandforhigh-speedrailwasthefocusofthe originalstudy.WecalculateandreporttheAMEofthemonetarycostandtraveltimeoftrain,air,andcar travel. ToconverttheAMEintoapproximateelasticities(whichmaybeeasiertointerpret),divideeach AMEbytheprobabilityoftraintravel(0.17)andmultiplybythesamplemeanofthefactor,reportedin thefirstcolumn. Youcancalculatethattheestimatedapproximateelasticityoftraintravelwithrespect totraincostisâ0.9,withrespecttotraintraveltimeisâ2.5,withrespecttoaircostis1.0,withrespect toairtraveltimeis0.25,withrespecttocarcostis0.6,andwithrespecttocartraveltimeis1.5. These estimatesindicatethattraintravelissensitivetoitstraveltime,issensitivewithrespecttoitsmonetary costandthatofairfare, andissensitivetothetraveltimeofcartravel. WecanusetheestimatedAME 6SeeExercise26.5. 7Thissymmetrybreaksdownifnonlineartransformationsareincludedinthemodel.",
    "page": 844,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER26. MULTIPLECHOICE 825 Table26.1:MultinomialModelsforTransportationChoice Variable Cond.Logit NestedLogit MixedLogit SimpleMulti.Probit Multi.Probit â0.022 â0.011 â0.023 â0.018 â0.005 Cost (0.003) (0.002) (0.004) (0.002) (0.002) â0.015 â0.005 â0.014 â0.011 â0.005 Intime (0.001) (0.001) (0.001) (0.001) (0.001) 0.0048 Ï(Intime) (0.0011) 0.036 0.024 0.040 0.027 0.018 Air Income (0.004) (0.003) (0.004) (0.003) (0.002) 0.29 0.28 0.35 0.29 â0.38 Urban (0.09) (0.09) (0.11) (0.07) (0.07) â2.15 â0.46 â2.72 â1.51 0.32 Constant (0.45) (0.35) (0.53) (0.32) (0.23) â0.051 â0.049 â0.050 â0.019 â0.008 Bus Income (0.018) (0.018) (0.018) (0.008) (0.007) â0.23 â0.21 â0.24 â0.13 â0.14 Urban (0.44) (0.45) (0.44) (0.21) (0.17) â1.79 â1.55 â1.82 â1.45 â0.23 Constant (0.79) (0.77) (0.79) (0.40) (0.59) 0.008 0.017 0.008 0.006 0.013 Car Income (0.003) (0.003) (0.003) (0.002) (0.003) â0.99 â0.58 â1.01 â0.73 â0.79 Urban (0.09) (0.08) (0.09) (0.07) (0.10) 1.86 1.19 1.89 1.44 1.51 Constant (0.19) (0.17) (0.19) (0.14) (0.20) 0.24 Ï(Car,Air) (0.05) 1.00 Ï(Train,Bus) (NA) Loglikelihood â2100.6 â2044.4 â2095.5 â2109.3 â2017.4 to calculate the rough effects of cost and travel time changes. For example, suppose high-speed rail reducestraintraveltimeby33%âanaveragereductionof75minutesâwhilepriceisunchanged. The estimatesimplythiswillincreasetraintravelprobabilityby0.14,thatis,from17%to31%,whichisclose toadoublingofusage. InmanycasesitisnaturaltoexpectthatthecoefficientsÎ³willvaryacrossindividuals. Wediscuss modelswithrandomÎ³inSection26.7. AsimplerspecificationistoallowÎ³tovarywiththeindividual characteristicW. For example in the transportation application the opportunity cost of travel time is likelyrelatedtoanindividualâswagewhichcanbeproxiedbyhouseholdincome. Wecanwritethisas Î³=Î³ +Î³ X.Substitutedinto(26.7)weobtainthemodel 1 2 U â=WÎ² +X Î³ +X WÎ³ +Îµ j j j 1 j 2 j whereforsimplicityweassumeW andX arescalar. Thiscanbewritteninform(26.7)byredefiningX j j as (X ,X W) and the same estimation methods apply. In our application this model yields a negative j j estimateforÎ³ ,indicatingthatthecostoftraveltimeisindeedincreasinginincome. 2",
    "page": 845,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER26. MULTIPLECHOICE 826 Table26.2:AMEofCostandTimeonTrainTravel Effectof Mean Cond.Logit MixedLogit SimpleMulti.Probit Multi.Probit â0.27 â0.28 â0.32 â0.08 TrainCost($) 56 (0.04) (0.05) (0.04) (0.03) â0.19 â0.20 â0.19 â0.09 TrainTime(min.) 224 (0.01) (0.01) (0.01) (0.01) 0.11 0.11 0.13 0.05 AirCost($) 153 (0.02) (0.02) (0.02) (0.02) 0.08 0.08 0.08 0.06 AirTime(min.) 54 (0.01) (0.01) (0.01) (0.01) 0.16 0.17 0.18 0.02 CarCost($) 65 (0.01) (0.03) (0.02) (0.01) 0.11 0.12 0.11 0.02 CarTime(min.) 232 (0.01) (0.01) (0.01) (0.01) Note:Foreaseofreading,thereportedAMEestimateshavebeenmultipliedby100. InStata,model(26.7)canbeestimatedusingcmclogit.Probabilitiescanbecalculatedbypredict, andmarginaleffectsbymargins.InR,usemlogit. 26.5 IndependenceofIrrelevantAlternatives Themultinomiallogitmodelhasanundesirablerestriction.Forfixedparametersandregressorsthe ratiooftheprobabilityoftwoalternativesis (cid:179) (cid:180) P j (W,X |Î¸) = exp W (cid:48)Î² j +X j (cid:48)Î³ . (26.11) P(cid:96)(W,X |Î¸) exp (cid:161) W (cid:48)Î² (cid:96) +X (cid:96) (cid:48)Î³(cid:162) ThisoddsratioisafunctiononlyoftheinputsX j andX(cid:96),doesnotdependonanyoftheinputsspecific totheotheralternatives,andisunalteredbythepresenceofotheralternatives. Thispropertyiscalled independenceofirrelevantalternatives(IIA),meaningthatthechoicebetweenoption j and(cid:96)isinde- pendentoftheotheralternativesandhencethelatterareirrelevanttothebivariatechoice.Thisproperty isstronglytiedtothemultinomiallogitmodelasthelatterwasderivedaxiomaticallybyLuce(1959)from anIIAassumption. TounderstandwhyIIAmaybeproblematicitishelpfultothinkthroughspecificexamples.Takethe transportation choice problem of the previous section. The IIA condition means that the ratio of the probabilityofselectingtraintothatofselectingcarisunaffectedbythepriceofanairplaneticket. This maymakesenseifindividualsviewthesetofchoicesassimilarlysubstitutable,butdoesnotmakesense iftrainandairareclosesubstitutes.Inthislattersettingalowairplaneticketmaymakeithighlyunlikely thatanindividualwillselecttraintravelwhileunaffectingtheirlikelihoodofselectingcartravel. Afamousexampleofthisproblemisthefollowingsetting. Supposethealternativesarecarandbus and suppose that the probability of the alternatives is split 50%-50%. Now suppose that we can split thebusalternativeintoâredbusâandâbluebusâsothereareatotalofthreealternatives. Supposethe bluebusandredbusarecloseequivalents: theyhavesimilarschedules,convenience,andcost. Inthis contextmostindividualswouldbenearindifferentbetweentheblueandredbussothesealternatives would receive similar probabilities. It would thus seem reasonable to expect that the probabilities of",
    "page": 846,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER26. MULTIPLECHOICE 827 thesethreechoiceswouldbecloseto50%-25%-25%. TheIIAcondition,however,impliesthattheratio of the first two probabilities must remain 1, so this implies that the probabilities of the three choices wouldbe33%-33%-33%. Wededucethatthemultinomiallogitmodelimpliesthataddingâredbusâto thechoicelistresultsinthereductionofcarusagefrom50%to33%. Thisdoesnâtmakesense; itisan unreasonableimplication.Thisexampleisknownastheâredbus/bluebuspuzzleâ. ThesourceoftheproblemisthattheIIAstructureandmultinomiallogitmodelexcludedifferenti- atedsubstitutabilityamongthealternatives. Thismaybeappropriatewhenthealternatives(e.g. bus, train,andcar)areclearlydifferentiatedandhavereasonablysimilardegreesofsubstitutability. Itisnot appropriatewhenasubsetofalternatives(e.g.redbusandbluebus)areclosesubstitutes. Partoftheproblemisduetotherestrictivecorrelationpatternimposedontheerrorsbythegener- alizedextremevaluedistribution.Toallowforcasessuchasredbus/bluebuswerequireamoreflexible correlationstructurewhichallowssubsetsofalternativestohavedifferentialcorrelations. 26.6 NestedLogit ThenestedlogitmodelcircumventstheIIAproblemdescribedintheprevioussectionbyseparating the alternatives into groups. Alternatives within groups are allowed to be correlated but are assumed uncorrelatedacrossgroups. ThemodelpositsthatthereareJ groupseachwithK alternatives.Weuse j todenotethegroup,kto j denotethealternativewithinagroup,andâjkâtodenoteaspecificalternative.LetW denoteindividual- specificregressorsandX denoteregressorswhichvarybyalternative.Theutilityofthe jkthalternative jk isafunctionoftheregressorsplusanerror: U â =W (cid:48)Î² +X (cid:48) Î³+Îµ . (26.12) jk jk jk jk â Themodelassumesthattheindividualselectsthealternative jk withthehighestutilityU . jk McFaddenâsNestedLogitmodelassumesthattheerrorshavethefollowingGEVjointdistribution F (cid:161)Îµ ,...,Îµ (cid:162)=exp (cid:195) â (cid:88) J (cid:34) (cid:88) Kj exp (cid:181) â Îµ jk (cid:182) (cid:35)Ï j(cid:33) . (26.13) 11 JKJ Ï j=1 k=1 j ThisisageneralizationoftheGEVdistribution(26.3). Thedistribution(26.13)istheproductof J GEV distributions(26.3)eachwithdissimilarityparameterÏ ,whichmeansthattheerrorswithineachgroup j areGEVdistributedwithdissimilarityparameterÏ . Acrossgroupstheerrorsareindependent. When j Ï =1forall j theerrorsaremutuallyindependentandthejointmodelequalsconditionallogit. When j Ï <1forsome j theerrorswithingroup j arecorrelatedbutnotwiththeothererrors. Ifagrouphasa j singlealternativeitsdissimilarityparameterisnotidentifiedsoshouldbesettoone. Thenestedlogitmodel(26.12)-(26.13)isstructurallyidenticaltotheconditionallogitmodelexcept thattheerrordistributionis(26.13)insteadof(26.3).ThecoefficientsÎ² andÎ³havethesameinterpre- jk tationasintheconditionallogitmodel. Aswritten,(26.12)allowsthecoefficientsÎ² tovaryacrossalternatives jk whilethecoefficientsÎ³ jk arecommonacross j andk. Otherspecificationsarepossible",
    "page": 847,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Ifagrouphasa j singlealternativeitsdissimilarityparameterisnotidentifiedsoshouldbesettoone. Thenestedlogitmodel(26.12)-(26.13)isstructurallyidenticaltotheconditionallogitmodelexcept thattheerrordistributionis(26.13)insteadof(26.3).ThecoefficientsÎ² andÎ³havethesameinterpre- jk tationasintheconditionallogitmodel. Aswritten,(26.12)allowsthecoefficientsÎ² tovaryacrossalternatives jk whilethecoefficientsÎ³ jk arecommonacross j andk. Otherspecificationsarepossible. Forexample,themodelcanbealteredto allowthecoefficientsÎ² and/orÎ³ tovaryacrossgroupsbutnotalternative.Thedegreeofvariabilityisa j j modelingchoicewithaflexibility/parsimonytrade-off. Itisalsopossible(butlesscommoninpractice) tohavevariablesW whichvarybygroupbutnotalternative. Thesecanbeincludedinthemodelwith j commoncoefficients. Thepartitionofalternativesintogroupsisamodelingdecision. Alternativeswithahighdegreeof substitutability should be placed in the same group. Alternatives with a low degree of substitutability shouldbeplacedindifferentgroups.",
    "page": 847,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER26. MULTIPLECHOICE 828 Toillustrate,consideraconsumerchoiceofanautomobilepurchase. Forsimplicitysupposethere are four choices: Honda Civic, Ford Fusion, Honda CR-V, and Ford Escape. The first two are compact carsandthelasttwoaresportsutilityvehicles(SUVs). Consequentlyitisreasonabletothinkofthefirst twoassubstitutesandthelasttwoassubstitutes. WedisplaythisnestingasatreediagramasinFigure 26.2(a).ThisshowsthedivisionofthedecisionâCarâintoâCompactâandâSportsUtilityVehicleâandthe furtherdivisionbymodel. OnlythedifferencesbetweenthecoefficientsÎ² areidentified. Identificationisachievedbysetting jk onealternative jk asthebasealternative. IfthecoefficientsÎ² areconstrainedtovarybybygroupthen j identificationisachievedbysettingabasegroup.Thescaleofthecoefficientsisnotidentifiedseparately fromthescalingoftheerrorsimplicitintheGEVdistribution(26.13). Some authors interpret model (26.12) as a nested sequential choice. An individual first selects a groupandsecondselectsthebestoptionwithinthegroup. Forexample,inthecarchoiceexampleyou couldimaginefirstdecidingonthestyleofcar(compactorSUV)andthendecidingonthespecificcar within each category (e.g. Civic vs. Fusion or CR-V vs. Escape). The sequential choice interpretation may help structure the groupings. However, sequential choice should be used cautiously as it is not technicallycorrect.Thecorrectinterpretationisdegreeofsubstitutabilitynotthetimingofdecisions. IfthecoefficientsÎ² onW areconstrainedtoonlyvaryacrossgroups(this,forexample,isthedefault j inStata)thentheeffectW (cid:48)Î² in(26.12)shiftstheutilitiesofallalternativeswithinagroup,andthusdoes j not affect the choice of an alternative within a group. In this case the variableW can be described as âaffectingthechoiceofgroupâ. Wenowdescribethenestedlogitresponseprobabilities. Theorem26.2 Assume the utility of alternative jk isU â =Âµ +Îµ and the jk jk jk errorvectorhasdistributionfunction(26.13). Thentheresponseprobabilities equalP jk =P k|j P j where exp (cid:161)Âµ /Ï (cid:162) P k|j = jk j (cid:88) Kj exp (cid:161)Âµ /Ï (cid:162) jm j m=1 and (cid:195) (cid:88) Kj exp (cid:161)Âµ /Ï (cid:162) (cid:33)Ï j jm j P = m=1 . j (cid:88) J (cid:195) (cid:88) Kj exp (cid:161)Âµ (cid:96)m /Ï (cid:96) (cid:162) (cid:33)Ï (cid:96) (cid:96)=1 m=1 Theorem 26.2 shows that the response probabilities equal the product of two terms: P k|j and P j . The first, P k|j , is the conditional probability of alternative k given the group j and takes the standard conditionallogitform.Thesecond,P ,istheprobabilityofgroup j. j LetÎ¸betheparameters.Thelog-likelihoodfunctionis (cid:96) n (Î¸)= (cid:88) n (cid:88) J (cid:88) Kj 1(cid:169) Y i =jk (cid:170)(cid:161) logP k|j (W i ,X i |Î¸)+logP j (W i ,X i |Î¸) (cid:162) . i=1j=1k=1 TheMLEÎ¸ (cid:98)maximizes(cid:96) n (Î¸).ThereisnoalgebraicsolutionsoÎ¸ (cid:98)needstobefoundnumerically.",
    "page": 848,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER26. MULTIPLECHOICE 829 Becausetheprobabilitystructureofanestedlogitmodelismorecomplicatedthantheconditional logitmodelitmaybedifficulttointerpretthecoefficientestimates.Marginaleffectscan(inprinciple)be calculatedbutthesearecomplicatedfunctionsofthecoefficients. Toillustrate,weestimateanestedlogitmodeloftransportationchoiceusingtheKoppelmandataset. Tofacilitatecomparisonsweestimatethesamespecificationasforconditionallogit. Thedifferenceis that we use the GEV distribution (26.13) with the groupings {car, air} and {train, bus}. This adds two dissimilarityparameters.TheresultsarereportedinthesecondcolumnofTable26.1. Thedissimilarityparameterestimatefor{car,air}is0.24whichissmall.Itimpliesacorrelationof0.94 betweenthecarandairutilityshocks. Thissuggeststhattheconditionallogitmodelâwhichassumes theutilityerrorsareindependentâismisspecified. Thedissimilarityparameterestimatefor{train,bus} isontheboundary81.00sohasnostandarderror. Nestedlogitmodelingislimitedbythenecessityofselectingthegroupings. Typicallythereisnota uniqueobviousstructure;consequentlyanyproposedgroupingissubjecttomisspecification. In this section we described the nested logit model with one nested layer. The model extends to multiplenestinglayers. Thedifferenceisthatthejointdistribution(26.13)ismodifiedtoallowhigher levels of interactions with additional dissimilarity parameters. An applied example is Goldberg (1995) whousedafive-levelnestedlogitmodeltoestimatethedemandforautomobilies.Thelevelsusedinher analysiswere(1)Buy/NotBuy;(2)New/Used;(3)CarClass;(4)Foreign/Domestic;and(5)CarModel. InStata,nestedlogitmodelscanbeestimatedbynlogit. Car P[Y=5] ` P[Y=4] Compact Sports Utility Vehicle P[Y=3] P[Y=2] P[Y=1] Honda Ford Honda Ford a 1 a 2 a 3 a 4 U* Civic Fusion CR-V Escape (a)NestedChoice (b)OrderedChoice Figure26.2:NestedChoiceandOrderedChoice 26.7 MixedLogit A generalization of the conditional logit model which allows the coefficients Î³ on the alternative- varyingregressorstoberandomacrossindividualsisknownasmixedlogit.Themodelisalsoknownas conditionalmixedlogitandrandomparameterslogit. 8Theuncontrainedmaximizerexceedsonewhichviolatestheparameterspacesothethemodeliseffectivelyestimated constrainingthisdissimilarityparametertoequalone.",
    "page": 849,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER26. MULTIPLECHOICE 830 Recall that the conditional logit model isU â =W (cid:48)Î² +X (cid:48)Î³+Îµ with Îµ i.i.d. extreme value. Now j j j j j replace Î³ with an individual-specific random variable Î· with distribution F(Î· | Î±) and parameters Î±. Thismodelis U â=W (cid:48)Î² +X (cid:48)Î·+Îµ j j j j Î·â¼F (cid:161)Î·|Î±(cid:162) . For example, in our transportation choice application the variables X are the cost and travel time of j eachalternative.Theabovemodelallowstheeffectofcostandtimeonutilitytobeheterogeneousacross individuals. The most common distributional assumption for Î· is N (cid:161)Î³,D (cid:162) with diagonal covariance matrix D. OthercommonspecificationsincludeN (cid:161)Î³,Î£(cid:162) withunconstrainedcovariancematrixÎ£,andlog-normally- distributedÎ·toenforceÎ·â¥0.(AconstraintÎ·â¤0canbeimposedbyfirstmultiplyingtherelevantregres- sor X by â1.) It is also commonto partition X so thatsome variables have random coefficients and j j othershavefixedcoefficients.Thereasonwhytheseconstraintsmaybedesirableisparsimonyandsim- plercomputation. UnderthenormalityspecificationsÎ·â¼N (cid:161)Î³,D (cid:162) andÎ·â¼N (cid:161)Î³,Î£(cid:162) themeanÎ³equalstheaverageran- domcoefficientinthepopulationandhasasimilarinterpretationtothecoefficientÎ³intheconditional logit model. The variances in D or Î£ control the dispersion of the distribution of Î· in the population. Smaller variances mean that Î· is mildly dispersed; larger variances mean high dispersion and hetero- geneity. Ausefulfeatureofthemixedlogitmodelisthattherandomcoefficientsinducecorrelationamong thealternatives.Toseethis,writeÎ³=(cid:69)(cid:163)Î·(cid:164) andV =X (cid:48) (Î·âÎ³)+Îµ .Thenthemodelcanbewrittenas j j j Y â=W (cid:48)Î² +X (cid:48)Î³+V j j j j whichistheconventionalrandomutilityframeworkbutwitherrorsV insteadofÎµ . Animportantdif- j j ferenceisthattheseerrorsareconditionallyheteroskedasticandcorrelatedacrossalternatives: (cid:69)(cid:163) V j V(cid:96) |X j ,X(cid:96) (cid:164)=X j (cid:48) var (cid:163)Î·(cid:164) X(cid:96). Thisnon-zerocorrelationmeansthattheIIApropertyispartiallybroken,givingthemixedlogitmodel moreflexibilitythantheconditionallogitmodeltocapturechoicebehavior. ConditionalonÎ·theresponseprobabilitiesfollowfrom(26.8) (cid:179) (cid:180) exp w (cid:48)Î² +x (cid:48)Î· j j P (w,x|Î·)= . j J (cid:88) exp (cid:161) w (cid:48)Î² (cid:96) +x (cid:96) (cid:48)Î·(cid:162) (cid:96)=1 Theunconditionalresponseprobabilitiesarefoundbyintegration. (cid:90) P (w,x)= P (w,x|Î·)dF(Î·|Î±). (26.14) j j Thelog-likelihoodfunctionis n J (cid:96) (Î¸)= (cid:88) (cid:88)1(cid:169) Y =j (cid:170) logP (W ,X |Î¸) (26.15) n i j i i i=1j=1 whereÎ¸isthelistofallparametersincludingÎ·.",
    "page": 850,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER26. MULTIPLECHOICE 831 The integral in (26.14) is not available in closed form. A standard numerical implementation9 is MonteCarlointegration(estimationbysimulation). Thistechniqueworksasfollows.Let{Î· ,...,Î· }bea 1 G setofi.i.d.pseudo-randomdrawsfromF(Î·|Î±).Thesimulationestimatorof(26.14)is 1 (cid:88) G P(cid:101)j (w,x)= P j (w,x|Î· g ). G g=1 AsG increasesthisconvergesinprobabilityto(26.14).MonteCarlointegrationiscomputationallymore efficientthannumericalintegrationwhenthedimensionofÎ·isthreeorlarger,butisconsiderablymore computationallyintensivethannon-randomconditionallogit. Toillustrate,weestimateamixedlogitmodelforthetransportationapplicationtreatingthecoeffi- cientontraveltimeasanormalrandomvariable. ThecoefficientestimatesarereportedinTable26.1 withestimatedmarginaleffectsinTable26.2. Theresultsaresimilartotheconditionallogitmodel. The coefficientontraveltimehasameanâ0.014whichisnearlyidenticaltotheconditionallogitestimate andastandarddeviationof0.005whichisaboutone-thirdofthevalueofthemean. Thissuggeststhat thecoefficientismildlyheterogenousamongtravelers. Aninterpretationofthisrandomcoefficientis thattravelershaveheterogeneouscostsassociatedwithtraveltime. InStata,mixedlogitcanbeestimatedbycmmixlogit. 26.8 SimpleMultinomialProbit The simplemultinomialprobit and simple conditionalmultinomialprobit models combine the latentutilitymodel U â=W (cid:48)Î² +Îµ (26.16) j j j or U â=W (cid:48)Î² +X (cid:48)Î³+Îµ (26.17) j j j j withtheassumptionthatÎµ isi.i.d. N(0,1). Theseareidenticaltothesimplemultinomiallogitmodelof j Section26.3andtheconditionallogitmodelofSection26.4exceptthattheerrordistributionisnormal insteadofextremevalue. Simple multinomial probit does not precisely satisfy IIA but its properties are similar to IIA. The modelassumesthattheerrorsareindependentandthusdoesnotallowtwoalternatives,e.g. âredbusâ andâbluebusâ,tobeclosesubstitutes. Thismeansthatinpracticethesimplemultinomialprobitwill produceresultswhicharesimilartosimplemultinomiallogit. Identification is identical to multinomial logit. The coefficients Î² and Î³ are only identified up to j scaleandthecoefficientsÎ² areonlyidentifiedrelativetoabasealternative. j TheresponseprobabilityP (W,X)isnotavailableinclosedform. However,itcanbeexpressedasa j one-dimensionalintegral,aswenowshow. Theorem26.3 Inthesimplemultinomialprobitandsimpleconditionalmulti- nomialprobitmodelstheresponseprobabilitiesequal P j (W,X)= (cid:90) â (cid:89) Î¦ (cid:179) W (cid:48)(cid:161)Î² j âÎ² (cid:96) (cid:162)+(cid:161) X j âX(cid:96) (cid:162)(cid:48) Î³+v (cid:180) Ï(v)dv (26.18) ââ(cid:96)(cid:54)=j whereÎ¦(v)andÏ(v)arethenormaldistributionanddensityfunctions. 9IftherandomcoefficientÎ·isscalaracomputationallymoreefficientmethodisintegrationbyquadrature.",
    "page": 851,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER26. MULTIPLECHOICE 832 TheproofispresentedinSection26.13. Theorem26.3showsthattheresponseprobabilityisaone- dimensionalnormalintegraloverthe Jâ1-foldproductofnormaldistributionfunctions. Thisintegral (26.18)isstraightforwardtonumericallyevaluatebyquadraturemethods. LetÎ¸=(cid:161)Î² ,...Î² ,Î³(cid:162) denotetheparameters.Giventhesample{Y ,W ,X }thelog-likelihoodis 1 J i i i n J (cid:96) (Î¸)= (cid:88) (cid:88)1(cid:169) Y =j (cid:170) logP (W ,X |Î¸). n i j i i i=1j=1 Themaximumlikelihoodestimator(MLE)Î¸ (cid:98)maximizes(cid:96) n (Î¸). Toillustrate, weestimateasimpleconditionalmultinomialprobitmodelfortransportationchoice usingthesamespecificationasbefore. TheresultsarereportedinthethirdcolumnofTable26.1. We reportaveragemarginaleffectsinTable26.2. WeseethattheestimatedAMEareveryclosetothoseof theconditionallogitmodel. InStata,simplemultivariateprobitcanbeestimatedbymprobit.Theresponseprobabilitiesandlog- likelihoodarecalculatedbyapplyingquadraturetotheintegral(26.18).Simpleconditionalmultinomial probit can be estimated by cmmprobit. The latter uses the method of simulated maximum likelihood (discussedinthenextsection)eventhoughnumericalcalculationcouldbeimplementedefficientlyus- ingtheone-dimensionalintegral(26.18). 26.9 GeneralMultinomialProbit Amodelwhichavoidsthecorrelationconstraintsofmultinomialandnestedlogitisgeneralmulti- nomialprobit,whichis(26.17)withtheerrorvectorÎµâ¼N(0,Î£)andunconstrainedÎ£. Identificationofthecoefficientsisthesameasmultinomiallogit. ThecoefficientsÎ² andÎ³areonly j identifieduptoscale,andthecoefficientsÎ² areonlyidentifiedrelativetoabasealternativeJ. j IdentificationofthecovariancematrixÎ£requiresmoreattention. Itturnsouttobeusefultorewrite themodelintermsofdifferencedutility,wheredifferencesaretakenwithrespecttothebasealternative J.Thedifferencedutilitiesare U ââU â=W (cid:48)(cid:161)Î² âÎ² (cid:162)+(cid:161) X âX (cid:162)(cid:48) Î³+Îµ (26.19) j J j J j J jJ whereÎµ =Îµ âÎµ . LetÎ£ bethecovariancematrixofÎµ for j =1,...,Jâ1. Forexample,supposethat jJ j J J jJ theerrorsÎµ arei.i.d.N(0,1).InthiscaseÎ£ equals j J ï£® ï£¹ 2 1 Â·Â·Â· 1 ï£¯ 1 2 Â·Â·Â· 1 ï£º Î£ J =ï£¯ ï£¯ ï£¯ . . . . . . ... . . . ï£º ï£º ï£º . (26.20) ï£° ï£» 1 1 Â·Â·Â· 2 Thescaleof(26.19)isnotidentifiedsoÎ£ isnormalizingbyfixingonediagonalelementofÎ£ . InStata, J J forexample,cmmprobitnormalizesthevarianceofoneelementâtheâscalealternativeââto2,inorder tomatchthecase(26.20).Consequently,Î£ has(Jâ1)J/2â1freecovarianceparameters. J Multinomial probitwith a general covariance matrix Î£ is more flexible thanconditional logitand J nestedlogit.ThisflexibilityallowsgeneralmultinomialprobittoescapetheIIArestrictions. Theresponseprobabilitiesdonothaveaclosed-formexpressionsbutcanbewrittenasJâ1dimen- sionalintegrals. Numericalevaluationofintegralsindimensionsthreeandgreateriscomputationally prohibitive.Afeasiblealternativeisnumericalsimulation.Theidea,roughly,istosimulatealargenum- berofrandomdrawsfromthemodelandcountthefrequencywhichsatisfythedesiredinequality. This",
    "page": 852,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER26. MULTIPLECHOICE 833 givesasimulationestimateoftheresponseprobability. Bruteforceimplementationofthisideacanbe inefficient, so clever tricks have been introduced to produce computationally efficient estimates. The standardimplementationwasdevelopedinaseriesofpapersbyGeweke,Hajivassiliou,andKeane,and isknownastheGHKsimulator. SeeTrain(2009)foradescriptionandreferences. TheGHKsimulator provides a feasible method to estimate the likelihood function and is known as simulated maximum likelihood. While feasible, simulated maximum likelihood is computationally intensive so optimizat- ingthelikelihoodtofindtheMLEiscomputationallyslow. Furthermorethelikelihoodisnotconcave intheparameterssoconvergencecanbedifficulttoobtaininsomeapplications. Consequentlyitmay be prudent to use simpler methods such as conditional and nested logit for exploratory analysis and multinomialprobitforfinal-stageestimation. Toillustrate, weestimatethegeneralmultinomialprobitmodelforthetransportationapplication. Wesetthebasealternativetotrainandthescalealternativetoair.Thecoefficientestimatesarereported inTable26.1andmarginaleffectsinTable26.2.Weseethattheestimatedmarginaleffectswithrespectto costandtraveltimeareconsiderablysmallerthanintheconditionallogitmodel. Thisindicatesgreatly reduced price elasticity (â0.3) and travel time elasticity (â1.1). Suppose (as we considered in Section 26.4)thathigh-speedrailreducestraintraveltimeby33%. Themultinomialprobitestimatesimplythat thisincreasestraintravelfrom17%to24%âabouta40%increase.Thisissubstantialbutone-halfofthe increaseestimatedbyconditionallogit. Amultinomialprobitmodelwithfouralternativeshasfivecovarianceparameters.Theestimatesfor thetransportationapplicationarereportedinthefollowing3Ã3table. Thediagonalelementsarethe variance estimates, the off-diagonal elements are the correlation estimates. One interesting finding is that the estimated correlation between air and car travel is 0.99, which is similar to the estimate from thenestedlogitmodel.Inbothframeworkstheestimatesindicateahighcorrelationbetweenairandcar travel,implyingthatspecificationswithindependenterrorsaremisspecified. ï£® Ï2 =2 ï£¹ (cid:98)Air ï£° Ï (cid:98)Air,Bus =0.60 Ï (cid:98) 2 Bus =0.41 ï£» Ï =0.99 Ï =0.60 Ï2 =3.8 (cid:98)Air,Car (cid:98)Car,Bus (cid:98)Car InStata,multivariateprobitcanbeestimatedbycmmprobit. ItusesGHKsimulatedmaximumlike- lihoodasdescribedabove. 26.10 OrderedResponse AmultinomialY isorderedifthealternativeshaveordinal(ordered)interpretation. Forexample,a studentmaybeaskedtoârateyour[econometrics]professorâwithpossibleresponses:poor,fair,average, good,orexcellent,codedas{1,2,3,4,5}.Theseresponsesarecategoricalbutarealsoordinallyrelated.We couldusestandardmultinomialmethods(e.g. multinomiallogitorprobit)butthisignorestheordinal structureandisthereforeinefficient. Thestandardapproachtoorderedresponseisbasedonthelatentvariableframework U â=X (cid:48)Î²+Îµ Îµâ¼G â where X doesnotincludeanintercept. ThemodelspecifiesthattheresponseY isdeterminedbyU",
    "page": 853,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER26. MULTIPLECHOICE 834 crossingaseriesoforderedthresholdsÎ± 1 <Î± 2 <Â·Â·Â·<Î± Jâ1 .Thus Y =1 if U ââ¤Î± 1 Y =2 if Î± <U ââ¤Î± 1 2 . . . . . . . . . Y =Jâ1 if Î± Jâ2 <U ââ¤Î± Jâ1 Y =J if Î± Jâ1 <U â . WritingÎ± 0 =ââandÎ± J =âwecanwritetheseJ equationsmorecompactlyasY =j ifÎ± jâ1 <U ââ¤Î± j . WhenJ=2thismodelspecializestobinarychoice. â ThestandardinterpretationisthatU isalatentcontinuousresponseandY isadiscretizedversion. â Consider again the example of ârate your professorâ. In the model,U is a studentâs true assessment. The response Y is a discretized version. The threshold crossing model postulates that responses are increasinginthelatentvariableandaredeterminedbythethresholds. InthestandardorderedresponseframeworkthedistributionG(x)oftheerrorÎµisassumedknown;in practiceeitherthenormalorlogisticdistributionisused. WhenÎµisnormalthemodeliscalledordered probit. When Îµ is logistic the model is called ordered logit. The coefficients and thresholds are only identifieduptoscale;thestandardnormalizationistofixthescaleofthedistributionofÎµ. Theresponseprobabilitiesare P (x)=(cid:80)(cid:163) Y =j |X =x (cid:164) j =(cid:80)(cid:163)Î± jâ1 <U ââ¤Î± j |X =x (cid:164) =(cid:80)(cid:163)Î± jâ1 âX (cid:48)Î²<eâ¤Î± j âX (cid:48)Î²|X =x (cid:164) =G (cid:161)Î± j âx (cid:48)Î²(cid:162)âG (cid:161)Î± jâ1 âx (cid:48)Î²(cid:162) . Itmaybeeasiertointerpretthecumulativeresponseprobabilities (cid:80)(cid:163) Y â¤j |X =x (cid:164)=G (cid:161)Î± âx (cid:48)Î²(cid:162) . j Themarginaleffectsare â âx P j (x)=Î²(cid:161) g (cid:161)Î± jâ1 âx (cid:48)Î²(cid:162)âg (cid:161)Î± j âx (cid:48)Î²(cid:162)(cid:162) andmarginalcumulativeeffectsare â (cid:80)(cid:163) Y â¤j |X =x (cid:164)=âÎ²g (cid:161)Î± âx (cid:48)Î²(cid:162) . âx j Toillustrate,Figure26.2(b)displayshowtheresponseprobabilitiesaredetermined. Thefigureplots thedistributionfunctionoflatentutilityU â withfourthresholdsÎ± ,Î± ,Î± andÎ± displayedonthex- 1 2 3 4 â axis. TheresponseY isdeterminedbyU crossingeachthreshold. Eachthresholdismappedtoapoint onthey-axis. Theprobabilityofeachoutcomeismarkedonthey-axisasthedifferencebetweeneach probabilitycrossing. TheparametersareÎ¸=(Î²,Î± 1 ,...Î± Jâ1 ).Giventhesample{Y i ,X i }thelog-likelihoodis n J (cid:96) (Î¸)= (cid:88) (cid:88)1(cid:169) Y =j (cid:170) logP (X |Î¸). n i j i i=1j=1 Themaximumlikelihoodestimator(MLE)Î¸ (cid:98)maximizes(cid:96) n (Î¸). InStata,orderedprobitandlogitcanbeestimatedbyoprobitandologit.",
    "page": 854,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER26. MULTIPLECHOICE 835 26.11 CountData Countdatareferstosituationswherethedependentvariableisthenumberofâeventsârecordedas positiveintegersY â{0,1,2,...}. Examplesincludethenumberofdoctorvisits,thenumberofaccidents, thenumberofpatentregistrations,thenumberofabsences,orthenumberofbankfailures.Countdata modelsaretypicallyemployedincontextswherethecountsaresmallintegers. AcountdatamodelspecifiestheresponseprobabilitiesP (x)=(cid:80)(cid:163) Y =j |x (cid:164) for j =0,1,2,...,withthe j property (cid:80)â P (x)=1. j=0 j ThebaselinemodelisPoissonregression. ThismodelspecifiesthatY isconditionallyPoissondis- tributedwithaPoissonparameterÎ»writtenasanexponentiallinkofalinearfunctionoftheregressors. TheexponentiallinkisusedtoensurethatthePoissonparameterisstrictlypositive.Thismodelis exp(âÎ»(x))Î»(x)j P (x)= j j! Î»(x)=exp(x (cid:48)Î²). The Poisson distribution has the property that its mean and variance equal the Poisson parameter Î». Thus (cid:69)[Y |X]=exp(X (cid:48)Î²) var[Y |X]=exp(X (cid:48)Î²). Thefirstequationshowsthattheconditionalmean(e.g.,theregressionfunction)equalsexp(X (cid:48)Î²). This iswhythemodeliscalledPoissonregression. Thelog-likelihoodfunctionis n n (cid:96) (Î²)= (cid:88) logP (cid:161) X |Î²(cid:162)= (cid:88)(cid:161)âexp(X (cid:48)Î²)+Y X (cid:48)Î²âlog(Y !) (cid:162) . n Yi i i i i i i=1 i=1 TheMLEÎ² (cid:98)isthevalueÎ²whichmaximizes(cid:96) n (Î²).Itsfirstandsecondderivativesare â n (cid:96) (Î²)= (cid:88) X (cid:161) Y âexp(X (cid:48)Î²) (cid:162) âÎ² n i i i i=1 â2 n (cid:96) (Î²)=â (cid:88) X X (cid:48) exp(X (cid:48)Î²). âÎ²âÎ²(cid:48) n i i i i=1 Sincethesecondderivativeisgloballynegativedefinitethelog-likelihoodfunctionisgloballyconcave. HencenumericaloptimizationtofindtheMLEiscomputationallystraightforward. IngeneralthereisnoreasontoexpectthePoissonmodeltobecorrectlyspecified.Henceweshould viewtheparameterÎ²asthebest-fittingpseudo-truevalue.Fromthefirst-orderconditionformaximiza- tionwefindthatthisvaluesatisfies (cid:69)(cid:163) X (cid:161) Y âexp(X (cid:48)Î²) (cid:162)(cid:164)=0. Thisholdsundertheconditionalmeanassumption(cid:69)[Y |X]=exp(X (cid:48)Î²). Ifthelatteriscorrectlyspeci- fied,PoissonregressioncorrectlyidentifiesthecoeffiicentÎ²,theMLEisconsistentforthisvalue,andthe estimatedresponseprobabilitiesareconsistentforthetrueresponseprobabilities. To explore this concept further, suppose the true conditional mean is nonparametric. Since it is non-negativewecanwriteitusinganexponentiallink10 as(cid:69)[Y |X]=exp(m(x)). Thefunctionm(x)is 10Or,equivalently,m(x)=log((cid:69)[Y |X]).",
    "page": 855,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER26. MULTIPLECHOICE 836 nonparametricallyidentifiedandcanbeapproximatedbyaseries x (cid:48) Î² . Thus(cid:69)[Y |X](cid:39)exp (cid:161) X (cid:48) Î² (cid:162) . K K K K WhatthisshowsisthatifPoissonregressionisimplementedusingaflexiblesetofregressors(asinseries regression)themodelwillapproximatethetrueconditionalmeanandhencewillconsistentlyestimate thetrueresponseprobabilities.ThisisabroadjustificationforPoissonregressionincountdataapplica- tionsifsuitableattentionispaidtothefunctionalformfortheincludedregressors. Sincethemodelisanapproximation,however,theconventionalcovariancematrixestimatorwillbe inconsistent. Consequentlyitisadvisedtousetherobustformulaforcovariancematrixandstandard errorestimation. For a greater degree of flexibility the Poisson model can be generalized. One approach, similar to mixed logit, is to treat the parameters as random variables, thereby obtaining a mixed probit model. One particular mixed model of importance is the negative binomial model which can be obtained as a mixed model as follows. Specify the Poisson parameter as Î»(X)=V exp(X (cid:48)Î²) where V is a random variable with a Gamma distribution. This is equivalent to treating the regression intercept as random withalog-Gammadistribution.IntegratingoutV,theresultingconditionaldistributionforY isNegative Binomial.TheNegativeBinomialisapopularmodelforcountdataregressionandhastheadvantagethat theconditionalmeanandvarianceareseparatelyvarying. FormoredetailseetheexcellentmonographoncountdatamodelsbyCameronandTrivedi(1998). InStata,PoissonandNegativeBinomialregressioncanbeestimatedbypoissonandnbreg.Gener- alizationstoallowtruncation,fixedeffects,andrandomeffectsarealsoavailable. 26.12 BLPDemandModel Amajordevelopmentinthe1990swastheextensionofconditionallogittomodelsofaggregatemar- ketdemand. ManyoftheideasweredevelopedintheseminalpapersofBerry(1994)andBerry,Levin- sohn, and Pakes (1995). For a review see Ackerberg, Benkard, Berry, and Pakes (2007). This model â widely known as the BLPmodel â has become popular in applied industrial organization. To discuss implementation we use as examples the applications in Berry, Levinsohn, and Pakes (1995) and Nevo (2001). Thecontextismarket-levelobservations. Aâmarketâistypicallyatimeperiodmatchedwithaloca- tion. Forexample,amarketinBerry,Levinsohn,andPakes(1995)istheUnitedStatesforonecalendar year. AmarketinNevo(2001)isoneof65U.S.citiesforonequarterofayear. Anobservationcontains asetof J goods. InBerry,Levinsohn,andPakes(1995)thegoodsare997distinctautomobilemodels. In Nevo(2001)thegoodsare25ready-to-eatbreakfastcereals.Observationstypicallyincludethepriceand salequantitiesofeachgood, asetofcharacteristicsofeachgood, andpossiblyinformationondemo- graphiccharacteristicsofthemarketpopulation. Themodelisderivedfromaconditionallogitspecificationofindividualbehavior. Thestandardas- sumptionisthateachindividualinthemarketpurchasesoneofthe J goodsormakesnopurchase(the latteriscalledtheoutsidealternative)",
    "page": 856,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". In Nevo(2001)thegoodsare25ready-to-eatbreakfastcereals.Observationstypicallyincludethepriceand salequantitiesofeachgood, asetofcharacteristicsofeachgood, andpossiblyinformationondemo- graphiccharacteristicsofthemarketpopulation. Themodelisderivedfromaconditionallogitspecificationofindividualbehavior. Thestandardas- sumptionisthateachindividualinthemarketpurchasesoneofthe J goodsormakesnopurchase(the latteriscalledtheoutsidealternative). Thisrequirestakingastandonthenumberofindividualsinthe market. Forexample,inBerry,Levinsohn,andPakes(1995)thenumberofindividualsistheentireU.S. population. Theirassumptionisthateachindividualmakesatmostoneautomobilepurchaseduring eachcalendaryear.InNevo(2001)thepopulationisthenumberofindividualsineachcity.Heassumes thateachindividualpurchasesaone-quarter(91-day)supplyofonebrandofbreakfastcereal, orpur- chasesnobreakfastcereal(theoutsidealternative).Byexplicitlyincludingtheoutsideoptionasachoice theseauthorsmodelaggregatedemand. Alternatively,theycouldhaveexcludedtheoutsideoptionand examinedchoiceamongtheJ goods.Thiswouldhavemodelledmarketshares(percentagesoftotalpur- chases)butnotaggregatedemand.Thetrade-offistheneedtotakeastandonthenumberofindividuals inthemarket.",
    "page": 856,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER26. MULTIPLECHOICE 837 Themodelisthateachindividualpurchasesoneofasetof J goodsindexed j =1,...,J oranunob- servedoutsidegood.Theutilityfromgood j takesamixedlogitform: U â=X (cid:48)Î·+Î¾ +Îµ (26.21) j j j j where X includes the price and characteristics of good j. The coefficient Î· is random (specific to an j individual)asinthemixedlogitmodel.ThevariablesÎ¾ andÎµ areunobservederrors.Î¾ ismarket-level j j j andÎµ isspecifictotheindividual. j ThemarketerrorÎ¾ maycontainunobservedproductcharacteristicssoislikelycorrelatedwithprod- j uctprice.IdentificationrequiresavectorofinstrumentsZ whichsatisfy j (cid:69)(cid:163) Z Î¾ (cid:164)=0. (26.22) j j Berry,Levinsohn,andPakes(1995)recommendasinstrumentsthenon-pricecharacteristicsin X ,the j sumofcharacteristicsofgoodssoldbythesamefirm, andthesumofcharacteristicsofgoodssoldby other firms. Nevo (2001) also included the prices of goods in other markets which is valid if demand shocks are uncorrelated across markets. There is considerable attention in the literature given to the choiceandconstructionofinstruments. WriteÎ³=(cid:69)(cid:163)Î·(cid:164) ,V =Î·âÎ³,andassumethatV hasdistributionF(V |Î±)withparametersÎ±(typically N(0,Î£)).Set Î´ =X (cid:48)Î³+Î¾ . (26.23) j j j Sincethemodelismixedlogit,(26.14)showsthattheresponseprobabilitiesgivenÎ´=(Î´ ,...,Î´ )are 1 J (cid:179) (cid:180) (cid:90) exp Î´ j +X j (cid:48) V P (Î´,Î±)= dF(V |Î±)dV. j J (cid:88) exp (cid:161)Î´ (cid:96) +X (cid:96) (cid:48) V (cid:162) (cid:96)=1 As discussed in Section 26.7 the integral in (26.14) is typically evaluated by numerical simulation. Let {V ,...,V }bei.i.d.pseudo-randomdrawsfromF(V |Î±).Thesimulationestimatoris 1 G (cid:179) (cid:180) 1 (cid:88) G exp Î´ j +X j (cid:48) V g P(cid:101)j (Î´,Î±)= . (26.24) G g=1 (cid:88) J exp (cid:161)Î´ (cid:96) +X (cid:96) (cid:48) V g (cid:162) (cid:96)=1 IneachmarketweobservethequantitypurchasedQ ofeachgoodandweareassumedtoknowthe j numberofindividualsM.Themarketshareofgood j isdefinedasS =Q /M whichisadirectestimate j j of the probability P . If the number of individuals M is large then S approximately equals P by the j j j WLLN. The BLP approach assumes that M is large enough that we can treat these two as equal. This impliesthesetofJ equalities S j =P(cid:101)j (Î´,Î±) (26.25) whereS=(S ,...,S ). Theleftsideof(26.25)istheobservedmarketshareofgood j (thatis,theratioof 1 J salestoindividualsinthemarket). Therightsideistheestimatedprobabilitythatthegoodisselected giventhemarketattributesandparameters. Asthereare J elementsineachofÎ´andS (andP(cid:101)j (Î´,Î±)is monotonicallyincreasingineachelementofÎ´)thereisaone-to-oneandinvertiblemappingbetween Î´andS. ThusgiventhemarketsharesS andparametersÎ±wecannumericallycalculatetheelements Î´whichsolvethe J equations(26.25). Berry,Levinsohn,andPakes(1995)showthatthesolutioncanbe obtainedbyiteratingon (cid:179) (cid:180) Î´i j =Î´i j â1+logS j âlogP(cid:101)j Î´iâ1,Î± . (26.26)",
    "page": 857,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER26. MULTIPLECHOICE 838 ThesolutionisanimplicitsetofJ equationsÎ´ =Î´ (S,Î±). j j WecombineÎ´ =Î´ (S,Î±)with(26.23)toobtaintheregression-likeexpressionÎ´ (S,Î±)=X (cid:48)Î³+Î¾ . j j j j j Combinedwith(26.22)weobtainthemomentequations (cid:104) (cid:179) (cid:180)(cid:105) (cid:69) Z Î´ (S,Î±)âX (cid:48)Î³ =0 j j j for j =1,...,J. EstimationisbynonlinearGMM.Theobservationsaremarketsindexedt =1,...,T,includingquan- titiesQ , pricesandcharacteristics X , andinstruments Z . MarketsharesareS =Q /M , where jt jt jt jt jt t M isthenumberofindividualsinthemarket.LetS =(S ,...,S ).Themomentequationis t t 1t Jt g(Î³,Î±)= 1 (cid:88) T (cid:88) J Z (cid:179) Î´ (S ,Î±)âX (cid:48) Î³ (cid:180) . TJ t=1j=1 jt jt t jt TheGMMestimator (cid:161)Î³,Î±(cid:162) minimizesthecriterion g(Î³,Î±) (cid:48) W g(Î³,Î±)foraweightmatrixW. (cid:98) (cid:98) Wementionedearlierthatobservationsmayincludedemographicinformation. Thiscanbeincor- porated as follows. We can add individual characteristics (e.g. income) to the utility model (26.21) as interactions with the product characteristics X . Since individual characteristics are unobserved they j can be treated as random but with a known distribution (taken from the known market-level demo- graphicdata). Forexample,Berry,Levinsohn,andPakes(1995)treatindividualincomeaslog-normally distributed. Theserandomvariablesarethentreatedjointlywiththerandomcoefficientswithnoeffec- tivechangeintheestimationmethod. An asymptotic theory developed by Berry, Linton, and Pakes (2004) shows that this GMM estima- torisconsistentandasymptoticallynormalas J ââundercertainassumptions. Thismeansthatthe estimatorcanbeappliedincontextswithsmallT andlargeJ,aswellasincontextswithlargeT. ToestimateaBLPmodelinStatathereisanadd-oncommandblp.InRthereisapackageBLPestimatoR. InPythonthereisapackagePyBLP. 26.13 TechnicalProofs* ProofofTheorem26.1:DefineÂµ j(cid:96) =X (cid:48)(cid:161)Î² j âÎ² (cid:96) (cid:162) .Itwillusefultoobservethat P (X)= exp (cid:161) X (cid:48)Î² j /Ï(cid:162) = (cid:195) (cid:88) J exp (cid:181) â Âµ j(cid:96) (cid:182) (cid:33)â1 . j J Ï (cid:88) exp (cid:161) X (cid:48)Î² (cid:96)/Ï(cid:162) (cid:96)=1 (cid:96)=1 Define â F (cid:161)Îµ ,...,Îµ (cid:162)= F (cid:161)Îµ ,...,Îµ (cid:162) j 1 J âÎµ 1 J j =exp (cid:195) â (cid:34) (cid:88) J exp (cid:179) â Îµ (cid:96)(cid:180) (cid:35)Ï(cid:33)(cid:34) (cid:88) J exp (cid:179) â Îµ (cid:96)(cid:180) (cid:35)Ïâ1 exp (cid:179) â Îµ j(cid:180) . Ï Ï Ï (cid:96)=1 (cid:96)=1 TheeventY =j occursifU j ââ¥U (cid:96) â forall(cid:96),whichoccurswhenÎµ (cid:96) â¤Îµ j +Âµ j(cid:96).Theprobability(cid:80)(cid:163) Y =j (cid:164) istheintegralofthejointdensity f (cid:161)Îµ 1 ,...,Îµ J (cid:162) overtheregionÎµ (cid:96) â¤Îµ j +Âµ j(cid:96).Thisis (cid:90) â(cid:183)(cid:90) Îµ +Âµ (cid:90) Îµ +Âµ (cid:184) (cid:80)(cid:163) Y =j (cid:164)=(cid:80)(cid:163)Îµ (cid:96) â¤Îµ j +Âµ j(cid:96),all(cid:96)(cid:164)= j j1 Â·Â·Â· J jJ f (cid:161)Îµ 1 ,...,Îµ J (cid:162) dÎµ 1 dÎµ 2 Â·Â·Â·dÎµ J dÎµ j ââ ââ ââ",
    "page": 858,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER26. MULTIPLECHOICE 839 wheretheouterintegralisoverÎµ .TheJâ1innersetofintegralsequalsF (cid:161)Îµ +Âµ ,...,Îµ +Âµ (cid:162) .Thus j j j j1 j jJ (cid:90) â (cid:80)(cid:163) Y =j (cid:164)= F (cid:161)Îµ +Âµ ,...,Îµ +Âµ (cid:162) dÎµ . (26.27) j j j1 j jJ j ââ Next,wesubstutetheaboveexpressionforF andcollecttermstofindthat(26.27)equals j (cid:90) â exp (cid:195) â (cid:34) (cid:88) J exp (cid:181) â Îµ (cid:96) +Âµ j(cid:96) (cid:182) (cid:35)Ï(cid:33)(cid:34) (cid:88) J exp (cid:181) â Îµ (cid:96) +Âµ j(cid:96) (cid:182) (cid:35)Ïâ1 exp (cid:179) â Îµ j(cid:180) dÎµ Ï Ï Ï j ââ (cid:96)=1 (cid:96)=1 = (cid:90) â exp (cid:161)âexp (cid:161)âÎµ (cid:162) P (X) âÏ(cid:162) P (X)1âÏ exp (cid:179) â Îµ j(cid:180)Ïâ1 exp (cid:179) â Îµ j(cid:180) dÎµ j j j Ï Ï j ââ (cid:90) â = exp (cid:161)âexp (cid:161)âÎµ âlogP (X) Ï(cid:162)(cid:162) P (X)1âÏ exp (cid:161)âÎµ (cid:162) dÎµ j j j j j ââ (cid:90) â =P (X)1âÏ exp (cid:161)âexp (cid:161)âÎµ âlogP (X) Ï(cid:162)(cid:162) exp (cid:161)âÎµ (cid:162) dÎµ j j j j j ââ (cid:90) â =P (X) exp (cid:161)âexp(âu) (cid:162) exp(âu)du j ââ =P (X). j The second-to-last equality makes the change of variables u =Îµ +logP (X) Ï . The final uses the fact j j that exp (cid:161)âexp(âu) (cid:162) exp(âu) is the Type I extreme value density which integrates to one. This shows (cid:80)(cid:163) Y =j (cid:164)=P (X),asclaimed. â  j ProofofTheorem26.2: TheproofmethodissimilartothatofTheorem26.1. Thejointdistributionof theerrorsis F (cid:161)Îµ ,...,Îµ (cid:162)=exp (cid:195) â (cid:88) J (cid:34) (cid:88) K(cid:96) exp (cid:181) â Îµ (cid:96)m (cid:182) (cid:35)Ï (cid:96)(cid:33) . 11 JKJ Ï (cid:96)=1 m=1 (cid:96) ThederivativewithrespecttoÎµ is jk â F (cid:161)Îµ ,...,Îµ (cid:162)= F (cid:161)Îµ ,...,Îµ (cid:162) jk 11 JKJ âÎµ 11 JKJ jk =exp (cid:195) â (cid:88) J (cid:34) (cid:88) K(cid:96) exp (cid:181) â Îµ (cid:96)m (cid:182) (cid:35)Ï (cid:96)(cid:33)(cid:34) (cid:88) Kj exp (cid:181) â Îµ jm (cid:182) (cid:35)Ï j â1 exp (cid:181) â Îµ jk (cid:182) . Ï Ï Ï (cid:96)=1 m=1 (cid:96) m=1 j j TheeventY jk =1occursifU j â k â¥U (cid:96) â m forall(cid:96)andm,whichoccurswhenÎµ (cid:96)m â¤Îµ jk +Âµ jk âÂµ lm",
    "page": 859,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Ï Ï Ï (cid:96)=1 m=1 (cid:96) m=1 j j TheeventY jk =1occursifU j â k â¥U (cid:96) â m forall(cid:96)andm,whichoccurswhenÎµ (cid:96)m â¤Îµ jk +Âµ jk âÂµ lm . Setting I = (cid:88) Kj exp (cid:161)Âµ /Ï (cid:162) andI =(cid:80)J I Ï (cid:96) wefindthat j jm j (cid:96)=1 (cid:96) m=1 (cid:90) â (cid:80)(cid:163) Y =1 (cid:164)= F (cid:161) v+Âµ âÂµ ,...,v+Âµ âÂµ (cid:162) dv jk ââ jk jk 11 jk JKJ = (cid:90) â exp (cid:195) â (cid:88) J (cid:34) (cid:88) K(cid:96) exp (cid:181) â v+Âµ jk âÂµ (cid:96)m (cid:182) (cid:35)Ï (cid:96)(cid:33)(cid:34) (cid:88) Kj exp (cid:181) â v+Âµ jk âÂµ jm (cid:182) (cid:35)Ï j â1 exp (cid:181) â v (cid:182) dv Ï Ï Ï ââ (cid:96)=1 m=1 (cid:96) m=1 j j =I Ï j j â1(cid:161) exp (cid:161)âÂµ jk (cid:162)(cid:162) Ï Ï j â j 1(cid:90) â â â exp (cid:195) âexp (cid:161)âvâÂµ jk (cid:162) (cid:96) (cid:88) = J 1 I (cid:96) Ï (cid:96) (cid:33) exp(âv)dv exp (cid:161)Âµ /Ï (cid:162) I Ï j â1 (cid:90) â = jk j j exp (cid:161)âexp (cid:161)âvâÂµ +logI (cid:162)(cid:162) exp (cid:161)âvâÂµ +logI (cid:162) dv jk jk I ââ exp (cid:161)Âµ /Ï (cid:162) I Ï j â1 = jk j j =P k|j P j I",
    "page": 859,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER26. MULTIPLECHOICE 840 asclaimed. â  ProofofTheorem26.3: WefollowtheproofofTheorem26.1through(26.27),whereinthiscaseÂµ j(cid:96) = X (cid:48)(cid:161)Î² j âÎ² (cid:96) (cid:162)+(cid:161) Z j âZ(cid:96) (cid:162)(cid:48) Î³and â F j (cid:161)Îµ 1 ,...,Îµ J (cid:162)= âÎµ F (cid:161)Îµ 1 ,...,Îµ J (cid:162)= (cid:89) Î¦(cid:161)Âµ j(cid:96) +Îµ j (cid:162)Ï(Îµ j ) j (cid:96)(cid:54)=j Thus (cid:90) â (cid:80)(cid:163) Y =j (cid:164)= (cid:89) Î¦(cid:161)Âµ j(cid:96) +v (cid:162)Ï(v)dv ââ(cid:96)(cid:54)=j asclaimed. â  _____________________________________________________________________________________________ 26.14 Exercises Exercise26.1 Forthemultinomiallogitmodel(26.2)showthat0â¤P (x)â¤1and (cid:80)J P (x)=1. j j=1 j Exercise26.2 Show that P (x) in the multinomial logit model (26.2) only depends on the coefficient j differencesÎ² âÎ² . j J Exercise26.3 Forthemultinomiallogitmodel(26.2)showthatthemarginaleffectsequal(26.4). Exercise26.4 Showthat(26.8)holdsfortheconditionallogitmodel. Exercise26.5 Fortheconditionallogitmodel(26.8)showthatthemarginaleffectsare(26.9)and(26.10). Exercise26.6 ShowthatP (w,x)intheconditionallogitmodel(26.8)onlydependsonthecoefficient j differencesÎ² âÎ² andvariabledifferencesx âx . j J j J Exercise26.7 IntheconditionallogitmodelfindanestimatorforAME . jj Exercise26.8 Show(26.11). Exercise26.9 Intheconditionallogitmodelwithnoalternative-invariantregressorsW showthat(26.11) impliesP j (x)/P(cid:96)(x)=exp (cid:179) (cid:161) x j âx(cid:96) (cid:162)(cid:48) Î³ (cid:180) . Exercise26.10 Takethenestedlogitmodel. Ifk and(cid:96)arealternativesinthesamegroup j, showthat theratioP jk /P j(cid:96)isindependentofvariablesintheothergroups.Whatdoesthismean? Exercise26.11 Takethenestedlogitmodel.Forgroupsj and(cid:96),showthattheratioP j /P(cid:96)isindependent ofvariablesintheothergroups.Whatdoesthismean? Exercise26.12 Usethecps09mardatasetandthesubsetofmen.Estimateamultinomiallogitmodelfor marriagestatussimilartoFigure26.1asafunctionofage. Howdoyourfindingscomparewiththosefor women? Exercise26.13 Usethecps09mardatasetandthesubsetofwomenwithagesupto35.Estimateamulti- nomiallogitmodelformarriagestatusaslinearfunctionsofageandeducation.Interpretyourresults.",
    "page": 860,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER26. MULTIPLECHOICE 841 Exercise26.14 Usethecps09mardatasetandthesubsetofwomen. Estimateanestedlogitmodelfor marriagestatusasafunctionofage.Describehowyoudecideonthegroupingofalternatives. Exercise26.15 UsetheKoppelmandataset. Estimateconditionallogitmodelssimilartothosereported inTable26.1butwiththefollowingmodifications. Foreachcasereporttheestimatedcoefficientsand standarderrorsforthecostandtimevariables,thelog-likelihood,anddescribehowtheresultschange. (a) ReplicatetheresultsofTable26.1forconditionallogitwiththesamevariables.Note:theregressors usedinTable26.1arecost,intime,income,andurban. (b) Addthevariableouttime,whichisout-of-vehicletime. (c) Replaceintimewithtime=intime+outtime. (d) Replacecostandintimewithlog(cost)andlog(intime). Exercise26.16 UsetheKoppelmandataset. Estimateanestedlogitmodelsimilartothosereportedin Table 26.1 but with the following modifications. For each case report the estimated coefficients and standarderrorsforthecostandtimevariables,thelog-likelihood,anddescribehowtheresultschange. (a) ReplicatetheresultsofTable26.1fornestedlogitwiththesamevariables. Note: Youwillneedto constrainthedissimilarityparameterfor{train,bus}. (b) Replacecostandintimewithlog(cost)andlog(intime). (c) Usethegroupings{car}and{train,bus,air}.Why(orwhynot)mightthisnestingmakesense? (d) Usethegroupings{air}and{train,bus,car}.Why(orwhynot)mightthisnestingmakesense? Exercise26.17 Use the Koppelman dataset. Estimate a mixed logit model similar to that reported in Table 26.1 but with the following modifications. For each case report the estimated coefficients and standarderrorsforthecostandtimevariables,thelog-likelihood,anddescribehowtheresultschange. (a) ReplicatetheresultsofTable26.1formixedlogitwiththesamevariables. (b) Replaceintimewithtime=intime+outtime. (c) Treat the coefficient on intime as the negative of a lognormal random variable. (Replace intime withnintime=-intimeandtreatthecoefficientaslognormallydistributed.) Howdoyoucompare theresultsoftheestimatedmodels? Exercise26.18 UsetheKoppelmandataset.Estimateageneralmultinomialprobitmodelsimilartothat reported in Table 26.1 but with the following modifications. For each case report the estimated coef- ficients and standard errors for the cost and time variables, the log-likelihood, and describe how the resultschange. (a) ReplicatetheresultsofTable26.1formultinomialprobitwiththesamevariables. (b) Replacecostandintimewithlog(cost)andlog(intime).",
    "page": 861,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Chapter 27 Censoring and Selection 27.1 Introduction Censored regression occurs when the dependent variable is constrained, resulting in a pile-up of observationsonaboundary.Selectionoccurswhensamplingisendogenous.Undereithercensoringor selection, conventional(e.g. leastsquares)estimatorsarebiasedforthepopulationparametersofthe uncensored/unselecteddistributions. Methodshavebeendevelopedtocircumventthisbias,including theTobit,CLAD,andsampleselectionestimators. FormoredetailseeMaddala(1983),Amemiya(1985),Gourieroux(2000),CameronandTrivedi(2005), andWooldridge(2010). 27.2 Censoring Itiscommonineconomicapplicationsforadependentvariabletohaveamixeddiscrete/continuous distribution,wherethediscretecomponentisontheboundaryofsupport.Mostcommonlythisbound- aryoccursat0. Forexample,Figure27.1(a)displaysthedensityoftabroad(transfersfromabroad)from thedatafileCHJ2004. Thisvariableistheamount1 ofremittancesreceivedbyaPhilippinohousehold fromaforeignsource. For80%ofhouseholdsthisvariableequals0. Theassociatedmasspointisdis- playedbythebaratzero.For20%ofhouseholdstabroadispositiveandcontinuouslydistributedwitha thickrighttail.Theassociateddensityisdisplayedbythelinegraph. Givensuchobservationsitisunclearhowtoproceedwitharegressionanalysis. Shouldweusethe full sample including the 0âs? Should we use only the sub-sample excluding the 0âs? Or should we do somethingelse? To answer these questions it is useful to have a statistical model. A classical framework is cen- soredregression,whichpositsthattheobservedvariableisacensoredversionofalatentcontinuously- distributedvariable.Withoutlossofgeneralitywefocusonthecaseofcensoringfrombelowatzero. ThecensoredregressionmodelwasproposedbyTobin(1958)toexplainhouseholdconsumptionof durablegoods.Tobinobservedthatinsurveydata,durablegoodconsumptioniszeroforapositivefrac- tionofhouseholds. Heproposedtreatingtheobservationsascensoredrealizationsfromacontinuous 1InthousandsofPhilippinopesos. 842",
    "page": 862,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER27. CENSORINGANDSELECTION 843 80% 20% 0 2 4 6 8 10 12 14 16 18 20 0 X'b Y* Thousands of Pesos (a)TransfersfromAbroadâCHJ2004dataset (b)CensoringProcess Figure27.1:CensoredDistributions distribution.Hismodelis Y â=X (cid:48)Î²+e eâ¼N(0,Ï2) Y =max (cid:161) Y â ,0 (cid:162) . (27.1) This model is known as Tobitregressionor censoredregression. It is also known as the Type1Tobit â model. ThevariableY islatent(unobserved). TheobservedvariableY iscensoredfrombelowatzero. Thismeansthatpositivevaluesareuncensoredandnegativevaluesaretransformedto0.Thiscensoring modelreplicatestheobservedphenomenonofapile-upofobservationsat0. The Tobit model can be justified by a latent choice framework where an individualâs optimal (un- â constrained)continuouslydistributedchoiceisY .Feasiblechoices,however,areconstrainedtosatisfy Y â¥0. (Forexample,negativepurchasesarenotallowed.) ConsequentlytherealizedvalueY isacen- â soredversionofY . Tojustifythisinterpretationofthemodelweneedtoenvisageacontextwherede- siredchoicesincludenegativevalues.Thismaybeastrainedinterpretationforconsumptionpurchases, butmaybereasonablewhennegativevaluesmakeeconomicsense. â ThecensoringprocessisdepictedinFigure27.1(b).ThelatentvariableY hasanormaldensitycen- teredatX (cid:48)Î².TheportionforY â>0ismaintainedwhiletheportionforY â<0istransformedtoapoint massatzero. Thelocationofthedensityandthedegreeofcensoringarecontrolledbytheconditional meanX (cid:48)Î².AsX (cid:48)Î²movestotherighttheamountofcensoringisdecreased.AsX (cid:48)Î²movestotheleftthe amountofcensoringisincreased. Acommonâremedyâtothecensoringproblemisdeletionofthecensoredobservations.Thiscreates atruncateddistributionwhichisdefinedbythefollowingtransformation (cid:189) Y ifY >0 Y#= missing ifY =0. InFigure27.1(a)andFigure27.1(b)thetruncateddistributionisthecontinuousportionabove0withthe masspointat0omitted.",
    "page": 863,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER27. CENSORINGANDSELECTION 844 The censoring and truncation processes are depicted in Figure 27.2(a) which plots 100 random2 â draws(Y ,X). Theuncensoredvariablesaremarkedbytheopencirclesandsquares. Theopensquares aretherealizationsforwhichY â>0andtheopencirclesaretherealizationsforwhichY â<0.Thecen- â soreddistributionreplacesthenegativevaluesofY with0,andthusreplacestheopenwiththefilled circles. The censored distribution thus consists of the open squares and filled circles. The truncated distributionisobtainedbydeletingthecensoredobservationssoconsistsofjusttheopensquares. â3 â2 â1 0 1 2 3 X 4 2 0 2â 4â Y* m#(x) m(x) m*(x) Y>0 (uncensored) Y<0 (unobserved) Y=0 (censored) â3 â2 â1 0 1 2 3 X (a)ConditionalDistrbutions 0.1 8.0 6.0 4.0 2.0 0.0 P[Y=0|X] (b)CensoringProbability Figure27.2:PropertiesofCensoredDistributions â To summarize: we distinguish between three distributions and variables: uncensored (Y ), cen- sored(Y),andtruncated(Y#). Thecensoredregressionmodel(27.1)makesseveralstrongassumptions: (1)linearityofthecondi- tionalmean;(2)independenceoftheerror;(3)normaldistribution.Thelinearityassumptionisnotcrit- icalaswecaninterpret X (cid:48)Î²asaseriesexpansionorsimilarflexibleapproximation. Theindependence assumption,however,isquiteimportantasitsviolation(e.g. heteroskedasticity)changestheproperties ofthecensoringprocess. Thenormalityassumptionisalsoquiteimportant,yetdifficulttojustifyfrom firstprinciples. 27.3 CensoredRegressionFunctions Wecancalculatesomepropertiesoftheconditionaldistributionofthecensoredrandomvariable. Theconditionalprobabilityofcensoringis (cid:80)(cid:163) Y â<0|X (cid:164)=(cid:80)(cid:163) e<âX (cid:48)Î²(cid:175) (cid:175) X (cid:164)=Î¦ (cid:181) â X (cid:48)Î²(cid:182) . Ï WeillustrateinFigure27.2(b). Thisplotsthecensoringprobabilityasafunctionof X fortheexample fromFigure27.2(a).Thecensoringprobabilityis98%forX =â3,50%forX =â1and2%forX =1. 2Xâ¼U[â3,3]andY â|Xâ¼N(1+X,1).",
    "page": 864,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER27. CENSORINGANDSELECTION 845 Theconditionalmeanoftheuncensored,censored,andtruncateddistributionsare m â (X)=(cid:69)(cid:163) Y â|X (cid:164)=X (cid:48)Î², m(X)=(cid:69)[Y |X]=X (cid:48)Î²Î¦ (cid:181) X (cid:48)Î²(cid:182) +ÏÏ (cid:181) X (cid:48)Î²(cid:182) (27.2) Ï Ï m#(X)=(cid:69)(cid:163) Y#|X (cid:164)=X (cid:48)Î²+ÏÎ» (cid:181) X (cid:48)Î²(cid:182) . (27.3) Ï ThefunctionÎ»(x)=Ï(x)/Î¦(x)in(27.3)iscalledtheinverseMillsratio. Toobtain(27.2)and(27.3)see Theorems5.8.2and5.7.6ofIntroductiontoEconometricsandExercise27.1. SinceY ââ¤Y â¤Y#itfollowsthat m â (x)â¤m(x)â¤m#(x) withstrictinequalityifthecensoringprobabilityispositive. Thisshowsthattheconditionalmeansof thetruncatedandcensoreddistributionsarebiasedfortheuncensoredconditionalmean. â WeillustrateinFigure27.2(a). Theuncensoredmeanm (x)ismarkedbythestraightline,thecen- soredmeanm(x)ismarkedwiththedashedline,andthetruncatedmeanm#(x)ismarkedwiththelong dashes.Thefunctionsarestrictlyrankedwiththetruncatedmeanexhibitingthehighestbias. 27.4 TheBiasofLeastSquaresEstimation Iftheobservations(Y,X)aregeneratedbythecensoredmodel(27.1)thenleastsquaresestimation usingeitherthefullsampleincludingthecensoredobservationsorthetruncatedsampleexcludingthe censoredobservationswillbebiased.Indeed,anestimatorwhichisconsistentfortheconditionalmean (suchasaseriesestimator)willestimatethecensoredmeanm(x)ortruncatedmeanm#(x)inthecen- â soredandtruncatedsamples,respectively,notthelatentconditionalmeanm (x). It is also interesting to consider the properties of the best linear predictor of Y on X, which is the estimand of the least squares estimator. In general, this depends on the marginal distribution of the regressors. However,whentheregressorsarenormallydistributedittakesasimpleformasdiscovered byGreene(1981).WritethemodelwithanexplicitinterceptasY â=Î±+X (cid:48)Î²+eandassumeX â¼N(0,Î£). Greeneshowedthatthebestlinearpredictorslopecoefficientis Î² =Î²(1âÏ) (27.4) BLP whereÏ=(cid:80)[Y =0]isthecensoringprobability.Wederive(27.4)attheendofthissection. Greeneâsformula(27.4)showsthattheleastsquaresslopecoefficientsareshrunktowardszeropro- portionately with the censoring percentage. While Greeneâs formula is special to normal regressors it gives a baseline estimate of the bias due to censoring. The censoring proportion Ï is easily estimated fromthesample(e.g.Ï=0.80inourtransfersexample)allowingaquickcalculationoftheexpectedbias duetocensoring. Thiscanbeusedasaruleofthumb. Iftheexpectedbiasissufficientlysmall(e.g. less than 5%) the resulting expected estimation bias (e.g. 5%) may be acceptable, leading to conventional least squares estimation using the full sample without an explicit treatment of censoring. However, if the censoring proportion Ï is sufficiently high (e.g. 10%) then estimation methods which correct for censoringbiasmaybedesired. Weclosethissectionbyderiving(27.4). ThecalculationissimplifiedbyatricksuggestedbyGold- berger (1981). Notice that Y â â¼N(Î±,Ï2) with Ï2 =Ï2+Î²(cid:48)Î£Î²",
    "page": 865,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". less than 5%) the resulting expected estimation bias (e.g. 5%) may be acceptable, leading to conventional least squares estimation using the full sample without an explicit treatment of censoring. However, if the censoring proportion Ï is sufficiently high (e.g. 10%) then estimation methods which correct for censoringbiasmaybedesired. Weclosethissectionbyderiving(27.4). ThecalculationissimplifiedbyatricksuggestedbyGold- berger (1981). Notice that Y â â¼N(Î±,Ï2) with Ï2 =Ï2+Î²(cid:48)Î£Î². Using the moments of the truncated Y Y normal distribution (Introduction to Econometrics, Theorems 5.7.6 and 5.7.8) and setting Î»=Î»(Î±/Ï ) Y",
    "page": 865,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER27. CENSORINGANDSELECTION 846 wecancalculatethat (cid:69)(cid:163)(cid:161) Y ââÎ±(cid:162) Y â|Y â>0 (cid:164)=var (cid:163) Y â|Y â>0 (cid:164)+(cid:161)(cid:69)(cid:163) Y â|Y â>0 (cid:164)âÎ±(cid:162)(cid:69)(cid:163) Y â|Y â>0 (cid:164) (cid:181) Î± (cid:182) =Ï2 1â Î»âÎ»2 +Ï Î»(Î±+Ï Î»)=Ï2. Y Ï Y Y Y Y TheprojectionofX onY â isX =(cid:69)[XY â ]Ïâ2(Y ââÎ±)+uwhereuisindependentofY â .Thisimplies Y (cid:69)(cid:163) XY â|Y â>0 (cid:164)=(cid:69)(cid:163)(cid:161)(cid:69)(cid:163) XY â(cid:164)Ïâ2(cid:161) Y ââÎ±(cid:162)+u (cid:162) Y â|Y â>0 (cid:164) Y =(cid:69)(cid:163) XY â(cid:164)Ïâ2(cid:69)(cid:163)(cid:161) Y ââÎ±(cid:162) Y â|Y â>0 (cid:164) Y =(cid:69)(cid:163) XY â(cid:164) . Hence Î² =(cid:69)(cid:163) XX (cid:48)(cid:164)â1(cid:69)[XY] BLP =(cid:69)(cid:163) XX (cid:48)(cid:164)â1(cid:69)(cid:163) XY â|Y â>0 (cid:164) (1âÏ) =(cid:69)(cid:163) XX (cid:48)(cid:164)â1(cid:69)(cid:163) XY â(cid:164) (1âÏ) =Î²(1âÏ) whichis(27.4)asclaimed. 27.5 TobitEstimator Tobin(1958)proposedestimationofthecensoredregressionmodel(27.1)bymaximumlikelihood. The censored variable Y has a conditional distribution function which is a mixture of continuous anddiscretecomponents: ï£± 0, y<0 ï£² F (cid:161) y|x (cid:162)= (cid:181) yâx (cid:48)Î²(cid:182) ï£³ Î¦ Ï , yâ¥0. Theassociateddensity3functionis f (cid:161) y|x (cid:162)=Î¦ (cid:181) â x (cid:48)Î²(cid:182)1{y=0}(cid:183) Ïâ1Ï (cid:181) yâx (cid:48)Î²(cid:182)(cid:184)1{y>0} . Ï Ï Thefirstcomponentistheprobabilityofcensoringandthesecondcomponentisthenormalregression density. Thelog-likelihoodisthesumofthelogdensityfunctionsevaluatedattheobservations: n (cid:96) (cid:161)Î²,Ï2(cid:162)= (cid:88) logf (Y |X ) n i i i=1 n (cid:195) (cid:34) (cid:195) Y âX (cid:48)Î²(cid:33)(cid:35)(cid:33) = (cid:88) 1 {Y =0}logf (Y |X )+1 {Y >0}log Ïâ1Ï i i i i i i Ï i=1 = (cid:88) logÎ¦ (cid:195) â X i (cid:48)Î²(cid:33) â 1 (cid:88) (cid:181) log (cid:161) 2ÏÏ2(cid:162)+ 1 (cid:161) Y âX (cid:48)Î²(cid:162)2 (cid:182) . Ï 2 Ï2 i i Yi =0 Yi >0 3Sincethedistributionfunctionisdiscontinuousaty=0thedensityistechnicallythederivativewithrespecttoamixed continuous/discretemeasure.",
    "page": 866,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER27. CENSORINGANDSELECTION 847 Thefirstcomponentisthesameasinaprobitmodel,andthesecondcomponentisthesameasforthe normalregressionmodel. The MLE (cid:161)Î² (cid:98),Ï (cid:98) 2(cid:162) are the values which maximize the log-likelihood (cid:96) n (cid:161)Î²,Ï2(cid:162) . This estimator was nicknamedâTobitâbyGoldbergerbecauseofitsconnectionwiththeprobitestimator. Amemiya(1973) establisheditsasymptoticnormality. Computationisimproved,asshownbyOlsen(1978),ifwetransformtheparameterstoÎ³=Î²/Ïand v=1/Ï.Thenthereparameterizedlog-likelihoodequals (cid:112) (cid:181) (cid:182) (cid:96) (cid:161)Î³,Î½(cid:162)= (cid:88) logÎ¦(cid:161)âX (cid:48)Î³(cid:162)+ (cid:88) log (cid:179) Î½/ 2Ï (cid:180) + â 1 (cid:88) (cid:161) Y Î½âX (cid:48)Î³(cid:162)2 . (27.5) n i 2 i i Yi =0 Yi >0 Yi >0 Thisisthesumofthreeterms,eachofwhichisgloballyconcavein(Î³,Î½)(aswenowdiscuss),so(cid:96) (cid:161)Î³,Î½(cid:162) n isgloballyconcavein(Î³,v)ensuringglobalconvergenceofNewton-basedoptimizers. Indeed,thethird termin(27.5)isthenegativeofaquadraticin(Î³,v),soisconcave. Thesecondtermin(27.5)islogarith- micinÎ½,whichisconcave.Thefirsttermin(27.5)isafunctiononlyofÎ³andhassecondderivative â2 (cid:88) logÎ¦(cid:161)âX (cid:48)Î³(cid:162)= (cid:88) X X (cid:48)Î»(cid:48)(cid:161)âX (cid:48)Î³(cid:162) âÎ³âÎ³(cid:48) i i i i Yi =0 Yi =0 whichisnegativedefinitesincetheMillsratiosatisfiesÎ»(cid:48) (u)<0(seeTheorem5.7.7inIntroductionto Econometrics).Hencethefirsttermin(27.5)isconcave. InStata,Tobitregressioncanbeestimatedwiththetobitcommand. InRthereareseveraloptions includingthetobitcommandintheAERpackage. JamesTobin James Tobin (1918-2002) of the United States was one of the leading macroe- conomistsofthemid-twentiethcenturyandwinnerofthe1981NobelMemorial PrizeinEconomicSciences.His1958paperintroducedcensoredregressionand its MLE, typically called the Tobit estimator. As a fascinating coincidence, the nameâTobitâalsoarisesinthe1951novelTheCaineMutiny,setonaU.S.Navy destroyer during World War II. At one point in the novel the author describes a crew member named âTobitâ who had âa mind like a spongeâbecause of his strongintellect. Itturnsouttheauthor(HermanWouk)andJamesTobinserved onthesameNavydestroyerduringWWII.Gofigure! 27.6 IdentificationinTobitRegression The Tobit model (27.1) makes several strong assumptions. Which are critical? To investigate this questionconsiderthenonparametriccensoredregressionframework Y â=m(X)+e (cid:69)[e]=0 Y =max (cid:161) Y â ,0 (cid:162)",
    "page": 867,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER27. CENSORINGANDSELECTION 848 where e â¼ F independent of X, and the regression function m(x) and distribution function F(u) are unknown.Whatisidentified? Suppose that the random variable m(X) has unbounded support on the real line (as occurs when m(X)=X (cid:48)Î²and X hasanunboundeddistributionsuchasthenormal). ThenwecanfindasetX â(cid:82) suchthatforxâX,(cid:80)[Y =0|X =x]=F(âm(x))(cid:39)0. Wecanthenimaginetakingthesubsampleofob- servationsforwhich X âX. Thefunctionm(x)isidentifiedforx âX,permittingtheidentificationof thedistributionF(u). Asthecensoringprobability(cid:80)[Y =0|X =x]=F(âm(x))isgloballyidentifiedthe functionm(x)isgloballyidentifiedaswell. Thisdiscussionshowsthatsolongaswemaintaintheas- sumptionthatX andeareindependent,theregressionfunctionm(x)anddistributionfunctionF(u)are nonparametricallyidentifiedwhenthemeanm(X)hasfullsupport. Thesetwoassumptions,however, areessentialaswenowdiscuss. Supposethefullsupportconditionfailsinthesensethattheregressionfunctionisboundedm(X)â¤ matavaluesuchthat(cid:80)[Y =0|X =x]=F(âm)>0.InthiscasetheerrordistributionF(u)isnotidenti- fiedforuâ¤âm.Thismeansthatthedistributionfunctioncantakeanyshapeforuâ¤âmsolongasitis weaklyincreasing.Thisimpliesthatthemean(cid:69)[e]isnotidentifiedsothelocationofm(x)(theintercept oftheregression)isnotidentified. Thesecondimportantassumptionisthate isindependentofX. Thisassumptionhasbeenrelaxed byPowell(1984,1986)intheconditionalquantileframework.Themodelis Y â=qÏ(X)+e (cid:81) Ï[e|X]=0 Y =max (cid:161) Y â ,0 (cid:162) forsomeÏâ(0,1).ThismodeldefinesqÏ(x)astheÏth conditionalquantilefunction.Sincequantilesare equivarianttomonotonetransformationswehavetherelationship (cid:81) Ï[Y |X =x]=max (cid:161) qÏ(x),0 (cid:162) . â Thus the conditional quantile function of Y is the censored quantile function of Y . The function (cid:81) Ï[Y |X =x]isidentifiedfromthejointdistributionof(Y,X). ConsequentlythefunctionqÏ(x)isiden- tifiedforanyx suchthatqÏ(x)>0. Thisisanimportantconceptualbreakthrough. Powellâsresultshows thatidentificationof qÏ(x)doesnotrequiretheerrortobeindependentof X norhaveaknowndistri- bution. Thekeyinsightisthatquantiles,notmeans,arenonparametricallyidentifiedfromacensored distribution. A limitation with Powellâs result is that the function qÏ(x) is only identifed on sub-populations for whichcensoringdoesnotexceedÏ%. Toillustrate,Figure27.3(a)displaysthecon (cid:112) ditionalquantilefunctionsqÏ(x)forÏ=0.3,0.5,0.7,and 0.9fortheconditionaldistributionY â|X â¼N (cid:161) xâ3,2+x (cid:162) .Theportionsabovezero(whichareidenti- 2 fiedfromthecensoreddistribution)areplottedwithsolidlines. Theportionsbelowzero(whicharenot identifiedfromthecensoreddistribution)areplottedwithdashedlines.Wecanseethatinthisexample thequantilefunctionq (x)isidentifiedforallvaluesofx,thequantilefunctionq (x)isnotidentified .9 .3 foranyvaluesofx,andthequantilefunctionsq (x)andq (x)areidentifiedforasubsetofvaluesofx",
    "page": 868,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Theportionsbelowzero(whicharenot identifiedfromthecensoreddistribution)areplottedwithdashedlines.Wecanseethatinthisexample thequantilefunctionq (x)isidentifiedforallvaluesofx,thequantilefunctionq (x)isnotidentified .9 .3 foranyvaluesofx,andthequantilefunctionsq (x)andq (x)areidentifiedforasubsetofvaluesofx. .7 .5 TheexplanationisthatforanyfixedvalueofX =x weonlyobservethecensoreddistributionY andso onlyobservethequantilesabovethecensoringpoint.Thereisnononparametricinformationaboutthe â distributionofY belowthecensoringpoint.",
    "page": 868,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER27. CENSORINGANDSELECTION 849 ( ) q0.9x ( ) q0.7x ( ) q0.5x ( ) q0.3x 0 50000 100000 150000 200000 Total Income (Pesos) (cid:112) (a)Y â|X â¼N (cid:161) Xâ3,2+X (cid:162) 2 )soseP( srefsnarT 00051 00021 0009 0006 0003 0 OLS Tobit LAD CLAD (b)EffectofIncomeonTransfers Figure27.3:CensoredRegressionQuantiles 27.7 CLADandCQREstimators Powell(1984,1986)appliedthequantileidentificationstrategydescribedintheprevioussectionto developstraightforwardcensoredregressionestimators. ThemodelinPowell(1984)iscensoredmedianregression: Y â=X (cid:48)Î²+e med[e|X]=0 Y =max (cid:161) Y â ,0 (cid:162) . InthismodelY â islatentwithmed[Y â|X]=X (cid:48)Î²andY iscensoredatzero.Asdescribedintheprevious sectiontheequivariancepropertyofthemedianimpliesthattheconditionalmedianofY equals med[Y |X]=max (cid:161) X (cid:48)Î²,0 (cid:162) . ThisisaparametricbutnonlinearmedianregressionmodelforY. The appropriate estimator for median regression is least absolute deviations (LAD). The censored leastabsolutedeviations(CLAD)criterionis M n (Î²)= n 1 (cid:88) n (cid:175) (cid:175)Y i âmax (cid:161) X i (cid:48)Î²,0 (cid:162)(cid:175) (cid:175). i=1 TheCLADestimatorminimizesM (Î²) n Î² (cid:98)CLAD =argminM n (Î²). Î² The CLAD criterion M (Î²) has similar properties as LAD criterion, namely that it is continuous, n faceted,andhasdiscontinuousfirstderivatives. Animportantdifference,however,isthatM (Î²)isnot n globallyconvex,sominimizationalgorithmsmayconvergetoalocalratherthanaglobalminimum.",
    "page": 869,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER27. CENSORINGANDSELECTION 850 Powell(1986)extendedCLADtocensoredquantileregression(CQR).Themodelis Y â=X (cid:48)Î²+e (cid:81) Ï[e|X]=0 Y =max (cid:161) Y â ,0 (cid:162) forÏâ(0,1).TheequivariancepropertyimpliesthattheconditionalquantilefunctionforY is (cid:81) Ï[Y |X]=max (cid:161) X (cid:48)Î²,0 (cid:162) . TheCQRcriterionis M n (Î²;Ï)= n 1 (cid:88) n Ï Ï (cid:161) Y i âmax (cid:161) X i (cid:48)Î²,0 (cid:162)(cid:162) i=1 whereÏ Ï(u)isthecheckfunction(24.10).TheCQRestimatorminimizesthiscriterion Î² (cid:98)CQR (Ï)=argminM n (Î²;Ï). Î² AsforCLAD,thecriterionisnotgloballyconcavesonumericalminimizationisnotguarenteedtocon- vergetotheglobalminimum. Powell(1984,1986)showsthattheCLADandCQRestimatorsareasymptoticallynormalbysimilar argumentsasforquantileregression. Animportanttechnicaldifferencewithquantileregressionisthat theCLADandCQRestimatorsrequirestrongerconditionsforidentification.Aswediscussedinthepre- vious section the quantile function X (cid:48)Î² is only identified for regions where it is positive. This means thatwerequireapositivefractionofthepopulationtosatisfyX (cid:48)Î²>0. Furthermore,therelevantdesign matrix(24.18)isdefinedonthissub-population,andmustbefullrankforconventionalinference. Es- sentially,theremustbesufficientvariationintheregressorsovertheregionofthesamplespacewhere thereisnocensoring. CLADcanbeestimatedinStatawiththeadd-onpackageclad.InR,CLADandCQRcanbeestimated withthecrqcommandinthepackagequantreg. 27.8 IllustratingCensoredRegression To illustrate the methods we revisit of the applications reported in Section 20.6, where we used a linear spline to estimate the impact of income on non-governmental transfers for a sample of 8684 Phillipino households. The least squares estimates indicated a sharp discontinuity in the conditional meanaround20,000pesos. Thedependentvariableisthesumoftransfersreceiveddomestically,from abroad,andin-kind,lessgifts. Eachofthesefoursub-variablesisnon-negative. Ifweapplythemodel toanyofthesesub-variablesthereissubstantialcensoring. Toillustrate,wesetthedependentvariable toequalthesumoftransfersreceiveddomestically,fromabroad,andin-kind,forwhichthecensoring proportionis18%.Thisproportionissufficientlyhighthatweshouldexpectsignificantcensoringbiasif censoringisignored. We estimate the same model as reported in Section 20.6 and displayed in Figure 20.2(b), which is alinearsplineinincomewith5knotsand15additionalcontrolregressors. Weestimatedtheequation usingfourmethods: (a)leastsquares;(b)Tobitregression;(c)LAD;(d)CLAD.Wedisplaytheestimated regressionasafunctionofincome(withremainingregressorssetatsamplemeans)inFigure27.3(b). Thebasicinsightâthattheregressionhasaslopeclosetoâ1forlowincomelevelsandisflatforhigh incomelevelswithasharpdiscontinuityatanincomelevelof20,000pesosâisremarkablyrobustacross",
    "page": 870,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER27. CENSORINGANDSELECTION 851 thefourestimates. Whatisnoticablydifferent,however,istheleveloftheregressionfunction. Theleast squaresestimateisseveralthousandpesosabovetheothers. ThefactthattheLADandCLADestimates haveameaningfullydifferentlevelshouldnotbesurprising. Thedependentvariableishighlyskewed, so the mean and median are quite different (the unconditional mean and median are 7700 and 1200, respectively). Thisimpliesalevelshiftoftheregressionfunction. Thisdoesnotexplain,however,why theTobitestimatealsoissubstantiallyshifteddown. Instead, thiscanbeexplainedbycensoringbias. Sincetheregressionfunctionisnegativelyslopedthecensoringprobabilityisincreasinginincome,so thebiasoftheleastsquaresestimatorispositiveandincreasingintheincomelevel.TheLADandCLAD estimates are quite similar even though the LAD estimates do not account for censoring. Overall, the CLADestimatesarethepreferredchoicebecausetheyarerobusttobothcensoringandnon-normality. 27.9 SampleSelectionBias Whileeconometricmodelstypicallyassumerandomsampling,actualobservationsaretypicallygath- ered non-randomly. This can induce estimation bias if selection (presence in the sample) is endoge- neous.Thefollowingareexamplesofpotentialsampleselection. 1. Wageregression. Wagesareonlyobservedforindividualswhohavewageincome,whichmeans thattheindividualisamemberofthelaborforceandhasawage-payingjob.Thedecisiontowork maybeendogenouslyrelatedtothepersonâsobservedandunobservedcharacteristics. 2. Programevaluation. Thegoalistomeasuretheimpactofaprogramsuchasworkforcetraining throughapilotprogram. Endogenousselectionariseswhenindividualsvolunteertoparticipate (ratherthanbeingrandomlyassigned).Individualswhovolunteerforatrainingprogrammayhave abilitieswhicharecorrelatedwithoutcomes. 3. Surveys. While a survey may be randomly distributed the act of completing the survey is non- random.Mostsurveyshavelowresponserates.Endogenousselectionariseswhenthedecisionto completeandreturnthesurveyiscorrelatedwiththesurveyresponses. 4. Ratings. Weareroutinelyaskedtorateproducts,services,andexperiences. Mostpeopledonot respondtotherequest. Endogenousselectionariseswhenthedecisiontoratetheproductiscor- relatedwiththeresponse. Tounderstandtheeffectofsampleselectionitisusefultoviewsamplingasatwo-stageprocess. In thefirststagetherandomvariables(Y,X)aredrawn. Inthesecondstagethepairiseitherselectedinto thesample(S =1)orunobserved(S =0). Thesamplethenconsistsofthepairs(Y,X)forwhichS =1. Suppose that the variables satisfy the latent regression model Y = X (cid:48)Î²+e with (cid:69)[e|X]=0. Then the conditionalmeanintheobserved(selected)sampleis (cid:69)[Y |X,S=1]=X (cid:48)Î²+(cid:69)[e|X,S=1]. Selection bias occurs when the second term is non-zero. To understand this further suppose that se- lectioncanbemodelledasS =1(cid:169) X (cid:48)Î³+u>0 (cid:170) forsomeerroru. Thisisconsistentwithalatentutility framework where X (cid:48)Î³+u is the latent utility of participation",
    "page": 871,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Suppose that the variables satisfy the latent regression model Y = X (cid:48)Î²+e with (cid:69)[e|X]=0. Then the conditionalmeanintheobserved(selected)sampleis (cid:69)[Y |X,S=1]=X (cid:48)Î²+(cid:69)[e|X,S=1]. Selection bias occurs when the second term is non-zero. To understand this further suppose that se- lectioncanbemodelledasS =1(cid:169) X (cid:48)Î³+u>0 (cid:170) forsomeerroru. Thisisconsistentwithalatentutility framework where X (cid:48)Î³+u is the latent utility of participation. Given this framework we can write the conditionalmeanofY intheselectedsampleas (cid:69)[Y |X,S=1]=X (cid:48)Î²+(cid:69)(cid:163) e|u>âX (cid:48)Î³(cid:164) .",
    "page": 871,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER27. CENSORINGANDSELECTION 852 Lete=Ïu+Îµbetheprojectionofe onu.SupposethattheerrorsareindependentofX,anduandÎµare mutuallyindependent.Thentheaboveexpressionequals (cid:69)[Y |X,S=1]=X (cid:48)Î²+Ï(cid:69)(cid:163) u|u>âX (cid:48)Î³(cid:164)=X (cid:48)Î²+Ïg (cid:161) X (cid:48)Î³(cid:162) forsomefunctiong(u). Whenuâ¼N(0,1),g(u)=Ï(u)/Î¦(u)=Î»(u)(seeExercise27.7)sotheexpression equals (cid:69)[Y |X,S=1]=X (cid:48)Î²+ÏÎ»(cid:161) X (cid:48)Î³(cid:162) . (27.6) Thisisthesameas(27.3)inthespecialcaseÏ=ÏandÎ³=Î²/Ï.This,asshowninFigure27.2(a),deviates fromthelatentconditionalmeanX (cid:48)Î². Onewaytointerpretthiseffectisthattheregressionfunction(27.6)containstwocomponents: X (cid:48)Î² andÏÎ»(cid:161) X (cid:48)Î³(cid:162) .AlinearregressiononX omitsthesecondtermandthusinheritsomittedvariablesbiasas X andÎ»(cid:161) X (cid:48)Î³(cid:162) arecorrelated.TheextentofomittedvariablesbiasdependsonthemagnitudeofÏwhich isthecoefficientfromtheprojectionofeonu.Whentheerrorseanduareindependent(whenselection isexogenous)thenÏ=0and(27.6)simplifiestoX (cid:48)Î²andthereisnoomittedterm.Thussampleselection biasarisesif(andonlyif)selectioniscorrelatedwiththeequationerror. Furthermore,theomittedselectiontermÎ»(cid:161) X (cid:48)Î³(cid:162) onlyimpactsestimatedmarginaleffectsiftheslope coefficientsÎ³arenon-zero.IncontrastsupposethatX (cid:48)Î³=Î³ ,aconstant.Then(27.6)equals(cid:69)[Y |X,S=1]= 0 X (cid:48)Î²+ÏÎ»(cid:161)Î³ (cid:162) sotheimpactofselectionisaninterceptshift. Ifourfocusisonmarginaleffectssample 0 selectionbiasonlyariseswhentheselectionequationhasnon-trivialdependenceontheregressorsX. InFigure27.2(a)wesawthatcensoringattenuates(flattens)theregressionfunction. Whilethese- lection mean (27.6) takes a similar form it is broader and can have a different impact. In contrast to thecensoringcase,selectioncanbothsteepenaswellasflattentheregressionfunction. Ingeneralitis difficulttopredicttheeffectofselectiononregressionfunctions. Aswehaveshown,endogenousselectionchangestheconditionalmean.Ifsamplesaregeneratedby endogenousselectionthenestimationwillbebiasedfortheparametersofinterest.Withoutinformation ontheselectionprocessthereislittlethatcanbedonetoâcorrectâthebiasotherthantobeawareofits presence.Inthenextsectionwediscussoneapproachwhichcorrectsforsampleselectionbiaswhenwe haveinformationontheselectionprocess. 27.10 HeckmanâsModel Heckman(1979)showedthatsampleselectionbiascanbecorrectedifwehaveasamplewhichin- cludes the non-selected observations. Suppose that the observations {Y ,X ,Z } are a random sample i i i whereY isaselectedvariable(suchaswage,whichisonlyobservedifapersonhaswageincome).Heck- manâsapproachistobuildajointmodelofthefullsample(notjusttheselectedsample)andusethisto estimatethemodelparameters. Heckmanâsmodelis Y â=X (cid:48)Î²+e S â=Z (cid:48)Î³+u S=1(cid:169) S â>0 (cid:170) (cid:189) Y â ifS=1 Y = missing ifS=0 with (cid:181) e (cid:182) (cid:181) (cid:181) Ï2 Ï (cid:182)(cid:182) â¼N 0, 21 . u Ï 1 21",
    "page": 872,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER27. CENSORINGANDSELECTION 853 â â ThemodelspecifiesthatthelatentvariablesY andS arelinearinregressors X and Z withstructural errorse andu. ThevariableS indicatesselectionandfollowsaprobitequation. ThevariableY equals thelatentvariableY â ifselected(S=1)andotherwiseismissing.Themodelspecifiesthattheerrorsare jointlynormalwithcovarianceÏ .Thevarianceofuisnotidentifiedsoisnormalizedtoequal1. 21 â InHeckmanâsclassicexample,Y isthewage(orlog(wage))anindividualwouldreceiveiftheywere employed, S is employment status, and Y is observed wage. The coefficients Î² are those of the wage regression;thecoefficientsÎ³arethosewhichdetermineemploymentstatus. Theerrore isunobserved ability and other unobserved factors which determine an individualâs wages; the error u is the unob- servedfactorswhichdetermineemploymentstatus;andthetwoarelikelytobecorrelated. Basedonthesamecalculationsasdiscussedintheprevioussection, theconditionalmeanofY in theselectedsampleis (cid:69)[Y |X,Z,S=1]=X (cid:48)Î²+Ï Î»(cid:161) Z (cid:48)Î³(cid:162) (27.7) 21 whereÎ»(x)istheinverseMillsratio. Heckmanproposedatwo-stepestimatorofthecoefficients. TheinsightisthatthecoefficientÎ³is identifiedbytheprobitregressionofS on Z. GivenÎ³thecoefficientsÎ²andÏ areidentifiedbyleast 21 squaresregressionofY on (cid:161) X,Î»(Z (cid:48)Î³) (cid:162) usingtheselectedsample.Thestepsareasfollows. 1. Construct(ifnecessary)thebinaryvariableSfromtheobservedseriesY. 2. EstimatethecoefficientÎ³byprobitregressionofSonZ. (cid:98) 3. ConstructthevariablesÎ» (cid:98)i =Î»(cid:161) Z i (cid:48)Î³ (cid:98) (cid:162) . 4. Estimatethecoefficients(Î² (cid:98),Ï (cid:98)21 )byleast-squaresregressionofY i on(X i ,Î» (cid:98)i )usingthesub-sample withS =1. i HeckmanshowedthattheestimatorÎ² (cid:98)isconsistentandasymptoticallynormal. ThevariableÎ» (cid:98)i is a generated regressor (see Section 12.26) which affects covariance matrix estimation. The method is sometimescalledâHeckitâasitisananalogofprobit,logit,andTobitregression. As a by-product we also obtain an estimator of the covariance Ï . This parameter indicates the 21 magnitudeofsampleselectionendogeneity. IfselectionisexogenousthenÏ =0. Thenullhypothesis 21 ofexogenousselectioncanbetestedbyexaminingthet-statisticforÏ . (cid:98)21 Analternativetotwo-stepestimationisjointmaximumlikelihood.ThejointdensityofSandY is f(s,y|x,z)=(cid:80)[S=0|x,z]1âs f (cid:161) y,S=1|x,z (cid:162)s . Theselectionprobabilityis(cid:80)[S=0|x,z]=1âÎ¦(cid:161) z (cid:48)Î³(cid:162) .Theconditionaldensitycomponentis (cid:90) â f (cid:161) y,S=1,|x,z (cid:162)= f (cid:161) y,s â|x,z (cid:162) ds â 0 (cid:90) â = f (cid:161) s â|y,x,z (cid:162) f (cid:161) y|x,z (cid:162) ds â 0 =(cid:161) 1âF (cid:161) s â|y,x,z (cid:162)(cid:162) f (cid:161) y|x,z (cid:162) . The first equality holds since S = 1 is the same as S â > 0. The second factors the joint density into â the product of the conditional of S given Y and the marginal of Y. The marginal density of Y is Ïâ1Ï(cid:161) (yâx (cid:48)Î²)/Ï(cid:162)",
    "page": 873,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". The first equality holds since S = 1 is the same as S â > 0. The second factors the joint density into â the product of the conditional of S given Y and the marginal of Y. The marginal density of Y is Ïâ1Ï(cid:161) (yâx (cid:48)Î²)/Ï(cid:162) . The conditional distribution of S â given Y is N (cid:161) Z (cid:48)Î³+Ï 21 (cid:161) Y âX (cid:48)Î²(cid:162) ,1âÏ 21 (cid:162) . Mak- Ï2 Ï2 ingthesesubstitutionsweobtainthejointmixeddensity ï£® ï£« ï£¶ ï£¹s f(s,y|x,z)=(cid:161) 1âÎ¦(cid:161) z (cid:48)Î³(cid:162)(cid:162)1âsï£¯Î¦ï£¬ z (cid:48)Î³+Ï Ï 2 2 1 (cid:161) yâx (cid:48)Î²(cid:162) ï£· 1 Ï (cid:181) yâx (cid:48)Î²(cid:182) ï£º . ï£° ï£­ (cid:113) 1âÏ 21 ï£¸Ï Ï ï£» Ï2",
    "page": 873,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER27. CENSORINGANDSELECTION 854 Evaluatedattheobservationsweobtainthelog-likelihoodfunction ï£® ï£« ï£¶ ï£¹ (cid:96) (cid:161)Î²,Î³,Ï2,Ï (cid:162)= (cid:88) log (cid:161) 1âÎ¦(cid:161) Z (cid:48)Î³(cid:162)(cid:162)+ (cid:88) ï£¯logÎ¦ï£¬ Z i (cid:48)Î³+Ï Ï 2 2 1 (cid:161) Y i âX i (cid:48)Î²(cid:162) ï£·â 1 log (cid:161) 2ÏÏ2(cid:162)â 1 (cid:161) Y âX (cid:48)Î²(cid:162)2ï£º. n 21 Si =0 i Si =1 ï£° ï£­ (cid:113) 1âÏ Ï 2 2 1 ï£¸ 2 2Ï2 i i ï£» Themaximumlikelihoodestimator (cid:161)Î² (cid:98),Î³ (cid:98) ,Ï (cid:98) 2,Ï (cid:98)21 (cid:162) maximizesthelog-likelihood. TheMLEisthepreferredestimationmethodforfinalreporting. Itcanbecomputationallydemand- inginsomeapplications,however,sothetwo-stepestimatorcanbeusefulforpreliminaryanalysis. InStatathetwo-stepestimatorandjointMLEcanbeobtainedwiththeheckmancommand. 27.11 NonparametricSelection Anonparametricselectionmodelis Y â=m(X)+e S â=g(Z)+u S=1(cid:169) S â>0 (cid:170) (cid:189) Y â ifS=1 Y = missing ifS=0 where the distribution of (e,u) is unknown. For simplicity we assume that (e,u) are independent of (X,Z). Selectionoccursifu>âg(Z).Thisisunaffectedbymonotonicallyincreasingtransformations.There- forethedistributionofu isnotseparatelyidentifiedfromthefunctiong(Z). Consequentlywecannor- malizethedistributionofutoaconvenientform.Hereweusethenormaldistribution:uâ¼Î¦(x). Since the functions m(X) and g(Z) are nonparametric we can use series methods to approximate them by linear models of the form m(X)= X (cid:48)Î² and g(Z)= Z (cid:48)Î³ after suitable variable transformation. Wewillusethislatternotationtolinkthemodelstoestimationmethods. Theconditionalprobabilityofselectionis p(Z)=(cid:80)[S=1|Z]=(cid:80)(cid:163) u>âZ (cid:48)Î³|Z (cid:164)=Î¦(cid:161) Z (cid:48)Î³(cid:162) . Theprobabilityp(Z)isknownasthepropensityscore;itisnonparametricallyidentifiedfromthejoint distributionof(S,Z),sothefunctiong(Z)=Z (cid:48)Î³isidentified.ThecoefficientÎ³andpropensityscorecan beestimatedbybinarychoicemethods,forexamplebyaseriesprobitregression. TheconditionalmeanofY givenselectionis (cid:69)[Y |X,Z,S=1]=X (cid:48)Î²+h (cid:161) Z (cid:48)Î³(cid:162) (27.8) 1 whereh (x)=(cid:69)[e|u>âx]. Ingeneralh (x)cantakearangeofpossibleshapes. When(e,u)arejointly 1 1 normalwithcovarianceÏ thenh (x)=Ï Î»(x)whereÎ»(x)=Ï(x)/Î¦(x)istheinverseMillsratio.There 21 1 21 aretwoalternativerepresentationsoftheconditionalmeanwhicharepotentiallyuseful. Since g(Z)= Î¦â1(cid:161) p(Z) (cid:162) wehavetherepresentation (cid:69)[Y |X,Z,S=1]=X (cid:48)Î²+h (cid:161) p(Z)) (cid:162) (27.9) 2 whereh (x)=h (cid:161)Î¦â1(x) (cid:162) .Also,sinceÎ»(x)isinvertiblewehavetherepresentation 2 1 (cid:69)[Y |X,Z,S=1]=X (cid:48)Î²+h (cid:161)Î»(Z (cid:48)Î³) (cid:162) (27.10) 3",
    "page": 874,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER27. CENSORINGANDSELECTION 855 whereh (x)=h (cid:161)Î»â1(x) (cid:162) . 3 1 Thethreeequations(27.8)-(27.10)suggestthreetwo-stepapproachestononparametricestimation whichwenowdescribe.Eachisbasedonafirst-stepbinarychoiceestimatorÎ³ofÎ³. (cid:98) Equation (27.8) suggests a regression of Y on X and a series expansion in Z (cid:48)Î³, for example a low- (cid:98) orderpolynomialinZ (cid:48)Î³. (cid:98) Equation(27.9)suggestsaregressionofY on X andaseriesexpansioninthepropensityscorep = (cid:98) Î¦(cid:161) Z (cid:48)Î³(cid:162) ,forexamplealow-orderpolynomialinp. (cid:98) (cid:98) Equation(27.10)suggestsaregressionofY onX andaseriesexpansioninÎ» (cid:98) =Î»(cid:161) Z (cid:48)Î³ (cid:98) (cid:162) ,forexamplea low-orderpolynomialinÎ» (cid:98). The advantage of expansions based on (27.10) is that it will be first-order accurate in the leading caseofthenormaldistribution.Thismeansthatfordistributionsclosetothenormal,seriesexpansions will be accurate even with a small number of terms. The advantage of expansions based on (27.9) is interpretability:Theregressionisexpressedasafunctionofthepropensityscore. Das,Newey,andVella(2003)provideadetailedasymptotictheoryforthisclassofestimatorsfocusing onthosebasedon(27.9).Theyprovideconditionsunderwhichthemodelsareidentified,theestimators consistent,andasymptoticallynormallydistributed. These nonparametric selection estimators are two-step estimators with generated regressors (see Section12.26). Thereforeconventionalcovariancematrixestimatorsandstandarderrorsareinconsis- tent. AsymptoticallyvalidcovariancematrixestimatorscanbeconstructedusingGMM.Analternative istousebootstrapmethods.Thelattershouldbeimplementedasanexplicittwo-stepestimatorsothat thefirst-stepestimationistreatedbythebootstrapdistribution. AstandardrecommendationisthattheregressorsZ intheselectionequationshouldincludeatleast onerelevantvariablewhichisavalidexclusionfromtheregressorsX inthemainequation. Thereason isthatotherwisetheseriesexpansionsform(x)andh(Z (cid:48)Î³)canbehighlycollinearandnotseparately identified.Thisinsightappliestotheparametriccaseaswell.Onedifficultyisthatinapplicationsitmay â â bechallengingtoidentifyvariableswhichaffectselectionS butnottheoutcomeY . 27.12 PanelData Apanelcensoredregression(panelTobit)equationis Y â=X (cid:48) Î²+u +e it it i it Y =max (cid:161) Y â ,0 (cid:162) . it it Theindividualeffectu canbetreatedasarandomeffect(uncorrelatedwiththeerrors)orafixedeffect i (unstructuredcorrelation). A random effects estimator can be derived under the assumption of joint normality of the errors. ThisisimplementedintheStatacommandxttobit. Theadvantageisthattheprocedureissimpleto implement.Thedisadvantagesarethosetypicallyassociatedwithrandomeffectsestimators. A fixed effects estimator was developed by HonorÃ© (1992). His key insight is the following, which we illustrate assuming T = 2",
    "page": 875,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". A random effects estimator can be derived under the assumption of joint normality of the errors. ThisisimplementedintheStatacommandxttobit. Theadvantageisthattheprocedureissimpleto implement.Thedisadvantagesarethosetypicallyassociatedwithrandomeffectsestimators. A fixed effects estimator was developed by HonorÃ© (1992). His key insight is the following, which we illustrate assuming T = 2. If the errors (e ,e ) are independent of (X ,X ,u ) then the distri- i1 i2 i1 i2 i â â bution of (Y ,Y ) conditional on (X ,X ) is symmetric about the 45 degree line through the point i1 i2 i1 i2 (âX (cid:48)Î²,0)in(Y ,Y )space. Thisdistributiondoesnotdependonthefixedeffectu . Fromthissymme- 1 2 i try and the censoring rules HonorÃ© derived moment conditions which identify the coefficients Î² and allowestimationbyGMM.HonorÃ©(1992)providesacompleteasymptoticdistributiontheory. HonorÃ© hasprovidedaStatacommandPantobwhichimplementshisestimatorandisavailableonhiswebsite. https://www.princeton.edu/~honore/stata/.",
    "page": 875,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER27. CENSORINGANDSELECTION 856 Apanelsampleselectionmodelis Y â=X (cid:48) Î²+u +e it it i it S â =Z (cid:48) Î³+Î· +v it it i it S =1(cid:169) S â >0 (cid:170) it it (cid:189) Y â ifS =1 Y = it it . it missing ifS =0 it AmethodtoestimatedthismodelispresentedinKyriazidou(1997). Againforexpositionwefocuson the T =2 case. Her estimator is motivated by the observation that Î² could be consistently estimated by least squares applied to the sub-sample where S = S = 1 (both observations are selected) and i1 i2 Z (cid:48) Î³=Z (cid:48) Î³(bothobservationshavesameprobabilityofselection). TheparameterÎ³isidentifiedupto i1 i2 scalebytheselectionequationsocanbeestimatedasÎ³bythemethodsdescribedinSection25.13(e.g. (cid:98) Chamberlain(1980,1984)). GivenÎ³weestimateÎ²bykernel-weightedleastsquaresonthesub-sample (cid:98) withS =S =1,withkernelweightsdependingon(Z âZ ) (cid:48)Î³.Kyriazidou(1997)providesacomplete i1 i2 i1 i2 (cid:98) distributiontheory. _____________________________________________________________________________________________ 27.13 Exercises Exercise27.1 Derive(27.2)and(27.3).Hint:UseTheorems5.7and5.8ofIntroductiontoEconometrics. Exercise27.2 Takethemodel Y â=X (cid:48)Î²+e eâ¼N (cid:161) 0,Ï2(cid:162) (cid:189) Y â ifY ââ¤Ï Y = missing ifY â>Ï . Inthismodel,wesaythatY iscappedfromabove.SupposeyouregressY onX.IsOLSconsistentforÎ²? Describethenatureoftheeffectofthemis-measuredobservationontheOLSestimator. Exercise27.3 Takethemodel Y =X (cid:48)Î²+e eâ¼N (cid:161) 0,Ï2(cid:162) . LetÎ² (cid:98)denotetheOLSestimatorforÎ²basedonanavailablesample. (a) Suppose that an observation is in the sample only if X 1 >0 where X 1 is an element of X. Is Î² (cid:98) consistentforÎ²?Obtainanexpressionforitsprobabilitylimit. (b) SupposethatanobservationisinthesampleonlyifY >0.IsÎ² (cid:98)consistentforÎ² (cid:98)?Obtainanexpres- sionforitsprobabilitylimit. Exercise27.4 Forthecensoredconditionalmean(27.2)proposeaNLLSestimatorof(Î²,Ï). Exercise27.5 Forthetruncatedconditionalmean(27.3)proposeaNLLSestimatorof(Î²,Ï).",
    "page": 876,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER27. CENSORINGANDSELECTION 857 â Exercise27.6 AlatentvariableY isgeneratedby Y â=Î² +XÎ² +e 0 1 e|X â¼N (cid:161) 0,Ï2(X) (cid:162) Ï2(X)=Î³ +X2Î³ 0 1 Y =max (cid:161) Y â ,0 (cid:162) . whereX isscalar. AssumeÎ³ >0andÎ³ >0. TheparametersareÎ²,Î³ ,Î³ . Findthelog-likelihoodfunc- 0 1 0 1 tionfortheconditionaldistributionofY givenX. Exercise27.7 Takethemodel S=1(cid:169) X (cid:48)Î³+u>0 (cid:170) (cid:189) X (cid:48)Î²+e ifS=1 Y = missing ifS=0 (cid:181) e (cid:182) (cid:181) (cid:181) Ï2 Ï (cid:182)(cid:182) â¼N 0, 21 u Ï 1 21 Show(cid:69)[Y |X,S=1]=X (cid:48)Î²+Ï Î»(cid:161) X (cid:48)Î³(cid:162) . 21 Exercise27.8 Show(27.7). Exercise27.9 TaketheCHJ2004dataset. Thevariablestinkindandincomearehouseholdtransfersre- ceivedin-kindandhouseholdincome,respectively.Dividebothvariablesby1000tostandardize.Create theregressorDincome=(income-1)Ã1 {income>1}. (a) EstimatealinearregressionoftinkindonincomeandDincome.Interprettheresults. (b) Calculatethepercentageofcensoredobservations(thepercentageforwhichtinkind=0. Doyou expectcensoringbiastobeaprobleminthisexample? (c) Supposeyoutryandfixtheproblembyomittingthecensoredobservations. Estimatetheregres- siononthesubsampleofobservationsforwhichtinkind>0. (d) EstimateaTobitregressionofoftinkindonincomeandDincome. (e) EstimatethesameregressionusingCLAD. (f) Interpretandexplainthedifferencesbetweenyourresultsin(a)-(e). Exercise27.10 Take the cps09mar dataset and the subsample of individuals with at least 12 years of education.Createwage=earnings/(hoursÃweeks)andlwage=log(wage). (a) Estimatealinearregressionoflwageoneducationandeducation^2.Interprettheresults. (b) Supposethewagedatahadbeencappedabout$30/hour. Createavariablecwagewhichislwage cappedat3.4. Estimatealinearregressionofcwage oneducationandeducation^2. Howwould youinterprettheseresultsifyouwereunawarethatthedependentvariablewascapped? (c) Supposeyoutryandfixtheproblembyomittingthecappedobservations.Estimatetheregression onthesubsampleofobservationsforwhichcwageislessthan3.4.",
    "page": 877,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER27. CENSORINGANDSELECTION 858 (d) EstimateaTobitregressionofcwageoneducationandeducation^2withuppercensoringat3.4. (e) EstimatethesameregressionusingCLAD.Youmayneedtoimposeanuppercensoringof3.3. (f) Interpretandexplainthedifferencesbetweenyourresultsin(a)-(e). Exercise27.11 TaketheDDK2011dataset. Createavariabletestscorewhichistotalscorestandardizedto havemeanzeroandvarianceone. Thevariabletracking isadummyindicatingthatthestudentswere tracked (separated by initial test score). The varible percentile is the studentâs percentile in the initial distribution.Forthefollowingregressionsclusterbyschool. (a) Estimatealinearregressionoftestscoreontracking,percentile,andpercentile^2. Interpretthere- sults. (b) Supposethescoreswerecensoredfrombelow. Createavariablectest whichistestscorecensored at0. Estimatealinearregressionofctest ontracking,percentile,andpercentile^2. Howwouldyou interprettheseresultsifyouwereunawarethatthedependentvariablewascensored? (c) Supposeyoutryandfixtheproblembyomittingthecensoredobservations. Estimatetheregres- siononthesubsampleofobservationsforwhichctestispositive. (d) Interpretandexplainthedifferencesbetweenyourresultsin(a),(b),and(c).",
    "page": 878,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Chapter 28 Model Selection, Stein Shrinkage, and Model Averaging 28.1 Introduction Thechapterreviewsmodelselection,James-Steinshrinkage,andmodelaveraging. Modelselectionisatoolforselectingonemodel(orestimator)outofasetofmodels.Differentmodel selectionmethodsaredistinguishedbythecriteriausedtorankandcomparemodels. Modelaveragingisageneralizationofmodelselection. Modelsandestimatorsareaveragedusing data-dependentweights. James-Steinshrinkagemodifiesclassicalestimatorsbyshrinkingtowardsareasonabletarget.Shrink- ingreducesmeansquarederror. TwoexcellentmonographsonmodelselectionandaveragingareBurnhamandAnderson(1998)and ClaeskensandHjort(2008).James-SteinshrinkagetheoryisthoroughlycoveredinLehmannandCasella (1998).SeealsoEfron(2010)andWasserman(2006). 28.2 ModelSelection In the course of an applied project an economist will routinely estimate multiple models. Indeed, most applied papers include tables displaying the results from different specifications. The question arises:Whichmodelisbest?Whichshouldbeusedinpractice?Howcanweselectthebestchoice?This isthequestionofmodelselection. Take, for example, a wage regression. Suppose we want a model which conditions on education, experience, region, and marital status. How should we proceed? Should we estimate a simple linear modelplusaquadraticinexperience?Shouldeducationenterlinearly,asimplesplineasinFigure2.6(a), or with separate dummies for each education level? Should marital status enter as a simple dummy (marriedornot)orallowingforallrecordedcategories? Shouldinteractionsbeincluded? Which? How many?Takentogetherweneedtoselectthespecificregressorstoincludeintheregressionmodel. Modelâselectionâmaybemis-named.Itwouldbemoreappropriatetocalltheissueâestimatorse- lectionâ. Whenweexamineatablecontainingtheresultsfrommultipleregressionswearecomparing multipleestimatesofthesameregression.Oneestimatormayincludefewervariablesthananother;that is a restricted estimator. One may be estimated by least squares and another by 2SLS. Another could be nonparametric. The underlying model is the same; the difference is the estimator. Regardless, the literaturehasadoptedthetermâmodelselectionâandwewilladheretothisconvention. 859",
    "page": 879,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 860 To gain some basic understanding it may be helpful to start with a stylzed example. Suppose that wehaveaK Ã1estimatorÎ¸ (cid:98)whichhasmeanÎ¸andknowncovariancematrixV. Analternativefeasible estimatorisÎ¸ (cid:101) =0. Thelattermayseemlikeasillyestimatorbutitcapturesthefeaturethatmodelse- lectiontypicallyconcernsexclusionrestrictions.Inthiscontextwecancomparetheaccuracyofthetwo estimatorsbytheirweightedmean-squarederror(WMSE).ForagivenweightmatrixW define wmse (cid:163)Î¸ (cid:98) (cid:164)=tr (cid:179) (cid:69) (cid:104) (cid:161)Î¸ (cid:98) âÎ¸(cid:162)(cid:161)Î¸ (cid:98) âÎ¸(cid:162)(cid:48)(cid:105) W (cid:180) =(cid:69) (cid:104) (cid:161)Î¸ (cid:98) âÎ¸(cid:162)(cid:48) W (cid:161)Î¸ (cid:98) âÎ¸(cid:162) (cid:105) . ThecalculationssimplifybysettingW =V â1whichwedoforourremainingcalculations. Forourtwoestimatorswecalculatethat wmse (cid:163)Î¸ (cid:98) (cid:164)=K (28.1) wmse (cid:163)Î¸ (cid:101) (cid:164)=Î¸(cid:48) V â1Î¸d=efÎ». (28.2) (SeeExercise28.1)TheWMSEofÎ¸ (cid:98)issmallerifK <Î»andtheWMSEofÎ¸ (cid:101)issmallerifK >Î». Oneinsight from this simple analysis is that we should prefer smaller (simpler) models when potentially omitted variableshavesmallcoefficientsrelativetoestimationvariance,andshouldpreferlarger(morecompli- cated)modelswhenthesevariableshavelargecoefficientsrelativetoestimationvariance. Thecomparisonbetween(28.1)and(28.2)isabasicbias-variancetrade-off. TheestimatorÎ¸ (cid:98)isun- biasedbuthasavariancecontributionofK. TheestimatorÎ¸ (cid:101)haszerovariancebuthasasquaredbias contributionÎ».TheWMSEcombinesthesetwocomponents. Selection based on WMSE suggests that we should select the estimator Î¸ (cid:98) if K < Î» and select Î¸ (cid:101) if K >Î». ThisisinfeasiblesinceÎ»isunknown. ItcanbeestimatedbyreplacingÎ¸ (cid:98)withÎ¸. Thisestimatoris Î» (cid:98) =Î¸ (cid:98) (cid:48) V â1Î¸ (cid:98) =W,theWaldstatisticforthetestofÎ¸=0.However,theestimatorÎ» (cid:98)hasexpectation (cid:69)(cid:163)Î» (cid:98) (cid:164)=(cid:69)(cid:163)Î¸ (cid:98) (cid:48) V â1(cid:48)Î¸ (cid:98) (cid:164)=Î¸(cid:48) V â1(cid:48)Î¸+(cid:69) (cid:104) (cid:161)Î¸ (cid:98) âÎ¸(cid:162)(cid:48) V â1(cid:161)Î¸ (cid:98) âÎ¸(cid:162) (cid:105) =Î»+K soisbiaseditself. AnunbiasedestimatorisÎ» (cid:101) =Î» (cid:98) âK. NoticethatÎ» (cid:101) >K isthesameasW >2K. This leadstothemodel-selectionrule:UseÎ¸ (cid:98)ifW >2K anduseÎ¸ (cid:101)otherwise. This is an overly-simplistic setting but highlights the fundamental ingredients of criterion-based modelselection. ComparingtheMSEofdifferentestimatorstypicallyinvolvesatrade-offbetweenthe biasandvariancewithmorecomplicatedmodelsexhibitinglessbiasbutincreasedestimationvariance. The actual trade-off is unknown since the bias depends on the unknown true parameters. The bias, however,canbeestimated,givingrisetoempiricalestimatesoftheMSEandempiricalmodelselection rules. Alargenumberofmodelselectioncriteriahavebeenproposed. Welistherethosemostfrequently usedinappliedeconometrics",
    "page": 880,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". ComparingtheMSEofdifferentestimatorstypicallyinvolvesatrade-offbetweenthe biasandvariancewithmorecomplicatedmodelsexhibitinglessbiasbutincreasedestimationvariance. The actual trade-off is unknown since the bias depends on the unknown true parameters. The bias, however,canbeestimated,givingrisetoempiricalestimatesoftheMSEandempiricalmodelselection rules. Alargenumberofmodelselectioncriteriahavebeenproposed. Welistherethosemostfrequently usedinappliedeconometrics. WefirstlistselectioncriteriaforthelinearregressionmodelY =X (cid:48)Î²+ewithÏ2=(cid:69)(cid:163) e2(cid:164) andakÃ1co- efficientvectorÎ².LetÎ² (cid:98)betheleastsquaresestimator,e (cid:98)i theleastsquaresresidual,andÏ (cid:98) 2=n â1(cid:80)n i=1 e (cid:98)i 2 thevarianceestimator.Thenumberofestimatedparameters(Î²andÏ2)isK =k+1. BayesianInformationCriterion BIC=n+nlog (cid:161) 2ÏÏ2(cid:162)+Klog(n). (28.3) (cid:98) AkaikeInformationCriterion AIC=n+nlog (cid:161) 2ÏÏ2(cid:162)+2K. (28.4) (cid:98)",
    "page": 880,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 861 Cross-Validation n CV= (cid:88) e2 (28.5) (cid:101)i i=1 wheree aretheleastsquaresleave-one-outpredictionerrors. (cid:101)i Wenextlisttwocommonly-usedselectioncriteriaforlikelihood-basedestimation. Let f(y,Î¸)bea parametricdensitywithaKÃ1parameterÎ¸.ThelikelihoodL (Î¸)=(cid:81)n f(Y ,Î¸)isthedensityevaluated n i=1 i attheobservations.ThemaximumlikelihoodestimatorÎ¸ (cid:98)maximizes(cid:96) n (Î¸)=logL n (Î¸). BayesianInformationCriterion BIC=â2(cid:96) n (Î¸ (cid:98))+Klog(n). (28.6) AkaikeInformationCriterion AIC=â2(cid:96) n (Î¸ (cid:98))+2K. (28.7) Inthefollowingsectionswederiveanddiscusstheseandothermodelselectioncriteria. 28.3 BayesianInformationCriterion TheBayesianInformationCriterion(BIC),alsoknownastheSchwarzCriterion,wasintroducedby Schwarz(1978). Itisappropriateforparametricmodelsestimatedbymaximumlikelihoodandisused toselectthemodelwiththehighestapproximateprobabilityofbeingthetruemodel. Let f(y,Î¸)betheparametricmodeldensity,andletÏ(Î¸)bethepriordensityforÎ¸. Thejointdensity ofY andÎ¸is f(y,Î¸)Ï(Î¸).ThemarginaldensityofY is (cid:90) p(y)= f(y,Î¸)Ï(Î¸)dÎ¸. Themarginaldensityp(Y)evaluatedattheobservationsisknownasthemarginallikelihood. Schwarz(1978)establishedthefollowingapproximation. Theorem28.1 Schwarz. Ifthemodel f(y,Î¸)satisfiesstandardregularitycon- ditionsandthepriorÏ(Î¸)isdiffusethen â2logp(Y)=â2(cid:96) n (Î¸ (cid:98))+Klog(n)+O(1) wheretheO(1)termisboundedasnââ. AheuristicprooffornormallinearregressionisgiveninSection28.32.Aâdiffuseâpriorisonewhich distributesweightuniformlyovertheparameterspace. Schwarzâs theorem shows that the marginal likelihood approximately equals the maximized likeli- hoodmultipliedbyanadjustmentdependingonthenumberofestimatedparametersandthesample size.Theapproximation(28.6)iscommonlycalledtheBayesianInformationCriterionorBIC.TheBIC isapenalizedloglikelihood.ThetermKlog(n)canbeinterpretedasanover-parameterizationpenalty. Themultiplicationoftheloglikelihoodbyâ2istraditionalasitputsthecriterionintothesameunitsas alog-likelihoodstatistic.",
    "page": 881,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 862 Inthecontextofnormallinearregressionwehavecalculatedin(5.6)that (cid:96) n (Î¸ (cid:98))=â n(cid:161) log(2Ï)+1 (cid:162)â n log (cid:161)Ï (cid:98) 2(cid:162) 2 2 whereÏ2istheresidualvarianceestimate.HenceBICequals(28.3)withK =k+1. (cid:98) Sincenlog(2Ï)+n doesnotvaryacrossmodelsthistermisoftenomitted. Itisbetter, however, to definetheBICasdescribedabovesothatdifferentparametricfamiliesarecomparable. Itisalsouseful toknowthatsomeauthorsdefinetheBICbydividingtheaboveexpressionbyn(e.g. BIC=log (cid:161) 2ÏÏ2(cid:162)+ (cid:98) Klog(n)/n) which does not change the rankings between models. However, this is an unwise choice becauseitaltersthescaling,makingitdifficulttocomparethedegreeofdifferencebetweenmodels. NowsupposethatwehavetwomodelsM andM whichhavemarginallikelihoodsp (Y)andp (Y). 1 2 1 2 Assumethatbothmodelshaveequalpriorprobability. BayesTheoremstatesthattheprobabilitythata modelistruegiventhedataisproportionaltoitsmarginallikelihood.Specifically p (Y) (cid:80)[M |Y]= 1 1 p (Y)+p (Y) 1 2 p (Y) (cid:80)[M |Y]= 2 . 2 p (Y)+p (Y) 1 2 Bayes selection picks the model with highest probability. Thus if p (Y)> p (Y) we select M . If 1 2 1 p (Y)<p (Y)weselectM . 1 2 2 Finding the model with highest marginal likelihood is the same as finding the model with lowest value of â2logp(Y). Theorem 28.1 shows that the latter approximately equals the BIC. BIC selection picksthemodelwiththelowest1valueofBIC.ThusBICselectionisapproximateBayesselection. The above discussion concerned two models but applies to any number of models. BIC selection picksthemodelwiththesmallestBIC.Forimplementationyousimplyestimateeachmodel,calculate itsBIC,andcompare. The BIC may be obtained in Stata by using the command estimates stats after an estimated model. 28.4 AkaikeInformationCriterionforRegression The Akaike Information Criterion (AIC) was introduced by Akaike (1973). It is used to select the modelwhoseestimateddensityisclosesttothetruedensity. Itisdesignedforparametricmodelsesti- matedbymaximumlikelihood. Let f(cid:98)(y)beanestimateoftheunknowndensity g(y)oftheobservationvectorY =(Y 1 ,...,Y n ). For example,thenormallinearregressionestimateofg(y)is f(cid:98)(y)=(cid:81)n i=1 Ï Ï (cid:98) (cid:161) Y i âX i (cid:48)Î² (cid:98) (cid:162) . To measure the distance between densities g and f Akaike used the Kullback-Leibler information criterion(KLIC) (cid:90) (cid:181) g(y) (cid:182) KLIC(g,f)= g(y)log dy. f(y) NoticethatKLIC(g,f)=0when f(y)=g(y).ByJensenâsinequality, (cid:90) (cid:181) f(y) (cid:182) (cid:90) KLIC(g,f)=â g(y)log dyâ¥âlog f(y)dy=0. g(y) ThusKLIC(g,f)isanon-negativemeasureofthedeviationof f from g,withsmallvaluesindicatinga smallerdeviation. 1WhentheBICisnegativethismeanstakingthemostnegativevalue.",
    "page": 882,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 863 ( ) gx ( ) fx (a) f(x)andg(x) (b)log(f(x)/g(x)) Figure28.1:Kullback-LeiblerDistanceMeasure Toillustrate,inFigure28.1wedisplaytwodensitiesandtheirlogratio. Inpanel(a)wedisplaytwo densities f(x)andg(x).Forconcreteness,thedensityg(x)isthenonparametricestimateofthelogwage density displayed in Figure 2.1. The density f(x) is the MLE of a log-normal parametric model. You canseethatthetwodensitesarequitesimilarandhavethesamegeneralshape. Theparametricmodel, however,issomewhatloweratthepeak,andmayover-statetherighttailofthedensity.Inpanel(b)you seethelogratiolog(f(x)/g(x)).Thedashedlineis0forreference.Ifthetwodensitieswerethesamethen theplotwouldequalthezeroline. Negativevaluesindicateregionswhere f(x)<g(x). Positivevalues indicateregionswhere f(x)>g(x). Inthisplotweseethatthelargestpercentagedeviationsareinthe righttail.TheKLICistheweightedintegralofthislogratiofunction.Itisaweightedaveragewithweights givenbythedensityg(x). Sinceg(x)putsmostprobabilitymassintheleft-middleoftheplotthisisthe regionemphasizedbytheKLICcalcuation.Thuswhiletherighttailhasthelargestdeviationsitdoesnot receivealargeweightintheKLICbecausethedensityg(x)haslittleprobabilitymassthere. TheKLICdistancebetweenthetrueandestimateddensitiesis (cid:90) (cid:195) g (cid:161) y (cid:162)(cid:33) KLIC(g,f(cid:98))= g(y)log dy (cid:161) (cid:162) f(cid:98) y (cid:90) (cid:90) = g(y)log (cid:161) g (cid:161) y (cid:162)(cid:162) dyâ g(y)log (cid:161) f(cid:98) (cid:161) y (cid:162)(cid:162) dy. Thisisrandomasitdependsontheestimator f(cid:98).AkaikeproposedtheexpectedKLICdistance (cid:90) (cid:183)(cid:90) (cid:184) (cid:69)(cid:163) KLIC(g,f(cid:98)) (cid:164)= g(y)log (cid:161) g (cid:161) y (cid:162)(cid:162) dyâ(cid:69) g(y)log (cid:161) f(cid:98) (cid:161) y (cid:162)(cid:162) dy . (28.8) Thefirsttermin(28.8)doesnotdependonthemodel.SominimizationofexpectedKLICdistanceis minimizationofthesecondterm.Multipliedby2(similarlytotheBIC)thisis (cid:183)(cid:90) (cid:184) T =â2(cid:69) g(y)log (cid:161) f(cid:98)(y) (cid:162) dy . (28.9)",
    "page": 883,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 864 Theexpectationisovertherandomestimator f(cid:98). Analternativeinterpretationistonoticethattheintegralin(28.9)isanexpectationoverY withre- specttothetruedatadensityg(y).Thuswecanwrite(28.9)as T =â2(cid:69)(cid:163) log (cid:161) f(cid:98) (cid:161) Y(cid:101) (cid:162)(cid:162)(cid:164) (28.10) whereY(cid:101) isanindependentcopyofY. Thekeytounderstandthisexpressionisthatboththeestimator f(cid:98)and the evaluation points Y(cid:101) are random and independently distributed. This is the expected log- likelihoodfitusingtheestimatedmodel f(cid:98)ofanout-of-samplerealizationY(cid:101). ThusT canbeinterpreted asanexpectedpredictiveloglikelihood. ModelswithlowvaluesofT havegoodfitbasedontheout-of- samplelog-likelihood. To gain further understanding we consider the simple case of the normal linear regression model withK regressors.Thelogdensityofthemodelfortheobservationsis logf(Y,X,Î¸)=â n log (cid:161) 2ÏÏ2(cid:162)â 1 (cid:88) n (cid:161) Y âX (cid:48)Î²(cid:162)2 . (28.11) 2 2Ï2 i i i=1 Theexpectedvalueatthetrueparametervaluesisânlog (cid:161) 2ÏÏ2(cid:162)ân.Thismeansthattheidealizedvalue 2 2 ofT isT =nlog (cid:161) 2ÏÏ2(cid:162)+n.Thiswouldbethevalueobtainediftherewerenoestimationerror. 0 We now add the assumption that the variance Ï2 is known. This is not realistic but simplifies the calculations. Theorem28.2 Suppose f(cid:98)(y) is an estimated normal linear regression model withK regressorsandaknownvarianceÏ2.Supposethatthetruedensityg(y) isaconditionallyhomoskedasticregressionwithvarianceÏ2.Then T =nlog (cid:161) 2ÏÏ2(cid:162)+n+K =T +K (28.12) 0 (cid:69)(cid:163)â2(cid:96) n (Î¸ (cid:98)) (cid:164)=nlog (cid:161) 2ÏÏ2(cid:162)+nâK =T 0 âK. (28.13) TheproofisgiveninSection28.32. Theseexpressionsareinteresting.Expression(28.12)showsthattheexpectedKLICdistanceT equals theidealizedvalueT plusK. Thelatteristhecostofparameterestimation, measuredintermsofex- 0 pectedKLICdistance. Byestimatingparameters(ratherthanusingthetruevalues)theexpectedKLIC distanceincreaseslinearlywithK. Expression (28.13) shows the converse story. It shows that the sample log-likelihood function is smaller than the idealized value T by K. This is the cost of in-sample over-fitting. The sample log- 0 likelihoodisanin-samplemeasureoffitandthereforeunderstatesthepopulationlog-likelihood. The twoexpressionstogethershowthatthesamplelog-likelihoodissmallerthanthetargetvalueT by2K. Thisisthecombinedcostofover-fittingandparameterestimation. CombiningtheseexpressionswecansuggestanunbiasedestimatorforT. Inthenormalregression modelweuse(28.4).Sincenlog(2Ï)+ndoesnotvaryacrossmodelsitareoftenomitted.Thusforlinear regressionitiscommontousethedefinitionAIC=nlog (cid:161)Ï2(cid:162)+2K. (cid:98) InterestinglytheAICtakesaverysimilarformtotheBIC.BoththeAICandBICarepenalizedloglike- lihoods,andbothpenaltiesareproportionaltothenumberofestimatedparametersK. Thedifference is thatthe AIC penalty is2K while the BIC penalty is Klog(n)",
    "page": 884,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". CombiningtheseexpressionswecansuggestanunbiasedestimatorforT. Inthenormalregression modelweuse(28.4).Sincenlog(2Ï)+ndoesnotvaryacrossmodelsitareoftenomitted.Thusforlinear regressionitiscommontousethedefinitionAIC=nlog (cid:161)Ï2(cid:162)+2K. (cid:98) InterestinglytheAICtakesaverysimilarformtotheBIC.BoththeAICandBICarepenalizedloglike- lihoods,andbothpenaltiesareproportionaltothenumberofestimatedparametersK. Thedifference is thatthe AIC penalty is2K while the BIC penalty is Klog(n). Since 2<log(n) ifn â¥8 the BIC uses a strongerparameterizationpenalty.",
    "page": 884,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 865 Selecting a model by the AIC is equivalent to calculating the AIC for each model and selecting the modelwiththelowest2value. Theorem28.3 Under the assumptions of Theorem 28.2, (cid:69)[AIC] = T. AIC is thusanunbiasedestimatorofT. One of the interesting features of these results are that they are exact â there is no approximation error â and they do not require that the true error is normally distributed. The critical assumption is conditionalhomoskedasticity. IfhomoskedasticityfailsthentheAIClosesitsvalidity. Inmoregeneral contextstheseresultsdonotholdexactlybutinsteadholdasapproximations(asdiscussedinthenext section). The AIC may be obtained in Stata by using the command estimates stats after an estimated model. 28.5 AkaikeInformationCriterionforLikelihood For the general likelihood context Akaike proposed the criterion (28.7). Here, Î¸ (cid:98) is the maximum likelihoodestimator,(cid:96) n (Î¸ (cid:98))isthemaximizedlog-likelihoodfunction,andK isthenumberofestimated parameters.Thisspecializesto(28.4)forthecaseofanormallinearregressionmodel. Asforregression,AICselectionisperformedbyestimatingasetofmodels,calculatingAICforeach, andselectingthemodelwiththesmallestAIC. TheadvantagesoftheAICarethatitissimpletocalculate,easytoimplement,andstraightforward tointerpret.Itisintuitiveasitisasimplepenalizedlikelihood. Thedisadvantageisthatitssimplicitymaybedeceptive. Theproofshowsthatthecriterionisbased onaquadraticapproximationtotheloglikelihoodandanasymptoticchi-squareapproximationtothe classical Wald statistic. When these conditions fail then the AIC may not be accurate. For example, if the model is an approximate (quasi) likelihood rather than a true likelihood then the failure of the information matrix equality implies that the classical Wald statistic is not asymptotically normal. In this case the accuracy of AIC fails. Another problem is that many nonlinear models have parameter regions where parametric identification fails. In these models the quadratic approximation to the log likelihoodfunctionfailstoholduniformlyintheparameterspacesotheaccuracyoftheAICfails.These qualificationspointtochallengesininterpretationoftheAICinnonlinearmodels. ThefollowingisananalogofTheorem28.3. Theorem28.4 Understandardregularityconditionsformaximumlikelihood estimation,plustheassumptionthatcertainstatistics(identifiedintheproof) areuniformlyintegrable, (cid:69)[AIC]=T +O (cid:161) n1/2(cid:162) . AICisthusanapproximately unbiasedestimatorofT. AsketchoftheproofisgiveninSection28.32. ThisresultshowsthattheAICis, ingeneral, areasonableestimatoroftheKLICfitofanestimated parametricmodel.ThetheoremholdsbroadlyformaximumlikelihoodestimationandthustheAICcan beusedinawidevarietyofcontexts. 2WhentheAICisnegativethismeanstakingthemostnegativevalue.",
    "page": 885,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 866 28.6 MallowsCriterion The Mallows Criterion was proposed by Mallows (1973) and is often called the C criterion. It is p appropriateforlinearestimatorsofhomoskedasticregressionmodels. Takethehomoskedasticregressionframework Y =m+e m=m(X) (cid:69)[e|X]=0 (cid:69)(cid:163) e2|X (cid:164)=Ï2. Writethefirstequationinvectornotationforthen observationsasY =m+e. Letm= AY bealinear (cid:98) estimatorofm,meaningthat AissomenÃn functionoftheregressormatrixX only. Theresidualsare e=Yâm.Theclassoflinearestimatorsincludesleastsquares,weightedleastsquares,kernelregression, (cid:98) (cid:98) locallinearregression,andseriesregression. Forexample,theleastsquaresestimatorusingaregressor matrixZ isthecase A=Z (cid:161) Z (cid:48) Z (cid:162)â1 Z (cid:48) . Mallows(1973)proposedthecriterion C =e (cid:48) e+2Ï2tr(A) (28.14) p (cid:98)(cid:98) (cid:101) whereÏ2 isapreliminaryestimatorofÏ2 (typicallybasedonfittingalargemodel). Inthecaseofleast (cid:101) squaresregressionwithK coefficientsthissimplifiesto C =nÏ2+2KÏ2. (28.15) p (cid:98) (cid:101) TheMallowscrierioncanbeusedsimilarlytotheAIC.Asetofregressionmodelsareestimatedand the criterionC calculated for each. The model with the smallest value ofC is the Mallows-selected p p model. MallowsdesignedthecriterionC asanunbiasedestimatorofthefollowingmeasureoffit p (cid:34) (cid:35) n R=(cid:69) (cid:88) (m âm )2 . (cid:98)i i i=1 Thisistheexpectedsquareddifferencebetweentheestimatedandtrueregressionevaluatedattheob- servations. AnalternativemotivationforR isintermsofpredictionaccuracy. Consideranindependentsetof observationsY(cid:101)i ,i =1,...,n,whichhavethesameregressors X i asthoseinsample. Considerprediction ofY(cid:101)i givenX i andthefittedregression. Theleastsquarespredictorism (cid:98)i . Thesumofexpectedsquared predictionerrorsis MSFE= (cid:88) n (cid:69) (cid:104) (cid:161) Y(cid:101)i âm (cid:98)i (cid:162)2 (cid:105) . i=1 Thebestpossible(infeasible)valueofthisquantityis MSFE 0 = (cid:88) n (cid:69) (cid:104) (cid:161) Y(cid:101)i âm i (cid:162)2 (cid:105) . i=1 Thedifferenceisthepredictionaccuracyoftheestimator: MSFEâMSFE 0 = (cid:88) n (cid:69) (cid:104) (cid:161) Y(cid:101)i âm (cid:98)i (cid:162)2 (cid:105) â (cid:88) n (cid:69) (cid:104) (cid:161) Y(cid:101)i âm i (cid:162)2 (cid:105) i=1 i=1 (cid:34) (cid:35) n =(cid:69) (cid:88) (m âm )2 (cid:98)i i i=1 =R",
    "page": 886,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 867 whichequalsMallowsâmeasureoffit.ThusR canbeviewedasameasureofpredictionaccuracy. We stated that the Mallows criterion is an unbiased estimator of R. More accurately, the adjusted criterionC â =C âe (cid:48) e is unbiased for R. When comparing modelsC andC â are equivalent so this p p p p substitutionhasnoconsequenceformodelselection. Theorem28.5 If m = AY is a linear estimator, the regression error is condi- (cid:98) tionallymeanzeroandhomoskedastic,andÏ2isunbiasedforÏ2,then (cid:101) (cid:104) (cid:105) (cid:69) C â =R p â sotheadjustedMallowscriterionC isanunbiasedestimatorofR. p TheproofisgiveninSection28.32. 28.7 Hold-OutCriterion Dividingthesampleintotwoparts,oneforestimationandthesecondforevaluation,createsasimple deviceformodelevaluationandselection. Thisprocedureisoftenlabelledhold-outevaluation. Inthe recentmachinelearningliteraturethedatadivisionistypicallydescribedasatrainingsampleandatest sample. The sample is typically divided randomly so that the estimation (training) sample has N observa- tions,theevaluation(test)samplehasP observations,whereN+P=n.Thereisnouniversalruleforthe choiceofN &P,butN =P=n/2isastandardchoice. Formorecomplicatedprocedures,suchastheevaluationofmodelselectionmethods,itisdesirable tomakeatripartitedivisionofthesampleinto(1)training,(2)modelselection,and(3)finalestimation andassessment.Thiscanbeparticularlyusefulwhenitisdesiredtoobtainaparameterestimatorwhose distributionisnotdistortedbythemodelselectionprocess.Suchdivisionsaremostsuitedforacontext ofanextremelylargesample. Takethestandardcaseofabipartitedivisionwhere1â¤i â¤N istheestimationsampleandN+1â¤ i â¤N+P istheevaluationsample. Ontheestimationsampleweconstructtheparameterestimates,for exampletheleastsquarescoefficients (cid:195) (cid:33)â1(cid:195) (cid:33) N N Î² (cid:101)N = (cid:88) X i X i (cid:48) (cid:88) X i Y i . i=1 i=1 Combiningthiscoefficientwiththeevaluationsamplewecalculatethepredictionerrorse (cid:101)N,i =Y i âX i (cid:48)Î² (cid:101)N fori â¥N+1. InSection4.14wedefinedthemeansquaredforecasterror(MSFE)basedonaestimationsampleof (cid:104) (cid:105) sizeN astheexpectationofthesquaredout-of-samplepredictionerrorMSFE =(cid:69) e2 .Thehold-out N (cid:101)N,i estimatoroftheMSFEistheaverageofthesquaredpredictionerrors Ï2 = 1 N (cid:88) +P e2 . (cid:101)N,P P (cid:101)N,i i=N+1 WecanseethatÏ2 isunbiasedforMSFE . (cid:101)N,P N",
    "page": 887,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 868 When N =P we can improve estimation of the MSFE by flipping the procedure. Exchanging the roles of estimation and evaluation samples we obtain a second MSFE estimator, say Ï2 . The global (cid:101)N,P (cid:179) (cid:180) estimator is their mean Ïâ2 = Ï2 +Ï2 /2. This estimator also has expectation MSFE but has (cid:101)N,P (cid:101)N,P (cid:101)N,P N reducedvariance. The estimated MSFE Ï2 can be used for model selection. The quantity Ï2 is calculated for a (cid:101)N,P (cid:101)N,P setofproposedmodels. TheselectedmodelistheonewiththesmallestvalueofÏ2 . Themethodis (cid:101)N,P intuitive,general,andflexible,anddoesnotrelyontechnicalassumptions. The hold-out method has two disadvantages. First, if our goal is estimation using the full sample, our desired estimate is MSFE , not MSFE . Hold-out estimation provides an estimator of the MSFE n N basedonestimationusingasubstantiallyreducedsamplesize, andisthusbiasedfortheMSFEbased on estimation using the full sample. Second, the estimator Ï2 is sensitive to the random sorting of (cid:101)N,P theobservationsintotheestimationandevaluationsamples. Thisaffectsmodelselection. Resultscan dependontheinitialsamplesortingandarethereforepartiallyarbitrary. 28.8 Cross-ValidationCriterion In applied statistics and machine learning the default method for model selection and tuning pa- rameter selection is cross-validation. We have introduced some of the concepts throughout the text- book,andreviewandunifytheconceptsatthispoint.Cross-validationiscloselyrelatedtothehold-out criterionintroducedintheprevioussection. InSection3.20wedefinedtheleave-one-outestimatorasthatobtainedbyapplyinganestimation formula to the sample omitting the ith observation. This is identical to the hold-out problem as de- scribed previously, where the estimation sample is N =nâ1 and the evaluation sample is P =1. The estimatorobtainedomittingobservationi iswrittenasÎ² (cid:98)(âi) . Thepredictionerrorise (cid:101)i =Y i âX i (cid:48)Î² (cid:98)(âi) . Theout-of-samplemeansquarederrorâestimateâise2.Thisisrepeatedntimes,onceforeachobserva- (cid:101)i tioni,andtheMSFEestimateistheaverageofthensquaredpredictionerrors CV= 1 (cid:88) n e2. n (cid:101)i i=1 The estimator CV is called the cross-validation (CV) criterion. It is a natural generalization of the hold-out criterion and eliminates the two disadvantages described in the previous section. First, the CV criterion is an unbiased estimator of MSFE nâ1 , which is essentially the same as MSFE n . Thus CV is essentially unbiased for model selection. Second, the CV criterion does not depend on a random sortingoftheobservations. Asthereisnorandomcomponentthecriteriontakesthesamevalueinany implementation. InleastsquaresestimationtheCVcriterionhasasimplecomputationalimplementation",
    "page": 888,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". First, the CV criterion is an unbiased estimator of MSFE nâ1 , which is essentially the same as MSFE n . Thus CV is essentially unbiased for model selection. Second, the CV criterion does not depend on a random sortingoftheobservations. Asthereisnorandomcomponentthecriteriontakesthesamevalueinany implementation. InleastsquaresestimationtheCVcriterionhasasimplecomputationalimplementation. Theorem 3.7showsthattheleave-one-outleastsquaresestimator(3.42)equals Î² (cid:98)(âi) =Î² (cid:98) â (1â 1 h ) (cid:161) X (cid:48) X (cid:162)â1 X i e (cid:98)i ii wheree aretheleastsquaresresidualsandh aretheleveragevalues.Thepredictionerrorthusequals (cid:98)i ii e (cid:101)i =Y i âX i (cid:48)Î² (cid:98)(âi) =(1âh ii ) â1e (cid:98)i wherethesecondequalityisfromTheorem3.7.ConsequentlytheCVcriterionis CV= 1 (cid:88) n e2= 1 (cid:88) n (1âh ) â2e2. n (cid:101)i n ii (cid:98)i i=1 i=1",
    "page": 888,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 869 Recall as well that in our study of nonparametric regression (Section 19.12) we defined the cross- validationcriterionforkernelregressionastheweightedaverageofthesquaredpredictionerrors CV= 1 (cid:88) n e2w(X ). n (cid:101)i i i=1 Theorem19.7showedthatCVisapproximatelyunbiasedfortheintegratedmeansquarederror(IMSE), whichisastandardmeasureofaccuracyfornonparametricregression.TheseresultsshowthatCVisan unbiasedestimatorforboththeMSFEandIMSE,showingacloseconnectionbetweenthesemeasures ofaccuracy. InSection20.17andequation(20.30)wedefinedtheCVcriterionforseriesregressionasin(28.5). Selectingvariablesforseriesregressionisidenticaltomodelselection. Theresultsasdescribedabove showthattheCVcriterionisanestimatorfortheMSFEandIMSEoftheregressionmodelandistherefore agoodcandidateforassessingmodelaccuracy.ThevalidityoftheCVcriterionismuchbroaderthanthe AIC as the theorems for CV do not require conditional homoskedasticity. This is not an artifact of the proofmethod;cross-validationisinherentlymorerobustthanAICorBIC. ImplementationofCVmodelselectionisthesameasfortheothercriteria.Asetofregressionmodels areestimated.ForeachtheCVcriterioniscalculated.ThemodelwiththesmallestvalueofCVistheCV- selectedmodel. The CV method is also much broader in concept and potential application. It applies to any esti- mationmethodsolongasaâleaveoneoutâerrorcanbecalculated. Itcanalsobeappliedtootherloss functionsbeyondsquarederrorloss.Forexample,across-validationestimateofabsolutelossis CV= 1 (cid:88) n |e |. (cid:101)i n i=1 Computationallyandconceptuallyitisstraightforwardtoselectmodelsbyminimizingsuchcriterion. However,thepropertiesofapplyingCVtogeneralcriterionisnotknown. StatadoesnothaveastandardcommandtocalculatetheCVcriterionforregressionmodels. 28.9 K-FoldCross-Validation TherearetwodeficiencieswiththeCVcriterionwhichcanbealleviatedbythecloselyrelatedK-fold cross-valiationcriterion.ThefirstdeficiencyisthatCVcalculationcanbecomputationallycostlywhen sample sizes are very large or the estimation method is other than least squares. For estimators other thanleastsquaresitmaybenecessarytocalculatenseparateestimationswhichcanbecomputationally prohibitive in some contexts. A second deficiency is that the CV criterion, viewed as an estimator of MSFE n ,hasahighvariance.Thesourceisthattheleave-one-outestimatorsÎ² (cid:98)(âi) haveminimalvariation acrossi andarethereforehighlycorrelated. AnalternativeisistosplitthesampleintoK groups(orâfoldsâ)andtreateachgroupasahold-out sample. Thiseffectivelyreducesthenumberofestimationsfromn toK. (ThisK isnotthenumberof estimatedcoefficients. Iapologizeforthepossibleconfusioninnotationbutthisisthestandardlabel.) AcommonchoiceisK =10,leadingtowhatisknownas10-foldcross-validation. The methodworksby thefollowingsteps. Thisdescriptionisfor estimationofaregressionmodel Y =g(X,Î¸)+ewithestimatorÎ¸ (cid:98). 1. Randomlysorttheobservations. 2",
    "page": 889,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". AnalternativeisistosplitthesampleintoK groups(orâfoldsâ)andtreateachgroupasahold-out sample. Thiseffectivelyreducesthenumberofestimationsfromn toK. (ThisK isnotthenumberof estimatedcoefficients. Iapologizeforthepossibleconfusioninnotationbutthisisthestandardlabel.) AcommonchoiceisK =10,leadingtowhatisknownas10-foldcross-validation. The methodworksby thefollowingsteps. Thisdescriptionisfor estimationofaregressionmodel Y =g(X,Î¸)+ewithestimatorÎ¸ (cid:98). 1. Randomlysorttheobservations. 2. Split the observations into folds k =1,...,K of (roughly) equal size n (cid:39)n/K. Let I denote the k k observationsinfoldk.",
    "page": 889,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 870 3. Fork=1,..,K (a) ExcludefoldI fromthedataset.Thisproducesasamplewithnân observations. k k (b) CalculatetheestimatorÎ¸ (cid:98)(âk) onthissample. (c) Calculatethepredictionerrorse (cid:101)i =Y i âg(X i ,Î¸ (cid:98)(âk) )fori âI k . (d) CalculateCV k =n k â1(cid:80) iâIk e (cid:101)i 2 4. CalculateCV=K â1(cid:80)K CV . k=1 k IfK =nthemethodisidenticaltoleave-one-outcrossvalidation. A useful feature of K-fold CV is that we can calculate an approximate standard error. It is based on the approximation var(CV)(cid:39)K â1var(CV ) which is based on the idea that CV are approximately k k uncorrelatedacrosfolds.Thisleadstothestandarderror (cid:118) (cid:117) (cid:117) 1 (cid:88) K s(CV)=(cid:116) (CV âCV)2. K(Kâ1) k k=1 Thisissimilartoaclusteredvarianceformula,wherethefoldsaretreatedasclusters.Thestandarderror s(CV)canbereportedtoassesstheprecisionofCVasanestimateoftheMSFE. OnedisadvantageofK-foldcross-validationisthatCVcanbesensitivetotheinitialrandomsorting oftheobservations, leadingtopartiallyarbitraryresults. Thisproblemcanbereducedbyatechnique calledrepeatedCV,whichrepeatstheK-foldCValgorithmM times(eachtimewithadifferentrandom sorting),leadingtoM valuesofCV.TheseareaveragedtoproducetherepeatedCVvalue.AsM increases therandomnessduetosortingiseliminated.Anassociatedstandarderrorcanbeobtainedbytakingthe squarerootoftheaveragesquaredstandarderrors. CV model selection is typically implemented by selecting the model with the smallest value of CV. Analternativeimplementationisknownastheonestandarderror(1se)ruleandselectsthemostpar- simonious model whose value of CV is within one standard error of the minimum CV. The (informal) idea is that models whose value of CV is within one standard error of one another are not statistically distinguishable, andallelseheldequalweshouldleantowardsparsimony. The1seruleisthedefault, forexample, inthepopularcv.glmnetRfunction. The1seruleisanoversmoothingchoice, meaning thatitleanstowardshigherbiasandreducedvariance.Incontrast,forinferencemanyeconometricians recommendundersmoothingbandwidths,whichmeansselectingalessparsimoniousmodelthanthe CVminimizingchoice. 28.10 ManySelectionCriteriaareSimilar For the linear regression model many selection criteria have been introduced. However, many of thesealternativecriteriaarequitesimilartooneanother. Inthissectionwereviewsomeofthesecon- nections.ThefollowingdiscussionisforthestandardregressionmodelY =X (cid:48)Î²+ewithnobservations, K estimatedcoefficients,andleastsquaresvarianceestimatorÏ2. (cid:98) Shibata(1980)proposedthecriteria (cid:181) (cid:182) 2K Shibata=Ï2 1+ (cid:98) n asanestimatoroftheMSFE.RecallingtheMallowscriterionforregression(28.15)weseethatShibata= C /nifwereplacethepreliminaryestimatorÏ2withÏ2.Thusthetwoarequitesimilarinpractice. p (cid:101) (cid:98)",
    "page": 890,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 871 Takinglogarithmsandusingtheapproximationlog(1+x)(cid:39)xforsmallx (cid:181) (cid:182) nlog(Shibata)=nlog (cid:161)Ï2(cid:162)+nlog 1+ 2K (cid:39)nlog (cid:161)Ï2(cid:162)+2K =AIC. (cid:98) (cid:98) n ThusminimizationofShibataâscriterionandAICaresimilar. Akaike(1969)proposedtheFinalPredictionErrorCriteria (cid:181) 1+K/n (cid:182) FPE=Ï2 . (cid:98) 1âK/n Usingtheexpansions(1âx) â1(cid:39)1+xand(1+x)2(cid:39)1+2xweseethatFPE(cid:39)Shibata. CravenandWahba(1979)proposedGeneralizedCrossValidation nÏ2 GCV= (cid:98) . (nâK)2 Bytheexpansion(1âx) â2(cid:39)1+2xwefindthat Ï2 (cid:181) 2K (cid:182) nGCV= (cid:98) (cid:39)Ï2 1+ =Shibata. (1âK/n)2 (cid:98) n TheabovecalculationsshowthattheWMSE,AIC,Shibata, FPE,GCV,andMallowscriterionareall closeapproximationstooneanotherwhenK/n issmall. DifferencesariseinfinitesamplesforlargeK. However,theaboveanalysisshowsthatthereisnofundamentaldifferencebetweenthesecriteria. They are all estimating the same target. This is in contrast to BIC which uses a different parameterization penaltyandisasymptoticallydistinct. InterestinglytherealsoisaconnectionbetweenCVandtheabovecriteria.Againusingtheexpansion (1âx) â2(cid:39)1+2xwefindthat n CV= (cid:88) (1âh ) â2e2 ii (cid:98)i i=1 n n (cid:39) (cid:88) e2+ (cid:88) 2h e2 (cid:98)i ii(cid:98)i i=1 i=1 n =nÏ2+2 (cid:88) X (cid:48)(cid:161) X (cid:48) X (cid:162)â1 X e2 (cid:98) i i(cid:98)i i=1 (cid:195) (cid:195) (cid:33)(cid:33) n =nÏ2+2tr (cid:161) X (cid:48) X (cid:162)â1 (cid:88) X X (cid:48) e2 (cid:98) i i(cid:98)i i=1 (cid:39)nÏ2+2tr (cid:179) (cid:161)(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:162)â1(cid:161)(cid:69)(cid:163) XX (cid:48) e2(cid:164)(cid:162) (cid:180) (cid:98) =nÏ2+2KÏ2 (cid:98) (cid:39)Shibata. Thethird-to-lastlineholdsasymptoticallybytheWLLN.Thefollowingequalityholdsunderconditional homoskedasiticity.ThefinalapproximationreplacesÏ2bytheestimatorÏ2.Thiscalculationshowsthat (cid:98) undertheassumptionofconditionalhomoskedasticitytheCVcriterionissimilartotheothercriteria.It differsunderheteroskedasticity,however,whichisoneofitsprimaryadvantages.",
    "page": 891,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 872 28.11 RelationwithLikelihoodRatioTesting SincetheAICandBICarepenalizedlog-likelihoods,AICandBICselectionarerelatedtolikelihood ratiotesting. SupposewehavetwonestedmodelsM 1 andM 2 withlog-likelihoods(cid:96) 1n (cid:161)Î¸ (cid:98)1 (cid:162) and(cid:96) 2n (cid:161)Î¸ (cid:98)2 (cid:162) andK <K estimatedparameters.AICselectsM ifAIC(K )<AIC(K )whichoccurswhen 1 2 1 1 2 â2(cid:96) 1n (cid:161)Î¸ (cid:98)1 (cid:162)+2K 1 <â2(cid:96) 2n (cid:161)Î¸ (cid:98)2 (cid:162) )+2K 2 or LR=2 (cid:161)(cid:96) 2n (cid:161)Î¸ (cid:98)2 (cid:162)â(cid:96) 1n (cid:161)Î¸ (cid:98)1 (cid:162)(cid:162)<2r wherer =K âK . ThusAICselectionissimilartoselectionbylikelihoodratiotestingwithadifferent 2 1 criticalvalue.Ratherthanusingacriticalvaluefromthechi-squaredistributiontheâcriticalvalueâis2r. ThisisnottosaythatAICselectionistesting(itisnot). Butratherthatthereisasimilarstructureinthe decision. There are two useful practical implications. One is that when test statistics are reported in their F form(whichdividebythedifferenceincoefficientsr)thentheAICâcriticalvalueâis2. TheAICselects therestricted(smaller)modelifF <2.Itselectstheunrestricted(larger)modelifF >2. Anotherusefulimplicationisinthecaseofconsideringasinglecoefficient(whenr =1). AICselects thecoefficient(thelargermodel)ifLR>2. Incontrasta5%significancetestâselectsâthelargermodel (rejects the smaller) if LR>3.84. Thus AIC is more generous in terms of selecting larger models. An equivalent way of seeing this is that AIC selects the coefficient if the t-ratio exceeds 1.41 while the 5% significancetestselectsifthet-ratioexceeds1.96. SimilarcommentsapplytoBICselectionthoughtheeffectivecriticalvaluesaredifferent. Forcom- paringmodelswithcoefficientsK <K theBICselectsM ifLR<log(n)r. TheâcriticalvalueâforanF 1 2 1 statisticislog(n).HenceBICselectionbecomesstricterassamplesizesincrease. 28.12 ConsistentSelection An important property of a model selection procedure is whether it selects a true model in large samples.Wecallsuchaprocedureconsistent. Todiscussthisfurtherweneedtothoughtfullydefinewhatisaâtrueâmodel. Theanswerdepends onthetypeofmodel. Whenamodelisaparametricdensityordistribution f(y,Î¸)withÎ¸âÎ(asinlikelihoodestimation) thenthemodelistrueifthereissomeÎ¸ âÎsuchthat f(y,Î¸ )equalsthetruedensityordistribution. 0 0 Noticethatitisimportantinthiscontextboththatthefunctionclass f(y,Î¸)andparameterspaceÎare appropriatelydefined. In a semiparametric conditional moment condition model which states (cid:69)(cid:163) g(Y,X,Î¸)|X (cid:164)=0 with Î¸âÎ then the model is true if there is some Î¸ âÎ such that (cid:69)(cid:163) g(Y,X,Î¸ )|X (cid:164)=0. This includes the 0 0 regressionmodelY =m(X,Î¸)+ewith(cid:69)[e|X]=0wherethemodelistrueifthereissomeÎ¸ âÎsuchthat 0 m(X,Î¸ )=(cid:69)[Y |X]. Italsoincludesthehomoskedasticregressionmodelwhichaddstherequirement 0 that(cid:69)(cid:163) e2|X (cid:164)=Ï2isaconstant",
    "page": 892,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". In a semiparametric conditional moment condition model which states (cid:69)(cid:163) g(Y,X,Î¸)|X (cid:164)=0 with Î¸âÎ then the model is true if there is some Î¸ âÎ such that (cid:69)(cid:163) g(Y,X,Î¸ )|X (cid:164)=0. This includes the 0 0 regressionmodelY =m(X,Î¸)+ewith(cid:69)[e|X]=0wherethemodelistrueifthereissomeÎ¸ âÎsuchthat 0 m(X,Î¸ )=(cid:69)[Y |X]. Italsoincludesthehomoskedasticregressionmodelwhichaddstherequirement 0 that(cid:69)(cid:163) e2|X (cid:164)=Ï2isaconstant. In a semiparametric unconditional moment condition model (cid:69)(cid:163) g(Y,X,Î¸) (cid:164) = 0 then the model is trueifthereissomeÎ¸ âÎsuchthat(cid:69)(cid:163) g(Y,X,Î¸ ) (cid:164)=0.Asubtleissuehereisthatwhenthemodelisjust 0 0 identifiedandÎisunrestrictedthenthisconditiontypicallyholdsandsothemodelistypicallytrue.This includesleastsquaresregressioninterpretedasaprojectionandjust-identifiedinstrumentalvariables regression. InanonparametricmodelsuchasY â¼ f âF whereF issomefunctionclass(suchassecond-order differentiabledensities)thenthemodelistrueifthetruedensityisamemberofthefunctionclassF.",
    "page": 892,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 873 Acomplicationarisesthattheremaybemultipletruemodels. Thiscannotoccurwhenmodelsare strictlynon-nested(meaningthatthereisnocommonelementinbothmodelclasses)butstrictlynon- nestedmodelsarerare. Mostmodelshavenon-trivialintersections. Forexample,thelinearregression models Y =Î±+X (cid:48)Î² +e and Y =Î±+X (cid:48)Î² +e with X and X containing no common elements may 1 1 2 2 1 2 appearnon-nestedbuttheyintersectwhenÎ² =0andÎ² =0. Asanotherexampleconsiderthelinear 1 2 modelY =Î±+X (cid:48)Î²+eandlog-linearmodellog(Y)=Î±+X (cid:48)Î²+e.Ifweaddtheassumptionthateâ¼N(0,Ï2) thenthemodelsarenon-intersecting. Butifwerelaxnormalityandinsteadusetheconditionalmean assumption(cid:69)[e|X]=0thenthemodelsareintersectingwhenÎ² =0andÎ² =0. 1 2 Themostcommontypeofintersectingmodelsarenested. Inregressionthisoccurswhenthetwo modelsareY =X (cid:48)Î² +e andY =X (cid:48)Î² +X (cid:48)Î² +e. IfÎ² (cid:54)=0thenonlythesecondmodelistrue. Butif 1 1 1 1 2 2 2 Î² =0thenbotharetruemodels. 2 â Ingeneral,givenasetofmodelsM ={M ,...,M }asubsetM aretruemodels(asdescribedabove) 1 M whiletheremainderarenottruemodels. A model selection rule M (cid:99)selects one model from the set M. We say a method is consistent if it asymptoticallyselectsatruemodel. Definition28.1 A model selection rule is model selection consistent if (cid:104) â(cid:105) (cid:80) M (cid:99) âM â1asnââ. This states that the model selection rule selects a true model with probability tending to 1 as the samplesizediverges. Abroadclassofmodelselectionmethodssatisfythisdefinitionofconsistency. Toseethisconsider theclassofinformationcriteria IC=â2(cid:96) n (cid:161)Î¸ (cid:98) (cid:162)+c(n,K). ThisincludesAIC(c =2K),BIC(c =Klog(n)),andtesting-basedselection(c equalsafixedquantileof theÏ2 distribution). K Theorem28.6 Understandardregularityconditionsformaximumlikelihood estimation,selectionbasedonICismodelselectionconsistentifc(n,K)=o(n) asnââ. TheproofisgiveninSection28.32. ThisresultcoversAIC,BICandtesting-basedselection.Thusallaremodelselectionconsistent. A major limitation with this result is that the definition of model selection consistency is weak. A model may be true but over-parameterized. To understand the distinction consider the models Y = X (cid:48)Î² +e and Y = X (cid:48)Î² +X (cid:48)Î² +e. If Î² =0 then both M and M are true, but M is the preferred 1 1 1 1 2 2 2 1 2 1 modelasitismoreparsimonious. Whentwonestedmodelsarebothtruemodelsitisconventionalto thinkofthemoreparsimoniousmodelasthecorrectmodel.Inthiscontextwedonotdescribethelarger modelasanincorrectmodelbutratherasover-parameterized. Ifaselectionruleasymptoticallyselects anover-parameterizedmodelwesaythatitâover-selectsâ.",
    "page": 893,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 874 Definition28.2 Amodelselectionruleasymptoticallyover-selectsifthereare (cid:104) (cid:105) modelsM 1 âM 2 suchthatliminf nââ (cid:80) M (cid:99) =M 2 |M 1 >0. Thedefinitionstatesthatover-selectionoccurswhentwomodelsarenestedandthesmaller(short) modelistrue(sobothmodelsaretruemodelsbutthesmallermodelismoreparsimonious)ifthelarger modelisasymptoticallyselectedwithpositiveprobability. Theorem28.7 Understandardregularityconditionsformaximumlikelihood estimation,selectionbasedonICasymptoticallyover-selectsifc(n,K)=O(1) asnââ. TheproofisgiveninSection28.32. This result includes both AIC and testing-based selection. Thus these procedures over-select. For example,ifthemodelsareY =X (cid:48)Î² +eandY =X (cid:48)Î² +X (cid:48)Î² +eandÎ² =0holds,thentheseprocedures 1 1 1 1 2 2 2 selecttheover-parameterizedregressionwithpositiveprobability. Following this line of reasoning, it is useful to draw a distinction between true and parsimonious models. WedefinethesetofparsimoniousmodelsM0âM â asthesetoftruemodelswiththefewest numberofparameters. WhenthemodelsinM â arenestedthenM0 willbeasingleton. Intheregres- sionexamplewithÎ² =0thenM istheuniqueparsimoniousmodelamong{M ,M }. Weintroducea 2 1 1 2 strongerconsistencydefinitionforprocedureswhichasymptoticallyselectparsimoniousmodels. Definition28.3 Amodelselectionruleisconsistentforparsimoniousmodels if(cid:80) (cid:104) M (cid:99) âM0(cid:105) â1asnââ. Ofthemethodswehavereviewed,onlyBICselectionisconsistentforparsimoniousmodels,aswe nowshow. Theorem28.8 Understandardregularityconditionsformaximumlikelihood estimation,selectionbasedonICisconsistentforparsimoniousmodelsiffor allK >K 2 1 c(n,K )âc(n,K )ââ (28.16) 2 1 asnââ,yetc(n,K)=o(n)asnââ. TheproofisgiveninSection28.32. TheconditionincludesBICsincec(n,K )âc(n,K )=(K âK )log(n)ââifK >K . 2 1 2 1 2 1 Some economists have interpreted Theorem 28.8 as indicating that BIC selection is preferred over theothermethods. Thisisanincorrectdeduction. Inthenextsectionweshowthattheotherselection proceduresareasymptoticallyoptimalintermsofmodelfitandintermsofout-of-sampleforecasting. Thusconsistentmodelselectionisonlyoneofseveraldesirablestatisticalproperties.",
    "page": 894,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 875 28.13 AsymptoticSelectionOptimality RegressorselectionbytheAIC/Shibata/Mallows/CVclassturnsouttobeasymptoticallyoptimalwith respect to out-of-sample prediction under quite broad conditions. This may appear to conflict with the results of the previous section but it does not as there is a critical difference between the goals of consistentmodelselectionandaccurateprediction. OuranalysiswillbeinthehomoskedasticregressionmodelconditioningontheregressormatrixX. Wewritetheregressionmodelas Y =m+e â (cid:88) m= X Î² j j j=1 (cid:69)[e|X]=0 (cid:69)(cid:163) e2|X (cid:164)=Ï2 whereX =(X ,X ,...).WecanalsowritetheregressionequationinmatrixnotationasY =m+e. 1 2 TheKthregressionmodelusesthefirstK regressorsX =(X ,X ,...,X ).Theleastsquaresestimates K 1 2 K inmatrixnotationare Y =X K Î² (cid:98)K + (cid:98) e K . AsinSection28.6definethefittedvaluesm (cid:98) =X K Î² (cid:98)K andregressionfit(sumofexpectedsquaredpredic- tionerrors)as R (K)=(cid:69)(cid:163) (mâm) (cid:48) (mâm)|X (cid:164) (28.17) n (cid:98) (cid:98) thoughnowweindexR bysamplesizenandmodelK forprecision. InanysamplethereisanoptimalmodelK whichminimizesR (K): n K opt=argminR (K). n n K opt ModelK obtainstheminimizedvalueofR (K) n n R opt=R (K opt )=minR (K). n n n n K NowconsidermodelselectionusingtheMallowâscriterionforregressionmodels C (K)=e (cid:48) e +2Ï2K p (cid:98)K(cid:98)K whereweexplicitlyindexbyK,andforsimplicityweassumetheerrorvarianceÏ2isknown.(Theresults areunchangedifitisreplacedbyaconsistentestimator.)Lettheselectedmodelbe K(cid:98)n =argminC p (K). K Prediction accuracy using the Mallows-selected model is R n (K(cid:98)n ). We say that a selection procedure is asymptoticallyoptimalifthepredictionaccuracyisasymptoticallyequivalentwiththeinfeasibleopti- mum.Thiscanbewrittenas R n (K(cid:98)n ) ââ1. (28.18) R opt p n opt Weconsiderconvergencein(28.18)intermsoftheriskratiosinceR divergesasthesamplesizein- n creases. Li(1987)establishedtheasymptoticoptimality(28.18). Hisresultdependsonthefollowingcondi- tions.",
    "page": 895,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 876 Assumption28.1 1. Theobservations(Y ,X ),i =1,...,n,areindependentandidenticallydis- i i tributed. 2. (cid:69)[e|X]=0. 3. (cid:69)(cid:163) e2|X (cid:164)=Ï2. 4. (cid:69)(cid:163)|e|4r |X (cid:164)â¤B<âforsomer >1. 5. R optââasnââ. n 6. Theestimatedmodelsarenested. Assumptions28.1.2and28.1.3statethatthetruemodelisaconditionallyhomoskedasticregression. Assumption28.1.4isatechnicalcondition,thataconditionalmomentoftheerrorisuniformlybounded. Assumption 28.1.5 is subtle. It effectively states that there is no correctly specified finite-dimensional model. To see this, suppose that there is a K such that the model is correctly specified, meaning that m =(cid:80)K X Î² . In this case we can show that R (K)=0, violating Assumption 28.1.5. Assumption i j=1 ji j n 28.1.6isatechnicalconditionthatrestrictsthenumberofestimatedmodels.Non-nestedmodelscanbe allowedbutthenanalternativerestrictiononthenumberofestimatedmodelsisneeded. Theorem28.9 Assumption 28.1 implies (28.18). Thus Mallows selection is asymptoticallyequivalenttousingtheinfeasibleoptimalmodel. TheproofisgiveninSection28.32. Theorem28.9statesthatMallowsselectioninaconditionalhomoskedasticregressionisasymptot- ically optimal. The key assumptions are homoskedasticity and that all finite-dimensional models are misspecified(incomplete), meaningthattherearealwaysomittedvariables. Thelattermeansthatre- gardless of the sample size there is always a trade-off between omitted variables bias and estimation variance. ThetheoremasstatedisspecificforMallowsselectionbutextendstoAIC,Shibata,GCV,FPE, andCVwithsomeadditionaltechnicalconsiderations.Theprimarymessageisthattheselectionmeth- odsdiscussedintheprevioussectionasymptoticallyselectasequenceofmodelswhicharebest-fitting inthesenseofminimizingthepredictionerror. Using a similar argument Andrews (1991c) showed that selection by cross-validation satisfies the sameasymptoticoptimalityconditionwithoutrequiringconditionalhomoskedasticity. Thetreatment is a bit more technical so we do not review it here. This indicates an important advantage for cross- validationselectionovertheothermethods. 28.14 FocusedInformationCriterion Claeskens and Hjort (2003) introduced the Focused Information Criterion (FIC) as an estimator oftheMSEofascalarparameter. Thecriterionisappropriateincorrectly-specifiedlikelihoodmodels",
    "page": 896,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 877 when one of the estimated models nests the other models. Let f(y,Î¸) be a parametric model density withaKÃ1parameterÎ¸. The class of models (sub-models) allowed are those defined by a set of differentiable restrictions r(Î¸)=0.LetÎ¸ (cid:101)betherestrictedMLEwhichmaximizesthelikelihoodsubjecttor(Î¸)=0. AkeyfeatureoftheFICisthatitfocusesonareal-valuedparameterÂµ=g(Î¸)where g issomedif- ferentiablefunction. ClaeskensandHjortcallÂµthetargetparameter. ThechoiceofÂµismadebythe researcher and is a critical choice. In most applications Âµ is the key coefficient in the application (for example, the returns to schooling in a wage regression). The unrestricted MLE for Âµ is Âµ (cid:98) = g(Î¸ (cid:98)), the restrictedMLEisÂµ (cid:101) =g(Î¸ (cid:101)). EstimationaccuracyismeasuredbytheMSEoftheestimatorofthetargetparameter,whichisthe squaredbiasplusthevariance: (cid:104) (cid:105) mse (cid:163)Âµ(cid:164)=(cid:69) (cid:161)ÂµâÂµ(cid:162)2 =(cid:161)(cid:69)(cid:163)Âµ(cid:164)âÂµ(cid:162)2+var (cid:163)Âµ(cid:164) . (cid:101) (cid:101) (cid:101) (cid:101) ItturnsouttobeconvenienttonormalizetheMSEbythatoftheunrestrictedestimator. Wedefinethis astheFocus F=mse (cid:163)Âµ(cid:164)âmse (cid:163)Âµ(cid:164) . (cid:101) (cid:98) TheClaeskens-HjortFICisanestimatorofF.Specifically, FIC=(cid:161)Âµ (cid:101) âÂµ (cid:98) (cid:162)2â2G(cid:98) (cid:48) V(cid:98)Î¸(cid:98) R(cid:98) (cid:179) R(cid:98) (cid:48) V(cid:98)Î¸(cid:98) R(cid:98) (cid:180)â1 R(cid:98) (cid:48) V(cid:98)Î¸(cid:98) G(cid:98) whereV(cid:98)Î¸(cid:98) ,G(cid:98) andR(cid:98) areestimatorsofvar (cid:163)Î¸ (cid:98) (cid:164) ,G= â â Î¸(cid:48) g(Î¸)andR= â â Î¸(cid:48) r(Î¸). In a least squares regression Y = XÎ²+e with a linear restriction R (cid:48)Î²=0 and linear parameter of interestÂµ=G (cid:48)Î²theFICequals FIC= (cid:181) G (cid:48) R (cid:179) R (cid:48)(cid:161) X (cid:48) X (cid:162)â1 R (cid:180)â1 R (cid:48)(cid:161) X (cid:48) X (cid:162)â1Î² (cid:98) (cid:182)2 â2Ï2G (cid:48)(cid:161) X (cid:48) X (cid:162)â1 R (cid:179) R (cid:48)(cid:161) X (cid:48) X (cid:162)â1 R (cid:180)â1 R (cid:48)(cid:161) X (cid:48) X (cid:162)â1 G. (cid:98) TheFICisusedsimilarlytoAIC.TheFICiscalculatedforeachsub-modelofinterestandthemodel withthelowestvalueofFICisselected. TheadvantageoftheFICisthatitisspecificallytargetedtominimizetheMSEofthetargetparameter. TheFICisthereforeappropriatewhenthegoalistoestimateaspecifictargetparameter.Adisadvantage isthatitdoesnotnecessarilyproduceamodelwithgoodestimatesoftheotherparameters.Forexample, inalinearregressionY =X Î² +X Î² +e,if X and X areuncorrelatedandthefocusparameterisÎ² 1 1 2 2 1 2 1 thentheFICwilltendtoselectthesub-modelwithout X , andthustheselectedmodelwillproducea 2 highlybiasedestimateofÎ² . ConsequentlywhenusingtheFICitisdubiousifattentionshouldbepaid 2 toestimatesotherthanthoseofÂµ. ComputationallyitmaybeconvenienttoimplementtheFICusinganalternativeformulation",
    "page": 897,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". ConsequentlywhenusingtheFICitisdubiousifattentionshouldbepaid 2 toestimatesotherthanthoseofÂµ. ComputationallyitmaybeconvenienttoimplementtheFICusinganalternativeformulation. De- finetheadjustedfocus F â=n (cid:161) F+2mse (cid:163)Âµ(cid:164)(cid:162)=n (cid:161) mse (cid:163)Âµ(cid:164)+mse (cid:163)Âµ(cid:164)(cid:162) . (cid:98) (cid:101) (cid:98) Thisaddsthesamequantitytoallmodelsandthereforedoesnotaltertheminimizingmodel. Multipli- cationbynputstheFICinunitswhichareeasierforreporting. Theestimateoftheadjustedfocusisan adjustedFICandcanbewrittenas FIC â=n (cid:161)Âµ (cid:101) âÂµ (cid:98) (cid:162)2+2nV(cid:98)Âµ (cid:101) (28.19) =n (cid:161)ÂµâÂµ(cid:162)2+2ns (cid:161)Âµ(cid:162)2 (28.20) (cid:101) (cid:98) (cid:101)",
    "page": 897,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 878 where (cid:48) (cid:181) (cid:179) (cid:48) (cid:180)â1 (cid:48) (cid:182) V(cid:98)Âµ (cid:101) =G(cid:98) I k âV(cid:98)Î¸(cid:98) R(cid:98) R(cid:98) V(cid:98)Î¸(cid:98) R(cid:98) R(cid:98) V(cid:98)Î¸(cid:98) G(cid:98) isanestimatorofvar (cid:163)Âµ (cid:101) (cid:164) ands (cid:161)Âµ (cid:101) (cid:162)=V(cid:98) 1 Âµ /2 isastandarderrorforÂµ (cid:101) . â (cid:101) ThismeansthatFIC canbeeasilycalculatedusingconventionalsoftwarewithoutadditionalpro- gramming.TheestimatorÂµcanbecalculatedfromthefullmodel(thelongregression)andtheestimator (cid:98) Âµanditsstandarderrors (cid:161)Âµ(cid:162) fromtherestrictedmodel(theshortregression). Theformula(28.20)can (cid:101) (cid:101) â thenbeappliedtoobtainFIC . â Theformula(28.19)alsoprovidesanintuitiveunderstandingoftheFIC.WhenweminimizeFIC we areminimizingthevarianceoftheestimatorofthetargetparameter(V(cid:98)Âµ)whilenotalteringtheestimate (cid:101) ÂµtoomuchfromtheunrestrictedestimateÂµ. (cid:101) (cid:98) Whenselectingfromamongstjusttwomodels,theFICselectstherestrictedmodelif (cid:161)Âµ (cid:101) âÂµ (cid:98) (cid:162)2+2V(cid:98)Âµ (cid:101) < 0whichisthesameas (cid:161)Âµ (cid:101) âÂµ (cid:98) (cid:162)2 /V(cid:98)Âµ (cid:101) <2. Thestatistictotheleftoftheinequalityisthesquaredt-statistic intherestrictedmodelfortestingthehypothesisthatÂµequalstheunrestrictedestimatorÂµbutignoring (cid:98) theestimationerrorinthelatter. Thusasimpleimplementation(whenjustcomparingtwomodels)is toestimatethelongandshortregressions,takethedifferenceinthetwoestimatesofthecoefficientof interest, and compute a t-ratio using the standard error from the short (restricted) regression. If this t-ratioexceeds1.4theFICselectsthelongregressionestimate. Ifthet-ratioissmallerthan1.4theFIC selectstheshortregressionestimate. ClaeskensandHjortmotivatetheFICusingalocalmisspecificationasymptoticframework.Weusea simplerheuristicmotivation. FirsttaketheunrestrictedMLE.UnderstandardconditionsÂµhasasymp- (cid:98) toticvarianceG (cid:48) VÎ¸G whereVÎ¸ =Iâ1.Astheestimatorisasymptoticallyunbiaseditfollowsthat mse (cid:163)Âµ (cid:98) (cid:164)(cid:39)var (cid:163)Âµ (cid:98) (cid:164)(cid:39)n â1G (cid:48) VÎ¸G. SecondtaketherestrictedMLE.UnderstandardconditionsÂµhasasymptoticvariance (cid:101) G (cid:48) (cid:179) VÎ¸ âVÎ¸R (cid:161) R (cid:48) VÎ¸R (cid:162)â1 RVÎ¸ (cid:180) G. Âµalsohasaprobabilitylimit,sayÂµ ,which(generally)differsfromÂµ.Togetherwefindthat (cid:101) R mse (cid:163)Âµ (cid:101) (cid:164)(cid:39)B+n â1G (cid:48) (cid:179) VÎ¸ âVÎ¸R (cid:161) R (cid:48) VÎ¸R (cid:162)â1 RVÎ¸ (cid:180) G whereB=(cid:161)ÂµâÂµ (cid:162)2 .Subtracting,wefindthattheFocusis R F(cid:39)Bân â1G (cid:48) VÎ¸R (cid:161) R (cid:48) VÎ¸R (cid:162)â1 RVÎ¸G",
    "page": 898,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Âµalsohasaprobabilitylimit,sayÂµ ,which(generally)differsfromÂµ.Togetherwefindthat (cid:101) R mse (cid:163)Âµ (cid:101) (cid:164)(cid:39)B+n â1G (cid:48) (cid:179) VÎ¸ âVÎ¸R (cid:161) R (cid:48) VÎ¸R (cid:162)â1 RVÎ¸ (cid:180) G whereB=(cid:161)ÂµâÂµ (cid:162)2 .Subtracting,wefindthattheFocusis R F(cid:39)Bân â1G (cid:48) VÎ¸R (cid:161) R (cid:48) VÎ¸R (cid:162)â1 RVÎ¸G. Theplug-inestimatorB(cid:98) =(cid:161)Âµ (cid:98) âÂµ (cid:101) (cid:162)2 ofB isbiasedsince (cid:69)(cid:163) B(cid:98) (cid:164)=(cid:161)(cid:69)(cid:163)Âµ (cid:98) âÂµ (cid:101) (cid:164)(cid:162)2+var (cid:163)Âµ (cid:98) âÂµ (cid:101) (cid:164) (cid:39)B+var (cid:163)Âµ(cid:164)âvar (cid:163)Âµ(cid:164) (cid:98) (cid:101) (cid:39)B+n â1G (cid:48) VÎ¸R (cid:161) R (cid:48) VÎ¸R (cid:162)â1 RVÎ¸G. ItfollowsthatanapproximatelyunbiasedestimatorforFis B(cid:98) â2n â1G (cid:48) VÎ¸R (cid:161) R (cid:48) VÎ¸R (cid:162)â1 RVÎ¸G. TheFICisobtainedbyreplacingtheunknownG,R,andn â1VÎ¸ byestimates.",
    "page": 898,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 879 28.15 BestSubsetandStepwiseRegression Supposethatwehaveasetofpotentialregressors{X ,...,X }andwewanttoselectasubsetofthe 1 K regressors to use in aregression. LetS denote asubsetofthe regressors, andletm =1,...,M denote m thesetofpotentialsubsets. Givenamodelselectioncriterion(e.g. AIC,Mallows,orCV)thebestsubset modelistheonewhichminimizesthecriterionacrosstheM models.Thisisimplementedbyestimating theM modelsandcomparingthemodelselectioncriteria. IfK issmallitiscomputationallyfeasibletocompareallsubsetmodelsbutitisnotfeasiblewhenK islarge. ThisisbecausethenumberofpotentialsubsetsisM =2K whichincreasesquicklywithK. For example, K =10implies M =1024, K =20implies M â¥1,000,000, andK =40implies M exceedsone trillion.Itsimplydoesnotmakesensetocontemplateestimatingallsubsetregressions! If the goal is to find the set of regressors which produces the smallest selection criterion it seems likelythatweshouldbeabletofindanapproximatingsetofregressorsatmuchreducedcomputation cost. Somespecificalgorithmstoimplementthisgoalareascalledstepwise,stagewise,andleastangle regression.Noneoftheseproceduresarebelievedtoactuallyachievethegoalofminimizinganyspecific selectioncriterion;rathertheyareviewedasusefulcomputationalapproximations. Thereisalsosome potentialconfusionasdifferentauthorsseemtousethesametermsforsomewhatdifferentimplemen- tations.WeusethetermshereasdescribedinHastie,Tibshirani,andFriedman(2008). In the following descriptions we use SSE(m) to refer to the sum of squared residuals from a fitted model and C(m) to refer to the selection criterion used for model comparison (AIC is most typically used). BackwardStepwiseRegression 1. Startwithallregressors{X ,...,X }includedintheâactivesetâ. 1 K 2. Form=0,...,Kâ1 (a) EstimatetheregressionofY ontheactiveset. (b) IdentifytheregressorwhoseomissionwillhavethesmallestimpactonC(m). (c) PutthisregressorinslotKâmanddeletefromtheactiveset. (d) CalculateC(m)andstoreinslotKâm. 3. ThemodelwiththesmallestvalueofC(m)istheselectedmodel. BackwarestepwiseregressionrequiresK <n sothatregressionwithallvariablesisfeasible. Itpro- duces an ordering of the regressors from âmost relevantâ to âleast relevantâ. A simplified version is to exittheloopwhenC(m)increases. (Thismaynotyieldthesameresultascompletingtheloop.) Forthe caseofAICselection, step(b)canbeimplementedbycalculatingtheclassical(homoskedastic)t-ratio foreachactiveregressorandfindtheregressorwiththesmallestabsolutet-ratio.(SeeExercise28.3.) ForwardStepwiseRegression â 1. Startwiththenullset{ }astheâactivesetâandallregressors{X ,...,X }astheâinactivesetâ. 1 K 2. Form=1,...,min(nâ1,K) (a) EstimatetheregressionofY ontheactiveset. (b) IdentifytheregressorintheinactivesetwhoseinclusionwillhavethelargestimpactonC(m).",
    "page": 899,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 880 (c) Putthisregressorinslotmandmoveitfromtheinactivetotheactiveset. (d) CalculateC(m)andstoreinslotm. 3. ThemodelwiththesmallestvalueofC(m)istheselectedmodel. AsimplifiedversionistoexittheloopwhenC(m)increases.(Thismaynotyieldthesameansweras completingtheloop.)ForthecaseofAICselectionstep(b)canbeimplementedbyfindingtheregressor intheinactivesetwiththelargestabsolutecorrelationwiththeresidualfromstep(a).(SeeExercise28.4.) Therearecombinedalgorithmswhichcheckbothforwardandbackwardmovementsateachstep. Thealgorithmscanalsobeimplementedwiththeregressorsorganizedingroups(sothatallelementsare eitherincludedorexcludedateachstep). Therearealsoold-fashionedversionswhichusesignificance testingratherthanselectioncriterion(thisisgenerallynotadvised). Stepwiseregressionbasedonold-fashionedsignificancetestingcanbeimplementedinStatausing thestepwisecommand.Ifattentionisconfinedtomodelswhichincluderegressorsone-at-a-time,AIC selection can be implemented by setting the significance level equal to p =0.32. Thus the command stepwise, pr(.32)implementsbackwardstepwiseregressionwiththeAICcriterion,andstepwise, pe(.32)implementsforwardstepwiseregressionwiththeAICcriterion. StepwiseregressioncanbeimplementedinRusingthelarscommand. 28.16 TheMSEofModelSelectionEstimators Modelselectioncanleadtoestimatorswithpoorsamplingperformance.Inthissectionweshowthat themeansquarederrorofestimationisnotnecessarilyimproved, andcanbeconsiderablyworsened, bymodelselection. Tokeepthingssimpleconsideranestimatorwithanexactnormaldistributionandknowncovariance matrix.Normalizingthelattertotheidentityweconsiderthesetting Î¸ (cid:98) â¼N(Î¸,I K ) andtheclassofmodelselectionestimators (cid:189) Î¸ (cid:98) if Î¸ (cid:98) (cid:48)Î¸ (cid:98) >c Î¸ = (cid:98)pms 0 if Î¸ (cid:98) (cid:48)Î¸ (cid:98) â¤c for some c. AIC sets c =2K, BICsets c =Klog(n), and 5% significance testing sets c to equal the 95% quantileoftheÏ2 K distribution.ItiscommontocallÎ¸ (cid:98)pms apost-model-selection(PMS)estimator WecanexplicitlycalculatetheMSEofÎ¸ (cid:98)pms . Theorem28.10 IfÎ¸ (cid:98) â¼N(Î¸,I K )then mse (cid:163)Î¸ (cid:98)pms (cid:164)=K+(2Î»âK)F K+2 (c,Î»)âÎ»F K+4 (c,Î») where F (x,Î») is the non-central chi-square distribution function with r de- r greesoffreedomandnon-centralityparameterÎ»=Î¸(cid:48)Î¸. TheproofisgiveninSection28.32.",
    "page": 900,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 881 TheMSEisdeterminedonlybyK,Î»,andc. Î»=Î¸(cid:48)Î¸turnsouttobeanimportantparameterforthe MSE.AsthesquaredEuclideanlength,itindexesthemagnitudeofthecoefficientÎ¸. Wecanseethefollowinglimitingcases. IfÎ»=0thenmse (cid:163)Î¸ (cid:98)pms (cid:164)=K(1âF K+2 (c,0)). AsÎ»ââthen mse (cid:163)Î¸ (cid:98)pms (cid:164)âK. Theunrestrictedestimatorobtainsifc =0, inwhichcasemse (cid:163)Î¸ (cid:98)pms (cid:164)=K. Asc ââ, mse (cid:163)Î¸ (cid:98)pms (cid:164)âÎ».ThelatterfactimpliesthatthePMSestimatorbasedontheBIChasMSEââasnââ. UsingTheorem28.10wecannumerically(cid:112)calculatetheMSE.InFigure28.2(a)and(b)weplottheMSE ofasetofestimatorsforarangeofvaluesof Î». Panel(a)isforK =1,panel(b)isforK =5. Thedash- dottedlinemarkstheMSEoftheunselectedestimatorÎ¸ (cid:98)whichisinvarianttoÎ». Theotherestimators plottedareAICselection(c =2K),5%significancetestingselection(chi-squarecriticalvalue),andBIC selection(c=Klog(n))forn=200andn=1000. 4 3 2 1 0 AIC 5% Sig Test BIC, n=200 BIC, n=1000 0 1 2 3 4 5 l (a)MSE,K =1 02 51 01 5 0 AIC 5% Sig Test BIC, n=200 BIC, n=1000 0 1 2 3 4 5 6 7 8 0.0 0.2 0.4 0.6 0.8 1.0 1.2 l (b)MSE,K =5 0.1 9.0 8.0 7.0 6.0 b 2 1 b rof ytilibaborP egarevoC r=0.0 r=0.3 r=0.5 r=0.7 r=0.8 (c)CoverageProbability Figure28.2:MSEandCoverageofPost-Model-SelectionEstimators In the plots you can see that the PMS estimators have lower MSE than the unselected estimator roughlyforÎ»<K buthigherMSEforÎ»>K. TheAICestimatorhasMSEwhichisleastdistortedfrom theunselectedestimator,reachingapeakofabout1.5forK =1.TheBICestimators,however,havevery largeMSEforlargervaluesofÎ»,andthedistortionisgrowingasn increases. TheMSEoftheselection estimatorsincreaseswithÎ»untilitreachesapeakandthenslowlydecreasesandasymptotesbacktoK. Furthermore, theMSEofBICisunboundedasn diverges.ThusforverylargesamplesizestheMSEof aBIC-selectedestimatorcanbeaverylargemultipleoftheMSEoftheunselectedestimator. Theplots showthatifÎ»issmallthereareadvantagestomodelselectionasMSEcanbegreatlyreduced.Howeverif Î»islargethenMSEcanbegreatlyincreasedifBICisused,andmoderatelyincreasedifAICisused.Asen- siblereadingoftheplotsleadstothepracticalrecommendationtonotusetheBICformodelselection, andusetheAICwithcare. The numerical calculations show that MSE is reduced by selection when Î» is small but increased whenÎ»ismoderatelylarge. Whatdoesthismeaninpractice? Î»issmallwhenÎ¸ issmallwhichmeans the compared models are similar in terms of estimation accuracy. In these contexts model selection canbevaluableasithelpsselectsmallermodelstoimproveprecision. HoweverwhenÎ»ismoderately large(whichmeansthatÎ¸ismoderatelylarge)thesmallermodelhasmeaningfulomittedvariablebias, yet the selection criteria have difficulty detecting which model to use",
    "page": 901,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Whatdoesthismeaninpractice? Î»issmallwhenÎ¸ issmallwhichmeans the compared models are similar in terms of estimation accuracy. In these contexts model selection canbevaluableasithelpsselectsmallermodelstoimproveprecision. HoweverwhenÎ»ismoderately large(whichmeansthatÎ¸ismoderatelylarge)thesmallermodelhasmeaningfulomittedvariablebias, yet the selection criteria have difficulty detecting which model to use. The conservative BIC selection proceduretendstoselectthesmallermodelandthusincursgreaterbiasresultinginhighMSE.These considerationssuggestthatitisbettertousetheAICwhenselectingamongmodelswithsimilarestima- tionprecision.Unfortunatelyitisimpossibletoknownaprioritheappropriatemodels. TheresultsofthissectionmayappeartocontradictTheorem28.8whichshowedthattheBICiscon-",
    "page": 901,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 882 sistentforparsimoniousmodelsasforallÎ»>0intheplotsthecorrectparsimoniousmodelisthelarger model. YetBICisnotselectingthismodelwithsufficientfrequencytoproducealowMSE.Thereisno contradiction. The consistency of the BIC appears in the lower portion of the plots where the MSE of theBICestimatoristheapproximatelystraightlineMSE=Î».ThisistheMSEoftherestrictedestimator. Thus for small Î» the BIC properly selects the true model. The fact that the MSE of the AIC estimator somewhatexceedsthatoftheBICinthisregionisillustratingtheover-selectionpropertyoftheAIC. 28.17 InferenceAfterModelSelection Economistsaretypicallyinterestedininferentialquestionssuchashypothesistestsandconfidence intervals. If an econometric model has been selected by a procedure such as AIC or CV what are the propertiesofstatisticaltestsappliedtotheselectedmodel? TobeconcreteconsidertheregressionmodelY =X Î² +X Î² +e andselectionofthevariable X . 1 1 2 2 2 That is, we compare Y = X Î² +e with Y = X Î² +X Î² +e. It is not too deep a realization that in 1 1 1 1 2 2 this context it is inappropriate to conduct conventional inference for Î² in the selected model. If we 2 selectthesmallermodelthereisnoestimateofÎ² . Ifweselectthelargeritisbecausethet-ratioforÎ² 2 2 exceedsthecriticalvalue.Thedistributionofthet-ratio,conditionalonexceedingacriticalvalue,isnot conventionallydistributedandthereseemslittlepointtopushthisissuefurther. ThemoreinterestingandsubtlequestionistheimpactoninferenceconcerningÎ² . Thisindeedis 1 acontextoftypicalinterest. Aneconomistisinterestedintheimpactof X onY givenasetofcontrols 1 X . Itiscommontoselectacrossthesecontrolstofindasuitableempiricalmodel. Oncethishasbeen 2 obtained we want to make inferential statements about Î² . Has selection over the controls impacted 1 inference? Weillustratetheissuenumerically. Supposethat(X ,X )arejointlynormalwithunitvariancesand 1 2 correlationÏ,e isindependentandstandardnormal,andn=30. WeestimatethelongregressionofY on(X ,X )andtheshortregressionofY onX alone. Weconstructthet-statistic3forÎ² =0inthelong 1 2 1 2 regressionandselectthelongregressionifthet-statisticissignificantatthe5%levelandselecttheshort regression if the t-statistic is not significant. We construct the standard 95% confidence interval4 for Î² intheselectedregression. Theseconfidenceintervalswillhaveexact95%coveragewhenthereisno 1 selectionandtheestimatedmodeliscorrect,sodeviationsfrom95%areduetomodelselectionandmis- specification. Wecalculatetheactualcoverageprobabilitybysimulationusingonemillionreplications, varying5Î² andÏ. 2 WedisplayinFigure28.2(c)thecoverageprobabilitiesasafunctionofÎ² forseveralvaluesofÏ. If 2 theregressorsareuncorrelated(Ï=0)thentheactualcoverageprobabilityequalsthenominallevelof 0.95. Thisisbecausethet-statisticforÎ² isindependentofthoseforÎ² inthisnormalregressionmodel 2 1 andthecoefficientsonX intheshortandlongregressionareidentical. 1 This invariance breaks down for Ï (cid:54)=0",
    "page": 902,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Wecalculatetheactualcoverageprobabilitybysimulationusingonemillionreplications, varying5Î² andÏ. 2 WedisplayinFigure28.2(c)thecoverageprobabilitiesasafunctionofÎ² forseveralvaluesofÏ. If 2 theregressorsareuncorrelated(Ï=0)thentheactualcoverageprobabilityequalsthenominallevelof 0.95. Thisisbecausethet-statisticforÎ² isindependentofthoseforÎ² inthisnormalregressionmodel 2 1 andthecoefficientsonX intheshortandlongregressionareidentical. 1 This invariance breaks down for Ï (cid:54)=0. As Ï increases the coverage probability of the confidence intervalsfallbelowthenominallevel. ThedistortionisstronglyaffectedbythevalueofÎ² . ForÎ² =0 2 2 thedistortionismild. ThereasonisthatwhenÎ² =0theselectiont-statisticselectstheshortregression 2 withhighprobability(95%)whichleadstoapproximatelyvalidinference. Also,asÎ² ââthecoverage 2 probabilityconvergestothenominallevel. ThereasonisthatforlargeÎ² theselectiont-statisticselects 2 thelongregressionwithhighprobability,againleadingtoapproximatelyvalidinference. Thedistortion is large, however, for intermediate values of Î² . For Ï =0.5 the coverage probability falls to 88%, and 2 3Usingthehomoskedasticvarianceformulaandassumingtheerrorvarianceisknown.Thisisdonetofocusontheselection issueratherthancovariancematrixestimation. 4Usingthehomoskedasticvarianceformulaandassumingthecorrecterrorvarianceisknown. 5ThecoverageprobabilityisinvarianttoÎ² 1.",
    "page": 902,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 883 forÏ=0.8theprobabilityislowas62%. ThereasonisthatforintermediatevaluesofÎ² theselection 2 t-statisticselectsbothmodelswithmeaningfulprobability,andthisselectiondecisioniscorrelatedwith thet-statisticsforÎ² .Thedegreeofunder-coverageisenormousandgreatlytroubling. 1 Themessagefromthisdisplayisthatinferenceaftermodelselectionisproblematic. Conventional inference procedures do not have conventional distributions and the distortions are potentially un- bounded. 28.18 EmpiricalIllustration Weillustratethe modelselectionmethodswithanapplication. Takethe CPS datasetandthesub- sampleofAsianwomenwhichhasn=1149observations. Consideralogwageregressionwithprimary interest on the return to experience measured as the percentage difference between expected wages between 0 and 30 years of experience. We consider and compare nine least squares regressions. All include an indicator for married and three indicators for the region. The estimated models range in complexityconcerningtheimpactofeducationandexperience. Table28.1:EstimatesofReturntoExperienceamongAsianWomen Model1 Model2 Model3 Model4 Model5 Model6 Model7 Model8 Model9 Return 13% 22% 20% 29% 40% 37% 33% 47% 45% s.e. 7 8 7 11 11 11 17 18 17 BIC 956 907 924 964 913 931 977 925 943 AIC 915 861 858 914 858 855 916 860 857 CV 405 387 386 405 385 385 406 387 386 FIC 86 48 53 58 32 34 86 71 68 Education College Spline Dummy College Spline Dummy College Spline Dummy Experience 2 2 2 4 4 4 6 6 6 Termsforexperience: â¢ Models1-3includeincludeexperienceanditssquare. â¢ Models4-6includepowersofexperienceuptopower4. â¢ Models7-9includepowersofexperienceuptopower6. Termsforeducation: â¢ Models1,4,and7includeasingledummyvariablecollegeindicatingthatyearsofeducationis16 orhigher. â¢ Models2,5,and8includealinearsplineineducationwithasingleknotateducation=9. â¢ Models3,6,and9includesixdummyvariables,foreducationequalling12,13,14,16,18,and20. Table28.1reportskeyestimatesfromtheninemodels.Reportedaretheestimateofthereturntoex- â perienceasapercentagewagedifference,itsstandarderror(HC1),theBIC,AIC,CV,andFIC ,thelatter",
    "page": 903,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 884 treatingthereturntoexperienceasthefocus. Whatwecanseeisthattheestimatesvarymeaningfully, rangingfrom13%to47%. Someoftheestimatesalsohavemoderatelylargestandarderrors. (Inmost modelsthereturntoexperienceisâstatisticallysignificantâ, butbylargestandarderrorswemeanthat itisdifficulttopindowntheprecisevalueofthereturntoexperience.) Wecanalsoseethatthemost importantfactorsimpactingthemagnitudeofthepointestimateisgoingbeyondthequadraticspecifi- cationforexperience,andgoingbeyondthesimplestspecificationforeducation.Anotheritemtonotice isthatthestandarderrorsaremostaffectedbythenumberofexperienceterms. TheBICpicksaparsimoniousmodelwiththelinearsplineineducationandaquadraticinexperi- ence.TheAICandCVselectalessparsimoniousmodelwiththefulldummyspecificationforeducation anda4th orderpolynomialinexperience.TheFICselectsanintermediatemodel,withalinearsplinein educationanda4th orderpolynomialinexperience. Whenselectingamodelusinginformationcriteriaitisusefultoexamineseveralcriteria.Inapplica- tionsdecisionsshouldbemadebyacombinationofjudgmentaswellastheformalcriteria.Inthiscase thecross-validationcriterionselectsmodel6whichhastheestimateof37%,butnear-similarvaluesof theCVcriterionareobtainedbymodels3and9whichhavetheestimates20%and45%.TheFIC,which focusesonthisspecificcoefficient,selectsmodel5whichhasthepointestimate40%whichissimilarto theCV-selectedmodel. OverallbasedonthisevidencetheCV-selectedmodelanditspointestimateof 37%seemsanappropriatechoice. However,theuncertaintyreflectedbytheflatnessoftheCVcriterion suggeststhatuncertaintyremainsinthechoiceofspecification. 28.19 ShrinkageMethods ShrinkagemethodsareabroadclassofestimatorswhichreducevariancebymovinganestimatorÎ¸ (cid:98) towardsapre-selectedpointsuchasthezerovector.Inhighdimensionsthereductioninvariancemore than compensates for the increase in bias resulting in improved efficiency when measured by mean squarederror.ThisandthenextfewsectionsreviewmaterialpresentedinChapter15ofIntroductionto Econometrics. ThesimplestshrinkageestimatortakestheformÎ¸ (cid:101) =(1âw)Î¸ (cid:98)forsomeshrinkageweight w â[0,1]. Setting w =0 we obtain Î¸ (cid:101) =Î¸ (cid:98)(no shrinkage) and setting w =1 we obtain Î¸ (cid:101) =0 (full shrinkage). It is straightforwardtocalculatetheMSEofthisestimator.AssumeÎ¸ (cid:98) â¼(Î¸,V).ThenÎ¸ (cid:101)hasbias bias (cid:163)Î¸ (cid:101) (cid:164)=(cid:69)(cid:163)Î¸ (cid:101) (cid:164)âÎ¸=âwÎ¸, (28.21) variance var (cid:163)Î¸ (cid:101) (cid:164)=(1âw)2V, (28.22) andweightedmeansquarederror(usingtheweightmatrixW =V â1) wmse (cid:163)Î¸ (cid:101) (cid:164)=K(1âw)2+w2Î» (28.23) whereÎ»=Î¸(cid:48) V â1Î¸. Theorem28.11 IfÎ¸ (cid:98) â¼(Î¸,V)andÎ¸ (cid:101) =(1âw)Î¸ (cid:98)then 1. wmse (cid:163)Î¸ (cid:101) (cid:164)<wmse (cid:163)Î¸ (cid:98) (cid:164) if0<w<2K/(K+Î»). 2. wmse (cid:163)Î¸ (cid:101) (cid:164) isminimizedbytheshrinkageweightw 0 =K/(K+Î»). 3. TheminimizedWMSEiswmse (cid:163)Î¸ (cid:101) (cid:164)=KÎ»/(K+Î»).",
    "page": 904,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 885 FortheproofseeExercise28.6. Part 1 of the theorem shows that the shrinkage estimator has reduced WMSE for a range of values of the shrinkage weight w. Part 2 of the theorem shows that the WMSE-minimizing shrinkage weight isasimplefunctionofK andÎ». ThelatterisameasureofthemagnitudeofÎ¸relativetotheestimation variance.WhenÎ»islarge(thecoefficientsarelarge)thentheoptimalshrinkageweightw issmall;when 0 Î»issmall(thecoefficientsaresmall)thentheoptimalshrinkageweightw islarge.Part3calculatesthe 0 associatedoptimalWMSE.ThiscanbesubstantiallylessthantheWMSEoftheoriginalestimatorÎ¸ (cid:98).For example,ifÎ»=K thenwmse (cid:163)Î¸ (cid:101) (cid:164)=K/2,one-halftheWMSEoftheoriginalestimator. To construct the optimal shrinkage weight we need the unknown Î». An unbiased estimator is Î» (cid:98) = Î¸ (cid:98) (cid:48) V â1Î¸ (cid:98) âK (seeExercise28.7)implyingtheshrinkageweight K w= . (28.24) (cid:98) Î¸ (cid:98) (cid:48) V â1Î¸ (cid:98) ReplacingK withafreeparameterc (whichwecalltheshrinkagecoefficient)weobtain (cid:181) (cid:182) c Î¸ (cid:101) = 1â Î¸ (cid:98). (28.25) Î¸ (cid:98) (cid:48) V â1Î¸ (cid:98) ThisclassofestimatorsisoftencalledaStein-Ruleestimator. Thisestimatorhasmanyappealingproperties. Itcanbeviewedasasmoothedselectionestimator. The quantity Î¸ (cid:98) (cid:48) V â1Î¸ (cid:98)is a Wald statistic for the hypothesis (cid:72) 0 :Î¸ =0. Thus when this Wald statistic is large(whentheevidencesuggeststhehypothesisofazerocoefficientisfalse)theshrinkageestimator is close to the original estimator Î¸ (cid:98). However when this Wald statistic is small (when the evidence is consistent with the hypothesis of a zero coefficient) then the shrinkage estimator moves the original estimatortowardszero. 28.20 James-SteinShrinkageEstimator JamesandStein(1961)madethefollowingdiscovery. Theorem28.12 AssumethatÎ¸ (cid:98) â¼N(Î¸,V),Î¸ (cid:101)isdefinedin(28.25),andK >2. 1. If0<c<2(Kâ2)thenwmse (cid:163)Î¸ (cid:101) (cid:164)<wmse (cid:163)Î¸ (cid:98) (cid:164) . 2. TheWMSEisminimizedbysettingc=Kâ2andequals wmse (cid:163)Î¸ (cid:101) (cid:164)=Kâ(Kâ2)2(cid:69)(cid:163) Q â1(cid:164) K whereQ â¼Ï2(Î»). K K SeeTheorem15.3ofIntroductiontoEconometrics. This result stunned the world of statistics. Part 1 shows that the shrinkage estimator has strictly smaller WMSE for all values of the parameters and thus dominates the original estimator. The latter is the MLE so this result shows that the MLE is dominated and thus inadmissible. This is a stunning resultbecauseithadpreviouslybeenassumedthatitwouldbeimpossibletofindanestimatorwhich dominatestheMLE.",
    "page": 905,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 886 Theorem28.12criticallydependsontheconditionK >2. Thismeansthatshrinkageachievesuni- formimprovementsonlyindimensionsthreeorlarger. Theminimizingchoicefortheshrinkagecoefficientc =K â2leadstowhatiscommonlyknownas theJames-Steinestimator (cid:181) Kâ2 (cid:182) Î¸ (cid:101) = 1â Î¸ (cid:98). Î¸ (cid:98) (cid:48) V â1Î¸ (cid:98) InpracticeV isunknownsowesubstituteanestimatorV(cid:98).Thisleadsto (cid:181) Kâ2 (cid:182) Î¸ (cid:101)JS = 1â Î¸ (cid:98) (cid:48) V(cid:98) â1Î¸ (cid:98) Î¸ (cid:98) whichisfullyfeasibleasitdoesnotdependonunknownsortuningparameters. ThesubstitutionofV(cid:98) forV canbejustifiedbyfinitesampleorasymptoticarguments. 28.21 InterpretationoftheSteinEffect TheJames-SteinTheoremappearstoconflictwithclassicalstatisticaltheory. Theoriginalestimator Î¸ (cid:98)isthemaximumlikelihoodestimator. Itisunbiased. Itisminimumvarianceunbiased. ItisCramer- Raoefficient. HowcanitbethattheJames-Steinshrinkageestimatorachievesuniformlysmallermean squarederror? Part of the answer is that classical theory has caveats. The Cramer-Rao Theorem, for example, re- strictsattentiontounbiasedestimatorsandthusprecludesconsiderationofshrinkageestimators. The James-SteinestimatorhasreducedMSE,butisnotCramer-Raoefficientsinceitisbiased.Thereforethe James-SteinTheoremdoesnotconflictwiththeCramer-RaoTheorem.Rather,theyarecomplementary results.Ontheonehand,theCramer-RaoTheoremdescribesthebestpossiblevariancewhenunbiased- nessisanimportantpropertyforestimation. Ontheotherhand,theJames-SteinTheoremshowsthat ifunbiasednessisnotacriticalpropertybutinsteadMSEisimportant,thentherearebetterestimators thantheMLE. The James-Stein Theorem may also appear to conflict with our results from Section 28.16 which showed that selection estimators do not achieve uniform MSE improvements over the MLE. This may appeartobeaconflictsincetheJames-Steinestimatorhasasimilarformtoaselectionestimator. The differenceisthatselectionestimatorsarehardthresholdrulesâtheyarediscontinuousfunctionsofthe dataâwhiletheJames-Steinestimatorisasoftthresholdruleâitisacontinuousfunctionofthedata. Hardthresholdingtendstoresultinhighvariance;softthresholdingtendstoresultinlowvariance. The James-Steinestimatorisabletoachievereducedvariancebecauseitisasoftthresholdfunction. The MSE improvements achieved by the James-Stein estimator are greatest when Î» is small. This occurswhentheparametersÎ¸aresmallinmagnituderelativetotheestimationvarianceV.Thismeans thattheuserneedstochoosethecenteringpointwisely. 28.22 PositivePartEstimator ThesimpleJames-Steinestimatorhastheoddpropertythatitcanâover-shrinkâ.WhenÎ¸ (cid:98) (cid:48) V â1Î¸ (cid:98) <Kâ2 thenÎ¸ (cid:101)hasoppositesignwithÎ¸ (cid:98). Thisdoesnotmakesenseandsuggeststhatfurtherimprovementscan be made. The standard solution is to use âpositive-partâ trimming by bounding the shrinkage weight",
    "page": 906,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 887 (28.24)belowone.Thisestimatorcanbewrittenas Î¸+= (cid:189) Î¸ (cid:101), Î¸ (cid:98) (cid:48) V â1Î¸ (cid:98) â¥Kâ2 (cid:101) 0, Î¸ (cid:98) (cid:48) V â1Î¸ (cid:98) <Kâ2 (cid:181) Kâ2 (cid:182) = 1â Î¸ (cid:98) Î¸ (cid:98) (cid:48) V â1Î¸ (cid:98) + where(a)+ =max[a,0]istheâpositive-partâfunction.Alternatively,itcanbewrittenas Î¸+=Î¸â (cid:181) Kâ2 (cid:182) Î¸ (cid:101) (cid:98) (cid:98) Î¸ (cid:98) (cid:48) V â1Î¸ (cid:98) 1 where(a) =min[a,1] 1 Thepositivepartestimatorsimultaneouslyperformsâselectionâaswellasâshrinkageâ. IfÎ¸ (cid:98) (cid:48) V â1Î¸ (cid:98)is sufficiently small, Î¸ (cid:101) + âselectsâ 0. When Î¸ (cid:98) (cid:48) V â1Î¸ (cid:98)is of moderate size, Î¸ (cid:101) + shrinks Î¸ (cid:98)towards zero. When Î¸ (cid:98) (cid:48) V â1Î¸ (cid:98)isverylarge,Î¸ (cid:101) + isclosetotheoriginalestimatorÎ¸ (cid:98). ConsistentwithourintuitionthepositivepartestimatorhasuniformlylowerWMSEthantheunad- justedJames-Steinestimator. Theorem28.13 UndertheassumptionsofTheorem28.12 wmse (cid:163)Î¸ (cid:101) +(cid:164)<wmse (cid:163)Î¸ (cid:101) (cid:164) . (28.26) ForaproofseeTheorem15.6ofIntroductiontoEconometrics.Theorem15.7ofIntroductiontoEcono- metricsprovidesanexplicitnumericalevaluationoftheMSEforthepositive-partestimator. InFigure28.3(a)weplotwmse (cid:163)Î¸ (cid:101) +(cid:164) /K asafunctionofÎ»/K forK =4,6,12,and48.Theplotsareuni- formlybelow1(thenormalizedWMSEoftheMLE)andsubstantiallysoforsmallandmoderatevaluesof Î».TheWMSEfunctionsfallasK increases,demonstratingthattheMSEreductionsaremoresubstantial whenK islarge. 0 1 2 3 4 5 0.1 8.0 6.0 4.0 2.0 0.0 l K K/ESM K= 4 K= 6 K= 12 K= 48 (a)WMSEofJames-Stein 1.3 0.3 9.2 8.2 7.2 6.2 5.2 Experience )egaw(gol Fixed Effects JamesâStein Model 3 Model 9 JamesâStein 0 10 20 30 40 50 â0.05 0.00 0.05 0.10 0.15 0.20 Individual Firm Effect (b)ExperienceProfile (c)FirmEffects Figure28.3:James-SteinEstimator In summary, the positive-part transformation is an important improvement over the unadjusted James-Stein estimator. It is more reasonable and reduces the mean squared error. The broader mes- sageisthatimposingboundaryconditionscanimproveestimationefficiency.",
    "page": 907,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 888 28.23 ShrinkageTowardsRestrictions TheclassicalJames-Steinestimatordoesnothavedirectuseinapplicationsbecauseitisrarethatwe wishtoshrinkanentireparametervectortowardsaspecificpoint.Rather,itismorecommontowishto shrinkaparametervectortowardsasetofrestrictions.Hereareafewexamples: 1. Shrinkalongregressiontowardsashortregression. 2. Shrinkaregressiontowardsanintercept-onlymodel. 3. Shrinktheregressioncoefficientstowardsasetofrestrictions. 4. Shrinkasetofestimates(orcoefficients)towardstheircommonmean. 5. Shrinkasetofestimates(orcoefficients)towardsaparametricmodel. 6. Shrinkanonparametricseriesmodeltowardsaparametricmodel. Thewaytothinkgenerallyabouttheseapplicationsitthattheresearcherwantstoallowforgenerality withthelargemodelbutbelievesthatthesmallermodelmaybeausefulapproximation. Ashrinkage estimator allows the data to smoothly select between these two options depending on the strength of informationforthetwospecifications. LetÎ¸ (cid:98) â¼N(Î¸,V)betheoriginalestimator, forexampleasetofregressioncoefficientestimates. The normalityassumptionisusedfortheexacttheorybutcanbejustifiedbasedonanasymtoticapproxi- mationaswell.Theresearcherconsidersasetofq>2linearrestrictionswhichcanbewrittenasR (cid:48)Î¸=r whereR isKÃq andr isqÃ1.AminimumdistanceestimatorforÎ¸is Î¸ (cid:98)R =Î¸ (cid:98) âVR (cid:161) R (cid:48) VR (cid:162)â1(cid:161) R (cid:48)Î¸ (cid:98) âr (cid:162) . TheJames-Steinestimatorwithpositive-parttrimmingis (cid:195) (cid:33) Î¸ (cid:101) +=Î¸ (cid:98) â (cid:161)Î¸ (cid:98) âÎ¸ (cid:98)R (cid:162)(cid:48) q V â â 2 1(cid:161)Î¸ (cid:98) âÎ¸ (cid:98)R (cid:162) 1 (cid:161)Î¸ (cid:98) âÎ¸ (cid:98)R (cid:162) . Thefunction(a) =min[a,1]boundstheshrinkageweightbelowone. 1 Theorem28.14 UndertheassumptionsofTheorem28.12,ifq>2then wmse (cid:163)Î¸ (cid:101) +(cid:164)<wmse (cid:163)Î¸ (cid:101) (cid:164) . The shrinkage estimator achieves uniformly smaller MSE if the number of restrictions is three or greater.Thenumberofrestrictionsq playsthesameroleasthenumberofparametersK intheclassical James-Steinestimator.Shrinkageachievesgreatergainswhentherearemorerestrictionsq,andachieves greatergainswhentherestrictionsareclosetobeingsatisfiedinthepopulation. Iftheimposedrestric- tions are far from satisfied then the shrinkage estimator will have similar performance as the original estimator.Itisthereforeimportanttoselecttherestrictionscarefully. InpracticethecovariancematrixV isunknownsoitisreplacedbyanestimatorV(cid:98).Thusthefeasible versionoftheestimatorsequal Î¸ (cid:98)R =Î¸ (cid:98) âV(cid:98)R (cid:161) R (cid:48) V(cid:98)R (cid:162)â1(cid:161) R (cid:48)Î¸ (cid:98) âr (cid:162)",
    "page": 908,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 889 and Î¸ (cid:101) +=Î¸ (cid:98) â (cid:181) qâ2 (cid:182) (cid:161)Î¸ (cid:98) âÎ¸ (cid:98)R (cid:162) (28.27) J 1 where J=(cid:161)Î¸ (cid:98) âÎ¸ (cid:98)R (cid:162)(cid:48) V(cid:98) â1(cid:161)Î¸ (cid:98) âÎ¸ (cid:98)R (cid:162) . It is insightful to notice that J is the minimum distance statistic for the test of the hypothesis (cid:72) : 0 R (cid:48)Î¸=r against(cid:72) :R (cid:48)Î¸(cid:54)=r. Thusthedegreeofshrinkageisasmoothedversionofthestandardtestof 1 therestrictions. When J islarge(sotheevidenceindicatesthattherestrictionsarefalse)theshrinkage estimatorisclosetotheunrestrictedestimatorÎ¸ (cid:98). WhenJ issmall(sotheevidenceindicatesthatthere- strictionscouldbecorrect)theshrinkageestimatorequalstherestrictedestimatorÎ¸ (cid:98)R .Forintermediate valuesofJ theshrinkageestimatorshrinksÎ¸ (cid:98)towardsÎ¸ (cid:98)R . WecansubstituteforJanysimilarasymptoticallychi-squarestatistic,includingtheWald,Likelihood Ratio,andScorestatistics.WecanalsousetheFstatistic(whichiscommonlyproducedbystatisticalsoft- ware)ifwemultiplybyq. Thesesubstitutionsdonotproducethesameexactfinitesampledistribution butareasymptoticallyequivalent. Inlinearregressionwehavesomeveryconvenientsimplificationsavailable. Ingeneral,V(cid:98) canbea heteroskedastic-robust or cluster-robust covariance matrix estimator. However, if the dimension K of the unrestricted estimator is quite large or has sparse dummy variables then these covariance matrix estimatorsareill-behavedanditmaybebettertouseaclassicalcovariancematrixestimatortoperform theshrinkage. IfthisisdonethenV(cid:98) =(cid:161) X (cid:48) X (cid:162)â1 s2,Î¸ (cid:98)R istheconstrainedleastsquaresestimator(inmost applicationstheleastsquaresestimatoroftheshortregression)andJ isaconventional(homoskedastic) Waldstatisticforatestoftherestrictions.WecanwritethelatterinFstatisticform n (cid:161)Ï2 âÏ2(cid:162) J= (cid:98)R (cid:98) (28.28) s2 where Ï2 and Ï2 are the least squares error variance estimators from the restricted and unrestricted (cid:98)R (cid:98) models.Theshrinkageweight((qâ2)/J) canbeeasilycalculatedfromstandardregressionoutput. 1 28.24 GroupJames-Stein TheJames-Steinestimatorcanbeappliedtogroups(blocks)ofparameters.Supposewehavethepa- rametervectorÎ¸=(Î¸ ,Î¸ ,...,Î¸ )partitionedintoGgroupseachofdimensionK â¥3.Wehaveastandard 1 2 G g estimatorÎ¸ (cid:98) =(cid:161)Î¸ (cid:98)1 ,Î¸ (cid:98)2 ,...,Î¸ (cid:98)G (cid:162) (forexample,leastsquaresregressionorMLE)withcovariancematrixV.The groupJames-Steinestimatoris Î¸ (cid:101) =(cid:161)Î¸ (cid:101)1 ,Î¸ (cid:101)2 ,...,Î¸ (cid:101)G (cid:162) (cid:195) (cid:33) K â2 Î¸ (cid:101)g =Î¸ (cid:98)g 1â Î¸ (cid:98) g (cid:48) V g â g 1Î¸ (cid:98)g + whereV g isthe gth diagonalblockofV. AfeasibleversionoftheestimatorreplacesV withV(cid:98) andV g withV(cid:98)g . ThegroupJames-Steinestimatorseparatelyshrinkseachblockofcoefficients. Theadvantagerela- tivetotheclassicalJames-Steinestimatoristhatthisallowstheshrinkageweighttovaryacrossblocks",
    "page": 909,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". AfeasibleversionoftheestimatorreplacesV withV(cid:98) andV g withV(cid:98)g . ThegroupJames-Steinestimatorseparatelyshrinkseachblockofcoefficients. Theadvantagerela- tivetotheclassicalJames-Steinestimatoristhatthisallowstheshrinkageweighttovaryacrossblocks. Someparameterblockscanusealargeamountofshrinkagewhileothersaminimalamount. Sincethe positive-parttrimmingisusedtheestimatorsimultaneouslyperformsshrinkageandselection. Blocks withsmalleffectswillbeshrunktozeroandeliminated. Thedisadvantageoftheestimatoristhatthe benefitsofshrinkagemaybereducedsincetheshrinkagedimensionisreduced. Thetrade-offbetween",
    "page": 909,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 890 thesefactorswilldependonhowheterogeneoustheoptimalshrinkageweightvariesacrosstheparam- eters. Thegroupsshouldbeselectedbasedontwocriteria.First,theyshouldbeselectedsothatthegroups separatevariablesbyexpectedamountofshrinkage. Thuscoefficientswhichareexpectedtobeâlargeâ relativetotheirestimationvarianceshouldbegroupedtogetherandcoefficientswhichareexpectedto beâsmallâshouldbegroupedtogether.Thiswillallowtheestimatedshrinkageweightstovaryaccording tothegroup.Forexample,aresearchermayexpecthigh-ordercoefficientsinapolynomialregressionto besmallrelativetotheirestimationvariance. Henceitisappropriatetogroupthepolynomialvariables into âlow orderâ and âhigh orderâ. Second, the groups should be selected so that the researcherâs loss (utility) is separable across groups of coefficients. This is because the optimality theory (given below) reliesontheassumptionthatthelossisseparable. Tounderstandtheimplicationsoftheserecommen- dationsconsiderawageregression. Ourinterpretationoftheeducationandexperiencecoefficientsare separableifweusethemforseparatepurposes,suchasforestimationofthereturntoeducationandthe returntoexperience. Inthiscaseitisappropriatetoseparatetheeducationandexperiencecoefficients intodifferentgroups. ForanoptimalitytheorywedefineweightedMSEwithrespecttotheblock-diagonalweightmatrix W =diag(V â1,...,V â1). 1 G Theorem28.15 UndertheassumptionsofTheorem28.12,ifWMSEisdefined withrespecttoW =diag(V â1,...,V â1)andK >2forallg =1,...,G then 1 G g wmse (cid:163)Î¸ (cid:101) (cid:164)<wmse (cid:163)Î¸ (cid:98) (cid:164) . TheproofisasimpleextensionoftheclassicalJames-Steintheory. Theblockdiagonalstructureof W meansthattheWMSEisthesumoftheWMSEofeachgroup. TheclassicalJames-Steintheorycan beappliedtoeachgroupfindingthattheWMSEisreducedbyshrinkagegroup-by-group.Thusthetotal WMSEisreducedbyshrinkage. 28.25 EmpiricalIllustrations WeillustrateJames-Steinshrinkagewiththreeempiricalapplications. ThefirstapplicationistothesampleusedinSection28.18, theCPSdatasetwiththesubsampleof Asianwomen(n=1149)focusingonthereturntoexperienceprofile. WeconsidershrinkageofModel9 (6th orderpolynomialinexperience)towardsModel3(2nd orderpolynomialinexperience). Thediffer- enceinthenumberofestimatedcoefficientsis4.WesetV(cid:98) toequaltheHC1covariancematrixestimator. Theshrinkageweightis0.46,meaningthattheSteinRuleestimatorisapproximatelyanequalweighted averageoftheestimatesfromthetwomodels.TheestimatedexperienceprofilesaredisplayedinFigure 28.3(b). The two least squares estimates are visually distinct. The 6th order polynomial (Model 9) shows a steep return to experience for the first 10 years, then a wobbly experience profile up to 40 years, and decliningabovethat. Italsoshowsadiparound25years. Thequadraticspecificationmissessomeof these features. The James-Stein estimator is essentially an average of the two profiles. It retains most featuresofthequarticspecification,exceptthatitsmoothsouttheunappealing25-yeardip",
    "page": 910,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". The two least squares estimates are visually distinct. The 6th order polynomial (Model 9) shows a steep return to experience for the first 10 years, then a wobbly experience profile up to 40 years, and decliningabovethat. Italsoshowsadiparound25years. Thequadraticspecificationmissessomeof these features. The James-Stein estimator is essentially an average of the two profiles. It retains most featuresofthequarticspecification,exceptthatitsmoothsouttheunappealing25-yeardip. The second application is to the Invest1993 data set used in Chapter 17. This is a panel data set ofannualobservationsoninvestmentdecisionsbycorporations. Wefocusonthefirm-specificeffects.",
    "page": 910,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 891 Theseareofinterestwhenstudyingfirmheterogeneityandisofparticularimportanceforfirm-specific forecasting. Accurateestimationoffirmeffectsischallengingwhenthenumberoftimeseriesobserva- tionsperfirmissmall. To keep the analysis focused we restrict attention to firms which are traded on either the NYSE or AMEXandtothelasttenyearsofthesample(1982-1991).Sincetheregressorsarelaggedthismeansthat thereareatmostninetime-seriesobservationsperfirm. Thesamplehasatotalof N =786firmsand n=5692observationsforestimation. Ourbaselinemodelisthetwo-wayfixedeffectslinearregression asreportedinthefourthcolumnofTable17.2. Ourrestrictedmodelreplacesthefirmfixedeffectswith 19 industry-specific dummy variables. This is similar to the first column of Table 17.2 except that the tradingdummyisomittedandtimedummiesareadded.TheSteinRuleestimatorthusshrinksthefixed effectsmodeltowardstheindustryeffectsmodel. Thelatterwilldowellifmostofthefixedeffectsare explainedbyindustryratherthanfirm-specificvariation. Duetothelargenumberofestimatedcoefficientsintheunrestrictedmodelweusethehomoskedas- ticweightmatrixasasimplification.Thisallowsthecalculationoftheshrinkageweightusingthesimple formula (28.28) for the statistic J. The heteroskedastic covariance matrix is not appropriate and the cluster-robustcovariancematrixwillnotbereliableduetothesparsedummyspecification. The estimated shrinkage weight is 0.35 which means that the Stein Rule estimator puts about 1/3 weightontheindustry-effectspecificationand2/3weightonthefirm-specificspecification. To report our results we focus on the distribution of the firm-specific effects. For the fixed effects model these are the estimated fixed effects. For the industry-effect model these are the estimated in- dustrydummycoefficients(foreachfirm). FortheSteinRuleestimatestheyareaweightedaverageof thetwo.Weestimate6thedensitiesoftheestimatedfirm-specificeffectsfromthefixed-effectsandStein Ruleestimators,andplottheminFigure28.3(c). You can see that the fixed-effects estimate of the firm-specific density is more dispersed while the Stein estimator is sharper and more peaked indicating that the fixed effects estimator attributes more variation in firm-specific factors than the Stein estimator. The Stein estimator pulls the fixed effects towardstheircommonmean,adjustingfortherandomnessduetotheirestimation. Ourexpectationis thattheSteinestimates,ifusedforanapplicationsuchasfirm-specificforecasting,willbemoreaccurate becausetheywillhavereducedvariancerelativetothefixedeffectsestimates. ThethirdapplicationusestheCPSdatasetwiththesubsampleofBlackmen(n=2413)focusingon thereturntoeducationacrossU.S.regions(Northeast,Midwest,South,West).Supposeyouareaskedto flexiblyestimatethereturntoeducationforBlackmenallowingforthereturntoeducationtovaryacross theregions.GiventhemodelselectioninformationfromSection28.18anaturalbaselineismodel6aug- mentedtoallowforgreatervariationacrossregions",
    "page": 911,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". ThethirdapplicationusestheCPSdatasetwiththesubsampleofBlackmen(n=2413)focusingon thereturntoeducationacrossU.S.regions(Northeast,Midwest,South,West).Supposeyouareaskedto flexiblyestimatethereturntoeducationforBlackmenallowingforthereturntoeducationtovaryacross theregions.GiventhemodelselectioninformationfromSection28.18anaturalbaselineismodel6aug- mentedtoallowforgreatervariationacrossregions. Aflexiblespecificationinteractsthesixeducation dummy variables with the four regional dummies (omitting the intercept), which adds 18 coefficients andallowsthereturntoeducationtovarywithoutrestrictionineachregion. TheleastsquaresestimateofthereturntoeducationbyregionisdisplayedinFigure28.4(a).Forsim- plicitywelabeltheomittededucationgroup(lessthan12yearseducation)asâ11yearsâ. Theestimates appear noisy due to the small samples. One feature which we can see is that the four lines track one anotherforyearsofeducationbetween12and18. Thatis,theyareroughlylinearinyearsofeducation withthesameslopebutdifferentintercepts. ToimprovetheprecisionoftheestimatesweshrinkthefourprofilestowardsModel6. Thismeans that we are shrinking the profiles not towards each other but towards the model with the same effect of education but regional-specific intercepts. Again we use the HC1 covariance matrix estimate. The number of restrictions is 18. The shrinkage weight is 0.49 which means that the Stein Rule estimator 6Thetwodensitiesareestimatedwithacommonbandwidthtoaidcomparison.Thebandwidthwasselectedtocompromise betweenthoseselectedforthetwosamples.",
    "page": 911,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 892 0.4 5.3 0.3 5.2 0.2 Education )egaw(gol Northeast Midwest South West 12 14 16 18 20 (a)LeastSquaresEstimates 0.4 5.3 0.3 5.2 0.2 Education )egaw(gol Northeast Midwest South West 12 14 16 18 20 (b)SteinRuleEstimates Figure28.4:SteinRuleEstimationofEducationProfilesAcrossRegions putsequalweightonthetwomodels. TheSteinRuleestimatesaredisplayedinFigure28.4(b). Theestimatesarelessnoisythanpanel(a) anditiseasiertoseethepatterns. Thefourlinestrackeachotherandareapproximatelylinearover12- 18. For20yearsofeducationthefourlinesdispersewhichseemslikelyduetosmallsamples. Inpanel (b) it is easier to see the patterns across regions. It appears that the northeast region has the highest wages(conditionaloneducation)whilethewestregionhasthelowestwages. Thisrankingisconstant fornearlyalllevelsofeducation. WhiletheSteinRuleestimatesshrinkthenonparametricestimatestowardsthecommon-education- factorspecificationitdoesnotimposethelatterspecification.TheSteinRuleestimatorhastheabilityto putnearzeroweightonthecommon-factormodel. Thefactthattheestimatesput1/2weightonboth modelsisthechoiceselectedbytheSteinRuleandisthusdata-driven. ThemessagefromthesethreeapplicationsisthattheJames-Steinshrinkageapproachcanbecon- structively used to reduce estimation variance in economic applications. These applications illustrate commonformsofpotentialapplications: Shrinkageofaflexiblespecificationtowardsasimplerspeci- fication;Shrinkageofheterogeneousestimatestowardshomogeneousestimates;Shrinkageoffixedef- fectstowardsgroupdummyestimates.Thesethreeapplicationsalsoemployedmoderatelylargesample sizes(n=1149,2413,and5692)yetfoundshrinkageweightsnear50%. Thisshowsthatthebenefitsof Steinshrinkagearenotconfinedtoâsmallâsamplesbutrathercanbeconstructiveusedinmoderately largesampleswithcomplicatedstructures. 28.26 ModelAveraging Recallthattheproblemofmodelselectionishowtoselectasinglemodelfromageneralsetofmod- els. The James-Stein shrinkage estimator smooths between two nested models by taking a weighted averageoftwoestimators. Moregenerallywecantakeanaverageofanarbitrarynumberofestimators. Theseestimatorsareknownasmodelaveragingestimators.Thekeyissueforestimationishowtoselect theaveragingweights.",
    "page": 912,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 893 SupposewehaveasetofMmodelsM ={M 1 ,...,M M }.ForeachmodelthereisanestimatorÎ¸ (cid:98)m ofthe parameterÎ¸.Thenaturalwaytothinkaboutmultiplemodels,parameters,andestimatorsisthesameas formodelselection.Allmodelsaresubsetsofageneralsuperset(overlapping)modelwhichcontainsall submodelsasspecialcases. Corresponding to the set of models we introduce a set of weights w ={w ,...,w }. It is common 1 M to restrict the weights to be non-negative and sum to one. The set of such weights is called the (cid:82)M probabilitysimplex. Definition28.4 Probability Simplex. The set S â (cid:82)M of vectors such that (cid:80)M w =1andw â¥1fori =1,...,M. m=1 m i Theprobabilitysimplexin(cid:82)2 and(cid:82)3 isshowninthetwopanelsofFigure28.5. Thesimplexin(cid:82)2 (theleftpanel)isthelinebetweenthevertices(1,0)and(0,1). Anexampleelementisthepoint(.7,.3) indicated by the dot. This is the weight vector which puts weight 0.7 on model 1 and weight 0.3 on model2.Thevertice(1,0)istheweightvectorwhichputsallweightonmodel1,correspondingtomodel selection,andsimilarlythevertice(0,1)istheweightvectorwhichputsallweightonmodel2. The simplex in (cid:82)3 (the right panel) is the equilateral triangle formed between (1,0,0), (0,1,0), and (0,0,1). An example element is the point (.1,.5,.4) indicated by the dot. The edges are weight vectors whichareaveragesbetweentwoofthethreemodels. Forexamplethebottomedgeareweightvectors whichdividetheweightbetweenmodels1and2,placingnoweightonmodel3.Theverticesareweight vectorswhichputallweightononeofthethreemodelsandcorrespondtomodelselection. w 2 wwww 3333 (0,1) (((((((0000000,,,,,,,0000000,,,,,,,1111111))))))) ww==((..11,,..55,,..44)) lll w=(.7,.3) ((((((((00000000,,,,,,,,11111111,,,,,,,,00000000)))))))) l wwwww 22222 wwwwww 111111 (((((((((111111111,,,,,,,,,000000000,,,,,,,,,000000000))))))))) w 1 (1,0) Figure28.5:ProbabilitySimplexin(cid:82)2and(cid:82)3 Since the weights on the probability simplex sum to one, an alternative representation is to elim- inate one weight by substitution. Thus we can set w = 1â(cid:80)Mâ1w and define the set of vectors M m=1 m w ={w 1 ,...,w Mâ1 } which lie in the (cid:82)Mâ1 unit simplex, which is the region bracketed by the probabil- itysimplexandtheorigin. Givenaweightvectorwedefinetheaveragingestimator M (cid:88) Î¸ (cid:98)(w)= w m Î¸ (cid:98)m . (28.29) m=1",
    "page": 913,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 894 Selectionestimatorsemergeasthespecialcasewheretheweightvectorw isaunitvector,e.g. thever- ticesinFigure28.5. It is not absolutely necessary to restrict the weight vector of an averaging estimator to lie in the probabilitysimplexS, butinmostcasesitisasensiblerestrictionwhichimprovesperformance. The unadjustedJames-Steinestimator,forexample,isanaveragingestimatorwhichdoesnotenforcenon- negativityoftheweights. Thepositive-partversion,however,imposesnon-negativityandachievesre- ducedMSEasaresult. In Section 28.19 and Theorem 28.11 we explored the MSE of a simple shrinkage estimator which shrinks an unrestricted estimator towards the zero vector. This is the same as a model averaging esti- mator where one of the two estimators is the zero vector. In Theorem 28.11 we showed that the MSE oftheoptimalshrinkage(modelaveraging)estimatorislessthantheunrestrictedestimator. Thisresult extends to the case of averaging between an arbitrary number of estimators. The MSE of the optimal averagingestimatorislessthantheMSEoftheestimatorofthefullmodelinanygivensample. Theoptimalaveragingweights,however,areunknown. Anumberofmethodshavebeenproposed forselectionoftheaveragingweights. One simple method is equal weighting. This is achieved by setting w =1/M and results in the m estimator Î¸ (cid:98) â= 1 (cid:88) M Î¸ (cid:98)m . M m=1 Theadvantagesofequalweightingarethatitissimple, easytomotivate, andnorandomnessisintro- duced by estimation of the weights. The variance of the equal weighting estimator can be calculated since the weights are fixed. Another important advantage is that the estimator can be constructed in contexts where it is unknown how to construct empirical-based weights, for example when averaging models from completely different probability families. The disadvantages of equal weighting are that themethodcanbesensitivetothesetofmodelsconsidered,thereisnoguaranteethattheestimatorwill performbetterthantheunrestrictedestimator,andsampleinformationisinefficientlyused.Inpractice, equalweightingisbestusedincontextswherethesetofmodelshavebeenpre-screenedsothatallare consideredâreasonableâmodels. Fromthestandpointofeconometricmethodologyequalweightingis notaproperstatisticalmethodasitisanincompletemethodology. Despitetheseconcernsequalweightingcanbeconstructivelyemployedwhensummarizinginfor- mation for a non-technical audience. The relevant context is when you have a small number of rea- sonable but distinct estimates typically made using different assumptions. The distinct estimates are presentedtoillustratetherangeofpossibleresultsandtheaveragetakentorepresenttheâconsensusâ orârecommendedâestimate. Asmentionedabove,anumberofmethodshavebeenproposedforselectionoftheaveragingweights. Inthefollowingsectionsweoutlinefourpopularmethods: SmoothedBIC,SmoothedAIC,Mallowsav- eraging,andJackknifeaveraging",
    "page": 914,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". The relevant context is when you have a small number of rea- sonable but distinct estimates typically made using different assumptions. The distinct estimates are presentedtoillustratetherangeofpossibleresultsandtheaveragetakentorepresenttheâconsensusâ orârecommendedâestimate. Asmentionedabove,anumberofmethodshavebeenproposedforselectionoftheaveragingweights. Inthefollowingsectionsweoutlinefourpopularmethods: SmoothedBIC,SmoothedAIC,Mallowsav- eraging,andJackknifeaveraging. 28.27 SmoothedBICandAIC RecallthatSchwarzâsTheorem28.1statesthatforaprobabilitymodel f(y,Î¸)andadiffusepriorthe marginallikelihoodp(Y)satisfies â2logp(Y)(cid:39)â2(cid:96) n (cid:161)Î¸ (cid:98) (cid:162)+Klog(n)=BIC. This has been been interpreted to mean that the model with the highest value of the right-hand-side approximatelyhasthehighestmarginallikelihoodandisthusthemodelwiththehighestprobabilityof beingthetruemodel.",
    "page": 914,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 895 ThereisanotherinterpretationofSchwarzâsresult.Themarginallikelihoodisapproximatelypropor- tionaltotheprobabilitythatthemodelistrue,conditionalonthedata. SchwarzâsTheoremimpliesthat thisisapproximately p(Y)(cid:39)exp(âBIC/2) which is a simple exponential transformation of the BIC. Weighting by posterior probability can be achievedbysettingmodelweightsproportionaltothistransformation.TheseareknownasBICweights andproducethesmoothedBICestimator. Todescribethemethodcompletely,wehaveasetofmodelsM ={M ,...,M }.Eachmodelf (y,Î¸ ) 1 M m m depends on a K Ã1 parameter vector Î¸ which is estimated by the maximum likelihood. The maxi- m m mizedlikelihoodisL m (Î¸ (cid:98)m )=f m (Y,Î¸ (cid:98)m ).TheBICformodelmisBIC m =â2logL m (Î¸ (cid:98)m )+K m log(n). TheBICweightsare exp(âBIC /2) w = m . m (cid:80)M exp (cid:161)âBIC /2 (cid:162) j=1 j SomepropertiesoftheBICweightsareasfollows. Theyarenon-negativesoallmodelsreceivepos- itiveweight. Somemodelscanreceiveweightarbitrarilyclosetozeroandinpracticemanyestimated modelsmayreceiveBICweightthatisessentiallyzero. ThemodelwhichisselectedbyBICreceivesthe greatestweightandmodelswhichhaveBICvaluesclosetotheminimumreceiveweightsclosesttothe largestweight.ModelswhoseBICisnotclosetotheminimumreceiveweightnearzero. TheSmoothedBIC(SBIC)estimatoris M (cid:88) Î¸ (cid:98)sbic = w m Î¸ (cid:98)m . m=1 TheSBICestimatorisasmootherfunctionofthedatathanBICselectionastherearenodiscontinuous jumpsacrossmodels. AnadvantageofthesmoothedBICweightsandestimatoristhatitcanbeusedtocombinemodels from different probability families. As for the BIC it is important that all models are estimated on the same sample. It is also important that the full formula is used for the BIC (no omission of constants) whencombiningmodelsfromdifferentprobabilityfamilies. ComputationallyitisbettertoimplementsmoothedBICwithwhatarecalledâBICdifferencesârather thantheactualvaluesoftheBIC,astheformulaaswrittencanproducenumericaloverflowproblems. Thedifficultyisduetotheexponentiationintheformula.Thisproblemcanbeeliminatedasfollows.Let BIC â= min BIC m 1â¤mâ¤M denotethelowestBICamongthemodelsanddefinetheBICdifferences âBIC =BIC âBIC â . m m Then exp(âBIC /2) w = m m (cid:80)M exp (cid:161)âBIC /2 (cid:162) j=1 j exp(âBIC /2)exp(BIC â /2) = m (cid:80)M exp (cid:161)âBIC /2 (cid:162) exp(BIC â /2) j=1 j exp(ââBIC /2) = m . (cid:80)M exp (cid:161)ââBIC /2 (cid:162) j=1 j",
    "page": 915,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 896 ThustheweightsarealgebraicallyidenticallywhethercomputedonBIC orâBIC .SinceâBIC areof m m m smallermagnitudethanBIC overflowproblemsarelesslikelytooccur. m Because of the properties of the exponential, if âBIC â¥10 then w â¤0.01. Thus smoothed BIC m m typicallyconcentratesweightonmodelswhoseBICvaluesareclosetotheminimum. Thismeansthat inpracticesmoothedBICputseffectivenon-zeroweightonasmallnumberofmodels. Burnham and Anderson (1998) follow a suggestion they credit to Akaike that if we make the same transformationtotheAICastotheBICtoobtainthesmoothedBICweightsweobtainfrequentistap- proximateprobabilitiesforthemodels.Specificallytheyproposetheweights exp(âAIC /2) w = m . m (cid:80)M exp (cid:161)âAIC /2 (cid:162) j=1 j They do not provide a strong theoretical justification for this specific choice of transformation but it seemsnaturalgiventhesmoothedBICformulaandworkswellinsimulations. ThealgebraicpropertiesoftheAICweightsaresimilartothoseoftheBICweights.Allmodelsreceive positiveweightthoughsomereceiveweightwhichisarbitrarilyclosetozero.Themodelwiththesmallest AICreceivesthegreatestAICweight,andmodelswithsimilarAICvaluesreceivesimilarAICweights. ComputationallytheAICweightsshouldbecomputedusingAICdifferences.Define AIC â= min AIC m 1â¤mâ¤M âAIC =AIC âAIC â . m m TheAICweightsalgebraicallyequal exp(ââAIC AIC /2) w = m m . m (cid:80)M exp (cid:161)ââAIC /2 (cid:162) j=1 j AsfortheBICweightsw â¤0.01ifâAIC â¥10sotheAICweightswillconcentratedonmodelswhose m m AIC values are close to the minimum. However, in practice it is common that the AIC criterion is less concentratedthantheBICcriterionastheAICputsasmallerpenaltyonlargepenalizations. TheAIC weightstendtobemorespreadoutacrossmodelsthanthecorrespondingBICweights. TheSmoothedAIC(SAIC)estimatoris M (cid:88) Î¸ (cid:98)saic = w m Î¸ (cid:98)m . m=1 TheSAICestimatorisasmootherfunctionofthedatathanAICselection. RecallthatbothAICselectionandBICselectionaremodelselectionconsistentinthesensethatas thesamplesizegetslargetheprobabilitythattheselectedmodelisatruemodelisarbtrarilyclosetoone. Furthermore,BICisconsistentforparsimoniousmodelsandAICasymptoticallyover-selects. ThesepropertiesextendtoSBICandSAIC.InlargesamplesSAICandSBICweightswillconcentrate exclusivelyontruemodels;theweightonincorrectmodelswillasymptoticallyapproachzero.However, SAIC will asymptotically spread weight across both parsimonious true models and overparameterized truemodels,whileSBICasymptoticallyconcentratesweightonlyonparsimonioustruemodels. An interesting property of the smoothed estimators is the possibility of asymptotically spreading weight across equal-fitting parsimonious models. Suppose we have two non-nested models with the samenumberofparametersandthesameKLICvaluesotheyareequalapproximations. Inlargesam- plesbothSBICandSAICwillbeweightedaveragesofthetwoestimatorsratherthansimplyselectingone ofthetwo.",
    "page": 916,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 897 28.28 MallowsModelAveraging InlinearregressiontheMallowscriterion(28.14)appliesdirectlytothemodelaveragingestimator (28.29).Thehomoskedasticregressionmodelis Y =m+e m=m(X) (cid:69)[e|X]=0 (cid:69)(cid:163) e2|X (cid:164)=Ï2. SupposethatthereareM modelsform(X),eachwhichtakestheformÎ²(cid:48) X forsomeK Ã1regression m m m vector X m . Themth modelestimatorofthecoefficientisÎ² (cid:98)m =(cid:161) X (cid:48) m X m (cid:162)â1 X (cid:48) m Y, andtheestimatorof thevectorm ism =P Y whereP =X (cid:161) X (cid:48) X (cid:162)â1 X (cid:48) . Thecorrespondingresidualvectorise = (cid:98)m m m m m m m (cid:98)m (I âP )Y. n m Themodelaveragingestimatorforfixedweightsis M (cid:88) m (w)= w P Y =P(w)Y (cid:98)m m m m=1 where M (cid:88) P(w)= w P . m m m=1 Themodelaveragingresidualis M (cid:88) e(w)=(I âP(w))Y = w (I âP )Y. (cid:98) n m n m m=1 Theestimatorm (w)islinearinY sotheMallowscriterioncanbeapplied.Itequals (cid:98)m C(w)=e(w) (cid:48) e(w)+2Ï2tr(P(w)) (cid:98) (cid:98) (cid:101) M =e(w) (cid:48) e(w)+2Ï2 (cid:88) w K (cid:98) (cid:98) (cid:101) m m m=1 whereÏ2isapreliminary7estimatorofÏ2. (cid:101) InthecaseofmodelselectiontheMallowspenaltyisproportionaltothenumberofestimatedcoeffi- cients.InthemodelaveragingcasetheMallowspenaltyistheaveragenumberofestimatedcoefficients. TheMallows-selectedweightvectoristhatwhichminimizestheMallowscriterion.Itequals w =argminC(w). (28.30) (cid:98)mma wâS ComputationallyitisusefultoobservethatC(w)isaquadratricfunctioninw. Indeed,bydefining thenÃM matrixE(cid:98) =[ (cid:98) e 1 ,..., (cid:98) e M ]ofresidualvectorsandtheMÃ1vectorK =[K 1 ,...,K M ]thecriterionis C(w)=w (cid:48) E(cid:98) (cid:48) E(cid:98)w+2Ï (cid:101) 2K (cid:48) w. The probability simplex S is defined by one equality and 2M inequality constraints. The minimiza- tion problem (28.30) falls in the category of quadratic programming which means optimization of a 7Itistypicaltousethebias-correctedleastsquaresvarianceestimatorfromthelargestmodel.",
    "page": 917,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 898 quadratic subject to linear equality and inequality constraints. This is a well-studied area of numeri- caloptimizationandnumericalsolutionsarewidelyavailable. InRusethecommandsolve.QPinthe packagequadprog.InMATLABusethecommandquadprog. Figure28.6illustratestheMallowsweightcomputationproblem.Displayedistheprobabilitysimplex S in (cid:82)3. The axes are the weight vectors. The ellipses are the contours of the unconstrained sum of squarederrorsasafunctionoftheweightvectorsprojectedontotheconstrainedset (cid:80)M w =1.Thisis m=1 m theextensionoftheprobabilitysimplexasatwo-dimensionalplanein(cid:82)3.Themidpointofthecontours istheminimizingweightvectorallowingforweightsoutside[0,1]. Thepointwherethelowestcontour ellipsehitstheprobabilitysimplexisthesolution(28.30),theMallowsselectedweightvector. Intheleft panelisdisplayedanexamplewherethesolutionisthevertex(0,1,0)sotheselectedweightvectorputs all weight on model 2. In the right panel is displayed an example where the solution lies on the edge between(1,0,0)and(0,0,1),meaningthattheselectedweightvectoraveragesmodels1and3butputs no weight onmodel 2. Since the contour sets are ellipses and the constraint setis a simplex, solution pointstendtobeonedgesandverticesmeaningthatsomemodelsreceivezeroweight. Infact,where there are a large number of models a generic feature of the solution is that most models receive zero weight;theselectedweightvectorputspositiveweightonasmallsubsetoftheeligiblemodels. w 3 w 3 l l l l w w 1 2 w w 1 2 Figure28.6:MallowsWeightSelection Once the weights w are obtained the model averaging estimator of the coefficients are found by (cid:98) averagingthemodelestimatesÎ² (cid:98)m usingtheweights. InthespecialcaseoftwonestedmodelstheMallowscriterioncanbewrittenas (cid:181) (cid:48) (cid:48) (cid:182)(cid:181) (cid:182) e e e e w C(w)=(w,1âw) (cid:98) e 1 (cid:48) (cid:98) e 1 (cid:98) e 1 (cid:48) (cid:98) e 2 1âw +2Ï (cid:101) 2(wK 1 +(1âw)K 2 ) (cid:98)2(cid:98)1 (cid:98)2(cid:98)2 (cid:181) e (cid:48) e e (cid:48) e (cid:182)(cid:181) 1âw (cid:182) =(w,1âw) (cid:98) (cid:48) 1(cid:98)1 (cid:98)2 (cid:48) (cid:98)2 +2Ï (cid:101) 2(wK 1 +(1âw)K 2 ) e e e e w (cid:98)2(cid:98)2 (cid:98)2(cid:98)2 =w2(cid:161) e (cid:48) e âe (cid:48) e (cid:162)+e (cid:48) e â2Ï2(K âK )w+2Ï2 (cid:98)1(cid:98)1 (cid:98)2(cid:98)2 (cid:98)2(cid:98)2 (cid:101) 2 1 (cid:101) whereweassumeK <K sothate (cid:48) e =Y (cid:48) (I âP )(I âP )Y =Y (cid:48) (I âP )Y =e (cid:48) e . Theminimizer 1 2 (cid:98)1(cid:98)2 n 1 n 2 n 2 (cid:98)2(cid:98)2 ofthiscriterionis (cid:181)Ï2(K âK ) (cid:182) w= (cid:101) 2 1 . (cid:98) e (cid:48) e âe (cid:48) e (cid:98)1(cid:98)1 (cid:98)2(cid:98)2 1 ThisisthesameastheSteinRuleweight(28.27)withaslightlydifferentshrinkageconstant. Thusthe MallowsaveragingestimatorforM=2isamemberoftheSteinRulefamily.HenceforM>2theMallows averagingestimatorisageneralizationoftheJames-Steinestimatortomultiplemodels.",
    "page": 918,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 899 Basedonthelatterobservation,B.E.Hansen(2014)showsthattheMMAestimatorhaslowerWMSE thantheunrestrictedleastsquaresestimatorwhenthemodelsarenestedlinearregressions,theerrors are homoskedastic, and the models are separated by 4 coefficients or greater. The latter condition is analogoustotheconditionsforimprovementsintheSteinRuletheory. B.E.Hansen(2007)showedthattheMMAestimatorasymptoticallyachievesthesameMSEasthein- feasibleoptimalbestweightedaverageusingthetheoryofLi(1987)undersimilarconditions.Thisshows thatusingmodelselectiontoolstoselecttheaveragingweightsisasymptoticallyoptimalforregression fittingandpointforecasting. 28.29 Jackknife(CV)ModelAveraging A disadvantage of Mallows selection is that the criterion is valid only when the errors are condi- tionally homoskedastic. In constrast, selection by cross-validation does not require homoskedasticity. Thereforeitseemssensibletousecross-validationratherthanMallowstoselecttheweightvectors. It turnsoutthatthisisasimpleextensionwithexcellentfinitesampleperformance.IntheMachineLearn- ingliteraturethismethodiscalledstacking. Afittedaveragingregression(withfixedweights)canbewrittenas M Y i = (cid:88) w m X m (cid:48) i Î² (cid:98)m +e (cid:98)i (w) m=1 whereÎ² (cid:98)m aretheleastsquarescoefficientestimatesfromModelm. Thecorrespondingleave-one-out equationis M Y i = (cid:88) w m X m (cid:48) i Î² (cid:98)m,(âi) +e (cid:101)i (w) m=1 where Î² (cid:98)m,(âi) are the least squares coefficient estimates from Model m when observation i is deleted. Theleave-one-outpredictionerrorssatisfythesimplerelationship M (cid:88) e (w)= w e (cid:101)i m(cid:101)mi m=1 wheree (cid:101)mi aretheleave-one-outpredictionerrorsformodelm. Inmatrixnotation (cid:101) e(w)=E(cid:101)w whereE(cid:101) isthenÃM matrixofleave-one-outpredictionerrors. This means that the jackknife estimate of variance (or equivalently the cross-validation criterion) equals CV(w)=w (cid:48) E(cid:101) (cid:48) E(cid:101)w whichisaquadraticfunctionoftheweightvector. Thecross-validationchoiceforweightvectoristhe minimizer w =argminCV(w). (28.31) (cid:98)jma wâS Given the weights the coefficient estimates (and any other parameter of interest) are found by taking weightedaveragesofthemodelestimatesusingtheweightvectorw . B.E.HansenandRacine(2012) (cid:98)jma callthistheJackknifeModelAveraging(JMA)estimator. ThealgebraicpropertiesofthesolutionaresimilartoMallows. Since(28.31)minimizesaquadratic function subject to a simplex constraint solutions tend to be on edges and vertices which means that many(ormost)modelsreceivezeroweight. HenceJMAweightselectionsimultaneouslyperformsse- lectionandshrinkage. Thesolutionisfoundnumericallybyquadraticprogrammingwhichiscomputa- tionallysimpleandfastevenwhenthenumberofmodelsM islarge.",
    "page": 919,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 900 B.E.HansenandRacine(2012)showedthattheJMAestimatorisasymptoticallyequivalenttothein- feasibleoptimalweightedaverageacrossleastsquaresestimatesbasedonaregressionfitcriteria. Their resultsholdunderquitemildconditionsincludingconditionalheteroskedasticity. Thisresultissimilar toAndrews(1991c)generalizationofLi(1987)âsresultformodelselection. TheimplicationofthistheoryisthatJMAweightselectioniscomputationallysimpleandhasexcel- lentsamplingperformance. 28.30 Granger-RamanathanAveraging A method similar to JMA based on hold-out samples was proposed for forecast combination by GrangerandRamanathan(1984),andhasemergedasapopularmethodinthemodernmachinelearning literature. Randomlysplitthesampleintotwoparts:anestimationananevaluationsample.Usingtheestima- tionsample,estimatetheM regressionmodels,obtainingthecoefficientsÎ² (cid:98)m . Usingthesecoefficients andtheevaluationsampleconstructthefittedvaluesY(cid:101)mi =X m (cid:48) i Î² (cid:98)m fortheM models.Thenestimatethe modelweightsbyaleastsquaresregressionofY i onY(cid:101)mi andnointerceptusingtheevaluationsample. Thisregressionis M (cid:88) Y i = w (cid:98)m Y(cid:101)mi +e (cid:98)i . m=1 Theleastsquarescoefficientsw aretheGranger-Ramanathanweights. (cid:98)m BasedonaninformalargumentGrangerandRamanathan(1984)recommendedanunconstrained leastsquaresregressiontoobtaintheweightsbutthisisnotadvisedasthisproducesextremelyerratic empiricalweights,especiallywhenM islarge. Itisrecommendedtouseconstrainedregression,impos- ingtheconstraintsw â¥0and (cid:80)M w =1. Toimposethenon-negativityconstraintsitisbesttouse (cid:98)m m=1 (cid:98)m quadraticprogramming. This Granger-Ramanathan approach is best suited for applications with a very large sample size wheretheefficiencylossfromthehold-outsamplesplitisnotaconcern. 28.31 EmpiricalIllustration WeillustratethemodelaveragingmethodswiththeempiricalapplicationfromSection28.18,which reportedwageregressionestimatesfortheCPSsub-sampleofAsianwomenfocusingonthereturnto experiencebetween0and30years. Table28.2reportsthemodelaveragingweightsobtainedusingthemethodsofSBIC,SAIC,Mallows modelaveraging(MMA),andjackknifemodelaveraging(JMA).Alsoreportedinthefinalcolumnisthe weightedaverageestimateofthereturntoexperienceasapercentage. Theresultsshowthatthemethodsputweightonsomewhatdifferentmodels. TheSBICputsnearly allweightonmodel2. TheSAICputsnearly1/2oftheweightonmodel6withmostoftheremainder split between models 5 and 9. MMA puts nearly 1/2 of the weight on model 9, 30% on 5, and 9% on model1. JMAissimilartoMMAbutmoreemphasisonparsimony,with1/2oftheweightonmodel5, 17%onmodel9,17%onmodel1,and8%onmodel3.OneoftheinterestingthingsabouttheMMA/JMA methodsisthattheycansplitweightbetweenquitedifferentmodels,e.g.models1and9. Theaveragingestimatorsfromthenon-BICmethodsaresimilartooneanotherbutSBICproducesa muchsmallerestimatethantheothermethods.",
    "page": 920,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 901 Table28.2:ModelAveragingWeightsandEstimatesofReturntoExperienceamongAsianWomen Model1 Model2 Model3 Model4 Model5 Model6 Model7 Model8 Model9 Return SBIC .02 .96 .00 .00 .04 .00 .00 .00 .00 22% SAIC .00 .02 .10 .00 .15 .44 .00 .06 .22 38% MMA .09 .02 .02 .00 .30 .00 .00 .00 .57 39% JMA .17 .00 .08 .00 .57 .01 .00 .00 .17 34% 28.32 TechnicalProofs* ProofofTheorem28.1Weestablishthetheoremunderthesimplifyingassumptionsofthenormallinear regressionmodelwithaKÃ1coefficientvectorÎ²andknownvarianceÏ2.Thelikelihoodfunctionis (cid:195) (cid:33) L (Î²)=(cid:161) 2ÏÏ2(cid:162)ân/2 exp â 1 (cid:88) n (cid:161) Y âX (cid:48)Î²(cid:162)2 . n 2Ï2 i i i=1 EvaluatedattheMLEÎ² (cid:98)thisequals L n (Î² (cid:98))=(cid:161) 2ÏÏ2(cid:162)ân/2 exp (cid:195) â (cid:80) 2 n i= Ï 1 2 e (cid:98)i 2(cid:33) . (28.32) Using(8.21)wecanwrite (cid:195) (cid:195) (cid:33)(cid:33) L n (Î²)=(cid:161) 2ÏÏ2(cid:162)ân/2 exp â 2Ï 1 2 (cid:88) n e (cid:98)i 2+(cid:161)Î² (cid:98) âÎ²(cid:162)(cid:48) X (cid:48) X (cid:161)Î² (cid:98) âÎ²(cid:162) i=1 (cid:181) (cid:182) =L n (Î² (cid:98))exp â 2Ï 1 2 (cid:161)Î² (cid:98) âÎ²(cid:162)(cid:48) X (cid:48) X (cid:161)Î² (cid:98) âÎ²(cid:162) . ForadiffusepriorÏ(Î²)=C themarginallikelihoodis p(Y)=L n (Î² (cid:98)) (cid:90) exp (cid:181) â 2Ï 1 2 (cid:161)Î² (cid:98) âÎ²(cid:162)(cid:48) X (cid:48) X (cid:161)Î² (cid:98) âÎ²(cid:162) (cid:182) CdÎ² =L n (Î² (cid:98))n âK/2(cid:161) 2ÏÏ2(cid:162)K/2 det (cid:181) 1 X (cid:48) X (cid:182)â1/2 C n wherethefinalequalityisthemultivariatenormalintegral.Rewritingandtakinglogs (cid:181) (cid:182) â2logp(Y)=â2logL n (Î² (cid:98))+KlognâKlog (cid:161) 2ÏÏ2(cid:162)+logdet 1 X (cid:48) X +logC n =â2(cid:96) n (Î² (cid:98))+Klogn+O(1). Thisisthetheorem. â  ProofofTheorem28.2From(28.11) (cid:90) g(y)logf(y,Î¸ (cid:98))dy=â n 2 log (cid:161) 2ÏÏ2(cid:162)â 2Ï 1 2 (cid:88) n (cid:90) (cid:161) yâX i (cid:48)Î² (cid:98) (cid:162)2 g (cid:161) y|X i (cid:162) dy i=1 =â n 2 log (cid:161) 2ÏÏ2(cid:162)â 2Ï 1 2 (cid:88) n (cid:179) Ï2+(cid:161)Î² (cid:98) âÎ²(cid:162)(cid:48) X i X i (cid:48)(cid:161)Î² (cid:98) âÎ²(cid:162) (cid:180) i=1 =â n log (cid:161) 2ÏÏ2(cid:162)â n â 1 e (cid:48) Pe. 2 2 2Ï2",
    "page": 921,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 902 Thus T =â2(cid:69) (cid:183)(cid:90) g(y)logf(cid:98)(y)dy (cid:184) =nlog (cid:161) 2ÏÏ2(cid:162)+n+ 1 (cid:69)(cid:163) e (cid:48) Pe (cid:164)=nlog (cid:161) 2ÏÏ2(cid:162)+n+K. Ï2 Thisis(28.12).Thefinalequalityholdsundertheassumptionofconditionalhomoskedasticity. Evaluating(28.11)atÎ² (cid:98)weobtaintheloglikelihood â2(cid:96) n (Î² (cid:98))=nlog (cid:161) 2ÏÏ2(cid:162)+ Ï 1 2 (cid:88) n e (cid:98)i 2=nlog (cid:161) 2ÏÏ2(cid:162)+ Ï 1 2 e (cid:48) Me. i=1 Thishasexpectation â(cid:69)(cid:163) 2(cid:96) n (Î² (cid:98)) (cid:164)=nlog (cid:161) 2ÏÏ2(cid:162)+ Ï 1 2 (cid:69)(cid:163) e (cid:48) Pe (cid:164)=nlog (cid:161) 2ÏÏ2(cid:162)+nâK. Thisis(28.13).Thefinalequalityholdsunderconditionalhomoskedasticity. â  ProofofTheorem28.4TheproofusesTaylorexpansionssimilartothoseusedfortheasymptoticdistri- butiontheoryoftheMLEinnonlinearmodels.Weavoidtechnicaldetailssothisisnotafullproof. Writethemodeldensityas f(y,Î¸)andtheestimatedmodelas f(cid:98)(y)=f(y,Î¸ (cid:98)).Recallfrom(28.10)that wecanwritethetargetT as T =â2(cid:69)(cid:163) logf(Y(cid:101),Î¸ (cid:98)) (cid:164) whereY(cid:101)isanindependentcopyofY.LetÎ¸ (cid:101)betheMLEcalculatedonthesampleY(cid:101).Î¸ (cid:101)isanindependent copyofÎ¸ (cid:98).BysymmetrywecanwriteT as T =â2(cid:69)(cid:163) logf(Y,Î¸ (cid:101)) (cid:164) . (28.33) DefinetheHessianH=â â (cid:69)(cid:163) logf(Y,Î¸) (cid:164)>0.Nowtakeasecond-orderTaylorseriesexpansionofthe âÎ¸âÎ¸(cid:48) loglikelihoodlogf(Y,Î¸ (cid:101))aboutÎ¸ (cid:98).Thisis logf(Y,Î¸ (cid:101))=logf(Y,Î¸ (cid:98))+ â â Î¸(cid:48) logf(Y,Î¸ (cid:98)) (cid:161)Î¸ (cid:101) âÎ¸ (cid:98) (cid:162)â 2 1(cid:161)Î¸ (cid:101) âÎ¸ (cid:98) (cid:162)(cid:48) H (cid:161)Î¸ (cid:101) âÎ¸ (cid:98) (cid:162)+O p (cid:161) n â1/2(cid:162) =logf(Y,Î¸ (cid:98))â n(cid:161)Î¸ (cid:101) âÎ¸ (cid:98) (cid:162)(cid:48) H (cid:161)Î¸ (cid:101) âÎ¸ (cid:98) (cid:162)+O p (cid:161) n â1/2(cid:162) . (28.34) 2 Thesecondequalityholdsbecauseofthefirst-orderconditionfortheMLEÎ¸ (cid:98)",
    "page": 922,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". (28.34) 2 Thesecondequalityholdsbecauseofthefirst-orderconditionfortheMLEÎ¸ (cid:98). IftheO (n â1/2)termin(28.34)isuniformlyintegrable(28.33)and(28.34)implythat p T =â(cid:69)(cid:163) 2logf(Y,Î¸ (cid:98)) (cid:164)+(cid:69) (cid:104) n (cid:161)Î¸ (cid:101) âÎ¸ (cid:98) (cid:162)(cid:48) H (cid:161)Î¸ (cid:101) âÎ¸ (cid:98) (cid:162) (cid:105) +O (cid:161) n â1/2(cid:162) =â(cid:69)(cid:163) 2logL(Î¸ (cid:98)) (cid:164)+(cid:69) (cid:104) n (cid:161)Î¸ (cid:101) âÎ¸(cid:162)(cid:48) H (cid:161)Î¸ (cid:101) âÎ¸(cid:162) (cid:105) +(cid:69) (cid:104) n (cid:161)Î¸ (cid:98) âÎ¸(cid:162)(cid:48) H (cid:161)Î¸ (cid:98) âÎ¸(cid:162) (cid:105) +2(cid:69) (cid:104) n (cid:161)Î¸ (cid:101) âÎ¸(cid:162)(cid:48) H (cid:161)Î¸ (cid:98) âÎ¸(cid:162) (cid:105) +O (cid:161) n â1/2(cid:162) =â(cid:69)(cid:163) 2(cid:96) n (Î¸ (cid:98)) (cid:164)+(cid:69)(cid:163)Ï2 K (cid:164)+(cid:69)(cid:163)Ï (cid:101) 2 K (cid:164)+O (cid:161) n â1/2(cid:162) =â(cid:69)(cid:163) 2(cid:96) n (Î¸ (cid:98)) (cid:164)+2K+O (cid:161) n â1/2(cid:162) whereÏ2 andÏ2 arechi-squarerandomvariableswithK degreesoffreedom.Thesecond-to-lastequal- K (cid:101)K ityholdsif n (cid:161)Î¸ (cid:98) âÎ¸(cid:162)(cid:48) H (cid:161)Î¸ (cid:98) âÎ¸(cid:162)ââÏ2 (28.35) K d and the Wald statistic on the left-side of (28.35) is uniformly integrable. The asymptotic convergence (28.35)holdsfortheMLEunderstandardregularityconditions(includingcorrectspecification). â ",
    "page": 922,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 903 ProofofTheorem28.5Usingmatrixnotationwecanwritemâm=â(I âA)m+Ae.Wecanthenwrite (cid:98) n thefitas R=(cid:69)(cid:163) (mâm) (cid:48) (mâm)|X (cid:164) (cid:98) (cid:98) =(cid:69)(cid:163) m (cid:48)(cid:161) I âA (cid:48)(cid:162) (I âA)mâ2m (cid:48)(cid:161) I âA (cid:48)(cid:162) Ae+e (cid:48) A (cid:48) Ae|X (cid:164) n n n =m (cid:48)(cid:161) I âA (cid:48)(cid:162) (I âA)m+Ï2tr (cid:161) A (cid:48) A (cid:162) . n n Noticethatthiscalculationreliesontheassumptionofconditionalhomoskedasticity. NowconsidertheMallowscriterion.Wefindthat C â=e (cid:48) e+2Ï2tr(A)âe (cid:48) e p (cid:98)(cid:98) (cid:101) =(m+e) (cid:48)(cid:161) I âA (cid:48)(cid:162) (I âA)(m+e)+2Ï2tr(A)âe (cid:48) e n n (cid:101) =m (cid:48)(cid:161) I âA (cid:48)(cid:162) (I âA)m+2m (cid:48)(cid:161) I âA (cid:48)(cid:162) (I âA)e+e (cid:48) A (cid:48) Aeâ2e (cid:48) Ae+2Ï2tr(A). n n n n (cid:101) Takingexpectationsandusingtheassumptionsofconditionalhomoskedasticityand(cid:69)(cid:163)Ï2|X (cid:164)=Ï2 (cid:101) (cid:104) (cid:105) (cid:69) C â|X =m (cid:48)(cid:161) I âA (cid:48)(cid:162) (I âA)m+Ï2tr (cid:161) A (cid:48) A (cid:162)=R. p n n Thisistheresultasstated. â  â â Proof of Theorem 28.6 Take any two models M and M where M â M and M â M . Let their 1 2 1 2 informationcriteriabewrittenas IC 1 =â2(cid:96) 1 (Î¸ (cid:98)1 )+c(n,K 1 ) IC 2 =â2(cid:96) 2 (Î¸ (cid:98)2 )+c(n,K 2 ). ModelM isselectedoverM if 1 2 LR<c(n,K )âc(n,K ) 2 1 where LR=2 (cid:161)(cid:96) 2 (Î¸ (cid:98)2 )â(cid:96)(Î¸ (cid:98)1 ) (cid:162) is the likelihood ratio statistic for testing M 1 against M 2 . Since we have assumedthatM isnotatruemodelwhileM istrue,thenLRdivergesto+âatraten.Thismeansthat 1 2 for any Î±>0, n â1+Î± LRââ+â. Furthermore, the assumptions imply n â1+Î± (c(n,K )âc(n,K ))ââ0. 1 2 p Fix(cid:178)>0.Thereisannsufficientlylargesuchthatn â1+Î± (c(n,K )âc(n,K ))<(cid:178).Thus 1 2 (cid:104) (cid:105) (cid:80) M (cid:99) =M 1 â¤(cid:80)(cid:163) n â1+Î± LR<n â1+Î± (c(n,K 2 )âc(n,K 1 )) (cid:164) â¤(cid:80)[LR<(cid:178)]â0. â â SincethisholdsforanyM âM wededucethattheselectedmodelisinM withprobabilityapproach- 1 ingone.Thismeansthattheselectioncriterionismodelselectionconsistentasclaimed. â  ProofofTheorem28.7TakethesettingasdescribedintheproofofTheorem28.6butnowassumeM â 1 â M andM ,M âM .ThelikelihoodratiostatisticsatisfiesLRââÏ2wherer =K âK .Let 2 1 2 r 2 1 d B=limsup(c(n,K )âc(n,K ))<â. 1 2 nââ LettingF (u)denotetheÏ2distributionfunction r r (cid:104) (cid:105) (cid:80) M (cid:99) =M 2 =(cid:80)[LR>(c(n,K 2 )âc(n,K 1 ))] â¥(cid:80)[LR>B] â(cid:80)(cid:163)Ï2>B (cid:164)=1âF (B)>0 r r",
    "page": 923,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 904 sinceÏ2hassupportoverthepositivereallineandB<â.Thisshowsthattheselectioncriterionasymp- r toticallyover-selectswithpositiveprobability. â  ProofofTheorem28.8Sincec(n,K)=o(n)theprocedureismodelselectionconsistent.Taketwomodels â M ,M âM with K <K . Since both models are true then LR=O (1). Fix (cid:178)>0. There is a B <â 1 2 1 2 p such that LRâ¤B with probability exceeding 1â(cid:178). By (28.16) there is an n sufficiently large such that c(n,K )âc(n,K )>B.Thus 2 1 (cid:104) (cid:105) (cid:80) M (cid:99) =M 2 â¤(cid:80)[LR>(c(n,K 2 )âc(n,K 1 ))]â¤(cid:80)[LR>B]â¤(cid:178). (cid:104) (cid:105) Since(cid:178)isarbitrary(cid:80) M (cid:99) =M 2 ââ0asclaimed. â  ProofofTheorem28.9First,weexamineR (K). Writethepredictedvaluesinmatrixnotationasm = n (cid:98)K X K Î² (cid:98)K =P K Y whereP K =X K (cid:161) X (cid:48) K X K (cid:162)â1 X (cid:48) K . Itisusefultoobservethatmâm (cid:98)K =M K mâP K e where M =I âP .Wefindthatthepredictionriskequals K K K R (K)=(cid:69)(cid:163) (mâm ) (cid:48) (mâm )|X (cid:164) n (cid:98)K (cid:98)K =(cid:69)(cid:163) (M mâP e) (cid:48) (M mâP e)|X (cid:164) K K K K =m (cid:48) M m+(cid:69)(cid:163) e (cid:48) P e|X (cid:164) K K =m (cid:48) M m+Ï2K. K (cid:48) ThechoiceofregressorsaffectsR (K)throughthetwotermsinthefinalline. Thefirsttermm M m is n K thesquaredbiasduetoomittedvariables.AsK increasesthistermdecreasesreflectingreducedomitted variablesbias. ThesecondtermÏ2K isestimationvariance. Itisincreasinginthenumberofregressors. Increasingthenumberofregressorsaffectsthequalityofout-of-samplepredictionbyreducingthebias butincreasingthevariance. WenextexaminetheadjustedMallowscriterion.Wefindthat C â (K)=e (cid:48) e +2Ï2Kâe (cid:48) e n (cid:98)K(cid:98)K =(m+e) (cid:48) M (m+e)+2Ï2Kâe (cid:48) e K =m (cid:48) M m+2m (cid:48) M eâe (cid:48) P e+2Ï2K. K K K Thenextstepistoshowthat sup (cid:175) (cid:175) (cid:175) C n â (K)âR n (K) (cid:175) (cid:175) (cid:175) ââ0 (28.36) (cid:175) R (K) (cid:175) p K n asnââ.Toestablish(28.36),observethat C â (K)âR (K)=2m (cid:48) M eâe (cid:48) P e+Ï2K. n n K K (cid:179) (cid:180)r Pick(cid:178)>0andsomesequenceB ââsuchthatB / R opt â0.(ThisisfeasiblebyAssumption28.1.5.) n n n ByBooleâsinequality(B.24),Whittleâsinequality(B.48),thefactsthatm (cid:48) M mâ¤R (K)andR (K)â¥Ï2K, K n n",
    "page": 924,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 905 B / (cid:179) R opt (cid:180)r â0,and (cid:80)â K âr <â n n K=1 (cid:183) (cid:175) (cid:48) (cid:175) (cid:175) (cid:184) â (cid:183)(cid:175) (cid:48) (cid:175) (cid:175) (cid:184) (cid:80) sup (cid:175) (cid:175) m M K e(cid:175) (cid:175) >(cid:178)(cid:175) (cid:175) X â¤ (cid:88) (cid:80) (cid:175) (cid:175) m M K e(cid:175) (cid:175) >(cid:178)(cid:175) (cid:175) X (cid:175) R (K) (cid:175) (cid:175) (cid:175) R (K) (cid:175) (cid:175) K n K=1 n â¤ C 1r (cid:88) â (cid:175) (cid:175)m (cid:48) M K m (cid:175) (cid:175) r (cid:178)2r R (K)2r K=1 n â â¤ C 1r (cid:88) 1 (cid:178)2r R (K)r K=1 n = C 1r (cid:88) Bn 1 + C 1r (cid:88) â 1 (cid:178)2r R (K)r (cid:178)2r R (K)r K=1 n K=Bn +1 n â â¤ C 1r B n + C 1r (cid:88) 1 (cid:178)2r (cid:179) R n opt (cid:180)r (cid:178)2rÏ2r K=Bn +1 Kr â0. ByasimilarargumentbutusingWhittleâsinequality(B.49),tr(P P )=tr(P )=K,andK â¤Ïâ2R (K) K K K n (cid:80) (cid:183) sup (cid:175) (cid:175) (cid:175) e (cid:48) P K eâÏ2K (cid:175) (cid:175) (cid:175) >(cid:178) (cid:175) (cid:175) (cid:175) X (cid:184) â¤ (cid:88) â (cid:80) (cid:183)(cid:175) (cid:175) (cid:175) e (cid:48) P K eâ(cid:69)(cid:161) e (cid:48) P K e (cid:162)(cid:175) (cid:175) (cid:175) >(cid:178) (cid:175) (cid:175) (cid:175) X (cid:184) (cid:175) R (K) (cid:175) (cid:175) (cid:175) R (K) (cid:175) (cid:175) K n K=1 n â¤ C 2r (cid:88) â tr(P K P K )r (cid:178)2r R (K)2r K=1 n = C 2r (cid:88) â Kr (cid:178)2r R (K)2r K=1 n â â¤ C 1r (cid:88) 1 (cid:178)2rÏ2r R (K)r K=1 n â0. Togethertheseimply(28.36). Finally we show that (28.36) implies (28.18). The argument is similar to the standard consistency â proof for nonlinear estimators. (28.36) states thatC (K) converges uniformly in probability to R (K). n n â opt ThisimpliesthattheminimizerofC (K)convergesinprobabilitytothatofR (K). Formally,sinceK n n n minimizesR (K) n 0â¤ R n (K(cid:98)n )âR n (K n opt ) R n (K(cid:98)n ) = C n â (K(cid:98)n )âR n (K n opt ) â C n â (K(cid:98)n )âR n (K(cid:98)n ) R n (K(cid:98)n ) âR n (K(cid:98)n ) â¤ C n â (K(cid:98)n )âR n (K n opt ) +o (1) p R n (K(cid:98)n ) C â (K opt )âR (K opt ) â¤ n n n n +o (1) opt p R (K ) n n â¤o (1). p â opt Thesecondinequalityis(28.36). ThefollowingusesthefactsthatK(cid:98)n minimizesC n (K)andK n mini- mizesR (K).Thefinalis(28.36).Thisis(28.18). â  n",
    "page": 925,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 906 Before providing the proof of Theorem 28.10 we present two technical results related to the non- central chi-square density function with degree of freedom K and non-centrality parameter Î» which equals (cid:88) â e âÎ»/2(cid:181)Î»(cid:182)i f K (x,Î»)= f K+2i (x) (28.37) i! 2 i=0 where f (x)= xr/2â1eâx/2 istheÏ2 densityfunction. r 2r/2Î(r/2)) K Theorem28.16 Thenon-centralchi-squaredensityfunction(28.37)obeysthe recursiverelationship f K (x,Î»)= K x f K+2 (x,Î»)+Î» x f K+4 (x,Î»). TheproofofTheorem28.16isastraightforwardmanipulationofthenon-centralchi-squaredensity function(28.37). ThesecondtechnicalresultisfromBock(1975,TheoremsA&B). Theorem28.17 IfX â¼N(Î¸,I )thenforanyfunctionh(u) K (cid:69)(cid:163) Xh (cid:161) X (cid:48) X (cid:162)(cid:164)=Î¸(cid:69)[h(Q K+2 )] (28.38) (cid:69)(cid:163) X (cid:48) Xh (cid:161) X (cid:48) X (cid:162)(cid:164)=K(cid:69)[h(Q K+2 )]+Î»(cid:69)[h(Q K+4 )] (28.39) whereÎ»=Î¸(cid:48)Î¸andQ â¼Ï2(Î»),anon-centralchi-squarerandomvariablewith r r r degreesoffreedomandnon-centralityparameterÎ». ProofofTheorem28.17Toshow(28.38)wefirstshowthatforZ â¼N(Âµ,1)thenforanyfunctiong(u) (cid:69)(cid:163) Zg (cid:161) Z2(cid:162)(cid:164)=Âµ(cid:69)(cid:163) g(Q ) (cid:164) . (28.40) 3 AssumeÂµ>0.Usingthechange-of-variablesy=x2 (cid:69)(cid:163) Zg (cid:161) Z2(cid:162)(cid:164)= (cid:90) â (cid:112) x g (cid:161) x2(cid:162) exp (cid:181) â 1(cid:161) xâÂµ(cid:162)2 (cid:182) dx ââ 2Ï 2 = (cid:90) â (cid:112) y e â(y+Âµ2)/2 (cid:179) e (cid:112) yÂµâe â (cid:112) yÂµ (cid:180) g (cid:161) y (cid:162) dy. (28.41) 0 2 2Ï ByexpansionandLegendreâsduplicationformula exâe âx=2 (cid:88) â x1+2i = (cid:112) Ïx (cid:88) â (x2/2)i . i=0 (1+2i)! i=0 2ii!Î(i+3/2) Then(28.41)equals Âµ (cid:90) â ye â(y+Âµ2)/2(cid:88) â (Âµ2/2)iyi+1/2 g (cid:161) y (cid:162) dy=Âµ (cid:90) â yf (y,Âµ2)g (cid:161) y (cid:162) dy=Âµ(cid:69)(cid:163) g(Q ) (cid:164) 0 i=0 23/2+ii!Î(i+3/2) 0 3 3",
    "page": 926,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 907 where f (y,Î»)isthenon-centralchi-squaredensity(28.37)with3degreesoffreedom.Thisis(28.40). 3 Takethe jth rowof(28.38). Write X (cid:48) X =X2+J,where X â¼N(Î¸ ,1)and J â¼Ï2 (Î»âÎ¸2)areinde- j j j Kâ1 j pendent.Settingg(u)=h(u+J)andusing(28.41) (cid:104) (cid:179) (cid:180)(cid:105) (cid:69)(cid:163) X h (cid:161) X (cid:48) X (cid:162)(cid:164)=(cid:69) X h X2+J j j j (cid:104) (cid:104) (cid:179) (cid:180) (cid:105)(cid:105) =(cid:69) (cid:69) X g X2 |J j j =(cid:69)(cid:163)Î¸ (cid:69)(cid:163) g(Q )|J (cid:164)(cid:164) j 3 =Î¸ (cid:69)[h(Q +J)] j 3 =Î¸ j (cid:69)[h(Q K+2 )] whichis(28.38).ThefinalequalityusesthefactthatQ 3 +Jâ¼Q K+2 . ObservethatX (cid:48) X hasdensity f (x,Î»).UsingTheorem28.16 K (cid:90) â (cid:69)(cid:163) X (cid:48) X (cid:161) X (cid:48) X (cid:162)(cid:164)= xh(x)f (x,Î»)dx K 0 (cid:90) â (cid:90) â =K h(x)f K+2 (x,Î»)dx+Î» h(x)f K+4 (x,Î»)dx 0 0 =K(cid:69)[h(Q K+2 )]+Î»(cid:69)[h(Q K+4 )] whichis(28.39). â  ProofofTheorem28.10Bythequadraticstructurewecancalculatethat MSE (cid:163)Î¸ (cid:98) â(cid:164)=(cid:69) (cid:104) (cid:161)Î¸ (cid:98) âÎ¸âÎ¸ (cid:98) 1(cid:169)Î¸ (cid:98) (cid:48)Î¸ (cid:98) â¤c (cid:170)(cid:162)(cid:48)(cid:161)Î¸ (cid:98) âÎ¸âÎ¸ (cid:98) 1(cid:169)Î¸ (cid:98) (cid:48)Î¸ (cid:98) â¤c (cid:170)(cid:162) (cid:105) =(cid:69) (cid:104) (cid:161)Î¸ (cid:98) âÎ¸(cid:162)(cid:48)(cid:161)Î¸ (cid:98) âÎ¸(cid:162) (cid:105) â(cid:69)(cid:163)Î¸ (cid:98) (cid:48)Î¸ (cid:98) 1(cid:169)Î¸ (cid:98) (cid:48)Î¸ (cid:98) â¤c (cid:170)(cid:164)+2(cid:69)(cid:163)Î¸(cid:48)Î¸ (cid:98) 1(cid:169)Î¸ (cid:98) (cid:48)Î¸ (cid:98) â¤c (cid:170)(cid:164) =KâK(cid:69)(cid:163)1(cid:169) Q K+2 â¤c (cid:170)(cid:164)âÎ»(cid:69)(cid:163)1(cid:169) Q K+4 â¤c (cid:170)(cid:164)+2Î»(cid:69)(cid:163)1(cid:169) Q K+2 â¤c (cid:170)(cid:164) =K+(2Î»âK)F K+2 (c,Î»)âÎ»F K+4 (c,Î»). ThethirdequalityusesthetworesultsfromTheorem28.17,settingh(u)=1 {uâ¤c}. â  _____________________________________________________________________________________________ 28.33 Exercises Exercise28.1 Verifyequations(28.1)-(28.2). Exercise28.2 FindtheMallowscriterionfortheweightedleastsquaresestimatorofalinearregression Y =X (cid:48)Î²+e withweightsÏ (assumeconditionalhomoskedasticity). i i i i Exercise28.3 BackwardStepwiseRegression.VerifytheclaimthatforthecaseofAICselection,step(b) ofthealgorithmcanbeimplementedbycalculatingtheclassical(homoskedastic)t-ratioforeachactive regressorandfindtheregressorwiththesmallestabsolutet-ratio. Hint: UsetherelationshipbetweenlikelihoodratioandFstatisticsandtheequalitybetweenFand Wald statistics to show that for tests on one coefficient the smallest change in the AIC is identical to identifyingthesmallestsquaredtstatistic.",
    "page": 927,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 908 Exercise28.4 ForwardStepwiseRegression. VerifytheclaimthatforthecaseofAICselection,step(b) of the algorithm can be implemented by identifying the regressor in the inactive set with the greatest absolutecorrelationwiththeresidualfromstep(a). Hint: Thisischallenging. Firstshowthatthegoalistofindtheregressorwhichwillmostdecrease SSE = e (cid:48) e = (cid:107)e(cid:107)2. Use a geometric argument to show that the regressor most parallel to e will most (cid:98)(cid:98) (cid:98) (cid:98) decreases(cid:107)e(cid:107).Showthatthisregressorhasthegreatestabsolutecorrelationwithe. (cid:98) (cid:98) Exercise28.5 Aneconomistestimatesseveralmodelsandreportsasingleselectedspecification, stat- ingthatâtheotherspecificationshadinsignificantcoefficientsâ. Howshouldweinterpretthereported parameterestimatesandt-ratios? Exercise28.6 VerifyTheorem28.11,including(28.21),(28.22),and(28.23). Exercise28.7 UndertheassumptionsofTheorem28.11,showthatÎ» (cid:98) =Î¸ (cid:98) (cid:48) V â1Î¸ (cid:98) âK isanunbiasedesti- matorofÎ»=Î¸(cid:48) V â1Î¸. Exercise28.8 ProveTheorem28.14forthesimplercaseoftheunadjusted(notpositivepart)Steinesti- matorÎ¸ (cid:101),V =I K andr =0. Extrachallenge:Showundertheseassumptionsthat wmse (cid:163)Î¸ (cid:101) (cid:164)=Kâ(qâ2)2J q (Î» R ) Î» =Î¸(cid:48) R (cid:161) R (cid:48) R (cid:162)â1 R (cid:48)Î¸. R Exercise28.9 SupposeyouhavetwounbiasedestimatorsÎ¸ (cid:98)1 andÎ¸ (cid:98)2 ofaparametervectorÎ¸ (cid:98)withcovari- ancematricesV andV .Takethegoalofminimizingtheunweightedmeansquarederror,e.g.trV for 1 2 1 Î¸ (cid:98)1 .AssumethatÎ¸ (cid:98)1 andÎ¸ (cid:98)2 areuncorrelated. (a) Showthattheoptimalweightedaverageestimatorequals 1 Î¸ + 1 Î¸ trV1 (cid:98)1 trV2 (cid:98)2 . 1 + 1 trV1 trV2 (b) GeneralizetothecaseofM unbiaseduncorrelatedestimators. (c) Interprettheformulae. Exercise28.10 YouestimateM linearregressionsY =X m (cid:48) Î² m +e m byleastsquares.LetY(cid:98)mi =X m (cid:48) i Î² (cid:98)m be thefittedvalues. (a) ShowthattheMallowsaveragingcriterionisthesameas n M (cid:88)(cid:161) Y i âw 1 Y(cid:98)1i âw 2 Y(cid:98)2i âÂ·Â·Â·âw M Y(cid:98)Mi (cid:162)2+2Ï2 (cid:88) w m k m . i=1 m=1 (b) AssumethemodelsarenestedwithM thelargestmodel.Ifthepreviouscriterionwereminimized overw intheprobabilitysimplexbutthepenaltywasomitted,whatwouldbethesolution?(What wouldbetheminimizingweightvector?)",
    "page": 928,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER28. MODELSELECTION,STEINSHRINKAGE,ANDMODELAVERAGING 909 Exercise28.11 YouestimateM linearregressionsY =X m (cid:48) Î² m +e m byleastsquares.LetY(cid:101)mi =X m (cid:48) i Î² (cid:98)m(âi) bethepredictedvaluesfromtheleave-one-outregressions.ShowthattheJMAcriterionequals n (cid:88)(cid:161) Y i âw 1 Y(cid:101)1i âw 2 Y(cid:101)2i âÂ·Â·Â·âw M Y(cid:101)Mi (cid:162)2 . i=1 Exercise28.12 Using the cps09mar dataset perform an analysis similar to that presented in Section 28.18butinsteadusethesub-sampleofHispanicwomen. Thissamplehas3003observations. Which modelsareselectedbyBIC,AIC,CVandFIC?(Thepreciseinformationcriteriayouexaminemaybelim- iteddependingonyoursoftware.) Howdoyouinterprettheresults?Whichmodel/estimatewouldyou selectasyourpreferredchoice?",
    "page": 929,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Chapter 29 Machine Learning 29.1 Introduction Thischapterreviewsmachinelearningmethodsforeconometrics. Thisisalargeandgrowingtopic soourtreatmentisselective. Thischapterbrieflycoversridgeregression, Lasso, elasticnet, regression trees, bagging, random forests, ensembling, Lasso IV, double-selection/post-regularization, and dou- ble/debiasedmachinelearning. AclassicreferenceisHastie,Tibshirani,andFriedman(2008).IntroductorytextbooksincludeJames, Witten,Hastie,andTibshirani(2013)andEfronandHastie(2017).ForatheoreticaltreatmentseeBÃ¼hlmann andvanderGeer(2011). ForreviewsofmachinelearningineconometricsseeBelloni, Chernozhukov andHansen(2014a),MullainathanandSpiess(2017),AtheyandImbens(2019),andBelloni,Chernozhukov, Chetverikov,Hansen,andKato(2021). 29.2 BigData,HighDimensionality,andMachineLearning Threeinter-relatedconceptsareâbigdataâ,âhighdimensionalityâ,andâmachinelearningâ. Bigdataistypicallyusedtodescribedatasetswhichareunusuallylargeand/orcomplexrelativeto traditionalapplications. Thedefinitionofâlargeâvariesacrossdisciplineandtime, buttypicallyrefers todatasetswithmillionsofobservations.Thesedatasetscanariseineconomicsfromhouseholdcensus data, government administrative records, and supermarket scanner data. Some challenges associated withbigdataarestorage,transmission,andcomputation. HighDimensionalistypicallyusedtodescribedatasetswithanunusuallylargenumberofvariables. Againthedefinitionofâlargeâvariesacrossapplications,buttypicallyreferstohundredsorthousands ofvariables. Inthetheoreticalliteratureâhighdimensionalityâisusedspecificallyforthecontextwhere p>>n,meaningthatthenumberofvariablesp greatlyexceedsthenumberofobservationsn. MachineLearningistypicallyusedtodescribeasetofalgorithmicapproachestostatisticallearn- ing.Themethodsareprimarilyfocusedonpointpredictioninsettingswithunknownstructure.Machine learningmethodsgenerallyallowforlargesamplesizes,largenumberofvariables,andunknownstruc- turalform.Theearlyliteraturewasalgorithmicwithnoassociatedstatisticaltheory.Thiswasfollowedby astatisticalliteratureexaminingthepropertiesofmachinelearningmethods,mostlyprovidingconver- genceratesundersparsityassumptions.Onlyrecentlyhastheliteratureexpandedtoincludeinference. Machine learning embraces a large and diverse set of tools for a variety of settings, including su- pervised learning (prediction rules for Y given high-dimensional X), unsupervisedlearning (uncov- ering structure amongst high-dimensional X), and classification (discrete choice analysis with high- dimensionalpredictors). Inthischapterwefocusonsupervisedlearningasitisanaturalextensionof 910",
    "page": 930,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 911 linearregression. Machinelearningarosefromthecomputerscienceliteratureandtherebyadoptedadistinctsetof labels to describe familar concepts. For example, it speaks of âtrainingâ rather than âestimationâ and âfeaturesâratherthanâregressorsâ. Inthischapter,however,wewillusestandardeconometriclanguage andterminology. Foreconometrics,machinelearningcanbethoughtofasâhighlynonparametricâ. Supposeweare interestedinestimatingtheconditionalmeanm(X)=(cid:69)[Y |X]whentheshapeofm(x)isunknown. A nonparametric analysis typically assumes that X is low-dimensional. In contrast, a machine learning analysismayallowforhundredsoreventhousandsofregressorsinX,anddoesnotrequirepriorinfor- mationaboutwhichregressorsaremostrelevant. Connectionsbetweennonparametricestimation, modelselection, andmachinelearningmethods ariseintuningparameterselectionbycross-validationandevaluationbyout-of-samplepredictionac- curacy.Theseissuesaretakenseriouslyinmachinelearningapplications;frequentlywithmultiplelevels ofhold-outsamples. 29.3 HighDimensionalRegression We are familiar with the linear regression model Y = X (cid:48)Î²+e where X and Î² are pÃ1 vectors1. In conventional regression models we are accustomed to thinking of the number of variables p as small relativetothesamplesizen.Traditionalparametricasymptotictheoryassumesthatp isfixedasnââ whichistypicallyinterpretedasimplyingthatp ismuchsmallerthann.Nonparametricregressionthe- oryassumesthatpââbutatamuchslowerratethann.Thisisinterpretedaspbeingmoderatelylarge butstillmuchsmallerthann. High-dimensionalregressionisusedtodescribethecontextwherep is verylarge,includingthecasewherepislargerthann.Itevenincludesthecasewherepisexponentially largerthann. It may seem shocking to contemplate an application with more regressors than observations. But thesituationarisesinanumberofcontexts. First,inourdiscussionofseriesregression(Chapter20)we describedhowaregressionfunctioncanbeapproximatedbyaninfiniteseriesexpansioninbasistrans- formations of the underlying regressors. Expressed as a linear model this implies a regression model with an infinite number of regressors. Practical models (as discussed in that chapter) use a moderate numberofregressors inestimatedregressionsbecausethis providesa balancebetweenbiasandvari- ance. Thislattermodels,however,arenotthetrueconditionalmean(whichhasaninfinitenumberof regressors)butratheralow-dimensionalbestlinearapproximation. Second, manyeconomicapplica- tionsinvolvealargenumberofbinary,discrete,andcategoricalvariables. Asaturatedregressionmodel convertsalldiscreteandcategoricalvariablesintobinaryvariablesandincludesallinteractions. Such manipulationscanresultinthousandsofregressors",
    "page": 931,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Thislattermodels,however,arenotthetrueconditionalmean(whichhasaninfinitenumberof regressors)butratheralow-dimensionalbestlinearapproximation. Second, manyeconomicapplica- tionsinvolvealargenumberofbinary,discrete,andcategoricalvariables. Asaturatedregressionmodel convertsalldiscreteandcategoricalvariablesintobinaryvariablesandincludesallinteractions. Such manipulationscanresultinthousandsofregressors. Forexample,tenbinaryvariablesfullyinteracted yields1024regressors.Twentybinaryvariablesfullyinteractedyieldsoveronemillionregressors.Third, manycontemporaryâbigâdatasetscontainthousandsofpotentialregressors.Manyofthevariablesmay below-informationbutitisdifficulttoknowaprioriwhicharerelevantandwhichirrelevant. When p >n the least squares estimator Î² (cid:98)ols is not uniquely defined since X (cid:48) X has deficient rank. Furthermore, for p <n but âlargeâ the matrix X (cid:48) X can be near-singular or ill-conditioned so the least squaresestimatorcanbenumericallyunstableandhighvariance. Consequentlyweturntoestimation methods other than least squares. In this chapter we discuss several alternative estimation methods, includingridgeregression,Lasso,elasticnet,regressiontrees,andrandomforests. 1InmostofthistextbookwehavedenotedthedimensionofX ask.Inthischapterwewillinsteaddenotethedimensionof X aspasthisisthecustominthemachinelearningliterature.",
    "page": 931,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 912 29.4 p-norms FordiscussionofridgeandLassoregressionwewillbemakingextensiveuseofthe1-normand2- norm, so it is useful to review the definition of the general p-norm. For a vector a = (a ,...,a ) (cid:48) the 1 k p-norm(pâ¥1)is (cid:195) (cid:33)1/p k (cid:107)a(cid:107) p = (cid:88)(cid:175) (cid:175)a j (cid:175) (cid:175) p . j=1 Importantspecialcasesincludethe1-norm k (cid:107)a(cid:107) 1 = (cid:88)(cid:175) (cid:175)a j (cid:175) (cid:175), j=1 the2-norm (cid:195) (cid:33)1/2 k (cid:107)a(cid:107) = (cid:88) a2 , 2 j j=1 andthesup-norm (cid:107)a(cid:107) â = max (cid:175) (cid:175)a j (cid:175) (cid:175). 1â¤jâ¤k Wealsodefinetheâ0-normâ k (cid:107)a(cid:107) = (cid:88)1(cid:169) a (cid:54)=0 (cid:170) , 0 j j=1 thenumberofnon-zeroelements.Thisisonlyheuristicallylabeledasaânormâ. Thep-normsatisfiesthefollowingadditivityproperty.Ifa=(a ,a )then 0 1 (cid:107)a(cid:107)p=(cid:107)a (cid:107)p+(cid:107)a (cid:107)p . p 0 p 1 p Thefollowinginequalitiesareuseful.TheHÃ¶lderinequalityfor1/p+1/q=1is (cid:175) (cid:175)a (cid:48) b (cid:175) (cid:175) â¤(cid:107)a(cid:107) p (cid:107)b(cid:107) q . (29.1) Thecasep=1andq=âis (cid:175) (cid:175)a (cid:48) b (cid:175) (cid:175) â¤(cid:107)a(cid:107) 1 (cid:107)b(cid:107) â. (29.2) TheMinkowskiinequalityforpâ¥1is (cid:107)a+b(cid:107) â¤(cid:107)a(cid:107) +(cid:107)b(cid:107) . (29.3) p p p Thep-normsforpâ¥1satisfynormmonotonicity.Inparticular (cid:107)a(cid:107) 1 â¥(cid:107)a(cid:107) 2 â¥(cid:107)a(cid:107) â. ApplyingHÃ¶lderâs(29.1)wealsohavetheinequality k (cid:107)a(cid:107) 1 = (cid:88)(cid:175) (cid:175)a j (cid:175) (cid:175) 1(cid:169) a j (cid:54)=0 (cid:170)â¤(cid:107)a(cid:107) 2 (cid:107)a(cid:107)1 0 /2. (29.4) j=1",
    "page": 932,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 913 29.5 RidgeRegression Ridgeregressionisashrinkage-typeestimatorwithsimilarbutdistinctpropertiesfromtheJames- Steinestimator(seeSection28.20). Therearetwocompetingmotivationsforridgeregression. Thetra- ditionalmotivationistoreducethedegreeofcollinearityamongtheregressors.Themodernmotivation (thoughinmathematicsitpre-datestheâtraditionalâmotivation)isregularizationofhigh-dimensional andill-posedinverseproblems.Wediscussbothinturn. As discussed in the previous section, when p is large the least squares coefficient estimate can be (cid:48) numericallyunreliableduetoanill-conditionedX X. AsanumericalimprovementHoerlandKennard (1970)proposedtheridgeregressionestimator Î² (cid:98)ridge =(cid:161) X (cid:48) X +Î»I p (cid:162)â1 X (cid:48) Y whereÎ»>0isashrinkageparameter.Thisestimatorhasthepropertythatitiswell-definedanddoesnot sufferfrommulticollinearityorill-conditioning. Thisevenholdsif p >n! Thatis, theridgeregression estimatoriswell-definedevenwhenthenumberofregressorsexceedsthesamplesize. TheconstantÎ»isatuningparameter.WediscusshowtoselectÎ»below. ToseehowÎ»>0ensuresthattheinverseproblemissolved,usethespectraldecompositiontowrite X (cid:48) X =H (cid:48) DH whereH isorthonormalandD =diag{r ,...,r }isadiagonalmatrixwiththeeigenvalues 1 p r ofX (cid:48) X onthediagonal.SetÎ=Î»I .Wecanwrite j p X (cid:48) X +Î»I =H (cid:48) DH+Î»H (cid:48) H=H (cid:48) (D+Î)H p whichhasstrictlypositiveeigenvaluesr +Î»>0. Thusalleigenvaluesareboundedawayfromzeroso j X (cid:48) X +Î»I isfullrankandwell-conditioned. p (cid:48) Thesecondmotivationisbasedonpenalization.WhenX X isill-conditioneditsinverseisill-posed. Techniquestodealwithill-posedestimatorsarecalledregularizationanddatebacktoTikhonov(1943). Aleadingmethodispenalization. Considerthesumofsquarederrorspenalizedbythesquared2-norm ofthecoefficientvector SSE 2 (cid:161)Î²,Î»(cid:162)=(cid:161) Y âXÎ²(cid:162)(cid:48)(cid:161) Y âXÎ²(cid:162)+Î»Î²(cid:48)Î²=(cid:176) (cid:176)Y âXÎ²(cid:176) (cid:176) 2 2 +Î»(cid:176) (cid:176) Î²(cid:176) (cid:176) 2 2 . TheminimizerofSSE (cid:161)Î²,Î»(cid:162) isaregularizedleastsquaresestimator. 2 ThefirstorderconditionforminimizationofSSE (cid:161)Î²,Î»(cid:162) overÎ²is 2 â2X (cid:48)(cid:161) Y âXÎ²(cid:162)+2Î»Î²=0. (29.5) ThesolutionisÎ² (cid:98)ridge . Thustheregularized(penalized)leastsquaresestimatorequalsridgeregression. Thisshowsthattheridgeregressionestimatorminimizesthesumofsquarederrorssubjecttoapenalty onthesquared2-normoftheregressioncoefficient. Penalizinglargecoefficientvectorskeepsthelatter frombeingtoolargeanderratic. HenceoneinterpretationofÎ»isasapenaltyonthemagnitudeofthe coefficientvector. Minimizationsubjecttoapenaltyhasadualrepresentationasconstrainedminimization.Thelatter is min (cid:161) Y âXÎ²(cid:162)(cid:48)(cid:161) Y âXÎ²(cid:162) Î²(cid:48)Î²â¤Ï forsomeÏ>0.Toseetheconnection,theLagrangianfortheconstrainedproblemis min (cid:161) Y âXÎ²(cid:162)(cid:48)(cid:161) Y âXÎ²(cid:162)+Î»(cid:161)Î²(cid:48)Î²âÏ(cid:162) Î²",
    "page": 933,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 914 whereÎ»isaLagrangemultiplier. Thefirstorderconditionis(29.5)whichisthefirstorderconditionfor thepenalizationproblem.Thisshowsthattheyhavethesamesolution. The practical difference between the penalization and constraint problems is that in the first you specifytheridgeparameterÎ»whileinthesecondyouspecifytheconstraintÏ.Theyareconnectedsince thevaluesofÎ»andÏsatisfytherelationship Y (cid:48) X (cid:161) X (cid:48) X +Î»I (cid:162)â1(cid:161) X (cid:48) X +Î»I (cid:162)â1 X (cid:48) Y =Ï. p p TofindÎ»givenÏitissufficientto(numerically)solvethisequation. b 2 Ridge Path l lOLS Ridge l b 1 b 2+b 2Â£t 1 2 Figure29.1:RidgeRegressionDualMinimizationSolution TovisualizetheconstraintproblemseeFigure29.1whichplotsanexamplein(cid:82)2. Theconstraintset Î²(cid:48)Î²â¤Ï is displayed as the ball about the origin and the contour sets of the sum of squared errors are displayedasellipses. Theleastsquaresestimatoristhecenteroftheellipses,whiletheridgeregression estimatoristhepointonthecirclewherethecontouristangent.Thisshrinkstheleastsquarescoefficient towardsthezerovector. UnliketheSteinestimator,however,itdoesnotshrinkalongthelinesegment connectingleastsquareswiththeorigin,ratheritshrinksalongatrajectorydeterminedbythedegreeof correlationbetweenthevariables. Thistrajectoryisdisplayedwiththedashedlines,markedasâRidge pathâ. ThisisthesequenceofridgeregressioncoefficientsobtainedasÎ»isvariedfrom0toâ. When Î»=0theridgeestimatorequalsleastsquares.ForsmallÎ»theridgeestimatormovesslightlytowardsthe originbyslidingalongtheridgeofthecontourset.AsÎ»increasestheridgeestimatortakesamoredirect path towards the origin. This is unlike the Stein estimator which shrinks the least squares estimator towardstheoriginalongtheconnectinglinesegment. Itisstraightforwardtogeneralizeridgeregressiontoallowdifferentpenaltiesondifferentgroupsof regressors.Takethemodel Y =X (cid:48)Î² +Â·Â·Â·+X (cid:48) Î² +e 1 1 G G andminimizetheSSEsubjecttothepenalty Î» Î²(cid:48)Î² +Â·Â·Â·+Î» Î²(cid:48) Î² . 1 1 1 G G G Thesolutionis Î² (cid:98)ridge =(cid:161) X (cid:48) X +Î(cid:162)â1 X (cid:48) Y where Î=diag (cid:169)Î» I ,...,Î» I (cid:170) . 1 p1 G pG Thisallowssomecoefficientstobepenalizedmore(orless)thanothercoefficients.Thisaddedflexibility comesatthecostofselectingtheridgeparametersÎ»=(Î» ,...,Î» ). OneimportantspecialcaseisÎ» =0, 1 G 1",
    "page": 934,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 915 thus one group of coefficients are not penalized. With G = 2 this partitions the coefficients into two groups:penalizedandnon-penalized. ThemostpopularmethodtoselecttheridgeparameterÎ»iscrossvalidation.Theleave-one-outridge regressionestimator,predictionerrors,andCVcriterionare (cid:195) (cid:33)â1(cid:195) (cid:33) Î² (cid:98)âi (Î»)= (cid:88) X j X j (cid:48)+Î (cid:88) X j Y i j(cid:54)=i j(cid:54)=i e (cid:101)i (Î»)=Y i âX i (cid:48)Î² (cid:98)âi (Î») n CV(Î»)= (cid:88) e (Î»)2. (cid:101)i i=1 TheCV-selectedridgeparameterÎ» (cid:98)minimizesCV(Î»). Thecross-validationridgeestimatoriscalculated usingÎ» (cid:98). InpracticeitmaybetrickytominimizeCV(Î»). TheminimummayoccuratÎ»=0(ridgeequalsleast squares),atÎ»=â(fullshrinkage),ortheremaybemultiplelocalminima.ThescaleoftheminimizingÎ» (cid:48) dependsonthescalingoftheregressorsandinparticularthesingularvaluesofX X.Itcanbeimportant toexploreCV(Î»)forverysmallvaluesofÎ». AsforleastsquaresthereisasimpleformulatocalculatetheCVcriterionforridgeregressionwhich greatlyspeedscomputation. Theorem29.1 Theleave-one-outridgeregressionpredictionerrorsare e (Î»)= (cid:179) 1âX (cid:48)(cid:161) X (cid:48) X +Î(cid:162)â1 X (cid:180)â1 e (Î») (29.6) (cid:101)i i i (cid:98)i wheree (cid:98)i (Î»)=Y i âX i (cid:48)Î² (cid:98)ridge (Î»)aretheridgeregressionresiduals. ForaproofseeExercise29.1 AnalternativemethodforselectionofÎ»istominimizetheMallowscriterionwhichequals C(Î»)= (cid:88) n e (Î»)2+2Ï2tr (cid:179) (cid:161) X (cid:48) X +Î(cid:162)â1(cid:161) X (cid:48) X (cid:162) (cid:180) . (29.7) (cid:98)i (cid:98) i=1 whereÏ2 isthevarianceestimatorfromleastsquaresestimation. Foraderivationof(29.7)seeExercise (cid:98) 29.2. The Mallows-selected ridge parameter Î» (cid:98) minimizesC(Î»). The Mallows-selected ridge estimator is calculated using Î» (cid:98). Li (1986) showed that in the normal regression model the ridge estimator with theMallows-selectedridgeparameterisasymptoticallyequivalenttotheinfeasiblebestridgeparameter intermsofregressionfit. Iamunawareofasimilaroptimalityresultforcross-validated-selectedridge estimation. An important caveat is that the ridge regression estimator is not invariant to rescaling the regres- sorsnorotherlineartransformations. Thereforeitiscommontoapplyridgeregressionafterapplying standardizingtransformationstotheregressors. RidgeregressioncanbeimplementedinRwiththeglmnetcommand. InStata,ridgeregressionis availableinthedownloadablepackagelassopack.",
    "page": 935,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 916 29.6 StatisticalPropertiesofRidgeRegression Undertheassumptionsofthelinearregressionmodelitisstraightforwardtocalculatetheexactbias andvarianceoftheridgeregressionestimator.Takethelinearregressionmodel Y =X (cid:48)Î²+e (cid:69)[e|X]=0. Thebiasoftheridgeestimatoris bias (cid:163)Î² (cid:98)ridge |X (cid:164)=âÎ»(cid:161) X (cid:48) X +Î»I p (cid:162)â1Î². (29.8) Underrandomsamplingitscovariancematrixis var (cid:163)Î² (cid:98)ridge |X (cid:164)=(cid:161) X (cid:48) X +Î»I p (cid:162)â1(cid:161) X (cid:48) DX (cid:162)(cid:161) X (cid:48) X +Î»I p (cid:162)â1 (29.9) where D =diag (cid:169)Ï2(X ),...,Ï2(X ) (cid:170) and Ï2(x)=(cid:69)(cid:163) e2|X =x (cid:164) . For a derivation of (29.8) and (29.9) see 1 n Exercise29.3.Underclusterorserialdependencethecentralcomponentmodifiesinthestandardway. Wecanmeasureestimationefficiencybythemeansquarederror(MSE)matrix mse (cid:163)Î² (cid:98) |X (cid:164)=(cid:69) (cid:104) (cid:161)Î² (cid:98) âÎ²(cid:162)(cid:161)Î² (cid:98) âÎ²(cid:162)(cid:48) |X (cid:105) . DefineÏ2=min xâX Ï2(x)whereX isthesupportofX. Theorem29.2 Inthelinearregressionmodel,if0<Î»<2Ï2/Î²(cid:48)Î², mse (cid:163)Î² (cid:98)ridge |X (cid:164)<mse (cid:163)Î² (cid:98)ols |X (cid:164) . ForaproofseeSection29.23. Theorem29.2showsthattheridgeestimatordominatestheleastsquaresestimatorforÎ»satisfyinga rangeofvalues.ThisholdsregardlessofthedimensionofÎ².Sincetheupperbound2Ï2/Î²(cid:48)Î²isunknown, however,itisuncleariffeasibleridgeregressiondominatesleastsquares.Theupperbounddoesnotgive practicalguidanceforselectionofÎ». Given (29.9) it is straightforward to construct estimators ofV Î²(cid:98) =var (cid:163)Î² (cid:98)ridge |X (cid:164) . I suggest the HC3 analog (cid:195) (cid:33) n V(cid:101)Î²(cid:98) =(cid:161) X (cid:48) X +Î»I p (cid:162)â1 (cid:88) X i X i (cid:48) e (cid:101)i (Î»)2 (cid:161) X (cid:48) X +Î»I p (cid:162)â1 (29.10) i=1 wheree (Î»)aretheridgeregressionpredictionerrors(29.6). Alternativelytheridgeregressionresiduals (cid:101)i e (Î») can be used but it is unclear how to make an appropriate degree-of-freedom correction. Under (cid:98)i clusteringorserialdependencethecentralcomponentofV(cid:101)Î²(cid:98) canbemodifiedasusual. Iftheregressors arehighlysparse(asinasparsedummyvariableregression)itmaybeprudenttousethehomoskedastic estimator V(cid:101) Î²(cid:98) 0=Ï (cid:101) 2(Î») (cid:161) X (cid:48) X +Î»I p (cid:162)â1(cid:161) X (cid:48) X (cid:162)(cid:161) X (cid:48) X +Î»I p (cid:162)â1 withÏ2(Î»)=n â1(cid:80)n e (Î»)2. (cid:101) i=1(cid:101)i Giventhattheridgeestimatorisexplicitlybiasedtherearenaturalconcernsabouthowtointerpret standarderrorscalculatedfromthesecovariancematrixestimators.Confidenceintervalscalculatedthe",
    "page": 936,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 917 usual way will have deficient coverage due to the bias. One answer is to interpret the ridge estimator Î² (cid:98)ridge and its standard errors similarly to those obtained in nonparametric regression. The estimators and confidence intervals are valid for the pseudo-true projections, e.g. Î²â =(cid:161) X (cid:48) X +Î»I (cid:162)â1 X (cid:48) XÎ², not p the coefficients Î² themselves. This is the same interpretation as we use for the projection model and fornonparametricregression. ForasymptoticallyaccurateinferenceonthetruecoefficientsÎ²theridge (cid:112) parameter Î» could be selected to satisfy Î»=o (cid:161) n (cid:162) analogously to an undersmoothing bandwidth in nonparametricregression. 29.7 IllustratingRidgeRegression 0 10 20 30 40 50 60 l VC 5.444 0.444 5.344 0.344 0 10 20 30 40 (a)Cross-ValidationFunction 6.3 4.3 2.3 0.3 8.2 Experience )egaw(gol Least Squares Ridge Regression (b)EstimatesofReturntoExperience Figure29.2:LeastSquaresandRidgeRegressionEstimatesoftheReturntoExperience To illustrate ridge regression we use the CPS dataset with the sample of Asian men with a college education(16yearsofeducationormore)toestimatetheexperienceprofile. Weconsiderafifth-order polynomialinexperiencefortheconditionalmeanoflogwages.Westartbystandardizingtheregressors. We first center experience at its mean, create powers up to order five, and then standardized each to havemeanzeroandvarianceone. Weestimatethepolynomialregressionbyleastsquaresandbyridge regression,thelattershrinkingthefivecoefficientsonexperiencebutnottheintercept. Wecalculatetheridgeparameterbycross-validation.Thecross-validationfunctionisdisplayedin Figure29.2(a)overtheinterval[0,60]. Sincewehavestandardizedtheregressorstohavezeromeanand unit variance the ridge parameter is scaled comparably with sample size, which in this application is n =875. The cross-validation function is uniquely minimized at Î»=19. I use this value of Î» for the followingridgeregressionestimation. Figure29.2(b)displaystheestimatedexperienceprofiles. Leastsquaresisdisplayedbydashesand ridgeregressionbythesolidline. Theridgeregressionestimateissmootherandmorecompelling. The greyshadedregionare95%normalconfidencebandscenteredattheridgeregressionestimate,calcu- latedusingtheHC3covariancematrixestimator(29.10).",
    "page": 937,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 918 29.8 Lasso Intheprevioussectionwelearnedthatridgeregressionminimizesthesumofsquarederrorsplusa 2-normpenaltyonthecoefficientvector. Modelselection(e.g. Mallows)minimizesthesumofsquared errors plus the 0-norm penalty (the number of non-zero coefficients). An intermediate case uses the 1-normpenalty. ThiswasproposedbyTibshirani(1996)andisknownastheLasso(forLeastAbsolute ShrinkageandSelectionOperator).Theleastsquarescriterionwitha1-normpenaltyis p SSE 1 (cid:161)Î²,Î»(cid:162)=(cid:161) Y âXÎ²(cid:162)(cid:48)(cid:161) Y âXÎ²(cid:162)+Î» (cid:88)(cid:175) (cid:175) Î² j (cid:175) (cid:175) =(cid:176) (cid:176)Y âXÎ²(cid:176) (cid:176) 2 2 +Î»(cid:176) (cid:176) Î²(cid:176) (cid:176) 1 . j=1 TheLassoestimatorisitsminimizer Î² (cid:98)Lasso =argminSSE 1 (cid:161)Î²,Î»(cid:162) . Î² Exceptforspecialcasesthesolutionmustbefoundnumerically.Fortunately,computationalalgorithms aresurprisinglysimpleandfast. AnimportantpropertyisthatwhenÎ»>0theLassoestimatoriswell- definedevenifp>n. TheLassominimizationproblemhasthedualconstrainedminimizationproblem Î² (cid:98)Lasso =argminSSE 1 (cid:161)Î²(cid:162) . (cid:107)Î²(cid:107) â¤Ï 1 Toseethatthetwoproblemsarethesameobservethattheconstrainedminimizationproblemhasthe Lagrangian (cid:195) (cid:33) p min (cid:161) Y âXÎ²(cid:162)(cid:48)(cid:161) Y âXÎ²(cid:162)+Î» (cid:88)(cid:175) (cid:175) Î² j (cid:175) (cid:175) âÏ Î² j=1 whichhasfirstorderconditions â2X (cid:48) (cid:161) Y âXÎ²(cid:162)+Î»sgn (cid:161)Î² (cid:162)=0. j j Thisisthesameasthoseforminimizationofthepenalizedcriterion.Thusthesolutionsareidentical. b 2 Lasso Path Lasso OLS b 1 b + b Â£t 1 2 Figure29.3:LassoDualMinimizationSolution The constraint set (cid:169)(cid:176) (cid:176) Î²(cid:176) (cid:176) â¤Ï(cid:170) for the dual problem is a cross-polytope resembling a multi-faceted 1 diamond. Theminimizationproblemin(cid:82)2 isillustratedinFigure29.3. Thesumofsquarederrorcon- toursetsaretheellipseswiththeleastsquaressolutionatthecenter. Theconstraintsetistheshaded polytope.TheLassoestimatoristheintersectionpointbetweentheconstraintsetandthelargestellipse",
    "page": 938,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 919 drawn. InthisexampleithitsavertexoftheconstraintsetandsotheconstrainedestimatorsetsÎ² (cid:98)1 =0. ThisisatypicaloutcomeinLassoestimation.Sinceweareminimizingaquadraticsubjecttoapolytope thesolutiontendstobeatvertices.Thiseliminatesasubsetofthecoefficients. The Lasso path is drawn with the dashed line. This is the sequence of solutions obtained as the constraintsetisvaried.Thesolutionpathhasthepropertythatitisastraightlinefromtheleastsquares estimator to the y-axis (in this example), at which point Î² is set to zero, and then the solution path 2 followsthey-axistotheorigin.Ingeneral,thesolutionpathislinearonsegmentsuntilacoefficienthits zero,atwhichpointthatcoefficientiseliminated. InthisparticularexamplethesolutionpathshowsÎ² 2 increasingwhile Î² decreases. ThuswhileLassois ashrinkage estimatoritdoesnotshrink individual 1 coefficientsmonotonically. ItisinstructivetocompareFigures29.1and29.3whichhavethesamesumofsquarescontours.The ridge estimator is generically an interior solution with no individual coefficient set to zero, while the Lasso estimator typically sets some coefficients equal to zero. However both estimators follow similar solutionpaths,followingtheridgeoftheSSEcriterionratherthantakingadirectpathtowardstheorigin. OnecasewherewecanexplicitlycalculatetheLassoestimatesiswhentheregressorsareorthogonal. SupposethatX (cid:48) X =I .Thenthefirstorderconditionforminimizationsimplifiesto p â2 (cid:161)Î² (cid:98)ols,j âÎ² (cid:98)Lasso,j (cid:162)+Î»sgn (cid:161)Î² (cid:98)Lasso,j (cid:162)=0 whichhastheexplicitsolution ï£± ï£´ ï£² Î² (cid:98)ols,j âÎ»/2 Î² (cid:98)ols,j >Î»/2 Î² (cid:98)Lasso,j = 0 (cid:175) (cid:175) Î² (cid:98)ols,j (cid:175) (cid:175) â¤Î»/2 ï£´ ï£³ Î² (cid:98)ols,j +Î»/2 Î² (cid:98)ols,j <âÎ»/2. ThisLassoestimateisacontinuoustransformationoftheleastsquaresestimate.Forsmallvaluesofthe leastsquaresestimatetheLassoestimateissettozero.ForallothervaluestheLassoestimatemovesthe leastsquaresestimatetowardszerobyÎ»/2. b^ b^ b^ ols ols ols b^ b^ ridge b^ select Lasso (a)Selection (b)Ridge (c)Lasso Figure29.4:TransformationsofleastsquaresestimatesbySelection,Ridge,andLasso It is constructive to contrast this behavior with ridge regression and selection estimation. When X (cid:48) X =I k theridgeestimatorequalsÎ² (cid:98)ridge =(1+Î») â1Î² (cid:98)ols soshrinksthecoefficientstowardszerobya common multiple. A selection estimator (for simplicity consider selection based on a homoskedastic t-test with Ï (cid:98) 2 =1 and critical value c) equals Î² (cid:98)select =1(cid:169)(cid:175) (cid:175) Î² (cid:98)ols,j (cid:175) (cid:175) >c (cid:170)Î² (cid:98)ols,j . Thus the Lasso, ridge, and",
    "page": 939,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 920 selectionestimatesarealltransformationsoftheleastsquarescoefficientestimate. Weillustratethese transformations in Figure 29.4. Panel (a) displays the selection transformation, panel (b) displays the ridgetransformation,andpanel(c)displaystheLassotransformation. TheLassoandridgeestimatorsarecontinuousfunctionswhiletheselectionestimatorisadiscontin- uousfunction.TheLassoandselectionestimatorsarethresholdingfunctions,meaningthatthefunction equals zero for a region about the origin. Thresholding estimators are selection estimators since they equalzerowhentheleastsquaresestimatorissufficientlysmall.TheLassofunctionisaâsoftthreshold- ingâruleasitisacontinuousfunctionwithboundedfirstderivative. Theselectionestimatorisaâhard thresholdingâruleasitisdiscontinuous. Hardthresholdingrulestendtohavehighvarianceduetothe discontinuoustransformation. Consequently,weexpecttheLassotohavereducedvariancerelativeto selectionestimators,permittingoveralllowerMSE. As for ridge regression, Lasso is not invariant to the scaling of the regressors. If you rescale a re- gressorthenthepenaltyhasadifferentmeaning. Consequently, itisimportanttoscaletheregressors appropriatelybeforeapplyingLasso. Itisconventionaltoscaleallthevariablestohavemeanzeroand unitvariance. Lasso is also not invariantto rotations of the regressors. For example, Lasso on (X ,X ) is not the 1 2 sameasLassoon(X âX ,X )despitehavingidenticalleastsquaressolutions.Thisistroublingastypi- 1 2 2 callythereisnodefaultspecification. Applications of Lasso estimation in economics are growing. Belloni, Chernozhukov and Hansen (2014)illustratethemethodusingthreeapplication:(1)theeffectofeminentdomainonhousingprices inaninstrumentalvariablesframework,(2)are-examinationoftheeffectofabortiononcrimeusingthe frameworkofDonohueandLevitt(2001),(3)are-examinationofthetheeffectofdemocracyongrowth usingtheframeworkofAcemoglu,JohnsonandRobinson(2001). MullainathanandSpiess(2017)illus- tratemachinelearningusingapredictionmodelforhousingpricesusingcharacteristics. Oster(2018) useshouseholdscannerdatatomeasuretheeffectofadiabetesdiagnosisonfoodpurchases. 29.9 LassoPenaltySelection CriticallyimportantforLassoestimationisthepenaltyÎ». ForÎ»closetozerotheestimatesareclose to least squares. As Î» increases the number of selected variables falls. Picking Î» induces a trade-off betweencomplexityandparsimony. It is common in the statistics literature to see coefficients plotted as a function of Î». This can be usedtovisualizethetrade-offbetweenparsimonyandvariableinclusion. Itdoesnot,however,provide astatisticalruleforselection. The most common selection method is minimization of K-fold cross validation (see Section 28.9). Leave-one-outCVisnottypicallyusedasitiscomputationallyexpensive.Manyprogramssetthedefault numberoffoldsasK =10,thoughsomeauthorsuseK =5,whileothersrecommendK =20. K-fold cross validation is an estimator of out-of-sample mean squared forecast error",
    "page": 940,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". This can be usedtovisualizethetrade-offbetweenparsimonyandvariableinclusion. Itdoesnot,however,provide astatisticalruleforselection. The most common selection method is minimization of K-fold cross validation (see Section 28.9). Leave-one-outCVisnottypicallyusedasitiscomputationallyexpensive.Manyprogramssetthedefault numberoffoldsasK =10,thoughsomeauthorsuseK =5,whileothersrecommendK =20. K-fold cross validation is an estimator of out-of-sample mean squared forecast error. Therefore penalty selection by minimization of the K-fold criterion is aimed to select models with good forecast accuracy,butnotnecessarilyforotherpurposessuchasaccurateinference. Conventionally,thevalueofÎ»selectedbyCVisthevaluewhichminimizestheCVcriterion.Another popular choice is called the â1seâ rule, which is the Î» which yields the most parsimonious model for Î» values within one standard error of the minimum. The idea is to select a model similar but more parsimoniousthantheCV-minimizingchoice. K-fold cross validation is implemented by first randomly dividing the observations into K groups. ConsequentlytheCVcriterionissensitivetotherandomsorting.Itisthereforeprudenttosettherandom numberseedforreplicabilityandtoassesssensitivityacrossinitialseeds. Ingeneral,selectingalarger",
    "page": 940,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 921 valueforK reducesthissensitivity. AsymptoticconsistencyofCVselectionforLassoestimationhasbeendemonstratedbyChetverikov, Liao,andChernozhukov(2020). 29.10 LassoComputation The constraint representation of Lasso is minimization of a quadratic subject to linear inequality constraints. This can be implemented by standard quadratic programming which is computationally simple.Forevaluationofthecross-validationfunction,however,itisusefultocomputetheentireLasso path. ForthisacomputationallyappropriatemethodisthemodifiedLARSalgorithm. (LARSstandsfor leastangleregression.) TheLARSalgorithmproducesapathofcoefficientsstartingattheoriginandendingatleastsquares. ThesequencecorrespondstothesequenceofconstraintsÏwhichcanbecalculatedbytheabsolutesum ofthecoefficients,butneitherthesevalues(norÎ»)areusedbythealgorithm.Thestepsareasfollows. 1. Startwithallcoefficientsequaltozero. 2. FindX mostcorrelatedwithY. j 3. IncreaseÎ² inthedirectionofcorrelation. j (a) Computeresidualsalongtheway. (b) StopwhensomeotherX(cid:96)hasthesamecorrelationwiththeresidualasX j . (c) If a non-zero coefficient hits zero, drop from the active set of variables and recompute the jointleastsquaresdirection. 4. Increase(Î² j ,Î² (cid:96))intheirjointleastsquaresdirectionuntilsomeotherX m hasthesamecorrelation withtheresidual. 5. Repeatuntilallpredictorsareinmodel. ThisalgorithmproducestheLassopath. Theequalitybetweenthetwoisnotimmediatelyapparent butthedemonstrationistedioussoisnotshownhere. ThemostpopularcomputationalimplementationforLassoistheRglmnetcommandintheglmnet package. PenaltyselectionbyK-foldcrossvalidationisimplmentedbythecv.glmnetcommand. The latterbydefaultreportsthepenaltyselectedbytheâ1seârule,andreportstheminimizingÎ»aslambda.min. ThedefaultnumberoffoldsisK =10. InStata,Lassoisavailablewiththecommandlasso. Bydefaultitselectsthepenaltybyminimizing theK-foldcrossvalidationcriterionwithK =10folds.Manyoptionsareavailable,includingconstraining the estimator to penalize only a subset of the coefficients. An alternative downloadable package with manyoptionsislassopack. 29.11 AsymptoticTheoryfortheLasso The current distribution theory of Lasso estimation is challenging and mostly focused on conver- gencerates. Theresultsarederivedundersparsityorapproximatesparsityconditions,theformerre- stricting the number of non-zero coefficients, and the second restricting how a sparse model can ap- proximateageneralparameterization. InthissectionweprovideabasicconvergenceratefortheLasso estimatorÎ² (cid:98)Lasso underamildmomentboundontheerrorsandasparsityassumptiononthecoefficients.",
    "page": 941,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 922 Themodelisthehigh-dimensionalprojectionframework: Y =X (cid:48)Î²+e (cid:69)[Xe]=0 (29.11) whereX ispÃ1withp>>n.ThetruecoefficientvectorÎ²isassumedtobesparseinthesensethatonly asubsetoftheelementsofÎ²arenon-zero. ForsomeÎ» letÎ² (cid:98)Lasso betheLassoestimatorwhichminimizesSSE 1 (cid:161)Î²,Î»(cid:162) . Definethescaleddesign matrixQ =n â1X (cid:48) X andtheregressionfit n (cid:161)Î² (cid:98)Lasso âÎ²(cid:162)(cid:48) Q n (cid:161)Î² (cid:98)Lasso âÎ²(cid:162)= n 1 (cid:88) n (cid:161) X i (cid:48)(cid:161)Î² (cid:98)Lasso âÎ²(cid:162)(cid:162)2 . (29.12) i=1 Weprovideaboundontheregressionfit(29.12),the1-normfit (cid:176) (cid:176) Î² (cid:98) âÎ²(cid:176) (cid:176) andthe2-normfit (cid:176) (cid:176) Î² (cid:98) âÎ²(cid:176) (cid:176) . 1 2 Theregressionfit(29.12)issimilartomeasuresoffitwehaveusedbefore,includingtheintegrated squarederror(20.22)inseriesregressionandtheregressionfitR (K)(equation(28.17))forevaluationof n modelselectionoptimality. Whenp>nthematrixQ issingular.Thetheory,however,requiresthatitnotbeâtoosingularâ.What n isrequiredisnon-singularityofallsub-matricesofQ correspondingtothenon-zerocoefficientsand n notâtoomanyâofthezerocoefficients.Thespecificrequirementisrathertechnical.PartitionÎ²=(Î² ,Î² ) 0 1 wheretheelementsofÎ² areall0andtheelementsofÎ² arenon-zero. (Thispartitionisatheoretical 0 1 deviceandunknowntotheeconometrician.) Letb =(b ,b )â(cid:82)p bepartitionedconformably. Define 0 1 theconeB =(cid:169) bâ(cid:82)p:(cid:107)b (cid:107) â¤3(cid:107)b (cid:107) (cid:170) .Thisisthesetofvectorsbsuchthatthesub-vectorb isnotâtoo 0 1 1 1 0 largeârelativetothesub-vectorb . 1 Assumption29.1 RestrictedEigenvalueCondition(REC) (cid:48) bQ b min n â¥c2>0. (29.13) (cid:48) bâB b b To gain some understanding of what the REC means, notice that if the minimum (29.13) is taken withoutrestrictionover(cid:82)p itequalsthesmallesteigenvalueofQ . Thuswhenp<n asufficientcondi- n tionfortheRECisÎ» (cid:161) Q (cid:162)â¥c2>0. Instead,theminimumin(29.13)iscalculatedonlyoverthecone min n B. In this sense this calculation is similar to a ârestricted eigenvalueâ which is the source of its name. TheRECtakesavarietyofformsinthetheoreticalliterautre;Assumption29.1isnottheweakestbutis themostintuitive.Assumption29.1hasbeenshowntoholdunderprimitiveconditionsonX,including normalityandboundedness.SeeLemmas1and2ofBelloniandChernozhukov(2011).",
    "page": 942,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 923 (cid:104) (cid:105) Theorem29.3 Supposemodel(29.11)holds,Ï=max 1â¤jâ¤p (cid:69) X j 2e2 <â,and Assumption29.1holds.TakeanyÎ±>0,andforsomeC â¥16Ï/Î±set (cid:113) Î»=C nlogp. (29.14) ThenthereisD<âsuchthatwithprobabilityexceeding1âÎ±, (cid:161)Î² (cid:98)Lasso âÎ²(cid:162)(cid:48) Q n (cid:161)Î² (cid:98)Lasso âÎ²(cid:162)â¤D (cid:176) (cid:176) Î²(cid:176) (cid:176) 0 lo n gp , (29.15) (cid:115) (cid:176) (cid:176) Î² (cid:98)Lasso âÎ²(cid:176) (cid:176) 1 â¤D (cid:176) (cid:176) Î²(cid:176) (cid:176) 0 lo n gp , (29.16) and (cid:115) (cid:176) (cid:176) Î² (cid:98)Lasso âÎ²(cid:176) (cid:176) 2 â¤D (cid:176) (cid:176) Î²(cid:176) (cid:176) 1 0 /2 lo n gp . (29.17) ForaproofseeSection29.23. Theorem 29.3 presents three convergence rates for the Lasso coefficient estimator Î² (cid:98)Lasso , for the regressionfit(29.12),the1-norm,andthe2-norm. Theseratesdependonthenumberofnon-zeroco- efficients (cid:176) (cid:176) Î²(cid:176) (cid:176) ,thenumberofvariablesp,andthesamplesizen. Supposethat (cid:176) (cid:176) Î²(cid:176) (cid:176) isfixed. Thenthe 0 0 bounds(29.15)-(29.17)areo(1)iflogp=o(n).ThisshowsthatLassoestimationisconsistentevenforan exponentiallylargenumberofvariables. Therates,however,allowthenumberofnon-zerocoefficients (cid:176) (cid:176) Î²(cid:176) (cid:176) toincreasewithnatthecostofslowingtheallowablerateofincreaseofp. 0 WestatedearlierinthissectionthatwehadassumedthatthecoefficientvectorÎ²issparse,meaning that only a subset of the elements of Î² are non-zero. This appears in the theory through the 0-norm (cid:176) (cid:176) Î²(cid:176) (cid:176) , the number of non-zero coefficients. If all elements of Î² are non-zero then (cid:176) (cid:176) Î²(cid:176) (cid:176) = p and the 0 0 (cid:161) (cid:162) bound(29.15)isO plogp/n ,whichissimilartotheboundforseriesregressionobtainedinTheorem 20.7,equation(20.24).Instead,theassumptionofsparsityenablestheLassoestimatortoachieveamuch improvedrateofconvergence. ThekeytoestablishingratessuchasTheorem29.3aremaximalinequalitiesappliedto 1X (cid:48) e. Our n proofusesthemaximalMarcinkiewicz-Zygmundinequality(B.47). AnimportantlimitationoftheorysuchasTheorem29.3isthesparsityassumption. Itisuntestable and counter-intuitive. Researchers in this field frequently use the phrase âimposing sparsityâ as if it issomethingunderthecontrolofthetheoristâbutsparsityisonlyapropertyofthetruecoefficients. Fortunatelytherearealternativestothesparsityassumptionaswediscussinthefollowingsection. 29.12 ApproximateSparsity Thetheoryoftheprevioussectionusedthestrongassumptionthatthetrueregressionissparse:only asubsetofthecoefficientsarenon-zero,andtheconvergenceratedependsonthecardinalityofthenon- zerocoefficients.AsshownbyBelloniandChernozhukov(2011)andBelloni,Chernozhukov,andHansen (2013),strictsparsityisnotrequired. Instead,similarconvergenceratesholdundertheassumptionof approximatesparsity. Onceagaintakethehigh-dimensionalregressionmodel(29.11)butdonotassumethatÎ²necessarily hasasparsestructure",
    "page": 943,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Instead,similarconvergenceratesholdundertheassumptionof approximatesparsity. Onceagaintakethehigh-dimensionalregressionmodel(29.11)butdonotassumethatÎ²necessarily hasasparsestructure. Instead,viewsparsemodelsasapproximations. ForeachintegerK >0letB = K",
    "page": 943,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 924 (cid:169) bâ(cid:82)p:(cid:107)b(cid:107) =K (cid:170) bethesetofvectorswithK non-zeroelements.Definethebestsparseapproximation 0 Î² K =argmin (cid:176) (cid:176)Q n (cid:161)Î²âb (cid:162)(cid:176) (cid:176)â bâBK withassociatedapproximationerror r K =(cid:176) (cid:176)Q n (cid:161)Î²âÎ² K (cid:162)(cid:176) (cid:176)â . Assumption29.2 ApproximateSparsity.Forsomes>1,r =O(K âs). K Assumption 29.2 states that the approximation error of the sparse approximation decreases at a powerlawrate. InSection20.8andTheorem20.1welearnedthatapproximationssimilartoAssump- tion29.2holdforpolynomialandsplineseriesregressionswithboundedregressorsifthetrueregression functionhasauniformsth derivative. Theprimarydifferenceisthatseriesregressionrequiresthatthe econometrician knows how to order the regressors while Assumption 29.2 does not impose a specific ordering.InthissenseAssumption29.2isweakerthantheapproximationconditionsofSection20.8. Weneedtomodifytherestrictedeigenvalueconditiontobesatisfiedalongthissequenceofapprox- imations. LetB =(cid:169) b=(b ,b )â(cid:82)p:(cid:107)b(cid:107) =K,b =0, (cid:107)b (cid:107) â¤3(cid:107)b (cid:107) (cid:170) . ThisistheanalogoftheconeB K 0 1 0 0 0 1 1 1 asdefinedforAssumption29.1,butfortheKth sparseapproximation. ExaminingtheproofofTheorem29.3,replacingÎ²withÎ² K (whichsatisfies (cid:176) (cid:176) Î² K (cid:176) (cid:176) 0 =K),andreplacing e withe+X (cid:48)(cid:161)Î²âÎ² (cid:162) ,weseethattheonlysubstantivedifferenceistheneedtoverifyr â¤Î»/4n,which K K holdsforÎ»satisfying(29.14)andK =A (cid:161) n/logp (cid:162)1/2s underAssumption29.2. Insertingthesevaluesinto Theorem29.3weobtainthefollowingrates. Theorem29.4 Suppose model (29.11) holds, Assumption 29.2 holds, and (cid:104) (cid:105) Ï = max 1â¤jâ¤p (cid:69) X j 2e2 < â. Take any Î± > 0, set Î» as (29.14), and K = (cid:161) (cid:162)1/2s A n/logp for Asufficientlylarge.Supposethatforthissequence (cid:48) bQ b min n â¥c2>0. (cid:48) bâBK b b ThenthereisD<âsuchthatwithprobabilityexceeding1âÎ±, (cid:161)Î² (cid:98)Lasso âÎ² K (cid:162)(cid:48) Q n (cid:161)Î² (cid:98)Lasso âÎ² K (cid:162)â¤D (cid:181) logp (cid:182)1â 2 1 s , n (cid:181) (cid:182)1â1 (cid:176) (cid:176) Î² (cid:98)Lasso âÎ² K (cid:176) (cid:176) 1 â¤D lo n gp 2 2s , and (cid:181) (cid:182)1â1 (cid:176) (cid:176) Î² (cid:98)Lasso âÎ² K (cid:176) (cid:176) 2 â¤D lo n gp 2 4s . Theorem29.4providesarateofconvergencefortheapproximatesparseprojectionmodel. Therate ofconvergenceisincreasingins. Thisisbecauseass increasestheregressionfunctioncanbeapproxi- matedwithasmallernumberK ofnon-zerocoefficients. Theorem29.4showsthatexactsparsityisnot",
    "page": 944,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 925 requiredforLassoestimation,ratherwhatisneededisapproximationpropertiessimilartothoseused inseriesregressiontheory. Theapproximatesparsityconditionfailswhentheregressorscannotbeeasilyordered. Suppose,for example,thatQ =I andallelementsofÎ²havecommonvalueÎ´. Inthiscaser =Î´whichdoesnot n p K decreasewithK.InthiscontextAssumption29.2doesnotholdandTheorem29.4doesnotapply. 29.13 ElasticNet ThedifferencebetweenLassoandridgeregressionisthattheLassousesthe1-normpenaltywhile ridgeusesthe2-normpenalty. Sincebothprocedureshaveadvantagesitseemsreasonablethatfurther improvementsmaybeobtainedbyacompromise.Takingaweightedaverageofthepenaltiesweobtain theElasticNetcriterion SSE (cid:161)Î²,Î»,Î±(cid:162)=(cid:161) Y âXÎ²(cid:162)(cid:48)(cid:161) Y âXÎ²(cid:162)+Î» (cid:179) Î±(cid:176) (cid:176) Î²(cid:176) (cid:176) 2+(1âÎ±) (cid:176) (cid:176) Î²(cid:176) (cid:176) (cid:180) 2 1 withweight0â¤Î±â¤1.ThisincludesLasso(Î±=0)andridgeregression(Î±=1)asspecialcases.Forsmall butpositiveÎ±theconstraintsetsaresimilartoâroundedâversionsoftheLassoconstraintsets. Typicallytheparameters(Î±,Î»)areselectedbyjointminimizationoftheK-foldcross-validationcri- terion.Sincetheelasticnetpenaltyislinear-quadraticthesolutioniscomputationallysimilartoLasso. Elastic net can be implemented in R with the glmnet command. In Stata use elasticnet or the downloadablepackagelassopack. 29.14 Post-Lasso TheLassoestimatorÎ² (cid:98)Lasso simultaneouslyselectsvariablesandshrinkscoefficients. Shrinkagein- troducesbiasintoestimation. ThisbiascanbereducedbyapplyingleastsquaresafterLassoselection. ThisisknownasthePost-Lassoestimator. Theproceduretakestwosteps. First, estimatethemodelY = X (cid:48)Î²+e byLasso. Let X denotethe S variablesin X whichhavenon-zerocoefficientsinÎ² (cid:98)Lasso . LetÎ² S denotethecorrespondingcoefficients in Î². Second, the coefficient Î² S is estimated by least squares, thus Î² (cid:98)S =(cid:161) X (cid:48) S X S (cid:162)â1(cid:161) X (cid:48) S Y (cid:162) . This is the Post-Lassoleastsquaresestimator. BelloniandChernozhukov(2013)provideconditionsunderwhich thepost-LassoestimatorhasthesameconvergenceratesastheLassoestimator. Thepost-Lassoisahardthresholdingorpost-model-selectionestimator. Indeed,whentheregres- sors are orthogonal the post-Lasso estimator precisely equals a selection estimator, transforming the leastsquarescoefficientestimatesusingthehardthresholdfunctiondisplayedinFigure29.4(a).Conse- quently,thepost-LassoestimatorinheritsthestatisticalpropertiesofPMSestimators(seeSections28.16 and28.17),includinghighvarianceandnon-standarddistributions. 29.15 RegressionTrees Regression trees were introduced by Breiman, Friedman, Olshen, and Stone (1984), and are also knownbytheacronymCARTforClassificationandRegressionTrees.Aregressiontreeisanonparamet- ricregressionusingalargenumberofstepfunctions. Theideaisthatwithasufficientlylargenumber ofsplitpoints,astepfunctioncanapproximateanyfunction",
    "page": 945,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". 29.15 RegressionTrees Regression trees were introduced by Breiman, Friedman, Olshen, and Stone (1984), and are also knownbytheacronymCARTforClassificationandRegressionTrees.Aregressiontreeisanonparamet- ricregressionusingalargenumberofstepfunctions. Theideaisthatwithasufficientlylargenumber ofsplitpoints,astepfunctioncanapproximateanyfunction. Regressiontreesmaybeespeciallyuseful whenthereareacombinationofcontinuousanddiscreteregressorssothattraditionalkernelandseries methodsarechallengingtoimplement.",
    "page": 945,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 926 Regression trees can be thought of as a 0th-order spline with free knots. They are also similar to thresholdregressionwithinterceptsonly(noslopecoefficients)andaverylargenumberofthresholds. The goal is to estimate m(x) = (cid:69)[Y |X =x] for scalar Y and vector X. The elements of X can be continuous,binary,orordinal.Ifaregressoriscategoricalisshouldbefirsttransformedtoasetofbinary variables. Theliteratureonregressiontreeshasdevelopedsomecolorfullanguagetodescribethetoolsbased onthemetaphorofalivingtree. 1. Asubsampleisabranch. 2. Terminalbranchesarenodesorleaves. 3. Increasingthenumberofbranchesisgrowingatree. 4. Decreasingthenumberofbranchesispruningatree. Thebasicalgorithmstartswithasinglebranch.Growalargetreebysequentiallysplittingthebranches. Thenprunebackusinganinformationcriterion. Thegoalofthegrowthstageistodeveloparichdata- determinedtreewhichhassmallestimationbias. Pruningbackisanapplicationofbackwardstepwise regressionwiththegoalofreducingover-parameterizationandestimationvariance. Theregressiontreealgorithmmakesextensiveuseoftheregressionsamplesplitalgorithm. Thisis asimplifiedversionofthresholdregression(Section23.7).ThemethodusesNLLStoestimatethemodel Y =Âµ 1(cid:169) X â¤Î³(cid:170)+Âµ 1(cid:169) X >Î³(cid:170)+e 1 d 2 d (cid:69)[e|X]=0 withtheindexdandparameterÎ³asfreeparameters2.TheNLLScriterionisminimizedover(d,Î³)bygrid search. Theestimatesproduceasamplesplit. Theregressiontreealgorithmappliessequentialsample splittingtomakealargenumberofsplits,eachonasub-sampleofobservations. Thebasicgrowthalgorithmisasfollows.Theobservationsare{Y ,X ,...,X :i =1,...,n}. i 1i ki 1. SelectaminimumnodesizeN (say5).Thisistheminimalnumberofobservationsoneachleaf. min 2. Sequentiallyapplyregressionsamplesplits. (a) Applytheregressionsamplesplitalgorithmtospliteachbranchintotwosub-branches,each withsizeatleastN . min (b) Oneachsub-branchb: i. TakethesamplemeanÂµ ofY forobservationsonthesub-branch. (cid:98)b i ii. Thisistheestimatoroftheregressionfunctiononthissub-branch. iii. Theresidualsonthesub-brancharee =Y âÂµ . (cid:98)i i (cid:98)b (c) Selectthebranchwhosesplitmostreducesthesumofsquarederrors. (d) Splitthisbranchintotwobranches.Makenoothersplit. (e) Repeat(a)-(d)untileachbranchcannotbefurthersplit. Theterminal(unsplit)branchesare theleaves. 2IfX â{0,1}isbinarythenÎ³=0isfixed. d",
    "page": 946,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 927 Afterthegrowthalgorithmhasbeenrun,theestimatedregressionisamulti-dimensionalstepfunc- tionwithalargenumberofbranchesandleaves. Thebasicpruningalgorithmisasfollows. 1. DefinetheMallows-typeinformationcriterion n C = (cid:88) e2+Î±N (cid:98)i i=1 whereN isthenumberofleavesandÎ±isapenaltyparameter. 2. ComputethecriterionC forthecurrenttree. 3. Usebackwardstepwiseregressiontoreducethenumberofleaves: (a) IdentifytheleafwhoseremovalmostdecreasesC. (b) Prune(remove)thisleaf. (c) IfthereisnoleafwhoseremovaldecreasesC thenstoppruning. (d) Otherwise,repeat(a)-(c). ThepenaltyparameterÎ±istypicallyselectedbyK-foldcross-validation. TheMallows-typecriterion isusedbecauseofitssimplicity,buttomyknowledgedoesnothaveatheoreticalfoundationforregres- siontreepenaltyselection. The advantage of regression trees is that they provide a highly flexible nonparametric approxima- tion.Theirmainuseisprediction.Onedisadvantageofregressiontreesisthattheresultsaredifficultto interpretastherearenoregressioncoefficients. Anotherdisadvantageisthatthefittedregressionm(x) (cid:98) isadiscretestepfunction,whichmaybeacrudeapproximationwhenm(x)iscontinuousandsmooth. Toobtainagoodapproximationaregressiontreemayrequireahighnumberofleaveswhichcanresult inanon-parsimoniousmodelwithhighestimationvariance. Thesamplingdistributionofregressiontreesisdifficulttoderive,inpartbecauseofthestrongcor- relation between the placement of the sample splits and the estimated means. This is similar to the problemsassociatedwithpost-model-selection.(SeeSections28.16and28.17.)Amethodwhichbreaks thisdependenceisthehonesttreeproposalofWagerandAther(2018).Splitthesampleintotwohalves A andB. Usethe A sampletoplacethesplitsandtheB sampletodowithin-leafestimation. Whilere- ducingestimationefficiency(thesampleiseffectivelyhalved)theestimatedconditionalmeanwillnot bedistortedbythecorrelationbetweentheestimatedsplitsandmeans. RegressiontreesalgorithmsareimplementedintheRpackagerpart. 29.16 Bagging Bagging(bootstrapaggregating)wasintroducedbyBreiman(1996)asamethodtoreducethevari- ance of a predictor. We focus here on its use for estimation of a conditional mean. The basic idea is simple. YougeneratealargenumberB ofbootstrapsamples, estimateyourregressionmodeloneach bootstrapsample,andtaketheaverageofthebootstrapregressionestimates.Themeanofthebootstrap estimatesisthebaggingestimatoroftheconditionalmean. Baggingisbelievedtobeusefulwhentheconditionalmeanestimatorhaslowbiasbuthighvariance. Thisoccursforhardthresholdingestimatorssuchasregressiontrees,modelselection,andpost-Lasso. Bagging is a smoothing operation which reduces variance. The resulting bagging estimator can have",
    "page": 947,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 928 lowerMSEasaresult.Baggingisbelievedtobelessusefulforestimatorswithhighbias,asbaggingmay exaggeratethebias. We first describe the estimation algorithm. Let m(x) = (cid:69)[Y |X =x] be the conditional mean and â m(x)anestimatorsuchasaregressiontree.Letm (x)bethesameestimatorconstructedonabootstrap (cid:98) (cid:98)b sample.Thebaggingestimatorofm(x)is m (x)= 1 (cid:88) b m â (x). (cid:98)bag B (cid:98)b B=1 AsB increasesthisconvergesinbootstrapprobabilitytotheidealbaggingestimator(cid:69)â [m â (x)]. (cid:98) TounderstandthebaggingprocessweuseanexamplefromBÃ¼hlmannandYu(2002). AsinSection 28.16supposethatÎ¸ (cid:98) â¼N(Î¸,1)andconsideraselectionestimatorbasedona5%test,Î¸ (cid:98)pms =Î¸ (cid:98) 1(cid:169)Î¸ (cid:98) 2â¥c (cid:170)= h(Î¸ (cid:98))wherec =3.84andh(t)=t 1(cid:169) t2â¥c (cid:170) . ApplyingTheorem28.17,equation(28.38),wecancalculate that(cid:69)(cid:163)Î¸ (cid:98)pms (cid:164)=g(Î¸)whereg(t)=t (cid:161) 1âF 3 (cid:161) c,t2(cid:162)(cid:162) andF r (x,Î»)isthenon-centralchi-squaredistribution function3.Thisrepresentationisnotintuitivesoitisbettertovisualizeitsgraph.Thefunctionsh(t)and g(t)areplottedinFigure29.5(a).Theselectionfunctionh(t)isidenticaltotheplotinFigure29.4(a).The functiong(t)isasmoothedversionofh(t),everywherecontinuousanddifferentiable. SupposethatthebaggingestimatorisconstructedusingtheparametricbootstrapÎ¸ (cid:98) ââ¼N(Î¸ (cid:98),1). The (cid:104) (cid:105) bootstrapselectionestimatorisÎ¸ (cid:98) p â ms =h (cid:161)Î¸ (cid:98) â(cid:162) . ItfollowsthatthebaggingestimatorisÎ¸ (cid:98)bag =(cid:69)â Î¸ (cid:98) p â ms = (cid:69)â(cid:163) h (cid:161)Î¸ (cid:98) â(cid:162)(cid:164)=g(Î¸ (cid:98)). ThuswhiletheselectionestimatorÎ¸ (cid:98)pms =h(Î¸ (cid:98))isthehardthresholdtransformation h(t) applied to Î¸ (cid:98), the bagging estimator Î¸ (cid:98)bag =g(Î¸ (cid:98)) is the smoothed transformation g(t) applied to Î¸ (cid:98). ThusFigure29.5(a)displayshowÎ¸ (cid:98)pms andÎ¸ (cid:98)bag aretransformationsofÎ¸ (cid:98), withthebaggingestimatora smoothtransformationratherthanahardthresholdtransformation. q^ () pms ht () gt q^ (a)SelectionandBaggingTransformations 5.2 0.2 5.1 0.1 5.0 0.0 MSE (q^ pms ) MSE (q^ bag ) 0 1 2 3 4 5 q (b)MSEofSelectionandBaggingEstimators Figure29.5:BaggingandSelection BÃ¼hlmann and Yu (2002) argue that smooth transformations generally have lower variances than hardthresholdtransformations, andthusarguethatÎ¸ (cid:98)bag willgenerallyhavelowervariancethanÎ¸ (cid:98)pms . Thisisdifficulttodemonstrateasageneralprinciplebutseemssatisfiedinspecificexamples. Forour 3BÃ¼hlmannandYu(2002),Proposition2.2,provideanalternativerepresentationusingthenormalcdfandpdffunctions.",
    "page": 948,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 929 examplewedisplay4inFigure29.5(b)theMSEoftheselectionestimatorÎ¸ (cid:98)pms anditsbaggedverionÎ¸ (cid:98)bag as functions of Î¸. As we learned in Section 28.16, the MSE of the selection estimator Î¸ (cid:98)pms is a hump- shapedfunctionofÎ¸. InFigure29.5(b)wecanseethattheMSEofthebaggedestimatorisconsiderably reducedrelativetotheselectionestimatorformostvaluesofÎ¸. ThereductioninMSEisgreatestinthe regionwheretheMSEofÎ¸ (cid:98)pms isgreatest. BÃ¼hlmannandYu(2002)alsocalculatethatmostofthisMSE reductionisduetoareductioninthevarianceofthebaggedestimator. Themostcommonapplicationofbaggingistoregressiontrees.Treeshaveasimilarstructuretoour exampleselectionestimatorÎ¸ (cid:98)pms andarethereforeexpectedtohaveasimilarreductioninestimation varianceandMSErelativetoregressiontreeestimation. Oneconvenientby-productofbaggingisaCVproxycalledtheout-of-bag(OOB)predictionerror.A typicalnonparametricbootstrapsamplecontainsabout63%oftheoriginalobservations,meaningthat about37%oftheobservationsarenotpresentinthatbootstrapsample. Thereforeabootstrapestimate of the regression function m(x) constructed on this bootstrap sample has âleft outâ about 37% of the observations, meaning that valid prediction errors can be calculated on these âleft outâ observations. Alternatively, for any given observation i, out of the B bootstrap samples about 0.63ÃB samples will containthisobservationandabout0.37ÃBsampleswillnotincludethisobservation.Thebaggingâleave i outâestimatorm (cid:98)âi (x)ofm(x)isobtainedbyaveragingjustthissecondset(the37%whichexcludethe observation).Theout-of-bagerrorise (cid:101)i =Y i âm (cid:98)âi (X i ).Theout-of-bagCVcriterionis (cid:80)n i=1 e (cid:101)i 2.Thiscan beusedasanestimatorofout-of-sampleMSFEandcanbeusedtocompareandselectmodels. Wager,Hastie,andEfron(2014)proposeestimatorsofV (x)=var (cid:163) m (x) (cid:164) .LetN denotethenum- n (cid:98)bag ib beroftimesobservationi appearsinthebootstrapsampleb andN =B â1(cid:80)B N . Theinfinitesimal i b=1 ib jackknifeestimatorofV is n (cid:195) (cid:33)2 V(cid:98)n (x)= (cid:88) n cov â(cid:161) N i ,m (cid:98)bag (x) (cid:162)2= (cid:88) n B 1 (cid:88) B (N ib âN i ) (cid:161) m (cid:98)b â (x)âm (cid:98)bag (x) (cid:162) . (29.18) i=1 i=1 b=1 ThisvarianceestimatorisbasedonEfron(2014). WhileBreimanâsproposalandmostapplicationsofbaggingareimplementedusingthenonparamet- ricbootstrap,analternativeistousesubsampling. Asubsamplingestimatorisbasedonsamplingwith- outreplacementratherthanwithreplacementasdoneintheconventionalbootstrap. Samplesofsize s <n are drawn from the original sample and used to construct the estimator m â (x). Otherwise the (cid:98)b methodsareidentical.Itturnsoutthatitissomewhateasiertodevelopadistributiontheoryforbagging undersubsampling,soasubsamplingassumptionisfrequentlyemployedintheoreticaltreatments. 29.17 RandomForests Randomforests, introducedbyBreiman(2001), areamodificationofbaggedregressiontrees",
    "page": 949,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Samplesofsize s <n are drawn from the original sample and used to construct the estimator m â (x). Otherwise the (cid:98)b methodsareidentical.Itturnsoutthatitissomewhateasiertodevelopadistributiontheoryforbagging undersubsampling,soasubsamplingassumptionisfrequentlyemployedintheoreticaltreatments. 29.17 RandomForests Randomforests, introducedbyBreiman(2001), areamodificationofbaggedregressiontrees. The modificationisdesignedtoreduceestimationvariance.Randomforestsarepopularinmachinelearning applicationsandhaveeffectivelydisplacedsimpleregressiontrees. Considertheprocedureofapplyingbaggingtoregressiontrees. Sincebootstrapsamplesaresimilar tooneanothertheestimatedbootstrapregressiontreeswillalsobesimilartooneanother,particularly inthesensethattheytendtohavethesplitsbasedonthesamevariables. Thismeansthatconditional onthesamplethebootstrapregressiontreesarepositivelycorrelated. Thiscorrelationmeansthatthe variance of the bootstrap average remains high even when the number of bootstrap replications B is large. Themodificationproposedbyrandomforestsistodecorrelate thebootstrapregressiontreesby 4ForÎ¸(cid:98)pmstheMSEiscalculatedusingTheorem28.10.ForÎ¸(cid:98)bag theMSEiscalculatedbynumericalintegration.",
    "page": 949,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 930 introducingextrarandomness.Thisdecorrelationreducesthevarianceofthebootstrapaverage,thereby reducingitsMSE. Thebasicrandomforestalgorithmisasfollows. Therecommendeddefaultsaretakenfromthede- scriptioninHastie,Tibshirani,andFriedman(2008). 1. Pick a minimum leaf size N (default =5), a minimal split fraction Î±â[0,1), and a sampling min numberm<p (default=p/3). 2. Forb=1,...,B: (a) Drawanonparametricbootstrapsample. (b) Growaregressiontreeonthebootstrapsampleusingthefollowingsteps: i. Selectmvariablesatrandomfromthep regressors. ii. Amongthesem variables,picktheonewhichproducesthebestregressionsplit,where eachsplitsubsamplehasatleast N observationsandatleastafractionÎ±oftheob- min servationsinthebranch. iii. Splitthebootstrapsampleaccordingly. (c) StopwheneachleafhasbetweenN and2N â1observations. min min (d) Setm (x)asthesamplemeanofY oneachleafofthebootstraptree. (cid:98)b 3. m (x)=B â1(cid:80)b m (x). (cid:98)rf B=1 (cid:98)b Usingrandomizationtoreducethenumberofvariablesfromptomateachstepaltersthetreestruc- ture and thereby reduces the correlation between the bootstrapped regression trees. This reduces the varianceofthebootstrapaverage. The infinitesimal jackknife (29.18) can be used for variance and standard error estimation, as dis- cussedinWager,Hastie,andEfron(2014). Whilerandomforestsarepopularinapplications,adistributionaltheoryhasbeenslowtodevelop. Someofthemorerecentresultshavemadeprogressbyfocusingonrandomforestsgeneratedbysub- samplingratherthanbootstrap(seethediscussionattheendoftheprevioussection). AvariantproposedbyWagerandAthey(2018)istousehonesttrees(seethediscussionattheendof Section29.15)toremovethedependencebetweenthesamplesplitsandthesamplemeans. Consistency and asymptotic normality has been established by Wager and Athey (2018). They as- sumethattheconditionalmeanandvarianceareLipschitz-continuousinx,X â¼U[0,1]p,andpisfixed5. Theyassumethattherandomforestiscreatedbysubsampling,estimatedbyhonesttrees,andthatthe minimalsplitfractionsatisfies0<Î±â¤0.2.Undertheseconditionstheyestablishthatpointwiseinx m (x)âm(x) (cid:98)rf(cid:112) ââN(0,1) V n (x) d forsomevariancesequenceV (x)â0.Theseresultsjustifyinferenceforrandomforestestimationofthe n regressionfunctionandstandarderrorcalculation. Theasymptoticdistributiondoesnotcontainabias component, indicating that the estimator is undersmoothed. The Wager-Athey conditions for asymp- toticnormalityaresurprisinglyweak. Thetheorydoesnotgiveinsight,however,intotheconvergence rateoftheestimator. Theessentialideaoftheresultisasfollows. Thesplittingalgorithmandrestric- tionsensurethattheregressorspaceis(inaroughsense)evenlysplitinto N â¼n Î³ leaveswhichgrows 5TheauthorsclaimthattheuniformdistributionassumptiononX canbereplacedbytheconditionthatthejointdensityis boundedawayfrom0andinfinity.",
    "page": 950,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 931 at a power rate. This ensures that the estimator is asymptotically unbiased and with suitable control overÎ³thesquaredbiascanbemadesmallerthanthevariance. TheassumptionthatÎ±>0ensuresthat thenumberofobservationsperleafincreaseswithnwhichcombinedwiththehonesttreeconstruction ensuresasymptoticnormalityoftheestimator. Furthermore,WagerandAthey(2018)assert(butdonotprovideaproof)thatthevarianceV (x)can n beconsistentlyestimatedbytheinfinitesimaljackknife(29.18),inthesensethatV(cid:98)n (x)/V n (x)ââ1. p ThestandardcomputationalimplementationofrandomforestsistheRrandomForestcommand. 29.18 Ensembling Ensemblingisthetermusedinmachinelearningformodelaveragingacrossmachinelearningalgo- rithms.Ensemblingispopularinappliedmachinelearning. Suppose you have a set of estimators (e.g., CV selection, James-Stein shrinkage, JMA, SBIC, PCA, kernelregression,seriesregression,ridgeregression,Lasso,regressiontree,baggedregressiontree,and randomforest). Whichshouldyouuse? Itisreasonabletoexpectthatonemethodmayworkwellwith sometypesofdataandothermethodsmayworkwellwithothertypesofdata. Theprincipleofmodel averagingsuggeststhatyoucandobetterbytakingaweightedaverageratherthanjustselectingone. We discussed model averaging models in Sections 28.26-28.31. Ensembling for machine learning canusemanyofthesamemethods. OnepopularmethodknownasstackingisthesameasJackknife ModelAveragingdiscussedinSection28.29. Thisselectsthemodelaveragingweightsbyminimizinga cross-validationcriterion,subjecttotheconstraintthattheweightsarenon-negativeandsumtoone. Unfortunately,thetheoreticalliteratureconcerningensemblingisthin.Muchoftheadviceconcern- ingspecificmethodsisbasedonempiricalperformance. 29.19 LassoIV Belloni,Chen,Chernozhukov,andHansen(2012)proposeLassoforestimationofthereducedform ofaninstrumentalvariablesregression. ThemodelislinearIV Y =X (cid:48)Î²+e (cid:69)[e|Z]=0 X =Î(cid:48) Z+U (cid:69)[U |Z]=0 whereÎ²iskÃ1(fixed)andÎis pÃn with p large. If p >n the2SLSestimatorequalsleastsquares. If p <n butlargethe2SLSestimatorsuffersfromtheâmanyinstrumentsâproblem. Theauthorsârecom- mendationistoestimateÎbyLassoorpost-Lasso6. The reduced form equations for the endogenous regressors are X =Î³(cid:48) Z +U . Each is estimated j j j separatelybyLassoyieldingcoefficientestimatesÎ³ (cid:98)j whicharestackedintothematrixÎ (cid:98)Lasso andusedto formthepredictedvaluesX(cid:98)Lasso =ZÎ (cid:98)Lasso .TheLassoIVestimatoris (cid:179) (cid:48) (cid:180)â1(cid:179) (cid:48) (cid:180) Î² (cid:98)LassoâIV = X(cid:98)Lasso X X(cid:98)Lasso Y . 6Astheydiscuss,anymachinelearningestimatorcanbeused,thoughthespecificassumptionslistedintheirpaperarefor Lassoestimation.",
    "page": 951,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 932 Thepaperdiscussesalternativeformulations. Oneisobtainedbysplit-sampleestimationasinAn- gristandKrueger(1995)(seeSection12.14). Dividethesamplerandomlyintotwoindependenthalves A andB. Use A toestimatethereduceformequationsbyLasso. ThenuseB toestimatethestructural coefficientÎ².Specifically,usingsampleAconstructtheLassocoefficientestimatematrixÎ (cid:98)Lasso,A .Com- binethiswithsampleB tocreatethepredictedvalues X(cid:98)Lasso,B =Z B Î (cid:98)Lasso,A . Finally,usingB construct theestimator (cid:179) (cid:48) (cid:180)â1(cid:179) (cid:48) (cid:180) Î² (cid:98)Lasso,B = X(cid:98)Lasso,B X B X(cid:98)Lasso,B Y B . Wecanreversetheprocedure. UseB toestimatethereducedformcoefficientmatrixÎ (cid:98)Lasso,B byLasso anduse A toestimatethestructuralcoefficient,thus X(cid:98)Lasso,A =Z A Î (cid:98)Lasso,B . Themomentsareaveraged toobtaintheLassoSSIVestimator (cid:179) (cid:48) (cid:48) (cid:180)â1(cid:179) (cid:48) (cid:48) (cid:180) Î² (cid:98)LassoâSSIV = X(cid:98)Lasso,B X B +X(cid:98)Lasso,A X A X(cid:98)Lasso,B Y B +X(cid:98)Lasso,A Y A . In later work (see Section 29.22) the authors describe Î² (cid:98)Lasso,B as a âsample splitâ and Î² (cid:98)LassoâSSIV as a âcross-fitâestimator. UsingtheasymptotictheoryforLassoestimationtheauthorsshowthattheseestimatorsareequiva- lenttoestimationusingtheinfeasibleinstrumentW =Î(cid:48) Z. Theorem29.5 Under the Assumptions listed in Theorem 3 of Belloni, Chen, Chernozhukov,andHansen(2012),including logp (cid:107)Î(cid:107) (cid:112) â0, (29.19) 0 n then (cid:112) (cid:161) Q â1â¦Q â1(cid:162) n (cid:161)Î² (cid:98)LassoâIV âÎ²(cid:162)ââN(0,I k ) (29.20) d whereQ=(cid:69)(cid:163) WW (cid:48)(cid:164) ,â¦=(cid:69)(cid:163) WW (cid:48) e2(cid:164) ,andW =Î(cid:48) Z. Furthermore,thestandard covariancematrixestimatorsareconsistentfortheasymptoticcovariancema- trix. ThesamedistributionresultholdsforÎ² (cid:98)LassoâSSIV undertheassumptions listedintheirTheorem7.Inparticular,(29.19)isreplacedby logp (cid:107)Î(cid:107) â0. (29.21) 0 n ForasketchoftheproofseeSection29.23. Equation(29.19)requiresthatthereducedformcoefficientÎissparseinthesensethatthenumber (cid:112) ofnon-zeroreducedformcoefficients(cid:107)Î(cid:107) growsmoreslowlythan n. Thisallowsforp togrowexpo- 0 nentiallywithn butatasomewhatslowerratethanallowedbyTheorem29.3. Condition(29.19)isone ofthekeyassumptionsneededforthedistributionresult(29.20). ForLassoSSIV,equation(29.21)replaces(29.19).Thisrateconditionisweaker,allowingp togrowat thesamerateasforregressionestimation. Thedifferenceisduetothesplit-sampleestimation,which breaksthedependencebetweenthereducedformcoefficientestimatesandthesecond-stagestructural estimates. Therearetwointerpretableimplicationsofthedifferencebetween(29.19)and(29.21). First, adirectimplicationisthatLassoSSIVallowsforlargernumberofvariablesp.Second,anindirectimpli- cationisthatforanysetofvariables,LassoSSIVwillhavereducedbiasrelativetoLassoIV.Bothinter- pretationssuggestthatLassoSSIVisthepreferredestimator.",
    "page": 952,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 933 Belloni, Chen, Chernozhukov, and Hansen (2012) extend Theorem 29.5 to allow for approximate sparsityasinSection29.12atthecostofmorerestrictiverateconditions. An important disadvantage of the split-sample and cross-fit estimators is that they depend on the randomsortingoftheobservationsintothesamplesAandB.Consequently,tworesearcherswillobtain twodifferentestimators. Furthermore,thesplit-sampleestimatorsusen/2observationsratherthann, whichmayimpactfinite-sampleperformance. Adeductionisthatthesplit-sampleestimatorsarenot appropriatewhennissmall. IVLassocanbeimplementedinStatausingthedownloadablepackageivlasso. 29.20 DoubleSelectionLasso Post-estimationinferenceisdifficultwithmostmachinelearningestimators. Forexample,consider thepost-Lassoestimator(leastsquaresappliedtotheregressorsselectedbytheLasso). Thisisapost- model-selection(PMS)estimator,asdiscussedinSections28.16and28.17. AsshowninSection28.17, the coverage probability of standard confidence intervals applied to PMS estimators can be far from thenominallevel. Belloni,Chernozhukov,andHansen(2014b)proposedanalternativeestimationand inferencemethodwhichachivesbettercoveragerates. Considerthelinearmodel Y =DÎ¸+X (cid:48)Î²+e (29.22) (cid:69)[e|D,X]=0 whereY andD arescalarandX ispÃ1. ThevariableD isthemainfocusoftheregression;thevariable X arecontrols.ThegoalisinferenceonÎ¸. Supposeyouestimatemodel(29.22)bygrouppost-Lasso,onlypenalizingÎ².Thisperformsselection onthevariablesX,resultinginaleastsquaresregressionofY onD andtheselectedvariablesinX.This isidenticaltothemodelstudiedinSection28.17(exceptthatinthatanalysisselectionwasperformed bytesting), whereFigure28.2(c)showsthatthecoverageprobabilitiesforÎ¸ aredownwardbiased, and thedistortionsareserious. Thedistortionsareprimarilyaffectedby(andincreasingin)thecorrelation betweenD andX. Belloni,Chernozhukov,andHansen(2014b)deducethatimprovedcoverageaccuracycanbeachieved ifthevariableX isincludedintheregression(29.22)wheneverX andD arecorrelated.Thisgivesriseto thepracticalsuggestiontoperformwhattheycalldouble-selection. Westartbyspecifyinganauxiliary equationforD: D=X (cid:48)Î³+V (29.23) (cid:69)[V |X]=0. Substituting(29.23)into(29.22)weobtainareducedformforY: Y =X (cid:48)Î·+U (29.24) (cid:69)[U |X]=0 whereÎ·=Î²+Î³Î¸andU =e+VÎ¸.Theproposeddouble-selectionalgorithmappliesmodelselection(e.g., Lassoselection)separatelytoequations(29.23)and(29.24), takestheunionoftheselectedregressors, and then estimates (29.22) by least squares using the selected regressors. This method ensures that a variableX isincludedifitisrelevantfortheregression(29.22)orifitiscorrelatedwithD. Thedouble-selectionestimatorasrecommendedbyBelloni,Chernozhukov,andHansen(2014b)is:",
    "page": 953,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 934 1. Estimate(29.23)byLasso.LetX betheselectedvariablesfromX. 1 2. Estimate(29.24)byLasso.LetX betheselectedvariablesfromX. 2 3. LetX(cid:101) =X 1 âªX 2 betheunionofthevariablesinX 1 andX 2 . 4. RegressY on(D,X(cid:101))toobtainthedouble-selectioncoefficientestimateÎ¸ (cid:98)DS . 5. Calculateaconventional(heteroskedastic)standarderrorforÎ¸ (cid:98)DS . Belloni,Chernozhukov,andHansen(2014b)showthatwhenboth(29.22)and(29.23)satisfyanap- proximatesparsitystructure(sothattheregressionsarewellapproximatedbyafinitesetofregressors) thenthedouble-selectionestimatorÎ¸ (cid:98)DS anditst-ratioareasymptoticallynormalsoconventionalinfer- erncemethodsarevalid. Theirproofistechnically tedioussonotrepeatedhere. Theessentialideais thatsinceX(cid:101)includesthevariablesinX 2 ,theestimatorÎ¸ (cid:98)DS isasymptoticallyequivalenttotheregression where D is replaced with the error V from (29.23). Since V is uncorrelated with the regressors X the estimatorandt-ratiosatisfytheconventionalnon-selectionasymptoticdistribution. Itshouldbeemphasizedthatthisdistributionalclaimisasymptotic;finitesampleinferencesremain distortedfromnominallevels.Furthermore,theresultrestsontheadequacyoftheapproximatesparsity assumptionforboththestructuralequation(29.22)andtheauxillaryregression(29.23). The primaryadvantage ofthedouble-selectionestimator isitssimplicity andclear intuitivestruc- ture. InStata,thedouble-selectionLassoestimatorcanbecomputedbythedsregresscommandorwith thepdslassoadd-onpackage.Double-selectionisavailableinRwiththehdmpackage. 29.21 Post-RegularizationLasso A potential improvement on double-selection Lasso is the post-regularization Lasso estimator of Chernozhukov,Hansen,andSpindler(2015),whichislabeledaspartialing-outLassointheStataman- ual. TheestimatorisessentiallythesameasRobinson(1988)forthepartiallylinearmodel(seeSection 19.24)butestimatedbyLassoratherthankernelregression. We first transform the structural equation (29.22) to eliminate the high-dimensional component. Taketheexpectedvalueof(29.22)conditionalonX,andsubtractfromeachside.Thisleadstotheequa- tion Y â(cid:69)[Y |X]=(Dâ(cid:69)[D|X])Î¸+e. Noticethatthiselminatestheregressor X andthehigh-dimensionalcoefficientÎ². Themodels(29.23)- (29.24)specify(cid:69)[Y |X]and(cid:69)[D|X]aslinearfunctionsofX.Substitutingtheseexpressionsweobtain Y âX (cid:48)Î·=(cid:161) DâX (cid:48)Î³(cid:162)Î¸+e. (29.25) IfÎ·andÎ³wereknownthecoefficientÎ¸couldbeestimatedbyleastsquares.AsÎ·andÎ³areunknownthey needtobeestimated. Chernozhukov,Hansen,andSpindler(2015)recommendestimationbyLassoor post-Lasso,separatelyforY andD. TheestimatorrecommendedbyChernozhukov,Hansen,andSpindler(2015)is: 1. Estimate(29.23)byLassoorpost-Lasso. LetÎ³ (cid:98) bethecoefficientestimatorandV(cid:98)i =D i âX i (cid:48)Î³ (cid:98) the residual. 2. Estimate(29.24)byLassoorpost-Lasso. LetÎ· (cid:98) bethecoefficientestimatorandU(cid:98)i =Y i âX i (cid:48)Î· (cid:98) the residual.",
    "page": 954,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 935 3. LetÎ¸ (cid:98)PR betheOLScoefficientfromtheregressionofU(cid:98)onV(cid:98). 4. Calculateaconventional(heteroskedastic)standarderrorforÎ¸ (cid:98)PR . Chernozhukov,Hansen,andSpindler(2015)introducethefollowinginsighttounderstandwhyÎ¸ (cid:98)PR mayberelativelyinsensitivetopost-model-selection.Thereasonwhymodelselectioninvalidatesinfer- enceisbecausewhenthevariablesD andX arecorrelatedthemomentconditionforÎ¸issensitivetoÎ². Specifically,themomentconditionforÎ¸basedon(29.22)is m(Î¸,Î²)=(cid:69)(cid:163) D (cid:161) Y âDÎ¸âX (cid:48)Î²(cid:162)(cid:164)=0. ItssensitivitywithrespecttoÎ²isitsderivativeevaluatedatthetruecoefficients â m(Î¸,Î²)=â(cid:69)(cid:163) DX (cid:48)(cid:164) âÎ² whichisnon-zerowhenD and X arecorrelated. Thismeansthatinclusion/exclusionofthevariable X hasanimpactonthemomentconditionforÎ¸andhenceitssolution. Incontrast,themomentconditionforÎ¸basedon(29.25)is m (Î¸,Î²)=(cid:69)(cid:163)(cid:161) DâX (cid:48)Î³(cid:162)(cid:161) Y âX (cid:48)Î·â(cid:161) DâX (cid:48)Î³(cid:162)Î¸(cid:162)(cid:164) PR =(cid:69)(cid:163)(cid:161) DâX (cid:48)Î³(cid:162)(cid:161) Y âDÎ¸âX (cid:48)Î²(cid:162)(cid:164) . ItssensitivitywithrespecttoÎ²is â m (Î¸,Î²)=â(cid:69)(cid:163)(cid:161) DâX (cid:48)Î³(cid:162) X (cid:48)(cid:164)=â(cid:69)(cid:163) VX (cid:48)(cid:164)=0. âÎ² PR This equals zero becauseV is a regression error as specified in (29.23) and thus uncorrelated with X. Sincethesensitivityofm (Î¸,Î²)withrespecttoÎ²iszero,inclusion/exclusionofthevariableX hasonly PR amildimpactonthemomentconditionforÎ¸anditsestimator. Theseinsightsareformalizedinthefollowingdistributiontheory. (cid:104) (cid:105) Theorem29.6 Supposemodel(29.22)-(29.23)holds, max (cid:69) X4 <â,(cid:69)(cid:163) e4(cid:164)< 1â¤jâ¤p j â,(cid:69)(cid:163) V4(cid:164)<â,Assumption29.1holdsforbothÎ²andÎ³,theLassoparameter satisfiesÎ»=C (cid:112) nlogp forC sufficientlylarge,and (cid:161)(cid:176) (cid:176) Î²(cid:176) (cid:176) +(cid:176) (cid:176) Î³(cid:176) (cid:176) (cid:162)lo (cid:112) gp =o(1). (29.26) 0 0 n Then (cid:112) (cid:195) (cid:69)(cid:163) V2e2(cid:164)(cid:33) n (cid:161)Î¸ (cid:98)PR âÎ¸(cid:162)â d âN 0, (cid:161)(cid:69)(cid:163) V2 (cid:164)(cid:162)2 . Furthermore, the standard variance estimator for Î¸ (cid:98)PR is consistent for the asymptoticvariance. ForaproofseeSection29.23.",
    "page": 955,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 936 Theorem29.6showsthatthepost-regularization(partialing-out)Lassoestimatorhasaconventional asymptoticdistribution,allowingconventionalinferenceforthecoefficientÎ¸. Thekeyrateconditionis (29.26),whichisstrongerthanrequiredforLassoestimation,andidenticalto(29.19)usedforLassoIV. (29.26)requiresthatbothÎ²andÎ³aresparse. Thecondition(29.26)canberelaxedtoallowapproximate sparsityasinTheorem29.4atthecostofamorerestrictiveratecondition. The advantage of the post-regularization estimator Î¸ (cid:98)PR over the double-selection estimator Î¸ (cid:98)DS is efficiency. Thepost-regularizationestimatorusesonlytherelevantcomponentsof X toseparatelyde- meanY andD,leadingtogreaterparsimony. Differentcomponentsof X mayberelevanttoD andY. The post-regularization estimator allows such distinctions and estimates each separately. In contrast, the double-selection estimator uses the union of the two regressor sets for estimation of Î¸, leading to a less parsimonious specification. As a consequence, an advantage of the double-selection estimator is reduced bias and robustness. Regarding the theory, the derivation of the asymptotic theory for the post-regularization estimator is considerably easier than that for the double-selection estimator, as it onlyinvolvesthemanipulationofratesofconvergence,whilethedouble-selectionestimatorrequiresa carefulattentiontothehandlingoftheunionoftheregressorsets. Thepartialing-outLassoestimatorisavailablewiththeporegresscommandinStata(implemented withpost-Lassoestimationonly),orwiththepdslassoadd-onpackage.Partialing-outLassoisavailable inRwiththehdmpackage. 29.22 Double/DebiasedMachineLearning Themostrecentcontributiontoinferencemethodsformodel(29.22)istheDouble/Debiasedma- chine learning (DML) estimator of Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, and Robins (2018). Our description will focus on linear regression estimated by Lasso, though their treat- mentisconsiderablymoregeneral.Thisestimationmethodhasreceivedconsiderableattentionamong econometriciansinrecentyearsandisconsideredthestate-of-the-artestimationmethod. TheDMLestimatorextendsthepost-regularizationestimatoroftheprevioussectionbyaddingsample- splittingsimilarlytothesplit-sampleIVestimator(seeSection29.19).Theauthorsarguethatthisreduces thedependencebetweentheestimationstagesandcanimproveperformance. Aspresentedintheprevioussection,thepost-regularizationestimatorfirstestimatesthecoefficients Î³andÎ·inthemodels(29.23)and(29.24)andthenestimatesthecoefficientÎ¸.Thesplit-sampleestimator performstheseestimationstepsusingseparatesamples.TheDMLestimatortakesthisastepfurtherby usingK-foldpartitioning.Theestimationalgorithmisasfollows. 1. RandomlypartitionthesampleintoK independentfoldsA ,k=1,...,K,ofroughlyequalsizen/K. k 2. Writethedatamatricesforeachfoldas(Y ,D ,X ). k k k 3. Fork=1,...,K (a) UseallobservationsexceptforfoldktoestimatethecoefficientsÎ³andÎ·in(29.23)and(29.24) byLassoorpost-Lasso",
    "page": 956,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". 1. RandomlypartitionthesampleintoK independentfoldsA ,k=1,...,K,ofroughlyequalsizen/K. k 2. Writethedatamatricesforeachfoldas(Y ,D ,X ). k k k 3. Fork=1,...,K (a) UseallobservationsexceptforfoldktoestimatethecoefficientsÎ³andÎ·in(29.23)and(29.24) byLassoorpost-Lasso. (b) Writetheseleave-fold-outestimatorsasÎ³ (cid:98)âk andÎ· (cid:98)âk . (c) SetV(cid:98)k =D k âX k Î³ (cid:98)âk andU(cid:98)k =Y k âX k Î· (cid:98)âk . ThesearetheestimatedvaluesofV andU for observationsinthekth foldusingtheleave-fold-outestimators. 4. SetÎ¸ (cid:98)DML = (cid:179) (cid:80)K k=1 V(cid:98) (cid:48) k V(cid:98)k (cid:180)â1(cid:179) (cid:80)K k=1 V(cid:98) (cid:48) k U(cid:98)k (cid:180) .Equivalently,stackV(cid:98)k andU(cid:98)k intonÃ1vectorsV(cid:98) andU(cid:98) (cid:179) (cid:48) (cid:180)â1(cid:179) (cid:48) (cid:180) andsetÎ¸ (cid:98)DML = V(cid:98) V(cid:98) V(cid:98) U(cid:98) .",
    "page": 956,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 937 5. Constructaconventional(heteroskedastic)standarderrorforÎ¸ (cid:98)DML . TheauthorscallÎ¸ (cid:98)DML across-fitestimatorasintheK =2caseitperformssamplesplittinginboth directionsandisthereforefullyasymptoticallyefficient.Theestimatorasdescribedaboveislabeledthe âDML2âestimatorbytheauthors. AnalternativetheylabelâDML1âisÎ¸ (cid:98)DML1 =(cid:80)K k=1 (cid:179) V(cid:98) (cid:48) k V(cid:98)k (cid:180)â1(cid:179) V(cid:98) (cid:48) k U(cid:98)k (cid:180) . TheyareasymptoticallyequivalentbutDML2ispreferred. The estimator requires the selection of the number of folds K. Similarly to K-fold CV the authors recommendK =10.ComputationalcostisroughlyproportionaltoK. Theorem29.7 UndertheassumptionsofTheorem29.6, (cid:112) (cid:195) (cid:69)(cid:163) V2e2(cid:164)(cid:33) n (cid:161)Î¸ (cid:98)DML âÎ¸(cid:162)â d âN 0, (cid:161)(cid:69)(cid:163) V2 (cid:164)(cid:162)2 . Furthermore, the standard variance estimator for Î¸ (cid:98)DML is consistent for the asymptoticvariance. Theorem29.7showsthattheDMLestimatorachievesastandardasymptoticdistribution.Theproof isastraightforwardextensionofthatforTheorem29.6soisomitted. TheauthorsarguethattheDMLestimatorhasimprovedsamplingperformanceduetoanimproved rate of convergence of certain error terms. If we examine the proof of Theorem 29.6, one of the error boundsis(29.42),whichshowsthat (cid:175) (cid:175) (cid:181) (cid:182) (cid:175) (cid:175) (cid:175) (cid:161)Î³ (cid:98)âk âÎ³(cid:162)(cid:48) (cid:112) 1 n X (cid:48) k e k (cid:175) (cid:175) (cid:175) â¤O p (cid:176) (cid:176) Î³(cid:176) (cid:176) 0 lo (cid:112) g n p =o p (1). (29.27) Undersamplesplitting,however,wehaveanimprovedrateofconvergence. ThecomponentsÎ³ (cid:98)âk and X (cid:48) k e k areindependent. Thustheleftsideof(29.27), conditionalonÎ³ (cid:98)âk and X k , ismeanzeroandhas conditional variance bounded by Ï2(cid:161)Î³ (cid:98)âk âÎ³(cid:162)(cid:48) n 1 X (cid:48) k X k (cid:161)Î³ (cid:98)âk âÎ³(cid:162) where Ï2 =sup x (cid:69)(cid:163) e2|X =x (cid:164) . This is O p (cid:179)(cid:176) (cid:176) Î³(cid:176) (cid:176) 0 lo n gp (cid:180) byTheorem29.3. Hence(29.27)isO p (cid:181)(cid:113) (cid:176) (cid:176) Î³(cid:176) (cid:176) 0 lo n gp (cid:182) , whichisofsmallerorder. Thisim- provement suggests that the deviations from the asymptotic approximation should be smaller under samplesplittingandtheDMLestimator. Theimprovements,however,donotleadtoarelaxationofthe regularityconditions.Theproofrequiresboundingtheterms(29.40)-(29.41)andthesearenotimproved bysamplesplititng. Consequentlyitisunclearifthedistributionalimpactofsamplesplittingislargeor small. TheadvantageoftheDMLestimatoroverthepost-regularizationestimatoristhatthesamplesplit- tingeliminatesthedependencebetweenthetwoestimationsteps,therebyreducingpost-model-selection bias. Theprocedurehasseveraldisadvantages,however. First,theestimatorisrandomduetothesam- plesplitting. Tworesearcherswiththesamedatasetbutmakingdifferentrandomsplitswillobtaintwo distinctestimators",
    "page": 957,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Consequentlyitisunclearifthedistributionalimpactofsamplesplittingislargeor small. TheadvantageoftheDMLestimatoroverthepost-regularizationestimatoristhatthesamplesplit- tingeliminatesthedependencebetweenthetwoestimationsteps,therebyreducingpost-model-selection bias. Theprocedurehasseveraldisadvantages,however. First,theestimatorisrandomduetothesam- plesplitting. Tworesearcherswiththesamedatasetbutmakingdifferentrandomsplitswillobtaintwo distinctestimators. Thisarbitrarinessisunsettling. Thisrandomnesscanbereducedbyusingalarger valueofK, butthisincreasescomputationcost. Anotherdisadvantageofsample-splittingisthatesti- mationofÎ³andÎ·isperformedusingsmallersampleswhichreducesestimationefficiency,thoughthis effectisminorifK â¥10.Regardless,theseconsiderationssuggestthatDMLmaybemostappropriatefor settingswithlargenandK â¥10.",
    "page": 957,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 938 AtthebeginningofthissectiontheDMLestimatorwasdescribedastheâstate-of-the-artâ.Thisfield israpidlydevelopingsothisspecificestimatormaybesooneclipsedbyafurtheriteration. InStata, theDMLestimatorisavailablewiththexporegresscommand. Bydefaultitimplements theDML2estimatorwithK =10folds.ThecoefficientsÎ³andÎ·areestimatedbypost-Lasso. 29.23 TechnicalProofs* ProofofTheorem29.2Combining(29.8)and(29.9)wefindthat mse (cid:163)Î² (cid:98)ridge |X (cid:164)=var (cid:163)Î² (cid:98)ridge |X (cid:164)+bias (cid:163)Î² (cid:98)ridge |X (cid:164) bias (cid:163)Î² (cid:98)ridge |X (cid:164)(cid:48) =(cid:161) X (cid:48) X +Î»I (cid:162)â1(cid:161) X (cid:48) DX +Î»2Î²Î²(cid:48)(cid:162)(cid:161) X (cid:48) X +Î»I (cid:162)â1 . p p TheMSEoftheleastsquaresestimatoris mse (cid:163)Î² (cid:98)ols |X (cid:164)=(cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48) DX (cid:162)(cid:161) X (cid:48) X (cid:162)â1 =(cid:161) X (cid:48) X +Î»I (cid:162)â1(cid:161) X (cid:48) X +Î»I (cid:162)(cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48) DX (cid:162)(cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48) X +Î»I (cid:162)(cid:161) X (cid:48) X +Î»I (cid:162)â1 p p p p =(cid:161) X (cid:48) X +Î»I (cid:162)â1 (cid:179) X (cid:48) DX +Î»(cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48) DX (cid:162)+Î»(cid:161) X (cid:48) DX (cid:162)(cid:161) X (cid:48) X (cid:162)â1 p +Î»2(cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48) DX (cid:162)(cid:161) X (cid:48) X (cid:162)â1 (cid:180) (cid:161) X (cid:48) X +Î»I (cid:162)â1 p â¥(cid:161) X (cid:48) X +Î»I (cid:162)â1 (cid:179) X (cid:48) DX +Î»(cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48) DX (cid:162)+Î»(cid:161) X (cid:48) DX (cid:162)(cid:161) X (cid:48) X (cid:162)â1 (cid:180) (cid:161) X (cid:48) X +Î»I (cid:162)â1 . p p Theirdifferenceis mse (cid:163)Î² (cid:98)ols |X (cid:164)âmse (cid:163)Î² (cid:98)ridge |X (cid:164)â¥Î»(cid:161) X (cid:48) X +Î»I p (cid:162)â1 A (cid:161) X (cid:48) X +Î»I p (cid:162)â1 (29.28) where A=(cid:161) X (cid:48) X (cid:162)â1(cid:161) X (cid:48) DX (cid:162)+(cid:161) X (cid:48) DX (cid:162)(cid:161) X (cid:48) X (cid:162)â1âÎ»Î²Î²(cid:48) . Theright-hand-sideof(29.28)ispositivedefiniteif A>0.Itssmallesteigenvaluesatisfies Î» (A)=2 minÎ±(cid:48)(cid:161) X (cid:48) X (cid:162)â1/2(cid:161) X (cid:48) DX (cid:162)(cid:161) X (cid:48) X (cid:162)â1/2Î±âÎ»Î²(cid:48)Î²â¥2minh (cid:48) DhâÎ»Î²(cid:48)Î²=2Ï2âÎ»Î²(cid:48)Î² min Î±(cid:48)Î±=1 h(cid:48)h=1 whichisstrictlypositivewhen0<Î»<2Ï2/Î²(cid:48)Î²asassumed. Thisshowsthat(29.28)ispositivedefinite",
    "page": 958,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Theright-hand-sideof(29.28)ispositivedefiniteif A>0.Itssmallesteigenvaluesatisfies Î» (A)=2 minÎ±(cid:48)(cid:161) X (cid:48) X (cid:162)â1/2(cid:161) X (cid:48) DX (cid:162)(cid:161) X (cid:48) X (cid:162)â1/2Î±âÎ»Î²(cid:48)Î²â¥2minh (cid:48) DhâÎ»Î²(cid:48)Î²=2Ï2âÎ»Î²(cid:48)Î² min Î±(cid:48)Î±=1 h(cid:48)h=1 whichisstrictlypositivewhen0<Î»<2Ï2/Î²(cid:48)Î²asassumed. Thisshowsthat(29.28)ispositivedefinite. â  ProofofTheorem29.3Thekeyisthefollowingmaximalinequality.ByMarkovâsinequality,themaximal Marcinkiewicz-Zygmundinequality(B.47),Jensenâsinequality,and(29.14), (cid:80) (cid:183)(cid:176) (cid:176) (cid:176) (cid:176)n 1 X (cid:48) e (cid:176) (cid:176) (cid:176) (cid:176)â > 4 Î» n (cid:184) â¤ Î» 4 (cid:69) (cid:34) 1 m â¤j a â¤ x p (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)i (cid:88) = n 1 X ji e i (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:35) (cid:112) (cid:34)(cid:195) (cid:33)1/2(cid:35) â¤ 16 logp max (cid:69) (cid:88) n X2 e2 Î» 1â¤jâ¤p i=1 ji i 16 (cid:112) logp(cid:112) â¤ nÏ Î» 16Ï = C â¤Î±",
    "page": 958,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 939 thefinalinequalitysinceC â¥16Ï/Î±.Thisshowsthat (cid:176) (cid:176) (cid:176) 1 X (cid:48) e (cid:176) (cid:176) (cid:176) â¤ Î» (29.29) (cid:176)n (cid:176)â 4n holdswithprobabilityexceeding1âÎ±.Theremainderoftheproofisalgebraic,basedonmanipulations oftheestimationcriterionfunction,conditionalontheevent(29.29). SinceÎ² (cid:98)minimizesSSE 1 (cid:161)Î²,Î»(cid:162) itsatisfiesSSE 1 (cid:161)Î² (cid:98),Î»(cid:162)â¤SSE 1 (cid:161)Î²,Î»(cid:162) or (cid:161) Y âXÎ² (cid:98) (cid:162)(cid:48)(cid:161) Y âXÎ² (cid:98) (cid:162)+Î»(cid:176) (cid:176) Î² (cid:98) (cid:176) (cid:176) â¤e (cid:48) e+Î»(cid:176) (cid:176) Î²(cid:176) (cid:176) . 1 1 Writingouttheleftside,dividingbyn,andre-arranginganddefiningR n =(cid:161)Î² (cid:98) âÎ²(cid:162)(cid:48) Q n (cid:161)Î² (cid:98) âÎ²(cid:162) ,thisimplies R n + n Î» (cid:176) (cid:176) Î² (cid:98) (cid:176) (cid:176) 1 â¤ n 2 e (cid:48) X (cid:161)Î² (cid:98) âÎ²(cid:162)+ n Î» (cid:176) (cid:176) Î²(cid:176) (cid:176) 1 â¤2 (cid:176) (cid:176) (cid:176) 1 X (cid:48) e (cid:176) (cid:176) (cid:176) (cid:176) (cid:176) Î² (cid:98) âÎ²(cid:176) (cid:176) + Î» (cid:176) (cid:176) Î²(cid:176) (cid:176) (cid:176)n (cid:176)â 1 n 1 Î» Î» â¤ (cid:176) (cid:176) Î² (cid:98) âÎ²(cid:176) (cid:176) + (cid:176) (cid:176) Î²(cid:176) (cid:176) . 2n 1 n 1 ThesecondinequalityisHÃ¶lderâs(29.2)andthethirdholdsby(29.29). PartitionÎ² (cid:98) =(cid:161)Î² (cid:98)0 ,Î² (cid:98)1 (cid:162) conformablywithÎ²=(cid:161)Î² 0 ,Î² 1 (cid:162) .Usingtheadditivitypropertyofthe1-normand thefactÎ² =0,theaboveexpressionimplies 0 Î» Î» Î» R n + 2n (cid:176) (cid:176) Î² (cid:98)0 âÎ² 0 (cid:176) (cid:176) 1 â¤ 2n (cid:176) (cid:176) Î² (cid:98)1 âÎ² 1 (cid:176) (cid:176) 1 + n (cid:161)(cid:176) (cid:176) Î² 1 (cid:176) (cid:176) 1 â(cid:176) (cid:176) Î² (cid:98)1 (cid:176) (cid:176) 1 (cid:162) â¤ 2 3 n Î» (cid:176) (cid:176) Î² (cid:98)1 âÎ² 1 (cid:176) (cid:176) 1 (29.30) the A se n co im nd pl i i n c e a q ti u o a n li o ty f( u 2 s 9 i . n 3 g 0) th is e (cid:176) (cid:176) fa Î² (cid:98) c 0 t â (cid:176) (cid:176) Î² Î² (cid:176) (cid:176) 0 (cid:176) (cid:176) 1 1 â¤ â¤ (cid:176) (cid:176) 3 Î² (cid:98) (cid:176) (cid:176) 1 Î² â (cid:98)1 Î² â 1 (cid:176) (cid:176) Î² 1 1 + (cid:176) (cid:176) 1 (cid:176) (cid:176) . Î² (cid:98) T 1 h (cid:176) (cid:176) u 1 s w Î² (cid:98) h â ic Î² h â fo B llo . w A s c f o r n o s m eq (2 u 9 e . n 3 c ). eisthatwecan applyAssumption29.1toobtain R n =(cid:161)Î² (cid:98) âÎ²(cid:162)(cid:48) Q n (cid:161)Î² (cid:98) âÎ²(cid:162)â¥c2(cid:176) (cid:176) Î² (cid:98) âÎ²(cid:176) (cid:176) 2 2 . (29.31) Thisistheonly(butkey)pointintheproofwhereAssumption29.1isused",
    "page": 959,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". Î² (cid:98) T 1 h (cid:176) (cid:176) u 1 s w Î² (cid:98) h â ic Î² h â fo B llo . w A s c f o r n o s m eq (2 u 9 e . n 3 c ). eisthatwecan applyAssumption29.1toobtain R n =(cid:161)Î² (cid:98) âÎ²(cid:162)(cid:48) Q n (cid:161)Î² (cid:98) âÎ²(cid:162)â¥c2(cid:176) (cid:176) Î² (cid:98) âÎ²(cid:176) (cid:176) 2 2 . (29.31) Thisistheonly(butkey)pointintheproofwhereAssumption29.1isused. Togetherwith(29.30),(29.31)implies c2(cid:176) (cid:176) Î² (cid:98) âÎ²(cid:176) (cid:176) 2 2 â¤ 2 3 n Î» (cid:176) (cid:176) Î² (cid:98)1 âÎ² 1 (cid:176) (cid:176) 1 â¤ 3Î» (cid:176)Î² âÎ² (cid:176) (cid:176)Î² âÎ² (cid:176)1/2 2n (cid:176)(cid:98)1 1(cid:176) 2 (cid:176)(cid:98)1 1(cid:176) 0 â¤ 3Î» (cid:176) (cid:176) Î² (cid:98) âÎ²(cid:176) (cid:176) (cid:176) (cid:176) Î²(cid:176) (cid:176) 1/2 . 2n 2 0 Thesecondinequalityis(29.4). Thethirdis (cid:176) (cid:176) Î² (cid:98)1 âÎ² 1 (cid:176) (cid:176) 2 â¤(cid:176) (cid:176) Î² (cid:98) âÎ²(cid:176) (cid:176) 2 and (cid:176) (cid:176) Î² (cid:98)1 âÎ² 1 (cid:176) (cid:176) 0 =(cid:176) (cid:176) Î² 1 (cid:176) (cid:176) 0 =(cid:176) (cid:176) Î²(cid:176) (cid:176) 0 . Rear- rangingandusing(29.14)weobtain (cid:115) (cid:176)Î²âÎ²(cid:176) â¤ 3Î» (cid:176)Î²(cid:176)1/2= 3C (cid:176)Î²(cid:176)1/2 logp (cid:176)(cid:98) (cid:176) (cid:176) (cid:176) (cid:176) (cid:176) 2 2c2n 0 2c2 0 n whichis(29.17)withD=3C/2c2.",
    "page": 959,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 940 (29.30),(29.4),(29.17)and(29.14)imply R n + 2 Î» n (cid:176) (cid:176) Î² (cid:98)0 âÎ² 0 (cid:176) (cid:176) 1 â¤ 2 3 n Î» (cid:176) (cid:176) Î² (cid:98)1 âÎ² 1 (cid:176) (cid:176) 2 (cid:176) (cid:176) Î² (cid:98)1 âÎ² 1 (cid:176) (cid:176) 1 0 /2 â¤ 3Î» (cid:176)Î²âÎ²(cid:176) (cid:176)Î²(cid:176)1/2 (cid:176)(cid:98) (cid:176) (cid:176) (cid:176) 2n 2 0 (cid:115) â¤ 9C Î» (cid:176) (cid:176) Î²(cid:176) (cid:176) logp (29.32) 4c2n 0 n = 9C2 (cid:176) (cid:176) Î²(cid:176) (cid:176) logp . 4c2 0 n Thisimplies(29.15)withD=9C2/4c2. Equation(29.32)alsoimplies (cid:115) (cid:176) (cid:176) Î² (cid:98)0 âÎ² 0 (cid:176) (cid:176) 1 â¤ 2 9 c C 2 (cid:176) (cid:176) Î²(cid:176) (cid:176) 0 lo n gp . Using(29.4)and(29.17) (cid:115) (cid:176) (cid:176) Î² (cid:98)1 âÎ² 1 (cid:176) (cid:176) 1 â¤(cid:176) (cid:176) Î² (cid:98)1 âÎ² 1 (cid:176) (cid:176) 2 (cid:176) (cid:176) Î² (cid:98)1 âÎ² 1 (cid:176) (cid:176) 1 0 /2â¤(cid:176) (cid:176) Î² (cid:98) âÎ²(cid:176) (cid:176) 2 (cid:176) (cid:176) Î²(cid:176) (cid:176) 1 0 /2â¤ 2 3 c C 2 (cid:176) (cid:176) Î²(cid:176) (cid:176) 0 lo n gp . Hence (cid:115) (cid:176)Î²âÎ²(cid:176) =(cid:176)Î² âÎ² (cid:176) +(cid:176)Î² âÎ² (cid:176) â¤ 6C (cid:176)Î²(cid:176) logp (cid:176)(cid:98) (cid:176) 1 (cid:176)(cid:98)0 0(cid:176) 1 (cid:176)(cid:98)1 1(cid:176) 1 c2 (cid:176) (cid:176) 0 n whichis(29.16)withD=6C/c2. â  ProofofTheorem29.5Weprovideasketchoftheproof.WestartwithLassoIV.First,considertheideal- izedestimatorÎ² (cid:98) =(cid:161) W (cid:48) X (cid:162)â1(cid:161) W (cid:48) Y (cid:162) whereW =ZÎ.IfthedistributionofW doesnotchangewithn(which holdswhenthenon-zerocoefficientsinÎdonotchangewithn)thenÎ² (cid:98)hastheasymptoticdistribution (29.20)understandardassumptions. Toallowthenon-zerocoefficientsinÎtochangewithn,Belloni, Chen,Chernozhukov,andHansen(2012)useatriangulararraycentrallimittheorywhichrequiressome additionaltechnicalconditions. Giventhis, (29.20)holdsifW canbereplacedbythepredictedvalues X(cid:98)Lasso withoutchanging(29.20).Thisholdsif 1(cid:161) X(cid:98)Lasso âW (cid:162)(cid:48) X ââ0 (29.33) n p (cid:112) 1 (cid:161) X(cid:98)Lasso âW (cid:162)(cid:48) eââ0. (29.34) n p Forsimplicityassumethatk=1. Theorem29.3showsthatundertheregularityconditionsfortheLasso appliedtothereducedform, (cid:175) (cid:175) (cid:181) (cid:182) (cid:181) (cid:182) (cid:175) (cid:175) 1(cid:161) X(cid:98)Lasso âW (cid:162)(cid:48)(cid:161) X(cid:98)Lasso âW (cid:162)(cid:175) (cid:175) =(cid:161)Î (cid:98) âÎ(cid:162)(cid:48) 1 Z (cid:48) Z (cid:161)Î (cid:98) âÎ(cid:162)â¤O p (cid:107)Î(cid:107) 0 logp (29.35) (cid:175)n (cid:175) n n and ï£« (cid:115) ï£¶ (cid:176) (cid:176) Î (cid:98) âÎ(cid:176) (cid:176) 1 â¤O pï£­ (cid:107)Î(cid:107) 0 lo n gp ï£¸. (29.36)",
    "page": 960,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 941 (cid:104) (cid:105) BytheMaximalMarchinkiewicz-Zygmundinequality(B.47),ifmax (cid:69) Z2e2 â¤Ï<â,then j j (cid:176) (cid:176) (cid:176)(cid:112) 1 Z (cid:48) e (cid:176) (cid:176) (cid:176) =O p (cid:179)(cid:113) logp (cid:180) . (29.37) (cid:176) n (cid:176) â BytheSchwarzinequalityand(29.35) (cid:175) (cid:175) (cid:175) 1(cid:161) X(cid:98)Lasso âW (cid:162)(cid:48) X (cid:175) (cid:175) (cid:175) â¤ (cid:175) (cid:175) (cid:175) 1(cid:161) X(cid:98)Lasso âW (cid:162)(cid:48)(cid:161) X(cid:98)Lasso âW (cid:162) (cid:175) (cid:175) (cid:175) 1/2(cid:175) (cid:175) (cid:175) 1 X (cid:48) X (cid:175) (cid:175) (cid:175) 1/2 (cid:175)n (cid:175) (cid:175)n (cid:175) (cid:175)n (cid:175) (cid:181) logp (cid:182)1/2 â¤O (cid:107)Î(cid:107) â¤o (1) p 0 p n thefinalinequalityunder(29.19).Thisestablishes(29.33). BytheHÃ¶lderinequality(29.2),(29.36),and(29.37), (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175)(cid:112) 1 (cid:161) X(cid:98)Lasso âW (cid:162)(cid:48) e (cid:175) (cid:175) =(cid:175) (cid:175) (cid:161)Î (cid:98) âÎ(cid:162)(cid:48) (cid:112) 1 Z (cid:48) e (cid:175) (cid:175) (cid:175) n (cid:175) (cid:175) n (cid:175) (cid:176) (cid:176) â¤(cid:176) (cid:176) Î (cid:98) âÎ(cid:176) (cid:176) (cid:176) (cid:176)(cid:112) 1 Z (cid:48) e (cid:176) (cid:176) 1(cid:176) n (cid:176) â ï£« (cid:115) ï£¶ logp (cid:179)(cid:113) (cid:180) â¤O pï£­ (cid:107)Î(cid:107) 0 ï£¸O p logp n (cid:181) (cid:182) logp =O (cid:107)Î(cid:107) (cid:112) p 0 n â¤o (1) (29.38) p thefinalinequalityunder(29.19).Thisestablishes(29.34). NowconsiderLassoSSIV.Thestepsareessentiallythesameexceptfor(29.38). Forthisweusethe factthatÎ (cid:98)Lasso,A isindependentofZ (cid:48) B e B .LetD B =diag (cid:161)(cid:69)(cid:163) e i 2|Z i (cid:164)(cid:162) forsampleB andassume(cid:69)(cid:163) e2|Z (cid:164)â¤ Ï2<â.Conditionallyon AandZ B var (cid:183) (cid:112) 1 n (cid:179) X(cid:98) (cid:48) Lasso,B âW B (cid:180)(cid:48) e B (cid:175) (cid:175) (cid:175) (cid:175) A,Z B (cid:184) =var (cid:183) (cid:161)Î (cid:98)Lasso,A âÎ(cid:162)(cid:48) (cid:112) 1 n Z (cid:48) B e B (cid:175) (cid:175) (cid:175) (cid:175) A,Z B (cid:184) =(cid:161)Î (cid:98)Lasso,A âÎ(cid:162)(cid:48) n 1 Z (cid:48) B DZ B (cid:161)Î (cid:98)Lasso,A âÎ(cid:162) â¤Ï2(cid:161)Î (cid:98)Lasso,A âÎ(cid:162)(cid:48) n 1 Z (cid:48) B Z B (cid:161)Î (cid:98)Lasso,A âÎ(cid:162) (cid:181) (cid:182) logp =O (cid:107)Î(cid:107) p 0 n â¤o (1) p thefinalboundsby(29.35)and(29.21).Thusn â1/2 (cid:179) X(cid:98) (cid:48) Lasso,B âW B (cid:180)(cid:48) e B ââ0asneeded. â  p ProofofTheorem29.6TheidealizedestimatorÎ¸ (cid:98)PR =(cid:161) V (cid:48) V (cid:162)â1(cid:161) V (cid:48) U (cid:162) satisfies (cid:112) n (cid:161)Î¸ (cid:98)PR âÎ¸(cid:162)=(cid:161) n â1V (cid:48) V (cid:162)â1(cid:161) n â1/2V (cid:48) e (cid:162) whichhasthestatedasymptoticdistribution. TheTheoremthereforeholdsifreplacementof(V,U)by (V(cid:98),U(cid:98))isasymptoticallynegligible.",
    "page": 961,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 942 SinceY =XÎ·+V(cid:98) Î¸+X (cid:161)Î³ (cid:98) âÎ³(cid:162)Î¸+e, (cid:112) (cid:112) (cid:48) (cid:112) 1 V(cid:98) (cid:48)(cid:161) V(cid:98) Î¸+X (cid:161)Î³ (cid:98) âÎ³(cid:162)Î¸âX (cid:161)Î· (cid:98) âÎ·(cid:162)+e (cid:162) n (cid:161)Î¸ (cid:98)PR âÎ¸(cid:162)= n V V (cid:98) (cid:98) (cid:48) U V (cid:98) (cid:98) = n 1 V(cid:98) (cid:48) V(cid:98) . (29.39) n Thedenominatorequals 1 V(cid:98) (cid:48) V(cid:98) = 1 V (cid:48) V â2 (cid:161)Î³ (cid:98) âÎ³(cid:162)(cid:48) 1 X (cid:48) V +(cid:161)Î³ (cid:98) âÎ³(cid:162)(cid:48) Q n (cid:161)Î³ (cid:98) âÎ³(cid:162) . n n n Thenumeratorequals (cid:112) 1 V(cid:98) (cid:48)(cid:161) V(cid:98) Î¸+X (cid:161)Î³ (cid:98) âÎ³(cid:162)Î¸âX (cid:161)Î· (cid:98) âÎ·(cid:162)+e (cid:162)= (cid:112) 1 V (cid:48) eâ(cid:161)Î³ (cid:98) âÎ³(cid:162)(cid:48) (cid:112) 1 X (cid:48) eâ(cid:161)Î· (cid:98) âÎ·(cid:162)(cid:48) (cid:112) 1 X (cid:48) V n n n n (cid:112) (cid:112) +Î¸(cid:161)Î³âÎ³(cid:162)(cid:48) (cid:112) 1 X (cid:48) V + n (cid:161)Î³âÎ³(cid:162)(cid:48) Q (cid:161)Î·âÎ·(cid:162)âÎ¸ n (cid:161)Î³âÎ³(cid:162)(cid:48) Q (cid:161)Î³âÎ³(cid:162) . (cid:98) (cid:98) n (cid:98) (cid:98) n (cid:98) n Thetermsontherightsidebeyondthefirstareasymptoticallynegligiblebecause (cid:112) (cid:181) (cid:182) n (cid:161)Î³ (cid:98) âÎ³(cid:162)(cid:48) Q n (cid:161)Î³ (cid:98) âÎ³(cid:162)â¤O p (cid:176) (cid:176) Î³(cid:176) (cid:176) 0 lo (cid:112) g n p =o p (1) (29.40) (cid:112) (cid:181) (cid:182) n (cid:161)Î· (cid:98) âÎ·(cid:162)(cid:48) Q n (cid:161)Î· (cid:98) âÎ·(cid:162)â¤O p (cid:176) (cid:176) Î·(cid:176) (cid:176) 0 lo (cid:112) g n p =o p (1) (29.41) byTheorem29.3andAssumption(29.26), (cid:112) n (cid:161)Î³âÎ³(cid:162)(cid:48) Q (cid:161)Î·âÎ·(cid:162)â¤ (cid:179)(cid:112) n (cid:161)Î³âÎ³(cid:162)(cid:48) Q (cid:161)Î³âÎ³(cid:162) (cid:180)1/2(cid:179)(cid:112) n (cid:161)Î·âÎ·(cid:162)(cid:48) Q (cid:161)Î·âÎ·(cid:162) (cid:180)1/2 (cid:98) n (cid:98) (cid:98) n (cid:98) (cid:98) n (cid:98) (cid:181) (cid:182) â¤O p (cid:176) (cid:176) Î³(cid:176) (cid:176) 1 0 /2(cid:176) (cid:176) Î·(cid:176) (cid:176) 1 0 /2 lo (cid:112) g n p =o p (1) bytheSchwarzinequalityandtheaboveresults,and (cid:175) (cid:175) (cid:176) (cid:176) (cid:175) (cid:175) (cid:175) (cid:161)Î³ (cid:98) âÎ³(cid:162)(cid:48) (cid:112) 1 n X (cid:48) e (cid:175) (cid:175) (cid:175) â¤(cid:176) (cid:176) Î³ (cid:98) âÎ³(cid:176) (cid:176) 1 (cid:176) (cid:176) (cid:176) (cid:112) 1 n X (cid:48) e (cid:176) (cid:176) (cid:176) â ï£« (cid:115) ï£¶ â¤O pï£­ (cid:176) (cid:176) Î³(cid:176) (cid:176) 0 lo n gp ï£¸O p (cid:179)(cid:113) logp (cid:180) =O p (cid:181) (cid:176) (cid:176) Î³(cid:176) (cid:176) 0 lo (cid:112) g n p (cid:182) =o p (1) (29.42) byHÃ¶lderâs(29.2),Theorem29.3,theMaximalMarchinkiewicz-Zygmundinequality(B.47)(asin(29.37)), andAssumption(29.26).Similarly (cid:161)Î³âÎ³(cid:162)(cid:48) (cid:112) 1 X (cid:48) V =o (1) (cid:98) p n (cid:161)Î·âÎ·(cid:162)(cid:48) (cid:112) 1 X (cid:48) V =o (1)",
    "page": 962,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". (cid:98) p n Togetherwehaveshownthatin(29.39),thereplacementof(V(cid:98),U(cid:98))by(V(cid:98),U(cid:98))isasymptoticallynegligible. â  _____________________________________________________________________________________________",
    "page": 962,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "CHAPTER29. MACHINELEARNING 943 29.24 Exercises Exercise29.1 ProveTheorem29.1.Hint:TheproofissimilartothatofTheorem3.7. Exercise29.2 Showthat(29.7)istheMallowscriterionforridgeregression. ForadefinitionoftheMal- lowscriterionseeSection28.6. Exercise29.3 Derivetheconditionalbias(29.8)andvariance(29.9)oftheridgeregressionestimator. Exercise29.4 Showthattheridgeregressionestimatorcanbecomputedasleast(cid:112)squaresappliedtoan augmenteddataset. Taketheoriginaldata(Y,X). Addp 0âstoY andp rowsof Î»I to X,applyleast p squares,andshowthatthisequalsÎ² (cid:98)ridge . Exercise29.5 WhichestimatorproducesahigherregressionR2,leastsquaresorridgeregression? Exercise29.6 DoesridgeregressionrequirethatthecolumnsofX linearlyindependent?Takeasample (Y,X).CreatetheaugmentedregressorsetX(cid:101) =(X,X)(addaduplicateofeachregressor)andlet(Î² (cid:98)1 ,Î² (cid:98)2 ) betheridgeregressioncoefficientsfortheregressionofY onX(cid:101).ShowthatÎ² (cid:98)1 =Î² (cid:98)2 = 2 1(cid:161) X (cid:48) X +I p Î» (cid:101) (cid:162)â1(cid:161) X (cid:48) Y (cid:162) withÎ» (cid:101) =Î»/2. Exercise29.7 Repeat the previous question for Lasso regression. Show that the Lasso coefficient esti- matesÎ² (cid:98)1 andÎ² (cid:98)2 areindividuallyindeterminatebuttheirsumsatisfiesÎ² (cid:98)1 +Î² (cid:98)2 =Î² (cid:98)Lasso ,thecoefficients fromtheLassoregressionofY onX. Exercise29.8 Youhavethecontinuousvariables(Y,X)withX â¥0andyouwanttoestimatearegression treefor(cid:69)[Y |X]. AfriendsuggestsaddingaquadraticX2tothevariablesforaddedflexibility. Doesthis makesense? Exercise29.9 Take the cpsmar09 dataset and the subsample of Asian women (n = 1149). Estimate a Lasso linear regression of log(wage) on the following variables: education; dummies for education equalling12,13,14,15,16,18,and20;experience/40inpowersfrom1to9;dummiesformarriagecate- goriesmarried,divorced,separated,widowed,nevermarried;dummiesforthefourregions;dummyfor unionmembership.Reporttheestimatedmodelandcoefficients. Exercise29.10 RepeattheaboveexerciseusingthesubsampleofHispanicmen(n=4547).",
    "page": 963,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Appendices 944",
    "page": 964,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Appendix A Matrix Algebra A.1 Notation Ascalaraisasinglenumber. AvectoraisakÃ1listofnumberstypicallyarrangedinacolumn.Wewritethisas ï£« ï£¶ a 1 ï£¬ a ï£· a=ï£¬ ï£¬ . 2 ï£· ï£·. ï£¬ . . ï£· ï£­ ï£¸ a k Equivalently,avectoraisanelementofEuclideank space,writtenasaâ(cid:82)k.Ifk=1thenaisascalar. Amatrix AisakÃr rectangulararrayofnumbers,writtenas ï£® ï£¹ a a Â·Â·Â· a 11 12 1r ï£¯ a a Â·Â·Â· a ï£º A=ï£¯ ï£¯ . 21 . 22 . 2r ï£º ï£º. ï£¯ . . . . . . ï£º ï£° ï£» a a Â·Â·Â· a k1 k2 kr Byconventiona referstotheelementintheith rowand jth columnof A. Ifr =1then A isacolumn ij vector.Ifk=1then Aisarowvector.Ifr =k=1,then Aisascalar. Astandardconvention(whichwewillfollowinthistextwheneverpossible)istodenotescalarsby lower-caseitalicsa,vectorsbylower-casebolditalicsa,andmatricesbyupper-casebolditalicsA.Some- timesamatrix Aisdenotedbythesymbol(a ). ij Amatrixcanbewrittenasasetofcolumnvectorsorasasetofrowvectors.Thatis, ï£® ï£¹ Î± 1 ï£¯ Î± ï£º A=(cid:163) a 1 a 2 Â·Â·Â· a r (cid:164)=ï£¯ ï£¯ ï£¯ . . . 2 ï£º ï£º ï£º ï£° ï£» Î± k where ï£® ï£¹ a 1i ï£¯ a ï£º a i =ï£¯ ï£¯ ï£¯ . . . 2i ï£º ï£º ï£º ï£° ï£» a ki 945",
    "page": 965,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXA. MATRIXALGEBRA 946 arecolumnvectorsand Î± =(cid:163) a a Â·Â·Â· a (cid:164) j j1 j2 jr arerowvectors. ThetransposeofamatrixA,denotedA (cid:48) ,A (cid:62) ,orAt,isobtainedbyflippingthematrixonitsdiagonal. (cid:48) Inmostoftheeconometricsliterature, andthistextbook, weuse A , butinthemathematicsliterature (cid:62) A istheconvention.Thus ï£® ï£¹ a a Â·Â·Â· a 11 21 k1 ï£¯ a a Â·Â·Â· a ï£º A (cid:48)=ï£¯ ï£¯ . 12 . 22 k . 2 ï£º ï£º. ï£¯ . . . . . . ï£º ï£° ï£» a a Â·Â·Â· a 1r 2r kr Alternatively,lettingB = A (cid:48) ,thenb =a . Notethatif A iskÃr,then A (cid:48) isrÃk. Ifa isakÃ1vector, ij ji thena (cid:48) isa1Ãk rowvector. Amatrixissquareifk=r.Asquarematrixissymmetricif A=A (cid:48) ,whichrequiresa =a .Asquare ij ji matrix is diagonal if the off-diagonal elements are all zero, so that a =0 if i (cid:54)= j. A square matrix is ij upper(lower)diagonalifallelementsbelow(above)thediagonalequalzero. An important diagonal matrix is the identity matrix, which has ones on the diagonal. The kÃk identitymatrixisdenotedas ï£® ï£¹ 1 0 Â·Â·Â· 0 ï£¯ 0 1 Â·Â·Â· 0 ï£º I k =ï£¯ ï£¯ ï£¯ . . . . . . . . . ï£º ï£º ï£º . ï£° ï£» 0 0 Â·Â·Â· 1 Apartitionedmatrixtakestheform ï£® ï£¹ A A Â·Â·Â· A 11 12 1r ï£¯ A A Â·Â·Â· A ï£º A=ï£¯ ï£¯ . 21 . 22 . 2r ï£º ï£º ï£¯ . . . . . . ï£º ï£° ï£» A A Â·Â·Â· A k1 k2 kr wherethe A denotematrices,vectorsand/orscalars. ij A.2 ComplexMatrices Scalars, vectors and matrices may contain real or complex numbers as entries. (However, most econometricapplicationsexclusivelyuserealmatrices.) Ifallelementsofavectorx arerealwesaythat x isarealvector,andsimilarlyformatrices. (cid:112) Recall that a complex number can be written as x = a+bi where where i= â1 and a and b are realnumbers. Similarlyavectorwithcomplexelementscanbewrittenasx =a+biwherea andb are realvectors, andamatrixwithcomplexelementscanbewrittenas X = A+Biwhere A andB arereal matrices. Recallthatthecomplexconjugateofx=a+biisx â=aâbi. Formatrices,theanalogousconceptis theconjugatetranspose. TheconjugatetransposeofX =A+BiisX â=A (cid:48)âB (cid:48) i. Itisobtainedbytaking thetransposeandtakingthecomplexconjugateofeachelement.",
    "page": 966,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXA. MATRIXALGEBRA 947 A.3 MatrixAddition Ifthematrices A=(cid:161) a (cid:162) andB=(cid:161) b (cid:162) areofthesameorder,wedefinethesum ij ij A+B=(cid:161) a +b (cid:162) . ij ij Matrixadditionfollowsthecommutativeandassociativelaws: A+B=B+A A+(B+C)=(A+B)+C. A.4 MatrixMultiplication If AiskÃr andc isreal,wedefinetheirproductas Ac=cA=(cid:161) a c (cid:162) . ij IfaandbarebothkÃ1thentheirinnerproductis k a (cid:48) b=a b +a b +Â·Â·Â·+a b = (cid:88) a b . 1 1 2 2 k k j j j=1 Notethata (cid:48) b=b (cid:48) a.Wesaythattwovectorsaandbareorthogonalifa (cid:48) b=0. IfAiskÃr andB isrÃs,sothatthenumberofcolumnsofAequalsthenumberofrowsofB,wesay that AandB areconformable. Inthiseventthematrixproduct AB isdefined. Writing Aasasetofrow vectorsandB asasetofcolumnvectors(eachoflengthr),thenthematrixproductisdefinedas ï£® a (cid:48) ï£¹ ï£® a (cid:48) b a (cid:48) b Â·Â·Â· a (cid:48) b ï£¹ 1 1 1 1 2 1 s ï£¯ a (cid:48) ï£º ï£¯ a (cid:48) b a (cid:48) b Â·Â·Â· a (cid:48) b ï£º AB=ï£¯ ï£¯ ï£¯ . . . 2 ï£º ï£º ï£º (cid:163) b 1 b 2 Â·Â·Â· b s (cid:164)=ï£¯ ï£¯ ï£¯ 2 . . . 1 2 . . . 2 2 . . . s ï£º ï£º ï£º . ï£° ï£» ï£° ï£» a (cid:48) a (cid:48) b a (cid:48) b Â·Â·Â· a (cid:48) b k k 1 k 2 k s Matrix multiplication is not commutative: in general AB(cid:54)=BA. However, it is associative and dis- tributive: A(BC)=(AB)C A(B+C)=AB+AC. Analternativewaytowritethematrixproductistousematrixpartitions.Forexample, (cid:183) A A (cid:184)(cid:183) B B (cid:184) (cid:183) A B +A B A B +A B (cid:184) AB= 11 12 11 12 = 11 11 12 21 11 12 12 22 . A A B B A B +A B A B +A B 21 22 21 22 21 11 22 21 21 12 22 22 Asanotherexample, ï£® ï£¹ B 1 ï£¯ B ï£º AB=(cid:163) A 1 A 2 Â·Â·Â· A r (cid:164)ï£¯ ï£¯ ï£¯ . . . 2 ï£º ï£º ï£º ï£° ï£» B r =A B +A B +Â·Â·Â·+A B 1 1 2 2 r r r (cid:88) = A B . j j j=1",
    "page": 967,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXA. MATRIXALGEBRA 948 Animportantpropertyoftheidentitymatrixisthatif AiskÃr then AI =AandI A=A. r k WesaytwomatricesAandB areorthogonalifA (cid:48) B=0.ThismeansthatallcolumnsofAareorthog- onalwithallcolumnsofB. ThekÃr matrixH,r â¤k,iscalledorthonormalifH (cid:48) H =I . ThismeansthatthecolumnsofH are r mutuallyorthogonalandeachcolumnisnormalizedtohaveunitlength. A.5 Trace ThetraceofakÃk squarematrix Aisthesumofitsdiagonalelements k (cid:88) tr(A)= a . ii i=1 Somestraightforwardpropertiesforsquarematrices AandB andrealc are tr(cA)=ctr(A) tr (cid:161) A (cid:48)(cid:162)=tr(A) tr(A+B)=tr(A)+tr(B) tr(I )=k. k Also,forkÃr AandrÃk B wehave tr(AB)=tr(BA). (A.1) Indeed, ï£® a (cid:48) b a (cid:48) b Â·Â·Â· a (cid:48) b ï£¹ 1 1 1 2 1 k ï£¯ a (cid:48) b a (cid:48) b Â·Â·Â· a (cid:48) b ï£º tr(AB)=tr ï£¯ ï£¯ 2 . 1 2 . 2 2 . k ï£º ï£º ï£¯ . . . . . . ï£º ï£° ï£» a (cid:48) b a (cid:48) b Â·Â·Â· a (cid:48) b k 1 k 2 k k k = (cid:88) a (cid:48) b i i i=1 k = (cid:88) b (cid:48) a i i i=1 =tr(BA). A.6 RankandInverse TherankofthekÃr matrix(r â¤k) A=(cid:163) a a Â·Â·Â· a (cid:164) 1 2 r isthenumberoflinearlyindependentcolumnsa andiswrittenasrank(A). Wesaythat Ahasfullrank j ifrank(A)=r. AsquarekÃk matrix A issaidtobenonsingularifitishasfullrank,e.g. rank(A)=k. Thismeans thatthereisnokÃ1c(cid:54)=0suchthat Ac=0. IfasquarekÃkmatrixAisnonsingularthenthereexistsauniquematrixkÃkmatrixA â1calledthe inverseof Awhichsatisfies AA â1=A â1A=I . k",
    "page": 968,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXA. MATRIXALGEBRA 949 Fornon-singular AandC someimportantpropertiesinclude AA â1=A â1A=I k (cid:161) A â1(cid:162)(cid:48) =(cid:161) A (cid:48)(cid:162)â1 (AC) â1=C â1A â1 (A+C) â1=A â1(cid:161) A â1+C â1(cid:162)â1 C â1 A â1â(A+C) â1=A â1(cid:161) A â1+C â1(cid:162)â1 A â1. IfakÃk matrixH isorthonormal(sothatH (cid:48) H =I )thenH isnonsingularandH â1=H (cid:48) . Further- k more,HH (cid:48)=I andH (cid:48)â1=H. k Anotherusefulresultfornon-singular AisknownastheWoodburymatrixidentity (A+BCD) â1=A â1âA â1BC (cid:161) C+CDA â1BC (cid:162)â1 CDA â1. Inparticular,forC =1,B=bandD=b (cid:48) forvectorbwefindtheShermanâMorrisonformula (cid:161) A+bb (cid:48)(cid:162)â1=A â1â(cid:161) 1+b (cid:48) A â1b (cid:162)â1 A â1bb (cid:48) A â1. andsimilarlyusingC =â1 (cid:161) Aâbb (cid:48)(cid:162)â1=A â1+(cid:161) 1âb (cid:48) A â1b (cid:162)â1 A â1bb (cid:48) A â1. (A.2) Thefollowingfactaboutinvertingpartitionedmatricesisquiteuseful. (cid:183) A A 11 A A 12 (cid:184)â1 d=ef (cid:183) A A 1 2 1 1 A A 1 2 2 2 (cid:184) = (cid:183) âA â1 A â 1 A 1 1 Â·2 A â1 âA â 11 1 A Â·2 â A 1 12 A â 22 1 (cid:184) (A.3) 21 22 22Â·1 21 11 22Â·1 where A 11Â·2 =A 11 âA 12 A â 22 1A 21 and A 22Â·1 =A 22 âA 21 A â 11 1A 12 .Therearealternativealgebraicrepresenta- tionsforthecomponents. Forexample,usingtheWoodburymatrixidentityyoucanshowthefollowing alternativeexpressions A11=A â1+A â1A A â1 A A â1 11 11 12 22Â·1 21 11 A22=A â1+A â1A A â1 A A â1 22 22 21 11Â·2 12 22 A12=âA â1A A â1 11 12 22Â·1 A21=âA â1A A â1 . 22 21 11Â·2 â EvenifamatrixAdoesnotpossessaninversewedefinetheMoore-PenrosegeneralizedinverseA asthematrixwhichsatisfies AA â A=A A â AA â=A â â AA issymmetric â A Aissymmetric. â Foranymatrix AtheMoore-Penrosegeneralizedinverse A existsandisunique. Forexample,if (cid:183) (cid:184) A 0 A= 11 0 0 and A â1existsthen 11 A â= (cid:183) A â 11 1 0 (cid:184) . 0 0",
    "page": 969,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXA. MATRIXALGEBRA 950 A.7 OrthogonalandOrthonormalMatrices WesaythattwokÃ1vectorsh andh areorthogonalifh (cid:48) h =0.Thismeansthattheyareperpen- 1 2 1 2 dicular. WesaythatakÃ1vectorhisaunitvectorifh (cid:48) h=1.Thismeansthatithasunitlengthin(cid:82)k. WesaythattwokÃ1vectorsh andh areorthonormaliftheyareorthogonalunitvectors. 1 2 WesaythatthekÃm andkÃm matricesH andH areorthogonalifH (cid:48) H =0. 1 2 1 2 1 2 WesaythatthekÃm(kâ¥m)matrixH isorthonormalifH (cid:48) H=I .Thismeansthatthecolumnsof m H areorthonormal.SomecallH anorthogonalmatrix. TypicallyanorthonormalmatrixiswrittenasH. IfH isakÃk orthogonalmatrixthenithasfullrankk,H (cid:48) H=I ,HH (cid:48)=I ,andH â1=H (cid:48) . k k A.8 Determinant Thedeterminantisameasureofthevolumeofasquarematrix. ItiswrittenasdetA or|A|. Inthis textbookweusedetAforclarity. Whilethedeterminantiswidelyused,itsprecisedefinitionisrarelyneeded.However,wepresentthe definitionhereforcompleteness.LetA=(cid:161) a (cid:162) beakÃkmatrix.LetÏ=(cid:161) j ,...,j (cid:162) denoteapermutation ij 1 k of(1,...,k). Therearek!suchpermutations. Thereisauniquecountofthenumberofinversionsofthe indicesofsuchpermutations(relativetothenaturalorder(1,...,k),andletÎµ Ï =+1ifthiscountiseven andÎµ Ï =â1ifthecountisodd.Thenthedeterminantof Aisdefinedas (cid:88) detA= Ï Îµ Ïa 1j1 a 2j2 Â·Â·Â·a kjk . Forexample,ifAis2Ã2thenthetwopermutationsof(1,2)are(1,2)and(2,1)forwhichÎµ =1and (1,2) Îµ =â1.Thus (2,1) detA=Îµ a a +Îµ a a (1,2) 11 22 (2,1) 21 12 =a a âa a . 11 22 12 21 ForasquarematrixAtheminorM oftheijthelementa isthedeterminantofthematrixobtained ij ij byremovingtheith rowand jth columnof A. Thecofactoroftheijth elementisC =(â1)i+jM . An ij ij importantrepresentationknownasLaplaceâsexpansionrelatesthedeterminantof Atoitscofactors: k (cid:88) detA= a C . ij ij j=1 Thisholdsforalli =1,,..,k.Thisisoftenpresentedasamethodforcomputationofadeterminant. TheoremA.1 Propertiesofthedeterminant 1. det(A)=det (cid:161) A (cid:48)(cid:162) 2. det(cA)=ckdetA 3. det(AB)=det(BA)=(detA)(detB) 4. det (cid:161) A â1(cid:162)=(detA) â1 (cid:183) (cid:184) 5. det A B =(detD)det (cid:161) AâBD â1C (cid:162) ifdetD(cid:54)=0 C D",
    "page": 970,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXA. MATRIXALGEBRA 951 (cid:183) (cid:184) (cid:183) (cid:184) A B A 0 6. det =det(A)(detD)anddet =det(A)(detD) 0 D C D 7. If AispÃq andB isqÃp thendet (cid:161) I +AB (cid:162)=det (cid:161) I +BA (cid:162) p q 8. If AandD areinvertiblethendet (cid:161) AâBD â1C (cid:162)= det(A) det (cid:161) DâCA â1B (cid:162) det(D) 9. detA(cid:54)=0ifandonlyif Aisnonsingular 10. If Aistriangular(upperorlower),thendetA=(cid:81)k a i=1 ii 11. If Aisorthonormal,thendetA=Â±1 12. A â1=(detA) â1C whereC =(C )isthematrixofcofactors ij A.9 Eigenvalues ThecharacteristicequationofakÃk squarematrix Ais det(Î»I âA)=0. k TheleftsideisapolynomialofdegreekinÎ»sohasexactlykroots,whicharenotnecessarilydistinctand mayberealorcomplex. Theyarecalledthelatentroots,characteristicroots,oreigenvaluesof A. IfÎ» isaneigenvalueofAthenÎ»I âAissingularsothereexistsanon-zerovectorhsuchthat(Î»I âA)h=0 k k or Ah=hÎ». Thevectorhiscalledalatentvector,characteristicvector,oreigenvectorofAcorrespondingtoÎ».They aretypicallynormalizedsothath (cid:48) h=1andthusÎ»=h (cid:48) Ah. SetH=[h Â·Â·Â· h ]andÎ=diag{Î» ,...,Î» }.Amatrixexpressionis 1 k 1 k AH=HÎ Wenowstatesomeusefulproperties. TheoremA.2 Propertiesofeigenvalues. LetÎ» andh ,i =1,...,k,denotethek eigenvaluesandeigen- i i vectorsofasquarematrix A. 1. det(A)=(cid:81)k Î» i=1 i 2. tr(A)=(cid:80)k Î» i=1 i 3. Aisnon-singularifandonlyifallitseigenvaluesarenon-zero. 4. Thenon-zeroeigenvaluesof AB andBAareidentical. 5. IfB isnon-singularthen AandB â1AB havethesameeigenvalues. 6. IfAh=hÎ»then(IâA)h=h(1âÎ»).SoIâAhastheeigenvalue1âÎ»andassociatedeigenvectorh.",
    "page": 971,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXA. MATRIXALGEBRA 952 ManyeigenvalueapplicationsineconometricsconcernthecasewherethematrixAisrealandsym- metric. Inthiscasealleigenvaluesof A arerealanditseigenvectorsaremutuallyorthogonal. Thus H isorthonormalsoH (cid:48) H =I andHH (cid:48)=I . Whentheeigenvaluesareallrealitisconventionaltowrite k k themindecendingorderÎ» â¥Î» â¥Â·Â·Â·â¥Î» . 1 2 k The following is an important property of real symmetric matrices which follows directly from the equations AH=HÎandH (cid:48) H=I . k TheoremA.3 Spectral Decomposition. If A is a kÃk real symmetric matrix then A = HÎH (cid:48) where H contains the eigenvectors and Î is a diagonal matrix with the eigenvalues on the diagaonal. The eigenvaluesareallrealandtheeigenvectormatrixsatisfiesH (cid:48) H=I . k ThespectraldecompositioncanbealternativelywrittenasH (cid:48) AH=Î. If A isreal,symmetric,andinvertible,thenbythespectraldecompositionandthepropertiesofor- thonormalmatrices, A â1=H (cid:48)â1Îâ1H â1=HÎâ1H (cid:48) .ThusthecolumnsofH arealsotheeigenvectorsof A â1,anditseigenvaluesareÎ»â1,Î»â1,...,Î»â1. 1 2 k A.10 PositiveDefiniteMatrices WesaythatakÃk realsymmetricsquarematrix Aispositivesemi-definiteifforallc (cid:54)=0,c (cid:48) Ac â¥0. ThisiswrittenasAâ¥0.WesaythatAispositivedefiniteifforallc(cid:54)=0,c (cid:48) Ac>0.ThisiswrittenasA>0. Somepropertiesinclude: TheoremA.4 Propertiesofpositivesemi-definitematrices 1. If A=G (cid:48) BG withB â¥0andsomematrixG then Aispositivesemi-definite. (Foranyc(cid:54)=0,c (cid:48) Ac= Î±(cid:48) BÎ±â¥0whereÎ±=Gc.)IfG hasfullcolumnrankandB>0then Aispositivedefinite. 2. If Aispositivedefinitethen Aisnon-singularand A â1exists.Furthermore, A â1>0. 3. A>0ifandonlyifitissymmetricandallitseigenvaluesarepositive. 4. Bythespectraldecomposition, A=HÎH (cid:48) where H (cid:48) H =I andÎisdiagonalwithnon-negative k diagonalelements.AlldiagonalelementsofÎarestrictlypositiveif(andonlyif) A>0. 5. Therankof Aequalsthenumberofstrictlypositiveeigenvalues. 6. If A>0then A â1=HÎâ1H (cid:48) . 7. If A â¥0 and rank(A)=r â¤k then the Moore-Penrose generalized inverse of A is A â = HÎâ H (cid:48) whereÎâ=diag (cid:161)Î»â1,Î»â1,...,Î»â1,0,...,0 (cid:162) . 1 2 r A.11 IdempotentMatrices AkÃksquarematrixAisidempotentifAA=A.Whenk=1theonlyidempotentnumbersare1and 0.Fork>1therearemanypossibilities.Forexample,thefollowingmatrixisidempotent (cid:183) 1/2 â1/2 (cid:184) A= . â1/2 1/2",
    "page": 972,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXA. MATRIXALGEBRA 953 If A is idempotent and symmetric with rank r then it has r eigenvalues which equal 1 and kâr eigenvalueswhichequal0. Toseethis,bythespectraldecompositionwecanwrite A=HÎH (cid:48) whereH isorthonormalandÎcontainstheeigenvalues.Then A=AA=HÎH (cid:48) HÎH (cid:48)=HÎ2H (cid:48) . WededucethatÎ2=ÎandÎ»2=Î» fori =1,...,k.HenceeachÎ» mustequaleither0or1.Sincetherank i i i of Aisr,andtherankequalsthenumberofpositiveeigenvalues,itfollowsthat (cid:183) (cid:184) I 0 Î= r . 0 0 kâr Thusthespectraldecompositionofanidempotentmatrix Atakestheform (cid:183) (cid:184) A=H I r 0 H (cid:48) (A.4) 0 0 kâr withH (cid:48) H=I . Additionally,tr(A)=rank(A)and Aispositivesemi-definite. k If Aisidempotentandsymmetricwithrankr <k thenitdoesnotpossessaninverse,butitsMoore- Penrosegeneralizedinversetakesthesimpleform A â= A. Thiscanbeverifiedbycheckingthecondi- tionsfortheMoore-Penrosegeneralizedinverse,forexample AA â A=AAA=A. If AisidempotentthenIâAisalsoidempotent. Oneusefulfactisthatif Aisidempotentthenforanyconformablevectorc, c (cid:48) Acâ¤c (cid:48) c (A.5) c (cid:48) (IâA)câ¤c (cid:48) c (A.6) Toseethis,notethat c (cid:48) c=c (cid:48) Ac+c (cid:48) (IâA)c. Since A and IâA areidempotenttheyarebothpositivesemi-definite,sobothc (cid:48) Ac andc (cid:48) (IâA)c are non-negative.Thustheymustsatisfy(A.5)-(A.6). A.12 SingularValues ThesingularvaluesÏ ofakÃr realmatrixAarethepositivesquarerootsoftheeigenvaluesofA (cid:48) A. j Thusfor j =1,...,r (cid:113) Ï = Î» (cid:161) A (cid:48) A (cid:162) . j j (cid:48) SinceA Aispositivesemi-definiteitseigenvaluesarenon-negative.Thussingularvaluesarealwaysreal andnon-negative. (cid:48) The non-zero singular values of A and A are the same. The number of non-zero singular values equalstherankof A. When Aispositivesemi-definitethenthesingularvaluesof Acorrespondtoitseigenvalues. ItisconventiontowritethesingularvaluesindecendingorderÏ â¥Ï â¥Â·Â·Â·â¥Ï . 1 2 r",
    "page": 973,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXA. MATRIXALGEBRA 954 A.13 MatrixDecompositions Thereareseveralusefulwaystodecomposeamatrixintotheproductsofsimplermatrices.Wehave alreadyintroducedthespectraldecomposition,whichwerepeathereforcompleteness. Thefollowing applytorealmatrices A. SpectralDecomposition: IfAiskÃkandsymmetricthenA=HÎH (cid:48) whereH (cid:48) H=I andÎisadiagonal k matrixwiththe(real)eigenvaluesonthediagaonal. Eigendecomposition:If AiskÃk andhasdistincteigenvaluesthereexistsanonsingularmatrixP such that A=PÎP â1 andP â1AP =Î. ThecolumnsofP aretheeigenvectors. Îisdiagonalwiththeeigen- valuesonthediagonal. MatrixSquareRoot:If AiskÃk andpositivedefinitewecanfindamatrixB suchthat A=BB (cid:48) .Wecall B amatrixsquarerootof AandistypicallywrittenasB=A1/2. The matrix B need not be unique. One matrix square root is obtained using the spectral decom- position A=HÎH (cid:48) . ThenB =HÎ1/2H (cid:48) isitselfsymmetricandpositivedefiniteandsatisfies A=BB. AnothermatrixsquarerootistheCholeskydecomposition,describedinSectionA.16. SingularValueDecomposition:If AiskÃr then A=UÎV (cid:48) whereU iskÃk,ÎiskÃr andV isrÃr.U andV areorthonormal(U (cid:48) U =I andV (cid:48) V =I ). Îisadiagonalmatrixwiththesingularvaluesof Aon k r thediagonal. CholeskyDecomposition:IfAiskÃkandpositivedefinitethenA=LL (cid:48) whereLislowertriangularand fullrank.SeeSectionA.16. QRDecomposition:If AiskÃr withkâ¥r andrankr thenA=QR.Q isakÃr andorthonormalmatrix (Q (cid:48) Q=I ).R isarÃr fullrankuppertriangularmatrix.SeeSectionA.17. r JordanMatrixDecomposition:IfAiskÃkthenA=PJP â1whereJ takestheJordannormalform.The latter is a block diagonal matrix J =diag{J ,...,J } where r is the number of distinct eigenvalues of A. 1 r TheJordanblocksJ arem Ãm wherem isthemultiplicityofÎ» (numberofeigenvaluesequallingÎ» ) i i i i i i andtaketheform ï£® Î» 1 0 ï£¹ i J i = ï£° 0 Î» i 1 ï£» (A.7) 0 0 Î» i illustratedhereform =3. i A.14 GeneralizedEigenvalues Let AandB bekÃk matrices.Thegeneralizedcharacteristicequationis det (cid:161)ÂµBâA (cid:162)=0. The solutions Âµ are known as generalized eigenvalues of A with respect to B. Associated with each generalizedeigenvalueÂµisageneralizedeigenvectorv whichsatisfies Av=BvÂµ. Theyaretypicallynormalizedsothatv (cid:48) Bv=1andthusÂµ=v (cid:48) Av.",
    "page": 974,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXA. MATRIXALGEBRA 955 Amatrixexpressionis AV =BVM whereM=diag (cid:169)Âµ ,...,Âµ (cid:170) . 1 k If AandB arerealandsymmetricthenthegeneralizedeigenvaluesarereal. Suppose in addition that B is invertible. Then the generalized eigenvalues of A with respect to B areequaltotheeigenvaluesofB â1/2AB â1/2(cid:48) . ThegeneralizedeigenvectorsV of AwithrespecttoB are relatedtotheeigenvectorsH ofB â1/2AB â1/2(cid:48) bytherelationshipV =B â1/2(cid:48) H. ThisimpliesV (cid:48) BV =I . k ThusthegeneralizedeigenvectorsareorthogonalizedwithrespecttothematrixB. IfAv=BvÂµthen(BâA)v=Bv(1âÂµ).SoageneralizedeigenvalueofBâAwithrespecttoB is1âÂµ withassociatedeigenvectorv. Generalizedeigenvalueequationshaveaninterestingdualproperty.ThefollowingisbasedonLemma A.9ofJohansen(1995). TheoremA.5 SupposethatB andC are invertiblepÃp andrÃr matrices,respectively,and AispÃr. Thenthegeneralizedeigenvalueproblems det (cid:161)ÂµBâAC â1A (cid:48)(cid:162)=0 (A.8) and det (cid:161)ÂµCâA (cid:48) B â1A (cid:162)=0 (A.9) havethesamenon-zerogeneralizedeigenvalues.Furthermore,foranysuchgeneralizedeigenvalueÂµ,if v andw aretheassociatedgeneralizedeigenvectorsof(A.8)and(A.9),then w=Âµâ1/2C â1A (cid:48) v. (A.10) Proof.LetÂµ(cid:54)=0beaneigenvalueof(A.8).UsingTheoremA.1.8 0=det (cid:161)ÂµBâAC â1A (cid:48)(cid:162) = det (cid:161)ÂµB (cid:162) det (cid:179) CâA (cid:48)(cid:161)ÂµB (cid:162)â1 A (cid:180) det(C) = det(B) det (cid:161)ÂµCâA (cid:48) B â1A (cid:162) . det(C) Sincedet(B)/det(C)(cid:54)=0thisimplies(A.10)holds.HenceÂµisaneigenvalueof(A.9),asclaimed. Wenextshowthat(A.10)isaneigenvectorof(A.9).Notethatthesolutionsto(A.8)and(A.9)satisfy BvÂµ=AC â1A (cid:48) v (A.11) and CwÂµ=A (cid:48) B â1Aw (A.12) andarenormalizedsothatv (cid:48) Bv=1andw (cid:48) Cw=1.Weshowthat(A.10)satisfies(A.12).Using(A.10),we findthattheleft-sideof(A.12)equals C (cid:161)Âµâ1/2C â1A (cid:48)(cid:162)Âµ=A (cid:48)Âµ1/2=A (cid:48) B â1BvÂµ1/2=A (cid:48) B â1AC â1A (cid:48) vÂµâ1/2=A (cid:48) B â1Aw. The third equality is (A.11) and the final is (A.10). This shows that (A.12) holds and thus (A.10) is an eigenvectorof(A.9)asstated. â ",
    "page": 975,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXA. MATRIXALGEBRA 956 A.15 ExtremaofQuadraticForms Theextremaofquadraticformsinrealsymmetricmatricescanbeconvenientlybewritteninterms ofeigenvaluesandeigenvectors. Let A denote a kÃk real symmetric matrix. Let Î» â¥Â·Â·Â·â¥Î» be the ordered eigenvalues of A and 1 k h ,...,h theassociatedorderedeigenvectors. 1 k (cid:48) Westartwithresultsfortheextremaofx Ax.ThroughoutthisSectionwhenwerefertotheâsolutionâ ofanextremumproblemitisthesolutiontothenormalizedexpression. (cid:48) x Ax â¢ maxx (cid:48) Ax=max =Î» .Thesolutionisx=h .(Thatis,themaximizerofx (cid:48) Ax overx (cid:48) x=1.) x(cid:48)x=1 x x (cid:48) x 1 1 (cid:48) x Ax â¢ minx (cid:48) Ax=min =Î» .Thesolutionisx=h . x(cid:48)x=1 x x (cid:48) x k k Multivariategeneralizationscaninvolveeitherthetraceorthedeterminant. â¢ max tr (cid:161) X (cid:48) AX (cid:162)=maxtr (cid:179) (cid:161) X (cid:48) X (cid:162)â1 X (cid:48) AX (cid:180) =(cid:80)(cid:96) Î» . X(cid:48)X=I(cid:96) X i=1 i ThesolutionisX =[h 1 ,...,h(cid:96)]. â¢ X m (cid:48)X i = n I(cid:96) tr (cid:161) X (cid:48) AX (cid:162)=m X intr (cid:179) (cid:161) X (cid:48) X (cid:162)â1 X (cid:48) AX (cid:180) =(cid:80)(cid:96) i=1 Î» kâi+1 . ThesolutionisX =[h kâ(cid:96)+1 ,...,h k ]. ForaproofseeTheorem11.13ofMagnusandNeudecker(2019). Supposeaswellthat A>0withorderedeigenvaluesÎ» â¥Î» â¥Â·Â·Â·â¥Î» andeigenvectors[h ,...,h ]. 1 2 k 1 k â¢ X m (cid:48)X a = x I(cid:96) det (cid:161) X (cid:48) AX (cid:162)=m X ax d d e e t t (cid:161) (cid:161) X X (cid:48) A (cid:48) X X (cid:162) (cid:162) = i (cid:89) = (cid:96) 1 Î» i .ThesolutionisX =[h 1 ,...,h(cid:96)]. â¢ X m (cid:48)X i = n I(cid:96) det (cid:161) X (cid:48) AX (cid:162)=m X in d d e e t t (cid:161) (cid:161) X X (cid:48) A (cid:48) X X (cid:162) (cid:162) = i (cid:89) = (cid:96) 1 Î» kâi+1 .ThesolutionisX =[h kâ(cid:96)+1 ,...,h k ]. â¢ X m (cid:48)X a = x I(cid:96) det (cid:161) X (cid:48) (IâA)X (cid:162)=m X ax det (cid:161) d X et (cid:48) (cid:161) (I X â (cid:48) X A (cid:162) )X (cid:162) = i (cid:89) = (cid:96) 1 (1âÎ» kâi+1 ).ThesolutionisX =[h kâ(cid:96)+1 ,...,h k ]. â¢ X m (cid:48)X i = n I(cid:96) det (cid:161) X (cid:48) (IâA)X (cid:162)=m X in det (cid:161) d X et (cid:48) (cid:161) (I X â (cid:48) X A (cid:162) )X (cid:162) = i (cid:89) = (cid:96) 1 (1âÎ» i ).ThesolutionisX =[h 1 ,...,h(cid:96)]. ForaproofseeTheorem11.15ofMagnusandNeudecker(2019). Wecanextendtheaboveresultstoincorporategeneralizedeigenvalueequations. LetAandB bekÃkrealsymmetricmatriceswithB>0.LetÂµ â¥Â·Â·Â·â¥Âµ betheorderedgeneralized 1 k eigenvaluesof AwithrespecttoB andv ,...,v theassociatedorderedeigenvectors. 1 k (cid:48) x Ax â¢ max x (cid:48) Ax=max =Âµ .Thesolutionisx=v . x(cid:48)Bx=1 x x (cid:48) x 1 1 (cid:48) x Ax â¢ min x (cid:48) Ax=min =Âµ .Thesolutionisx=v . x(cid:48)Bx=1 x x (cid:48) x k k",
    "page": 976,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXA. MATRIXALGEBRA 957 â¢ max tr (cid:161) X (cid:48) AX (cid:162)=maxtr (cid:179) (cid:161) X (cid:48) BX (cid:162)â1 X (cid:48) AX (cid:180) =(cid:80)(cid:96) Âµ . X(cid:48)BX=I(cid:96) X i=1 i ThesolutionisX =[v 1 ,...,v(cid:96)]. â¢ X(cid:48) m BX in =I(cid:96) tr (cid:161) X (cid:48) AX (cid:162)=m X intr (cid:179) (cid:161) X (cid:48) BX (cid:162)â1 X (cid:48) AX (cid:180) =(cid:80)(cid:96) i=1 Âµ kâi+1 . ThesolutionisX =[v kâ(cid:96)+1 ,...,v k ]. Supposeaswellthat A>0. â¢ max det (cid:161) X (cid:48) AX (cid:162)=max det (cid:161) X (cid:48) AX (cid:162) = (cid:89) (cid:96) Âµ . X(cid:48)BX=I(cid:96) X det (cid:161) X (cid:48) BX (cid:162) i=1 i ThesolutionisX =[v 1 ,...,v(cid:96)]. â¢ X(cid:48) m BX in =I(cid:96) det (cid:161) X (cid:48) AX (cid:162)=m X in d d e e t t (cid:161) (cid:161) X X (cid:48) (cid:48) B AX X (cid:162) (cid:162) = i (cid:89) = (cid:96) 1 Âµ kâi+1 . ThesolutionisX =[v kâ(cid:96)+1 ,...,v k ]. â¢ X(cid:48) m BX a = x I(cid:96) det (cid:161) X (cid:48) (IâA)X (cid:162)=m X ax det d (cid:161) e X t (cid:161) (cid:48) ( X I (cid:48) â BX A) (cid:162) X (cid:162) = i (cid:89) = (cid:96) 1 (cid:161) 1âÂµ kâi+1 (cid:162) . ThesolutionisX =[v kâ(cid:96)+1 ,...,v k ]. â¢ min det (cid:161) X (cid:48) (IâA)X (cid:162)=min det (cid:161) X (cid:48) (IâA)X (cid:162) = (cid:89) (cid:96) (cid:161) 1âÂµ (cid:162) . X(cid:48)BX=I(cid:96) X det (cid:161) X (cid:48) BX (cid:162) i=1 i ThesolutionisX =[v 1 ,...,v(cid:96)]. Bychange-of-variableswecanre-expressoneeigenvalueproblemintermsofanother.Forexample, let A>0,B>0,andC >0.Then (cid:161) (cid:48) (cid:162) (cid:161) (cid:48) (cid:162) det X CACX det X AX max =max (cid:161) (cid:48) (cid:162) (cid:161) (cid:48) (cid:162) X det X CBCX X det X BX and (cid:161) (cid:48) (cid:162) (cid:161) (cid:48) (cid:162) det X CACX det X AX min =min . (cid:161) (cid:48) (cid:162) (cid:161) (cid:48) (cid:162) X det X CBCX X det X BX A.16 CholeskyDecomposition ForakÃk positivedefinitematrix AitsCholeskydecompositiontakestheform A=LL (cid:48) whereLis lowertriangularandfullrank. Alowertriangularmatrix(alsoknownasalefttriangularmatrix)takes theform ï£® ï£¹ L 0 Â·Â·Â· 0 11 ï£¯ L L Â·Â·Â· 0 ï£º L=ï£¯ ï£¯ ï£¯ . . . 21 . . . 22 ... . . . ï£º ï£º ï£º . ï£° ï£» L L Â·Â·Â· L k1 k2 kk ThediagonalelementsofL areallstrictlypositive. TheCholeskydecompositionisunique(forpositive definite A).",
    "page": 977,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXA. MATRIXALGEBRA 958 Thedecompositionisveryusefulforarangeofcomputations,especiallywhenamatrixsquareroot isrequired. Algorithmsforcomputationareavailableinstandardpackages(forexample,cholineither MATLABorR). LowertriangularmatricessuchasL havespecialproperties. Oneisthatitsdeterminantequalsthe productofthediagonalelements. ProofsofuniquenessoftheCholeskydecomposition(aswellascomputation)arealgorithmic. Here arethedetailsforthecasek=3.Writeout ï£® ï£¹ ï£® ï£¹ï£® ï£¹ A A A L 0 0 L L L 11 21 31 11 11 21 31 ï£° A 21 A 22 A 32 ï£» =A=LL (cid:48)= ï£° L 21 L 22 0 ï£»ï£° 0 L 22 L 32 ï£» A A A L L L 0 0 L 31 32 33 31 32 33 33 ï£® L2 L L L L ï£¹ 11 11 21 11 31 = ï£° L 11 L 21 L2 21 +L2 22 L 31 L 21 +L 32 L 22 ï£». L L L L +L L L2 +L2 +L2 11 31 31 21 32 22 31 32 33 Therearesixequations,sixknowns(theelementsof A),andsixunknowns(theelementsofL). Wecan solveforthelatterbystartingwiththefirstcolumn,movingfromtoptobottom. Thefirstelementhas (cid:112) thesimplesolutionL = A .Thishasarealsolutionsince A >0.Movingdown,sinceL isknown, 11 11 11 11 fortheentriesbeneathL wesolveandfind 11 A A L = 21 = (cid:112) 21 21 L 11 A 11 A A L = 31 = (cid:112) 31 . 31 L 11 A 11 Nextwemovetothesecondcolumn.WeobservethatL isknown.ThenwesolveforL 21 22 (cid:115) (cid:113) A2 L = A âL2 = A â 21. 22 22 21 22 A 11 Thishasarealsolutionsince A>0.ThensinceL isknownwecanmovedownthecolumntofind 22 L = A 32 âL 31 L 21 = A 32 â A3 A 1 1 A 1 21 . 32 (cid:114) L 22 A â A2 21 22 A11 Finallywetakethethirdcolumn.AllelementsexceptL areknown.Sowesolvetofind 33 (cid:118) (cid:117) (cid:179) (cid:180)2 L = (cid:113) A âL2 âL2 = (cid:117) (cid:117) (cid:117)A â A2 31 â A 32 â A3 A 1 1 A 1 21 . 33 33 31 32 (cid:116) 33 A 11 A â A2 21 22 A11 A.17 QRDecomposition TheQRdecompositioniswidelyusedfornumericalproblemssuchasmatrixinversionandsolving systemsoflinearequations. Let A be an kÃr matrix, with k â¥r and rank r. The QR decomposition of A is A =QR whereQ isakÃr orthonormalmatrixandR isar Ãr fullrankuppertriangularmatrix(alsoknownasaright triangularmatrix).",
    "page": 978,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXA. MATRIXALGEBRA 959 To show that the QR decomposition exists, observe that A (cid:48) A is r Ãr and positive definite. Apply the Cholesky decomposition to find A (cid:48) A =LL (cid:48) where L is lower triangular and full rank. We then set Q=A (cid:161) L (cid:48)(cid:162)â1 andR=L (cid:48) .ThematrixR isuppertriangularbyconstruction.Also, Q (cid:48) Q=(cid:161) L (cid:48)(cid:162)â1(cid:48) A (cid:48) A (cid:161) L (cid:48)(cid:162)â1=L â1LL (cid:48)(cid:161) L (cid:48)(cid:162)â1=I k soQ isorthonormalasclaimed. NumericalcomputationoftheQRdecompositiondoesnotusetheabovematrixoperations.Instead itisdonealgorithmically. StandardmethodsincludetheGram-SchmidtandHouseholderalgorithms. TheGram-SchmidtissimpletodescribeandimplementbuttheHouseholderisnumericallymorestable andisthereforethestandardimplementation.Sincethealgorithmisinvolvedwedonotdescribeithere. A.18 SolvingLinearSystems Alinearsystemofk equationswithk unknownsis a b +a b +Â·Â·Â·+a b =c 11 1 12 2 1k k 1 a b +a b +Â·Â·Â·+a b =c 21 1 22 2 2k k 2 . . . a b +a b +Â·Â·Â·+a b =c k1 1 k2 2 kk k k orinmatrixnotation Ab=c (A.13) whereAiskÃk,andbandcarekÃ1.IfAisfullrankthenthesolutionb=A â1cisunique.Inthissection wedescribethreealgorithmstonumericallyfindthesolutionb.ThefirstusesGaussianelimination,the secondusestheQRdecomposition,andthethirdusestheCholeskydecomposition. (1)SolvingbyGaussianelimination The solution b in (A.13) is invariant to row operations; including multiplying an equation by non- zeronumbers,andaddingandsubtractingequationsfromoneanother. Toexploitthisinsightcombine theknownconstants Aandc intoakÃ(k+1)augmentedmatrix [A|c]. (A.14) Therowoperationsdescribedabovearethesameasmultiplyingrowsof[A|c]bynon-zeronumbersand addingandsubtractingrowsof[A|c]fromoneanother. Suchoperationsdonotchangethesolutionb. Gaussianeliminationworksbyapplyingrowoperationsto[A|c]untiltheleftsectionequalstheidentity matrixI andthusequals k [I |d]. (A.15) k Since row operations do not alter the solution, this means that the solution b in (A.13) also satisfies I b=d whichimpliesb=d.Thusthesolutionbcanbefoundastheright-mostvectord in(A.15). k TheGauss-Jordanalgorithmimplementsasequenceofrowoperationswhichobtainsthesolution foranypair(A.14)suchthat Aisfullrank.Thealgorithmisasfollows. Forr =1,...,k: 1. Dividetheelementsofrowr bya .Thusmakethechanges rr",
    "page": 979,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXA. MATRIXALGEBRA 960 (a) a (cid:55)âa /a fori =1,...,k ri ri rr (b) c (cid:55)âc /a r r rr 2. Forrows j (cid:54)=r,subtracta timesrowr fromrow j.Thusmakethechanges jr (a) a (cid:55)âa âa a fori =1,...,k ji ji jr ri (b) c (cid:55)âc âa c j j jr r Eachpairofoperationstransformsacolumnofthematrix A intoancolumnoftheidentitymatrix I ,startingwiththefirstcolumnandworkingsequentiallytotheright. Thefirstoperation(dividingby k a )normalizestherthdiagonalelementtounity.Thesecondsetofoperationsmakesrowoperationsto rr transformtheremainingelementsoftherth columntoequalzero. Sincethepreviouscolumnsareunit vectorstheyareunaffectedbytheseoperations. (2)SolvingbyQRDecomposition First,computetheQRdecomposition A=QR whereQ isakÃk orthogonalmatrix,andR iskÃk anduppertriangular. Thisisisdonenumerically(typicallybytheHouseholderalgorithm)asdescribed inSectionA.17. Thismeansthat(A.13)canbewrittenasQRb=c. PremultiplyingbyQ (cid:48) andobserving Q (cid:48) Q=I weobtainRb=Q (cid:48) c d=ef d.Thissystemcanbewrittenas k r 11 b 1 +r 12 b 2 +Â·Â·Â·+r 1,kâ1 b kâ2 +r 1k b k =d 1 r 22 b 2 +Â·Â·Â·+r 2,kâ1 b kâ2 +r 2k b k =d 2 . . . r kâ1,kâ1 b kâ2 +r kâ1,k b k =d kâ1 r b =d . kk k k Thiscanbesolvedbybackwardsrecursion b =d /r k k kk b kâ1 =(cid:161) d kâ1 âr kâ1,k b k (cid:162) /r kâ1,kâ1 . . . b =(d âr b âÂ·Â·Â·âr b )/r . 1 1 12 2 1k k 11 Tosummarize,theQRsolutionmethodis 1. NumericallycomputetheQRdecomposition A=QR. 2. Calculated =Q (cid:48) c. 3. Solveforbbybackwardrecursion. (3)SolvingbyCholeskyDecompositionforpositivedefinite A First,computetheCholeskydecompositionA=LR whereLiskÃkandlowertriangular,andR=L (cid:48) isuppertriangular. ThisisisdonenumericallyasdescribedinSectionA.16. Thismeansthat(A.13)can bewrittenasLRb=c orLd =c whered =Rb. Thevectord canbesolvedfromL andc usingforward recursion.TheequationRb=d canthenbesolvedforbbybackwardsrecursion.",
    "page": 980,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXA. MATRIXALGEBRA 961 We have described three algorithms. Which should be used in practice? For positive definite A, solvingbytheCholeskydecompositionisthepreferredmethodasitisnumericallymostefficientand stable. When A is not positive definite, solving by the QR decomposition is the preferred method as itisnumericallymoststable. TheadvantageoftheGauss-Jordanalgorithmisthatitisthesimplestto program. A.19 AlgorithmicMatrixInversion Numericalmethodsforsolvinglinearsystemscanbeusedtocalculatetheinverseofafull-rankkÃk matrix A. LetB =A â1betheinverseof A. Thematricessatisfy AB =I whichisamatrixgeneralization k of(A.13).ThegoalistosolvethissystemtoobtainB. (1)SolvingbyGaussianelimination Replacec in(A.14)withI andapplytheGauss-Jordaneliminationalgorithm.ThesolutionisB. k (2)SolvingbyQRdecomposition NumericallycomputetheQRdecomposition A=QR. ThisimpliesQRB =I . PremultiplyingbyQ (cid:48) k andobservingQ (cid:48) Q =I weobtainRB =Q (cid:48) . WriteB =[b ,...,b ]andQ (cid:48)=(cid:163) q ,...,q (cid:164) . For j =1,...,k, k 1 k 1 k Rb =q .SinceR isuppertriangularthevectorb canbefoundbybackwardsrecursionasdescribedin j j j SectionA.18. (3)SolvingbyCholeskydecompositionforpositivedefinite A Compute the Cholesky decomposition A =LR where L is kÃk and lower triangular and R =L (cid:48) is upper triangular. This implies LRB = I or LC = I whereC =RB. Applying forward recursion one k k columnatatimewecansolveforC. WethenhaveRB =C. Applyingbackwardsrecursiononecolumn atatimewecansolveforB. A.20 MatrixCalculus Letx=(x ,...,x ) (cid:48) bekÃ1andg(x)=g(x ,...,x ):(cid:82)kâ(cid:82).Thevectorderivativeis 1 k 1 k ï£« â ï£¶ g(x) â âx1 âx g(x)=ï£¬ ï£¬ ï£­ . . . ï£· ï£· ï£¸ â g(x) âxk and â (cid:179) (cid:180) g(x)= â g(x) Â·Â·Â· â g(x) . âx (cid:48) âx1 âxk Somepropertiesarenowsummarized. TheoremA.6 Propertiesofmatrixderivatives 1. â (cid:161) a (cid:48) x (cid:162)= â (cid:161) x (cid:48) a (cid:162)=a âx âx 2. â (cid:161) x (cid:48) A (cid:162)=Aand â (Ax)=A âx âx(cid:48) 3. â (cid:161) x (cid:48) Ax (cid:162)=(cid:161) A+A (cid:48)(cid:162) x âx 4. â2 (cid:161) x (cid:48) Ax (cid:162)=A+A (cid:48) âxâx(cid:48) 5. â tr(BA)=B (cid:48) âA",
    "page": 981,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXA. MATRIXALGEBRA 962 6. â logdet(A)=(A â ) (cid:48) âA Toshowpart1,notethat â â (cid:161) a (cid:48) x (cid:162)= (a x +Â·Â·Â·+a x )=a . âx âx 1 1 k k j j j Thus ï£« ï£¶ a 1 â âx (cid:161) a (cid:48) x (cid:162)=ï£¬ ï£­ . . . ï£· ï£¸ =a a k asclaimed. Forpart2,write A=[a ,...,a ]sothat 1 m â â (cid:183) â â (cid:184) (cid:161) x (cid:48) A (cid:162)= (cid:163) x (cid:48) a ,...,x (cid:48) a (cid:164)= (cid:161) x (cid:48) a (cid:162) ,..., (cid:161) x (cid:48) a (cid:162) =[a ,...,a ]=A âx âx 1 m âx 1 âx m 1 m usingpart1. â (Ax)=Afollowsbytakingthetranspose. âx(cid:48) Forpart3,noticex (cid:48) Ax=x (cid:48) A (cid:48) x andapplytheproductruleandthenpart2, â â â (cid:161) x (cid:48) Ax (cid:162)= (cid:161) x (cid:48) I (cid:162) Ax+ (cid:161) x (cid:48) A (cid:48)(cid:162) x=I Ax+A (cid:48) x=(cid:161) A+A (cid:48)(cid:162) x. âx âx k âx k Forpart4,applyingpart3wefind â2 â â â (cid:161) x (cid:48) Ax (cid:162)= (cid:161) x (cid:48) Ax (cid:162)= x (cid:48)(cid:161) A+A (cid:48)(cid:162)=A+A (cid:48) . âxâx (cid:48) âx âx (cid:48) âx Forpart5,recallfromSectionA.5thatwecanwriteoutexplicitly (cid:88)(cid:88) tr(BA)= a b . ij ji i j Thusifwetakethederivativewithrespecttoa wefind ij â tr(BA)=b . âa ji ij whichistheijth elementofB (cid:48) ,establishingpart5. Forpart6,recallLaplaceâsexpansion k (cid:88) detA= a C . ij ij j=1 whereC istheijth cofactorof A.SetC =(C ).ObservethatC for j =1,...,k arenotfunctionsofa . ij ij ij ij Thusthederivativewithrespecttoa is ij â â logdet(A)=(detA) â1 detA=(detA) â1C . âa âa ij ij ij Togetherthisimplies â logdet(A)=(detA) â1C =A â1 âA wherethesecondequalityisTheoremA.1.12.",
    "page": 982,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXA. MATRIXALGEBRA 963 A.21 KroneckerProductsandtheVecOperator Let A=[a a Â·Â·Â· a ]bemÃn.Thevecof A,denotedbyvec(A),isthemnÃ1vector 1 2 n ï£« ï£¶ a 1 ï£¬ a ï£· vec(A)=ï£¬ ï£¬ . 2 ï£· ï£·. ï£¬ . . ï£· ï£­ ï£¸ a n LetA=(cid:161) a (cid:162) beanmÃnmatrixandletBbeanymatrix.TheKroneckerproductofAandB,denoted ij AâB,isthematrix ï£® ï£¹ a B a B Â·Â·Â· a B 11 12 1n ï£¯ a B a B Â·Â·Â· a B ï£º AâB=ï£¯ ï£¯ 2 . 1 2 . 2 2n . ï£º ï£º. ï£¯ . . . . . . ï£º ï£° ï£» a B a B Â·Â·Â· a B m1 m2 mn Some important properties are now summarized. These results hold for matrices for which all matrix multiplicationsareconformable. TheoremA.7 PropertiesoftheKroneckerproduct 1. (A+B)âC =AâC+BâC 2. (AâB)(CâD)=ACâBD 3. Aâ(BâC)=(AâB)âC 4. (AâB) (cid:48)=A (cid:48)âB (cid:48) 5. tr(AâB)=tr(A)tr(B) 6. If AismÃmandB isnÃn,det(AâB)=(det(A))n(det(B))m 7. (AâB) â1=A â1âB â1 8. If A>0andB>0then AâB>0 9. vec(ABC)=(cid:161) C (cid:48)âA (cid:162) vec(B) 10. tr(ABCD)=vec (cid:161) D (cid:48)(cid:162)(cid:48)(cid:161) C (cid:48)âA (cid:162) vec(B) A.22 VectorNorms GivenavectorspaceV (suchasEuclideanspace(cid:82)m)anormisafunctionÏ:V â(cid:82)withtheproper- ties 1. Ï(ca)=|c|Ï(a)foranycomplexnumberc andaâV 2. Ï(a+b)â¤Ï(a)+Ï(b) 3. IfÏ(a)=0thena=0.",
    "page": 983,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXA. MATRIXALGEBRA 964 AseminormonV isafunctionwhichsatisfiesthefirsttwoproperties.Thesecondpropertyisknown as the triangle inequality. It is the one property which typically needs a careful demonstration (as the othertwopropertiestypicallyholdbyinspection). ThetypicalnormusedforEuclideanspace(cid:82)m istheEuclideannorm (cid:195) (cid:33)1/2 m (cid:107)a(cid:107)=(cid:161) a (cid:48) a (cid:162)1/2= (cid:88) a2 . i i=1 Analternativenormisthepânorm(forpâ¥1) (cid:195) (cid:33)1/p m (cid:107)a(cid:107) = (cid:88)|a |p . p i i=1 SpecialcasesincludetheEuclideannorm(p=2),the1ânorm m (cid:107)a(cid:107) = (cid:88)|a | 1 i i=1 andthesup-norm (cid:107)a(cid:107) â =max(|a 1 |,...,|a m |). Forrealnumbers(m=1)thesenormscoincide. A.23 MatrixNorms TwocommonnormsusedformatrixspacesaretheFrobeniusnormandthespectralnorm.Wewill use(cid:107)A(cid:107) fortheFrobeniusand(cid:107)A(cid:107)forthespectralnorm. Ingeneralthespectralnormismorewidely F used. TheFrobeniusnormofanmÃk matrix AistheEuclideannormappliedtoitselements (cid:195) (cid:33)1/2 m k (cid:107)A(cid:107) =(cid:107)vec(A)(cid:107)=(cid:161) tr (cid:161) A (cid:48) A (cid:162)(cid:162)1/2= (cid:88) (cid:88) a2 . F ij i=1j=1 WhenmÃm Aisrealsymmetricthen (cid:195) (cid:33)1/2 m (cid:107)A(cid:107) = (cid:88) Î»2 F (cid:96) (cid:96)=1 whereÎ» (cid:96),(cid:96)=1,...,maretheeigenvaluesofA.Toseethis,bythespectraldecompositionA=HÎH (cid:48) with H (cid:48) H=I andÎ=diag{Î» ,...,Î» },so 1 m (cid:195) (cid:33)1/2 m (cid:107)A(cid:107) =(cid:161) tr (cid:161) HÎH (cid:48) HÎH (cid:48)(cid:162)(cid:162)1/2=(tr(ÎÎ))1/2= (cid:88) Î»2 . (A.16) F (cid:96) (cid:96)=1 AusefulcalculationisforanymÃ1vectorsaandb,using(A.1), (cid:176) (cid:176)ab (cid:48)(cid:176) (cid:176) =tr (cid:161) ba (cid:48) ab (cid:48)(cid:162)1/2=(cid:161) b (cid:48) ba (cid:48) a (cid:162)1/2=(cid:107)a(cid:107)(cid:107)b(cid:107) F andinparticular (cid:176) (cid:176)aa (cid:48)(cid:176) (cid:176) =(cid:107)a(cid:107)2. (A.17) F",
    "page": 984,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXA. MATRIXALGEBRA 965 ThespectralnormofanmÃk realmatrix Aisitslargestsingularvalue (cid:107)A(cid:107)=Ï (A)=(cid:161)Î» (cid:161) A (cid:48) A (cid:162)(cid:162)1/2 max max whereÎ» (B)denotesthelargesteigenvalueofthematrixB.Noticethat max Î» max (cid:161) A (cid:48) A (cid:162)=(cid:176) (cid:176)A (cid:48) A (cid:176) (cid:176) so (cid:107)A(cid:107)=(cid:176) (cid:176)A (cid:48) A (cid:176) (cid:176) 1/2 . If AismÃmandsymmetricwitheigenvaluesÎ» j then(cid:107)A(cid:107)=max jâ¤m (cid:175) (cid:175) Î» j (cid:175) (cid:175). TheFrobeniusandspectralnormsarecloselyrelated. Theyareequivalentwhenappliedtoamatrix ofrank1,since (cid:176) (cid:176)ab (cid:48)(cid:176) (cid:176) =(cid:107)a(cid:107)(cid:107)b(cid:107)=(cid:176) (cid:176)ab (cid:48)(cid:176) (cid:176) .Ingeneral,formÃk matrix Awithrankr F (cid:195) (cid:33)1/2 k (cid:107)A(cid:107)=(cid:161)Î» (cid:161) A (cid:48) A (cid:162)(cid:162)1/2â¤ (cid:88) Î» (cid:161) A (cid:48) A (cid:162) =(cid:107)A(cid:107) . (A.18) max j F j=1 (cid:48) Since A Aalsohasrankatmostr ithasatmostr non-zeroeigenvalues,andhence (cid:195) k (cid:33)1/2 (cid:195) r (cid:33)1/2 (cid:112) (cid:107)A(cid:107) = (cid:88) Î» (cid:161) A (cid:48) A (cid:162) = (cid:88) Î» (cid:161) A (cid:48) A (cid:162) â¤(cid:161) rÎ» (cid:161) A (cid:48) A (cid:162)(cid:162)1/2= r(cid:107)A(cid:107). (A.19) F j j max j=1 j=1 Givenanyvectornorm(cid:107)a(cid:107)theinducedmatrixnormisdefinedas (cid:107)Ax(cid:107) (cid:107)A(cid:107)= sup (cid:107)Ax(cid:107)=sup . x(cid:48)x=1 x(cid:54)=0 (cid:107)x(cid:107) Toseethatthisisanormweneedtocheckthatitsatisfiesthetriangleinequality.Indeed (cid:107)A+B(cid:107)= sup (cid:107)Ax+Bx(cid:107)â¤ sup (cid:107)Ax(cid:107)+ sup (cid:107)Bx(cid:107)=(cid:107)A(cid:107)+(cid:107)B(cid:107). x(cid:48)x=1 x(cid:48)x=1 x(cid:48)x=1 Foranyvectorx bythedefinitionoftheinducednorm(cid:107)Ax(cid:107)â¤(cid:107)A(cid:107)(cid:107)x(cid:107),apropertywhichiscalledcon- sistentnorms. LetAandB beconformableand(cid:107)A(cid:107)aninducedmatrixnorm.Thenusingthepropertyofconsistent norms (cid:107)AB(cid:107)= sup (cid:107)ABx(cid:107)â¤ sup (cid:107)A(cid:107)(cid:107)Bx(cid:107)=(cid:107)A(cid:107)(cid:107)B(cid:107). x(cid:48)x=1 x(cid:48)x=1 Amatrixnormwhichsatisfiesthispropertyiscalledasub-multiplicativenormandisamatrixformof theSchwarzinequality. ThematrixnorminducedbytheEuclideanvectornormisthespectralnorm.Indeed, sup (cid:107)Ax(cid:107)2= sup x (cid:48) A (cid:48) Ax=Î» (cid:161) A (cid:48) A (cid:162)=(cid:107)A(cid:107)2. max x(cid:48)x=1 x(cid:48)x=1 ItfollowsthatthespectralnormisconsistentwiththeEuclideannormandissub-multiplicative.",
    "page": 985,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "Appendix B Useful Inequalities InthisAppendixwelistasetofinequalitiesandboundswhichareusedfrequentlyineconometric theory,predominantlyinasymptoticanalysis.TheproofsarepresentedinSectionB.5. B.1 InequalitiesforRealNumbers TriangleInequality.Foranyrealnumbersx j (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:88) m x j (cid:175) (cid:175) (cid:175) â¤ (cid:88) m (cid:175) (cid:175)x j (cid:175) (cid:175). (B.1) (cid:175)j=1 (cid:175) j=1 JensenâsInequality.Ifg(Â·):(cid:82)â(cid:82)isconvex,thenforanynon-negativeweightsa suchthat (cid:80)m a =1, j j=1 j andanyrealnumbersx j (cid:195) (cid:33) m m g (cid:88) a x â¤ (cid:88) a g (cid:161) x (cid:162) . (B.2) j j j j j=1 j=1 Inparticular,settinga =1/m,then j (cid:195) (cid:33) g 1 (cid:88) m x â¤ 1 (cid:88) m g (cid:161) x (cid:162) . (B.3) j j m m j=1 j=1 Ifg(Â·):(cid:82)â(cid:82)isconcavethentheinequalitiesin(B.2)and(B.3)arereversed. GeometricMeanInequality. Foranynon-negativerealweightsa suchthat (cid:80)m a =1,andanynon- j j=1 j negativerealnumbersx j m x a1x a2Â·Â·Â·x am â¤ (cid:88) a x . (B.4) 1 2 m j j j=1 LoÃ¨veâsc Inequality.Foranyrealnumbersx ,if0<r â¤1 r j (cid:175) (cid:175)r (cid:175) (cid:175) (cid:175) (cid:88) m x j (cid:175) (cid:175) (cid:175) â¤ (cid:88) m (cid:175) (cid:175)x j (cid:175) (cid:175) r (B.5) (cid:175)j=1 (cid:175) j=1 andifr â¥1 (cid:175) (cid:175)r (cid:175) (cid:175) (cid:175) (cid:88) m x j (cid:175) (cid:175) (cid:175) â¤mrâ1(cid:88) m (cid:175) (cid:175)x j (cid:175) (cid:175) r . (B.6) (cid:175)j=1 (cid:175) j=1 966",
    "page": 986,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXB. USEFULINEQUALITIES 967 Fortheimportantspecialcasem=2wecancombinethesetwoinequalitiesas |a+b|r â¤C (cid:161)|a|r+|b|r(cid:162) (B.7) r whereC =max (cid:163) 1,2râ1(cid:164) . r NormMonotonicity.If0<tâ¤s,andanyrealnumbersx j (cid:175) (cid:175)1/s (cid:175) (cid:175)1/t (cid:175) (cid:175) (cid:175) (cid:88) m (cid:175) (cid:175)x j (cid:175) (cid:175) s (cid:175) (cid:175) (cid:175) â¤ (cid:175) (cid:175) (cid:175) (cid:88) m (cid:175) (cid:175)x j (cid:175) (cid:175) t (cid:175) (cid:175) (cid:175) . (B.8) (cid:175)j=1 (cid:175) (cid:175)j=1 (cid:175) B.2 InequalitiesforVectors TriangleInequality.Ifa=(a ,...,a ) (cid:48) 1 m m (cid:107)a(cid:107)â¤ (cid:88)(cid:175) (cid:175)a j (cid:175) (cid:175). (B.9) j=1 c Inequality.ForanymÃ1vectorsaandb, 2 (a+b) (cid:48) (a+b)â¤2a (cid:48) a+2b (cid:48) b. (B.10) HÃ¶lderâsInequality.Ifp>1,q>1,and1/p+1/q=1,thenforanymÃ1vectorsaandb, (cid:175) (cid:175)a (cid:48) b (cid:175) (cid:175) â¤(cid:107)a(cid:107) p (cid:107)b(cid:107) q . (B.11) SchwarzInequality.ForanymÃ1vectorsaandb, (cid:175) (cid:175)a (cid:48) b (cid:175) (cid:175) â¤(cid:107)a(cid:107)(cid:107)b(cid:107). (B.12) MinkowskiâsInequality.ForanymÃ1vectorsaandb,ifpâ¥1,then (cid:107)a+b(cid:107) â¤(cid:107)a(cid:107) +(cid:107)b(cid:107) . (B.13) p p p TriangleInequality.ForanymÃ1vectorsaandb, (cid:107)a+b(cid:107)â¤(cid:107)a(cid:107)+(cid:107)b(cid:107). (B.14)",
    "page": 987,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXB. USEFULINEQUALITIES 968 B.3 InequalitiesforMatrices SchwarzMatrixInequality. For any mÃk and kÃm matrices A and B, and both the Frobenius and spectralnorms, (cid:107)AB(cid:107)â¤(cid:107)A(cid:107)(cid:107)B(cid:107). (B.15) TriangleInequality.ForanymÃk matrices AandB,andboththeFrobeniusandspectralnorms, (cid:107)A+B(cid:107)â¤(cid:107)A(cid:107)+(cid:107)B(cid:107). (B.16) TraceInequality.ForanymÃmmatrices AandB suchthat AissymmetricandBâ¥0 tr(AB)â¤(cid:107)A(cid:107)tr(B). (B.17) QuadraticInequality.ForanymÃ1bandmÃmsymmetricmatrix A b (cid:48) Abâ¤(cid:107)A(cid:107)b (cid:48) b. (B.18) StrongSchwarzMatrixInequality.Foranyconformablematrices AandB (cid:107)AB(cid:107) â¤(cid:107)A(cid:107)(cid:107)B(cid:107) . (B.19) F F NormEquivalence.ForanymÃk matrix Aofrankr (cid:112) (cid:107)A(cid:107)â¤(cid:107)A(cid:107) â¤ r(cid:107)A(cid:107). (B.20) F EigenvalueProductInequality.ForanymÃmrealsymmetricmatricesAâ¥0andBâ¥0,theeigenvalues Î» (cid:96)(AB)arerealandsatisfy Î» min (A)Î» min (B)â¤Î» (cid:96)(AB)â¤Î» max (A)Î» max (B). (B.21) (ZhangandZhang,2006,Corollary11). B.4 ProbabilityInequalities MonotoneProbabilityInequality.Foranyevents AandB suchthat AâB, (cid:80)[A]â¤(cid:80)[B]. (B.22) Inclusion-ExclusionPrinciple.Foranyevents AandB, (cid:80)[AâªB]=(cid:80)[A]+(cid:80)[B]â(cid:80)[Aâ©B]. (B.23) BooleâsInequality(UnionBound).Foranyevents AandB, (cid:80)[AâªB]â¤(cid:80)[A]+(cid:80)[B]. (B.24)",
    "page": 988,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXB. USEFULINEQUALITIES 969 BonferroniâsInequality.Foranyevents AandB, (cid:80)[Aâ©B]â¥(cid:80)[A]+(cid:80)[B]â1. (B.25) ExpectationEquality.IfX â(cid:82)isanon-negativerandomvariablethen (cid:90) â (cid:69)[X]= (cid:80)[X >u]du. (B.26) 0 JensenâsInequality. Ifg(x):(cid:82)m â(cid:82)isconvex,thenforanyrandomvectorX â(cid:82)k forwhich(cid:69)(cid:107)X(cid:107)<â and(cid:69)(cid:175) (cid:175)g(X) (cid:175) (cid:175) <â, g((cid:69)[X])â¤(cid:69)(cid:163) g(X) (cid:164) . (B.27) Ifg(Â·)isconcavetheinequalityisreversed. Conditional Jensenâs Inequality. If g(x) : (cid:82)m â (cid:82) is convex, then for any random variables (Y,X) â (cid:82)mÃ(cid:82)k forwhich(cid:69)(cid:107)Y(cid:107)<âand(cid:69)(cid:176) (cid:176)g(Y) (cid:176) (cid:176) <â, g((cid:69)[Y |X])â¤(cid:69)(cid:163) g(Y)|X (cid:164) . (B.28) Ifg(x)isconcavetheinequalityisreversed. ConditionalExpectationInequality. For any r â¥1 and for any random variables (Y,X)â(cid:82)Ã(cid:82)k such that(cid:69)|Y|r <â,then (cid:69)|(cid:69)(Y |X)|r â¤(cid:69)|Y|r <â. (B.29) ExpectationInequality.ForanyrandomvectorY â(cid:82)m with(cid:69)(cid:107)Y(cid:107)<â then (cid:107)(cid:69)[Y](cid:107)â¤(cid:69)(cid:107)Y(cid:107). (B.30) HÃ¶lderâsInequality.Ifp>1andq>1and 1 + 1 =1,thenforanyrandommÃnmatricesX andY, p q (cid:69)(cid:176) (cid:176)X (cid:48) Y (cid:176) (cid:176) â¤(cid:161)(cid:69)(cid:107)X(cid:107)p(cid:162)1/p(cid:161)(cid:69)(cid:107)Y(cid:107)q(cid:162)1/q . (B.31) Cauchy-SchwarzInequality.ForanyrandommÃnmatricesX andY, (cid:69)(cid:176) (cid:176)X (cid:48) Y (cid:176) (cid:176) â¤(cid:161)(cid:69)(cid:107)X(cid:107)2(cid:162)1/2(cid:161)(cid:69)(cid:107)Y(cid:107)2(cid:162)1/2 . (B.32) MatrixCauchy-SchwarzInequality.Tripathi(1999).ForanyrandomX â(cid:82)m andY â(cid:82)(cid:96) , (cid:69)(cid:163) YX (cid:48)(cid:164)(cid:161)(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:162)â (cid:69)(cid:163) XY (cid:48)(cid:164)â¤(cid:69)(cid:163) YY (cid:48)(cid:164) . (B.33) MinkowskiâsInequality.ForanyrandommÃnmatricesX andY, (cid:161)(cid:69)(cid:107)X+Y(cid:107)p(cid:162)1/pâ¤(cid:161)(cid:69)(cid:107)X(cid:107)p(cid:162)1/p+(cid:161)(cid:69)(cid:107)Y(cid:107)p(cid:162)1/p . (B.34)",
    "page": 989,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXB. USEFULINEQUALITIES 970 LyapunovâsInequality.ForanyrandommÃnmatrixX and0<r â¤p, (cid:161)(cid:69)(cid:107)X(cid:107)r(cid:162)1/r â¤(cid:161)(cid:69)(cid:107)X(cid:107)p(cid:162)1/p . (B.35) MarkovâsInequality(standardform).ForanyrandomX â(cid:82)k,non-negativefunctiong(x)â¥0,and(cid:178)>0 (cid:80)(cid:163) g(X)>(cid:178)(cid:164)â¤(cid:178)â1(cid:69)(cid:163) g(X) (cid:164) . (B.36) MarkovâsInequality(strongform).ForanyrandomX â(cid:82)k,non-negativefunctiong(x)â¥0,and(cid:178)>0 (cid:80)(cid:163) g(X)>(cid:178)(cid:164)â¤â1(cid:69)(cid:163) g(X) 1(cid:169) g(X)>(cid:178)(cid:170)(cid:164) . (B.37) ChebyshevâsInequality.ForanyrandomX â(cid:82)and(cid:178)>0 (cid:80)[|Xâ(cid:69)[X]|>(cid:178)]â¤(cid:178)â2var[X]. (B.38) GaussianTailInequality(ChernoffBound).IfX â¼N(0,1),thenforanyt>0, (cid:181) t2(cid:182) (cid:80)[X >t]â¤exp â . (B.39) 2 BernsteinâsInequality.IfX â(cid:82)areindependent,(cid:69)[X ]=0,Ï2=(cid:80)n (cid:69)(cid:163) X2(cid:164) ,and|X |â¤M<â,thenfor i i i=1 i i all(cid:178)>0 (cid:34)(cid:175) (cid:175) (cid:35) (cid:175)(cid:88) n (cid:175) (cid:181) (cid:178)2 (cid:182) (cid:80) (cid:175) X (cid:175)>(cid:178) â¤2exp â . (B.40) (cid:175) (cid:175)i=1 i(cid:175) (cid:175) 4Ï2+4M(cid:178) Bernsteinâs Inequality for Vectors. If X = (X ,...,X ) (cid:48) are independent random vectors, (cid:69)[X ] = 0, i 1i mi i Ï2=max j (cid:80)n i=1 (cid:69) (cid:104) X j 2 i (cid:105) ,and (cid:175) (cid:175)X ji (cid:175) (cid:175) â¤M<â,thenforallÎµ>0 (cid:34)(cid:176) (cid:176) (cid:35) (cid:176)(cid:88) n (cid:176) (cid:181) Îµ2 (cid:182) (cid:80) (cid:176) X (cid:176)>(cid:178) â¤2mexp â . (B.41) (cid:176) (cid:176)i=1 i(cid:176) (cid:176) 4m2Ï2+4mMÎµ Bahr-EsseenInequality.IfX â(cid:82)areindependentand(cid:69)[X ]=0,thenforany0<r â¤2 i i (cid:175) (cid:175)r (cid:69) (cid:175) (cid:175) (cid:88) n X (cid:175) (cid:175) â¤2 (cid:88) n (cid:69)|X |r. (B.42) (cid:175) i(cid:175) i (cid:175)i=1 (cid:175) i=1 SomeofthefollowinginequalitiesmakeuseofRademacherrandomvariablesÎµwhicharetwo-point discretevariablessatisfying 1 (cid:80)[Îµ=1]= 2 1 (cid:80)[Îµ=â1]= . 2 ExponentialInequality.IfÎµ areindependentRademacherrandomvariablesthenforanyconstantsa i i (cid:34) (cid:195) n (cid:33)(cid:35) (cid:195)(cid:80)n a2(cid:33) (cid:69) exp (cid:88) a Îµ â¤exp i=1 i . (B.43) i i 2 i=1",
    "page": 990,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXB. USEFULINEQUALITIES 971 SymmetrizationInequality. If X â(cid:82)areindependent,(cid:69)[X ]=0,andÎµ areindependentRademacher i i i randomvariables,thenforanyr â¥1 (cid:175) (cid:175)r (cid:175) (cid:175)r (cid:69) (cid:175) (cid:175) (cid:88) n X (cid:175) (cid:175) â¤2r(cid:69) (cid:175) (cid:175) (cid:88) n X Îµ (cid:175) (cid:175) . (B.44) (cid:175) i(cid:175) (cid:175) i i(cid:175) (cid:175)i=1 (cid:175) (cid:175)i=1 (cid:175) KhintchineâsInequality.IfÎµ areindependentRademacherrandomvariablesthenforanyr >0andany i realnumbersa i (cid:175) (cid:175)r (cid:195) (cid:33)r/2 (cid:69) (cid:175) (cid:175) (cid:88) n a Îµ (cid:175) (cid:175) â¤K (cid:88) n a2 . (B.45) (cid:175) i i(cid:175) r i (cid:175)i=1 (cid:175) i=1 whereK =1forr â¤2andK =2r/2Î((r+1)/2))/Ï1/2forr â¥2. r r Marcinkiewicz-ZygmundInequality.IfX â(cid:82)areindependentand(cid:69)[X ]=0thenforanyr â¥1 i i (cid:175) (cid:175)r (cid:175) (cid:175)r/2 (cid:69) (cid:175) (cid:175) (cid:88) n X (cid:175) (cid:175) â¤M (cid:69) (cid:175) (cid:175) (cid:88) n X2 (cid:175) (cid:175) (B.46) (cid:175) i(cid:175) r (cid:175) i (cid:175) (cid:175)i=1 (cid:175) (cid:175)i=1 (cid:175) whereM =2rK withK fromtheKhintchineinequality. r r r MaximalMarcinkiewicz-ZygmundInequality. If X â(cid:82), j =1,...,J areindependentacrossi =1,...,n, ji and(cid:69)(cid:163) X (cid:164)=0,forJâ¥2, ji (cid:34) (cid:175) (cid:175)(cid:35) (cid:175) (cid:175)1/2 (cid:69) max (cid:175) (cid:175) (cid:88) n X (cid:175) (cid:175) â¤4 (cid:161) logJ (cid:162)1/2 max(cid:69) (cid:175) (cid:175) (cid:88) n X2 (cid:175) (cid:175) . (B.47) 1â¤jâ¤J (cid:175) (cid:175)i=1 ji(cid:175) (cid:175) 1â¤jâ¤J (cid:175) (cid:175)i=1 ji(cid:175) (cid:175) WhittleâsInequalities. (Whittle,1960)IfX â(cid:82)areindependent,(cid:69)[X ]=0,andforsomer â¥2,(cid:69)|X |r â¤ i i i B <â,thenthereisaconstantC <âsuchthatforanyrealnumbersa 1r 1r i (cid:175) (cid:175)r (cid:175) (cid:175)r/2 (cid:69) (cid:175) (cid:175) (cid:88) n a X (cid:175) (cid:175) â¤C (cid:175) (cid:175) (cid:88) n a2 (cid:175) (cid:175) . (B.48) (cid:175) i i(cid:175) 1r(cid:175) i(cid:175) (cid:175)i=1 (cid:175) (cid:175)i=1 (cid:175) Furthermore,if(cid:69)|X |2r â¤B <âthenthereisaconstantC <âsuchthatforanyrealnÃn matrix A i 2r 2r andsettingX =(X ,...,X ) (cid:48) 1 n (cid:69)(cid:175) (cid:175)X (cid:48) AXâ(cid:69)(cid:163) X (cid:48) AX (cid:164)(cid:175) (cid:175) r â¤C 2r tr (cid:161) A (cid:48) A (cid:162)r/2 . (B.49) Our proof shows that we can setC =M B andC =4rK C1/2B1/2 where M and K are from the 1r r 1r 2r r 1r 2r r r Marcinkiewicz-ZygmundandKhinchineinequalities. RosenthalâsInequality. Foranyr >0thereisaconstantR <âsuchthatifX â(cid:82)areindependentand r i (cid:69)[X ]=0then i (cid:175) (cid:175)r (cid:40)(cid:195) (cid:33)r/2 (cid:41) (cid:69) (cid:175) (cid:175) (cid:88) n X (cid:175) (cid:175) â¤R (cid:88) n (cid:69)(cid:163) X2(cid:164) + (cid:88) n (cid:69)|X |r",
    "page": 991,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": ". (B.49) Our proof shows that we can setC =M B andC =4rK C1/2B1/2 where M and K are from the 1r r 1r 2r r 1r 2r r r Marcinkiewicz-ZygmundandKhinchineinequalities. RosenthalâsInequality. Foranyr >0thereisaconstantR <âsuchthatifX â(cid:82)areindependentand r i (cid:69)[X ]=0then i (cid:175) (cid:175)r (cid:40)(cid:195) (cid:33)r/2 (cid:41) (cid:69) (cid:175) (cid:175) (cid:88) n X (cid:175) (cid:175) â¤R (cid:88) n (cid:69)(cid:163) X2(cid:164) + (cid:88) n (cid:69)|X |r . (B.50) (cid:175) i(cid:175) r i i (cid:175)i=1 (cid:175) i=1 i=1 Ourproofestablishesthat(B.50)holdswithR =2r(râ2)/8M whereM isfromtheMarcinkiewicz-Zygmund r r r inequality. ForageneralizationofRosenthalâsinequalitytothematrixcaseseeB.E.Hansen(2015).",
    "page": 991,
    "chunk_id": 1,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXB. USEFULINEQUALITIES 972 KolmogorovâsInequality.IfX areindependent,(cid:69)[X ]=0,and(cid:69)(cid:163) X2(cid:164)<â,thenforall(cid:178)>0 i i i (cid:34) (cid:175) (cid:175) (cid:35) (cid:80) max (cid:175) (cid:175) (cid:88) j X (cid:175) (cid:175)>(cid:178) â¤(cid:178)â2(cid:88) n (cid:69)(cid:163) X2(cid:164) . (B.51) 1â¤jâ¤n (cid:175) (cid:175)i=1 i(cid:175) (cid:175) i=1 i BillingsleyâsInequality. AssumeX areindependentandmeanzero. SetS =(cid:80)i X andÏ2 =var[S ]. (cid:112) i i j=1 j n n ThenforanyÎ»>2 2, (cid:183) (cid:184) (cid:183) Ï Î»(cid:184) (cid:80) max |S |>Ï Î» â¤2(cid:80) |S |> n . (B.52) i n n 1â¤iâ¤n 2 Billingsley(1968,Lemma10.7). B.5 Proofs* ProofofTriangleInequality(B.1).Takethecasem=2.Observethat â|x |â¤x â¤|x | 1 1 1 â|x |â¤x â¤|x |. 2 2 2 Adding,wefind â|x |â|x |â¤x +x â¤|x |+|x | 1 2 1 2 1 2 whichis(B.1)form=2.Form>2,weapply(B.1)mâ1times. â  ProofofJensenâsInequality(B.2).SeeIntroductiontoEconometrics,Theorem2.12. Proofof GeometricMeanInequality(B.4).SeeIntroductiontoEconometrics,Theorem2.13. ProofofLoÃ¨veâsc Inequality(B.5).SeeIntroductiontoEconometrics,Theorem2.14. r ProofofNormMonotonicity(B.8).SeeIntroductiontoEconometrics,Theorem2.15. ProofofTriangleInequality(B.9).Applythec inequality(B.5)withr =1/2tofind r (cid:175) (cid:175)1/2 (cid:107)a(cid:107)= (cid:175) (cid:175) (cid:175) (cid:88) m a2 j (cid:175) (cid:175) (cid:175) â¤ (cid:88) m (cid:175) (cid:175)a j (cid:175) (cid:175). (cid:175)j=1 (cid:175) j=1 â  Proofofc Inequality(B.10).Bythec inequality(B.6)withr =2, (cid:161) a +b (cid:162)2â¤2a2+2b2.Thus 2 r j j j j m m m (a+b) (cid:48) (a+b)= (cid:88)(cid:161) a +b (cid:162)2â¤2 (cid:88) a2+2 (cid:88) b2=2a (cid:48) a+2b (cid:48) b. j j j j j=1 j=1 j=1 â  ProofofHÃ¶lderâsInequality(B.11).Withoutlossofgeneralityassume (cid:80)m j=1 (cid:175) (cid:175)a j (cid:175) (cid:175) p=(cid:107)a(cid:107)p p =1and (cid:80)m j=1 (cid:175) (cid:175)b j (cid:175) (cid:175) q= (cid:107)b(cid:107)q=1.Bythegeometricmeaninequality(B.4) q (cid:175) (cid:175)p (cid:175) (cid:175)q (cid:175) (cid:175)a j b j (cid:175) (cid:175) â¤(cid:175) (cid:175)a j (cid:175) (cid:175) (cid:175) (cid:175)b j (cid:175) (cid:175) â¤ (cid:175)a j(cid:175) + (cid:175)b j(cid:175) . (B.53) p q",
    "page": 992,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXB. USEFULINEQUALITIES 973 BytheTriangleinequality(B.1),(B.53),theassumptions (cid:80)m j=1 (cid:175) (cid:175)a j (cid:175) (cid:175) p=1, (cid:80)m j=1 (cid:175) (cid:175)b j (cid:175) (cid:175) q=1,and1/p+1/q=1 (cid:175) (cid:175) (cid:175) (cid:175)a (cid:48) b (cid:175) (cid:175) = (cid:175) (cid:175) (cid:175) (cid:88) m a j b j (cid:175) (cid:175) (cid:175) (cid:175)j=1 (cid:175) m â¤ (cid:88)(cid:175) (cid:175)a j b j (cid:175) (cid:175) j=1 â¤ (cid:88) m (cid:195)(cid:175) (cid:175)a j (cid:175) (cid:175) p + (cid:175) (cid:175)b j (cid:175) (cid:175) q(cid:33) p q j=1 1 1 = + =1 p q whichis(B.11). â  ProofofSchwarzInequality(B.12).ThisisaspecialcaseofHÃ¶lderâsinequality(B.11)withp=q=2. m (cid:175) (cid:175)a (cid:48) b (cid:175) (cid:175) â¤ (cid:88)(cid:175) (cid:175)a j b j (cid:175) (cid:175) â¤(cid:107)a(cid:107)(cid:107)b(cid:107). j=1 â  ProofofMinkowskiâsInequality(B.13). Set q =p/(pâ1)sothat1/p+1/q =1. Usingthetrianglein- equalityforrealnumbers(B.1)andtwoapplicationsofHÃ¶lderâsinequality(B.11) m (cid:107)a+b(cid:107)p p = (cid:88)(cid:175) (cid:175)a j +b j (cid:175) (cid:175) p j=1 m = (cid:88)(cid:175) (cid:175)a j +b j (cid:175) (cid:175) (cid:175) (cid:175)a j +b j (cid:175) (cid:175) pâ1 j=1 m m â¤ (cid:88)(cid:175) (cid:175)a j (cid:175) (cid:175) (cid:175) (cid:175)a j +b j (cid:175) (cid:175) pâ1+ (cid:88)(cid:175) (cid:175)b j (cid:175) (cid:175) (cid:175) (cid:175)a j +b j (cid:175) (cid:175) pâ1 j=1 j=1 (cid:195) (cid:33)1/q (cid:195) (cid:33)1/q m m â¤(cid:107)a(cid:107) p (cid:88)(cid:175) (cid:175)a j +b j (cid:175) (cid:175) (pâ1)q +(cid:107)b(cid:107) p (cid:88)(cid:175) (cid:175)a j +b j (cid:175) (cid:175) (pâ1)q j=1 j=1 =(cid:161)(cid:107)a(cid:107) +(cid:107)b(cid:107) (cid:162)(cid:107)a+b(cid:107)pâ1 . p p p Solving,wefind(B.13). â  ProofofTriangleInequality(B.14).ThisisMinkowskiâsinequality(B.13)withp=2. â  Proof of Schwarz Matrix Inequality (B.15). The inequality holds for the spectral norm since it is an inducednorm. NowconsidertheFrobeniusnorm. Partition A (cid:48)=[a ,...,a ]andB =[b ,...,b ]. Then 1 n 1 n bypartitionedmatrixmultiplication, thedefinitionoftheFrobeniusnormandtheSchwarzinequality",
    "page": 993,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXB. USEFULINEQUALITIES 974 (B.12) (cid:176) (cid:176) a (cid:48) 1 b 1 a (cid:48) 1 b 2 Â·Â·Â· (cid:176) (cid:176) (cid:107)AB(cid:107) F = (cid:176) (cid:176) (cid:176) a (cid:48) 2 b 1 a (cid:48) 2 b 2 Â·Â·Â· (cid:176) (cid:176) (cid:176) (cid:176) (cid:176) . . . . . . ... (cid:176) (cid:176) F (cid:176) (cid:176) (cid:107)a 1 (cid:107)(cid:107)b 1 (cid:107) (cid:107)a 1 (cid:107)(cid:107)b 2 (cid:107) Â·Â·Â· (cid:176) (cid:176) â¤ (cid:176) (cid:176) (cid:107)a 2 (cid:107)(cid:107)b 1 (cid:107) (cid:107)a 2 (cid:107)(cid:107)b 2 (cid:107) Â·Â·Â· (cid:176) (cid:176) (cid:176) (cid:176) (cid:176) (cid:176) . . . . . . ... (cid:176) (cid:176) F (cid:195) (cid:33)1/2 m m = (cid:88) (cid:88)(cid:107)a i (cid:107)2(cid:176) (cid:176)b j (cid:176) (cid:176) 2 i=1j=1 (cid:195) (cid:33)1/2(cid:195) (cid:33)1/2 m m = (cid:88)(cid:107)a (cid:107)2 (cid:88)(cid:107)b (cid:107)2 i i i=1 i=1 (cid:195) (cid:33)1/2(cid:195) (cid:33)1/2 k m m k = (cid:88) (cid:88) a2 ji (cid:88) (cid:88)(cid:176) (cid:176)b ji (cid:176) (cid:176) 2 i=1j=1 i=1j=1 =(cid:107)A(cid:107) (cid:107)B(cid:107) . F F â  ProofofTriangleInequality(B.16). The inequality holds for the spectral norm since it is an induced norm. NowconsidertheFrobeniusnorm. Leta=vec(A)andb=vec(B). Thenbythedefinitionofthe Frobeniusnormandthetriangleinequality(B.14) (cid:107)A+B(cid:107) =(cid:107)vec(A+B)(cid:107) =(cid:107)a+b(cid:107)â¤(cid:107)a(cid:107)+(cid:107)b(cid:107)=(cid:107)A(cid:107) +(cid:107)B(cid:107) . F F F F â  Proof of Trace Inequality (B.17). By the spectral decomposition for symmetric matices, A = HÎH (cid:48) where Î has the eigenvalues Î» of A on its diagonal and H is orthonormal. DefineC = H (cid:48) BH which j hasnon-negativediagonalelementsC sinceB ispositivesemi-definite.Then jj m m tr(AB)=tr(ÎC)= (cid:88) Î» j C jj â¤max (cid:175) (cid:175) Î» j (cid:175) (cid:175) (cid:88) C jj =(cid:107)A(cid:107) 2 tr(C) j=1 j j=1 wheretheinequalityusesthefactthatC â¥0.Notethat jj tr(C)=tr (cid:161) H (cid:48) BH (cid:162)=tr (cid:161) HH (cid:48) B (cid:162)=tr(B) sinceH isorthonormal.Thustr(AB)â¤(cid:107)A(cid:107) tr(B)asstated. â  2 ProofofQuadraticInequality(B.18). Inthetraceinequality(B.17)setB =bb (cid:48) andnotetr(AB)=b (cid:48) Ab andtr(B)=b (cid:48) b. â  ProofofStrongSchwarzMatrixInequality(B.19).BythedefinitionoftheFrobeniusnorm,theproperty (cid:48) (cid:48) ofthetrace,thetraceinequality(B.17)(notingthatboth A AandBB aresymmetricandpositivesemi- definite),andtheSchwarzmatrixinequality(B.15) (cid:107)AB(cid:107) =(cid:161) tr (cid:161) B (cid:48) A (cid:48) AB (cid:162)(cid:162)1/2 F =(cid:161) tr (cid:161) A (cid:48) ABB (cid:48)(cid:162)(cid:162)1/2 â¤(cid:161)(cid:176) (cid:176)A (cid:48) A (cid:176) (cid:176) tr (cid:161) BB (cid:48)(cid:162)(cid:162)1/2 2 =(cid:107)A(cid:107) (cid:107)B(cid:107) . 2 F",
    "page": 994,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXB. USEFULINEQUALITIES 975 â  Proof of Norm Equivalence (B.20). The first inequality was established in (A.18), and the second in (A.19). â  ProofofMonotoneProbabilityInequality(B.22).SeeIntroductiontoEconometrics,Theorem1.2.4. ProofofInclusion-ExclusionPrinciple(B.23).SeeIntroductiontoEconometrics,Theorem1.2.5. ProofofBooleâsInequality(B.29).SeeIntroductiontoEconometrics,Theorem1.2.6. ProofofBonferrroniâsInequality(B.25).SeeIntroductiontoEconometrics,Theorem1.2.7. Proof of Expectation Equality (B.26). Let F â (u)=(cid:80)[X >u]=1âF(u) where F(u) is the distribution function.Byintegrationbyparts (cid:90) â (cid:90) â (cid:90) â (cid:90) â (cid:69)[X]= udF(u)=â udF â (u)=â(cid:163) uF â (u) (cid:164)â + F â (u)du= (cid:80)[X >u]du 0 0 0 0 0 asstated. â  ProofofJensenâsInequality(B.27).SeeIntroductiontoEconometrics,Theorem2.9. ProofofConditionalJensenâsInequality(B.28). Apply Jensenâs inequality to the conditional distribu- tion. ProofofConditionalExpectationInequality(B.29). Asthefunction|u|r isconvexforr â¥1,thecondi- tionalJensenâsinequality(B.28)implies |(cid:69)[Y |X]|r â¤(cid:69)(cid:163)|Y|r |X (cid:164) . Takingunconditionalexpectationsandthelawofiteratedexpectations,weobtain (cid:69)(cid:163)|(cid:69)[Y |X]|r(cid:164)â¤(cid:69)(cid:163)(cid:69)(cid:163)|Y|r |X (cid:164)(cid:164)=(cid:69)(cid:163)|Y|r(cid:164)<â asrequired. â  ProofofExpectationInequality(B.30).SeeIntroductiontoEconometrics,Theorem2.10. ProofofHÃ¶lderâsInequality(B.31).SeeIntroductiontoEconometrics,Theorem4.15. ProofofCauchy-SchwarzInequality(B.32).SeeIntroductiontoEconometrics,Theorem4.11. Proof of Matrix Cauchy-Schwarz Inequality (B.33). Define e = Y â(cid:69)(cid:163) YX (cid:48)(cid:164)(cid:161)(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:162)â X. Note that (cid:69)(cid:163) ee (cid:48)(cid:164)â¥0ispositivesemi-definite.Wecancalculatethat (cid:69)(cid:163) ee (cid:48)(cid:164)=(cid:69)(cid:163) YY (cid:48)(cid:164)â(cid:161)(cid:69)(cid:163) YX (cid:48)(cid:164)(cid:162)(cid:161)(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:162)â (cid:69)(cid:163) XY (cid:48)(cid:164) . Sincetheleft-hand-sideispositivesemi-definite,soistheright-hand-side.Thismeans (cid:69)(cid:163) YY (cid:48)(cid:164)â¥(cid:69)(cid:163) YX (cid:48)(cid:164)(cid:161)(cid:69)(cid:163) XX (cid:48)(cid:164)(cid:162)â (cid:69)(cid:163) XY (cid:48)(cid:164) asstated. â ",
    "page": 995,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXB. USEFULINEQUALITIES 976 ProofofMinkowskiâsInequality(B.34).SeeIntroductiontoEconometrics,Theorem4.16. ProofofLyapunovâsInequality(B.35).SeeIntroductiontoEconometrics,Theorem2.11. ProofofMarkovâsInequality(B.36)and(B.37).SeeIntroductiontoEconometrics,Theorem7.3. ProofofChebyshevâsInequality(B.38).SeeIntroductiontoEconometrics,Theorem7.1. ProofofGaussianTailInequality(B.39). Sincet >0,usingMarkovâsinequality(B.36),andthemoment generatingfunctionofthestandardnormal(SeeIntroductiontoEconometrics,Theorem5.3) (cid:179) (cid:180) (cid:80)[X >t]=(cid:80)(cid:163) exp(tX)>exp (cid:161) t2(cid:162)(cid:164)â¤ (cid:69)(cid:163) exp(tX) (cid:164) = exp t 2 2 (cid:161) (cid:162) (cid:161) (cid:162) exp t2 exp t2 whichsimpliesto(B.39). â  ProofofBernsteinsâsInequality(B.40).Wefirstshow (cid:34) (cid:35) n (cid:181) (cid:178)2 (cid:182) (cid:88) (cid:80) X >(cid:178) â¤exp â . (B.54) i=1 i 4Ï2+4M(cid:178) SetÏ2=(cid:69)(cid:163) X2(cid:164) andt=(cid:178)/ (cid:161) 2Ï2+2M(cid:178)(cid:162)>0.Observethat i i (cid:178) (cid:181) 1 (cid:182)kâ2 tkâ¤ t (B.55) 2Ï2 2M and|X |â¤M implies i (cid:104) (cid:105) (cid:69) Xk â¤Ï2(2M)kâ2. (B.56) i i UsingMarkovâsinequality(B.36)theleftsideof(B.54)equals (cid:34) (cid:195) (cid:33) (cid:35) (cid:34) (cid:195) (cid:33)(cid:35) (cid:80) exp t (cid:88) n X >exp(t(cid:178)) â¤e ât(cid:178)(cid:69) exp t (cid:88) ni X =e ât(cid:178)(cid:89) n (cid:69)(cid:163) exp(tX ) (cid:164) . (B.57) i i i i=1 i=1 i=1 Expandingtheexponentialfunction,usingtheassumption(cid:69)[X ]=0,(B.55),and(B.56) i â tk(cid:69)(cid:163) Xk(cid:164) (cid:69)(cid:163) exp(tX ) (cid:164)=1+t(cid:69)[x ]+ (cid:88) i i i k! k=2 â¤1+ Ï2 i (cid:178)t (cid:88) â 1 2Ï2 k=2 k! Ï2(cid:178)t â¤1+ i 2Ï2 (cid:195)Ï2(cid:178)t (cid:33) â¤exp i . 2Ï2 Thismeansthattherightsideof(B.57)islessthan (cid:181) (cid:178)t (cid:182) (cid:181) (cid:178)t (cid:182) (cid:181) (cid:178)2 (cid:182) exp ât(cid:178)+ =exp â =exp â 2 2 4Ï2+4M(cid:178)/3",
    "page": 996,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXB. USEFULINEQUALITIES 977 thesecondequalityusingthedefintiont=(cid:178)/ (cid:161) 2Ï2+2M(cid:178)(cid:162) .Thisestablishes(B.54). ReplacingX withâX intheaboveargumentweobtain i i (cid:34) (cid:35) n (cid:181) (cid:178)2 (cid:182) (cid:88) (cid:80) X <â(cid:178) â¤exp â . (B.58) i=1 i 4Ï2+4M(cid:178) Together,(B.54)and(B.58)establish(B.40). â  ProofofBernsteinsâsInequalityforVectors(B.41). By the triangle inequality (B.9), Booleâs inequality (B.24),andthenBernsteinâsinequality(B.40) (cid:34)(cid:176) (cid:176) (cid:35) (cid:34) (cid:175) (cid:175) (cid:35) (cid:176)(cid:88) n (cid:176) (cid:88) m (cid:175)(cid:88) n (cid:175) (cid:80) (cid:176) X (cid:176)>(cid:178) â¤(cid:80) (cid:175) X (cid:175)>(cid:178) (cid:176) i(cid:176) (cid:175) ji(cid:175) (cid:176)i=1 (cid:176) j=1(cid:175)i=1 (cid:175) (cid:34) (cid:40)(cid:175) (cid:175) (cid:41)(cid:35) â¤(cid:80) (cid:91) m 1 (cid:175) (cid:175) (cid:88) n X (cid:175) (cid:175)>(cid:178)/m (cid:175) ji(cid:175) j=1 (cid:175)i=1 (cid:175) (cid:34)(cid:175) (cid:175) (cid:35) (cid:88) m (cid:175)(cid:88) n (cid:175) â¤ (cid:80) (cid:175) X (cid:175)>(cid:178)/m (cid:175) ji(cid:175) j=1 (cid:175)i=1 (cid:175) (cid:181) ((cid:178)/m)2 (cid:182) â¤2mexp â 4Ï2+4M((cid:178)/m) whichis(B.41). â  ProofofBahr-EsseenInequality(B.42).OurproofistakenfromvonBahrandEsseen(1965).For0<r â¤ 1(B.42)holdsbythec inequality(B.5). Forr =2,(B.42)holdsbyindependenceanddirectcalculation. r Wethusfocusonthecase1<r <2. Westartwithacharacterizationofanabsolutemoment.Definethegeneralizedcosineintegral (cid:90) â K(r)= t â(r+1)(1âcos(t))dt (B.59) 0 whichisfinitefor0<r <2.Makingthechangeofvariablet=sxandrearrangingweobtain (cid:90) â |x|r =K(r) â1 t â(r+1)(1âcos(tx))dt. 0 LetC (t)denotethecharacteristicfunctionofX andletR (t)=(cid:69)[cos(tX )]denoteitsrealpart.Thusfor i i i i any0<r <2 (cid:90) â (cid:69)|X |r =K(r) â1 t â(r+1)(1âR (t))dt. (B.60) i i 0 LetY beanindependentcopyofX . ThecharacteristicfunctionofX âY isC (t)C (ât)=|C (t)|2. i i i i i i i Thefacts|C (t)|2â¥|R (t)|2and|R (t)|â¤1imply i i i 1â|C (t)|2â¤1âR (t)2=(1âR (t))(1+R (t))â¤2(1âR (t)). (B.61) i i i i i Usingthecharacterization(B.60),(B.61),and(B.60)againwefindthatfor0<r <2 (cid:90) â (cid:69)|X âY |r =K(r) â1 t â(r+1)(cid:161) 1â|C (t)|2(cid:162) dt i i i 0 n (cid:90) â â¤2 (cid:88) K(r) â1 t â(r+1)(1âR (t))dt i i=1 0 =2(cid:69)|X |r. (B.62) i",
    "page": 997,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXB. USEFULINEQUALITIES 978 Let (cid:69) denote expectation over Y . Since X and Y have the same distribution, (cid:69) [Y ]=0. Using Y i i i Y i Jensenâsinequality(B.27)since|u|r isconvexforr â¥1, (cid:175) (cid:175)r (cid:175) (cid:175)r (cid:175)(cid:88) n (cid:175) (cid:175) (cid:88) n (cid:175) (cid:69)(cid:175) X (cid:175) =(cid:69)(cid:175)(cid:69) (X âY )(cid:175) (cid:175) i(cid:175) (cid:175) Y i i (cid:175) (cid:175)i=1 (cid:175) (cid:175) i=1 (cid:175) (cid:34) (cid:175) (cid:175)r(cid:35) (cid:175)(cid:88) n (cid:175) â¤(cid:69) (cid:69) (cid:175) (X âY )(cid:175) Y (cid:175) i i (cid:175) (cid:175)i=1 (cid:175) (cid:175) (cid:175)r (cid:175)(cid:88) n (cid:175) =(cid:69)(cid:175) Z (cid:175) (B.63) (cid:175) i(cid:175) (cid:175)i=1 (cid:175) wherethefinalequalitysetsZ =X âY . i i i n SincetheZ aremutuallyindependent,thecharacteristicfunctionof (cid:80)n Z is (cid:89)|C (t)|2,whichis i i=1 i i i=1 real.Wenextemploythefollowinginequality.If|a |â¤1then i n n (cid:89) (cid:88) 1â a â¤ (1âa ). (B.64) i i i=1 i=1 To establish (B.64) first do so for n = 2 and then apply induction. The inequality follows from 0 â¤ (1âa )(1âa )=1âa âa +a a andrearranging.Since|C (t)|â¤1(B.64)implies 1 2 1 2 1 2 i n n 1â (cid:89)|C (t)|2â¤ (cid:88)(cid:161) 1â|C (t)|2(cid:162) . (B.65) i i i=1 i=1 n Recallingthecharacterization(B.60)andthefactthatthethecharacteristicfunctionof (cid:80)n Z is (cid:89)|C (t)|2, i=1 i i i=1 applying(B.65),andthenusing(B.60)again (cid:69) (cid:175) (cid:175) (cid:175) (cid:88) n Z (cid:175) (cid:175) (cid:175) r =K(r) â1 (cid:90) â t â(r+1) (cid:195) 1â (cid:89) n |C (t)|2 (cid:33) dt (cid:175) i(cid:175) i (cid:175)i=1 (cid:175) 0 i=1 n (cid:90) â â¤ (cid:88) K(r) â1 t â(r+1)(cid:161) 1â|C (t)|2(cid:162) dt i i=1 0 n = (cid:88) (cid:69)|Z |r i i=1 n = (cid:88) (cid:69)|X âY |r i i i=1 n â¤2 (cid:88) (cid:69)|X |r. i i=1 Thefinalinequalityis(B.62).Combinedwith(B.63)thisis(B.42). â  ProofofExponentialInequality(B.43).SeeIntroductiontoEconometrics,Theorem18.4. ProofofSymmetrizationInequality(B.44).SeeIntroductiontoEconometrics,Theorem18.3. ProofofKhintchineâsInequality(B.45). Forr â¤2byLyapunovâsinequality(B.35) (cid:195) (cid:175) (cid:175)r(cid:33)1/r (cid:195) (cid:175) (cid:175)2(cid:33)1/2 (cid:195) (cid:33)1/2 (cid:195) (cid:33)1/2 (cid:69) (cid:175) (cid:175) (cid:88) n a Îµ (cid:175) (cid:175) â¤ (cid:69) (cid:175) (cid:175) (cid:88) n a Îµ (cid:175) (cid:175) = (cid:88) n (cid:69)(cid:163) a2Îµ2(cid:164) = (cid:88) n a2 (cid:175) i i(cid:175) (cid:175) i i(cid:175) i i i (cid:175)i=1 (cid:175) (cid:175)i=1 (cid:175) i=1 i=1",
    "page": 998,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXB. USEFULINEQUALITIES 979 whichis(B.45)withK =1. r Taker â¥2.Letb =a / (cid:161)(cid:80)n a2(cid:162)1/2 so (cid:80)n b2=1.Then i i i=1 i i=1 i (cid:175) (cid:175)r (cid:195) (cid:33)r/2 (cid:175) (cid:175)r (cid:69) (cid:175) (cid:175) (cid:88) n a Îµ (cid:175) (cid:175) = (cid:88) n a2 (cid:69) (cid:175) (cid:175) (cid:88) n b Îµ (cid:175) (cid:175) . (B.66) (cid:175) i i(cid:175) i (cid:175) i i(cid:175) (cid:175)i=1 (cid:175) i=1 (cid:175)i=1 (cid:175) Weshowbelowthattheexpectationisboundedbyreplacingtheb withthecommonvaluen â1/2.Thus i (cid:175) (cid:175)r (cid:175) (cid:175)r (cid:175)(cid:88) n (cid:175) (cid:175) 1 (cid:88) n (cid:175) (cid:69)(cid:175) b Îµ (cid:175) â¤(cid:69)(cid:175)(cid:112) Îµ (cid:175) (B.67) (cid:175) i i(cid:175) (cid:175) i(cid:175) (cid:175)i=1 (cid:175) (cid:175) n i=1 (cid:175) (cid:175) (cid:175)r (cid:175) 1 (cid:88) n (cid:175) â¤limsup(cid:69)(cid:175)(cid:112) Îµ (cid:175) (cid:175) i(cid:175) nââ (cid:175) n i=1 (cid:175) =(cid:69)|Z|r =2r/2Î((r+1)/2))/Ï1/2. Thesecond-to-lastequalityfollowsfromthecentrallimittheoremandthefactthatÎµ areboundedand i thus uniformly integrable. The final equality is Theorem 5.1.4. Together with (B.66) this is (B.45) with K =2r/2Î((r+1)/2))/Ï1/2. r Theproofiscompletedbyshowing(B.67). Withoutlossofgeneralityassumeb â¥0andareordered i ascending,sothatb isthesmallestandb isthelargest.Theargumentbelowshowsthattheleftsideof 1 n (cid:113) (B.67)isincreasedifwereplaceb andb bythecommonvalue (cid:161) b2+b2(cid:162) /2(whichdoesnotalterthe 1 n 1 n sum (cid:80)n b2).Iterativelythisimplies(B.67). i=1 i SetS=(cid:80)nâ1b Îµ .Then i=2 i i (cid:34)(cid:175) (cid:175)r(cid:175) (cid:35) (cid:69) (cid:175) (cid:175) (cid:175) (cid:88) n b i Îµ i (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) (cid:175) S =(cid:69)(cid:163)|b 1 Îµ 1 +b n Îµ n +S|r(cid:175) (cid:175)S (cid:164) (cid:175)i=1 (cid:175) (cid:175) = 1(cid:163)|b +b +S|r+|b âb +S|r+|âb +b +S|r+|âb âb +S|r(cid:164) 1 n 1 n 1 n 1 n 4 =g(u )+g(u ) (B.68) 1 2 where (cid:112) (cid:112) g(u)= 1(cid:161)(cid:175) (cid:175)S+ u (cid:175) (cid:175) r+(cid:175) (cid:175)Sâ u (cid:175) (cid:175) r(cid:162) 4 u =(b +b )2 1 1 n u =(b âb )2. 2 n 1 Thefunctiong(u)isconvexonuâ¥0sinceSâ¥0andr â¥2.(ForaformalproofseeWhittle(1960,Lemma 1.)Setc=2 (cid:161) b2+b2(cid:162) .Wefind 1 n (cid:179)u u (cid:180) (cid:179)u u (cid:180) g(u )+g(u )=g 1 w +(1â 1 )0 +g 2 c+(1â 2 )0 1 2 1 c c c c u u u u â¤ 1 g(c)+(1â 1 )g(0)+ 2 g(c)+(1â 2 )g(0) c c c c =g(c)+g(0). TheinequalityistwoapplicationsofJensenâs(B.2)andthefinalequalityisu +u =c. Combinedwith 1 2 (B.68)wehaveshownthat (cid:34)(cid:175) (cid:175)r(cid:175) (cid:35) (cid:175)(cid:88) n (cid:175) (cid:175) (cid:69) (cid:175) b Îµ (cid:175) (cid:175)S â¤g(c)+g(0). (cid:175) i i(cid:175) (cid:175) (cid:175)i=1 (cid:175) (cid:175)",
    "page": 999,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXB. USEFULINEQUALITIES 980 (cid:112) Theright-hand-sideis(B.68)whenb =b = c/2. 1 n Thismeansthattheleftsideof(B.67)canbeincreasedbyreplacingb andb bythecommonvalue (cid:112) 1 n c/2asdescribedearlier.Iteratively,wereplacethesmallestandlargestb bytheircommonvalue,with i eachstepincreasingtheexpectation.Inthelimitweobtain(B.67). â  ProofofMarcinkiewicz-ZygmundInequality(B.46). LetÎµ beindependentRademacherrandomvari- i ables. Let(cid:69) X and(cid:69) Îµ denoteexpectationsconditionalonX i andÎµ i ,respectively. Bythesymmetrization inequality(B.44) (cid:175) (cid:175)r (cid:175) (cid:175)r (cid:34) (cid:175) (cid:175)r(cid:35) (cid:69) (cid:175) (cid:175) (cid:175) (cid:88) n X i (cid:175) (cid:175) (cid:175) â¤2r(cid:69) (cid:175) (cid:175) (cid:175) (cid:88) n Îµ i X i (cid:175) (cid:175) (cid:175) =2r(cid:69) Îµ (cid:69) X (cid:175) (cid:175) (cid:175) (cid:88) n Îµ i X i (cid:175) (cid:175) (cid:175) . (B.69) (cid:175)i=1 (cid:175) (cid:175)i=1 (cid:175) (cid:175)i=1 (cid:175) The expectation (cid:69) treats X as fixed, so we can apply Khintchineâs inequality (B.45). Thus (B.69) is X i boundedby (cid:195) (cid:33)r/2 (cid:195) (cid:33)r/2 n n 2rK r (cid:69) Îµ (cid:88) X i 2 =2rK r (cid:69) (cid:88) X i 2 . i=1 i=1 Thisis(B.46). â  ProofofMaximalMarcinkiewicz-ZygmundInequality(B.47). SeeIntroductiontoEconometrics,Theo- rem18.12. ProofofWhittleâsInequality(B.48). BytheMarcinkiewicz-Zygmundinequality(B.46),Minkowskiâsin- equality(B.34),and(cid:69)|X |r â¤B i 1r (cid:175) (cid:175)r (cid:175) (cid:175)r/2 (cid:69) (cid:175) (cid:175) (cid:88) n a X (cid:175) (cid:175) â¤M (cid:69) (cid:175) (cid:175) (cid:88) n a2X2 (cid:175) (cid:175) (cid:175) i i(cid:175) r (cid:175) i i (cid:175) (cid:175)i=1 (cid:175) (cid:175)i=1 (cid:175) (cid:195) (cid:33)r/2 n â¤M (cid:88) a2(cid:161)(cid:69)|X |r(cid:162)2/r r i i i=1 (cid:195) (cid:33)r/2 n â¤B M (cid:88) a2 . 1r r i i=1 whichis(B.48)withC =B M asclaimed. â  1r 1r r ProofofWhittleâsInequality(B.49).Asshownin(B.63) (cid:69)(cid:175) (cid:175)X (cid:48) AXâ(cid:69)(cid:163) X (cid:48) AX (cid:164)(cid:175) (cid:175) r â¤(cid:69)(cid:175) (cid:175)X (cid:48) AXâY (cid:48) AY (cid:175) (cid:175) r (B.70) whereY =(Y ,...,Y ) (cid:48) isanindependentcopyofX.Wecanwrite 1 n (X+Y) (cid:48) A(XâY)=Î¾(cid:48) (XâY) whereÎ¾=A(X+Y). IndependenceimpliesexchangeabilitywhichimpliesthedistributionofX âY conditionalonX +Y i i i i issymmetricabouttheorigin.Toseethisformally,byexchangeability (cid:80)[X âY â¤u|X +Y =v]=(cid:80)[Y âX â¤u|Y +X =v] i i i i i i i i =1â(cid:80)[Y âX >u|Y +X =v] i i i i =1â(cid:80)[X âY <âu|X +Y =v] i i i i =(cid:80)[X âY â¥âu|X +Y =v] i i i i",
    "page": 1000,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXB. USEFULINEQUALITIES 981 whichisthedefinitionofasymmetricdistribution.ThuswecanwriteX âY =Î· Îµ whereÎ· =|X âY | i i i i i i i andÎµ isanindependentRademacherrandomvariable. i Denote the ith element of Î¾ as Î¾ = a (cid:48) (X+Y) where a (cid:48) is the ith column of A. Then Î¾(cid:48) (XâY)= i i i (cid:80)n Î¾ Î· Îµ .ApplyingKhintchineâsinequality(B.45) i=1 i i i (cid:175) (cid:175)r (cid:69)(cid:175) (cid:175)X (cid:48) AXâY (cid:48) AY (cid:175) (cid:175) r =(cid:69) (cid:175) (cid:175) (cid:175) (cid:88) n Î¾ i Î· i Îµ i (cid:175) (cid:175) (cid:175) (cid:175)i=1 (cid:175) (cid:34)(cid:195) (cid:33)r/2(cid:35) n â¤K (cid:69) (cid:88) Î¾2Î·2 r i i i=1 (cid:195) (cid:33)r/2 n â¤K r (cid:88)(cid:161)(cid:69)(cid:163)|Î¾ i |r(cid:175) (cid:175) Î· i (cid:175) (cid:175) r(cid:164)(cid:162)2/r i=1 (cid:195) (cid:33)r/2 n â¤K (cid:88)(cid:161)(cid:69)(cid:163)Î¾2r(cid:164)(cid:162)1/r(cid:161)(cid:69)(cid:163)Î·2r(cid:164)(cid:162)1/r . (B.71) r i i i=1 ThefirstinequalityisMinkowskiâs(B.34),thesecondisCauchy-Schwarz(B.32).Observethat (cid:69)(cid:163)Î·2r(cid:164)â¤22r(cid:69)(cid:163) X2r(cid:164)â¤22rB . i i 2r ByWhittleâsfirstinequality(B.48) (cid:175) (cid:175)2r (cid:195) (cid:33)r (cid:69)(cid:163)Î¾2r(cid:164)=(cid:69) (cid:175) (cid:175) (cid:88) n a (cid:161) X +Y (cid:162) (cid:175) (cid:175) â¤22rC (cid:88) n a2 . i (cid:175) ji j j (cid:175) 1r ji (cid:175)j=1 (cid:175) j=1 Hence(B.71)isboundedby (cid:195) (cid:33)r/2 4rK C1/2B1/2 (cid:88) n (cid:88) n a2 =4rK C1/2B1/2 (cid:179) tr (cid:161) A (cid:48) A (cid:162)r/2 (cid:180)r/2 . r 1r 2r ji r 1r 2r i=1j=1 Thisis(B.49)withC =4rK C1/2B1/2. â  2r r 1r 2r ProofofRosenthalâsInequality(B.50).DefineÂµ =(cid:80)n (cid:69)|X |s foranys>0. s i=1 i Take0<r â¤2.ByLyapunovâsinequality(B.35) (cid:195) (cid:175) (cid:175)r(cid:33)1/r (cid:195) (cid:175) (cid:175)2(cid:33)1/2 (cid:69) (cid:175) (cid:175) (cid:88) n X (cid:175) (cid:175) â¤ (cid:69) (cid:175) (cid:175) (cid:88) n X (cid:175) (cid:175) =Âµ1/2. (cid:175) i(cid:175) (cid:175) i(cid:175) 2 (cid:175)i=1 (cid:175) (cid:175)i=1 (cid:175) Raisingtothepowerr implies(B.50).Fortheremainderassumer >2. BytheMarcinkiewicz-Zygmundinequality(B.46) (cid:175) (cid:175)r (cid:69) (cid:175) (cid:175) (cid:88) n X (cid:175) (cid:175) â¤M (cid:69)|S |r/2. (B.72) (cid:175) i(cid:175) r n (cid:175)i=1 (cid:175) whereS =(cid:80)n X2.Foranyi,usingthec inequality(B.7) n i=1 i r (cid:175) (cid:175)r/2â1 (cid:195) (cid:175) (cid:175)r/2â1(cid:33) |S n |r/2â1= (cid:175) (cid:175) (cid:175) X i + (cid:88) X j (cid:175) (cid:175) (cid:175) â¤c r/2â1 |X i |r/2+ (cid:175) (cid:175) (cid:175) (cid:88) X j (cid:175) (cid:175) (cid:175) . (cid:175) j(cid:54)=i (cid:175) (cid:175)j(cid:54)=i (cid:175)",
    "page": 1001,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXB. USEFULINEQUALITIES 982 Thus (cid:69)|S |r/2=(cid:69)(cid:163) S |S |r/2â1(cid:164) n n n n = (cid:88) (cid:69)(cid:163) X2|S |râ2(cid:164) i n i=1 (cid:34) (cid:195) (cid:175) (cid:175)r/2â1(cid:33)(cid:35) â¤c r/2â1 (cid:88) n (cid:69) X i 2 |X i |r/2+ (cid:175) (cid:175) (cid:175) (cid:88) X j (cid:175) (cid:175) (cid:175) i=1 (cid:175)j(cid:54)=i (cid:175) (cid:195) (cid:34) (cid:175) (cid:175)r/2â1(cid:35)(cid:33) â¤c r/2â1 (cid:88) n (cid:69)|X i |r+ (cid:88) n (cid:69) X i 2 (cid:175) (cid:175) (cid:175) (cid:88) X j 2 (cid:175) (cid:175) (cid:175) i=1 i=1 (cid:175)j(cid:54)=i (cid:175) (cid:195) (cid:175) (cid:175)r/2â1(cid:33) =c r/2â1 Âµ r + (cid:88) n (cid:69)(cid:163) X i 2(cid:164)(cid:69) (cid:175) (cid:175) (cid:175) (cid:88) X j 2 (cid:175) (cid:175) (cid:175) i=1 (cid:175)j(cid:54)=i (cid:175) â¤c r/2â1 (cid:161)Âµ r +Âµ 2 (cid:69)|S n |r/2â1(cid:162) . (B.73) Thesecond-to-lastlineholdssinceX i 2isindependentof (cid:80) j(cid:54)=i X j 2.Thefinalinequalityholdssince (cid:80) j(cid:54)=i X j 2â¤ S and (cid:80)n (cid:69)(cid:163) X2(cid:164)=Âµ . n i=1 i 2 Suppose2â¤r â¤4. Thenr/2â1â¤1. ByJensenâsinequality(B.27)(cid:69)|S |r/2â1â¤((cid:69)|S |)r/2â1=Âµr/2â1. n n 2 Also,c r/2â1 =1.Together,wecanbound(B.73)by (cid:80)n i=1 (cid:69)|X i |r+Âµr 2 /2.Thisimplies n (cid:69)|S |r/2â¤ (cid:88) (cid:69)|X |r+Âµr/2. (B.74) n i 2 i=1 Wenowestablish (cid:69)|S |s/2â¤2s(sâ2)/8(cid:161)Âµ +Âµs/2(cid:162) (B.75) n s 2 forallsâ¥2byarecursiveargument.(B.74)showsthat(B.75)holdsfor2â¤sâ¤4.Wenowshowthat(B.75) fors=râ2implies(B.75)fors=r.Taker >4.Using(B.73),c r/2â1 =2r/2â2and(B.75) (cid:69)|S |r/2â¤2r/2â2(cid:161)Âµ +Âµ (cid:69)|S |r/2â1(cid:162) n r 2 n (cid:179) (cid:179) (cid:180)(cid:180) â¤2r/2â2 Âµ r +Âµ 2 2(râ2)(râ4)/8 Âµ râ2 +Âµ( 2 râ2)/2 â¤2r/2â22(râ2)(râ4)/8(cid:161)Âµ r +Âµ 2 Âµ râ2 +Âµr 2 /2(cid:162) =2r(râ2)/8â1(cid:161)Âµ r +Âµ 2 Âµ râ2 +Âµr 2 /2(cid:162) . UsingHÃ¶lderâsinequality(B.31),HÃ¶lderâsinequalityforvectors(B.11),andthegeometricmeaninequal- ity(B.4) Âµ 2 Âµ râ2 â¤Âµ 2 (cid:88) n (cid:161)(cid:69)(cid:163) X i 2(cid:164)(cid:162)2/(râ2)(cid:161)(cid:69)|X i |r(cid:162)(râ4)/(râ2) i=1 (cid:195) (cid:33)2/(râ2)(cid:195) (cid:33)(râ4)/(râ2) n n â¤Âµ (cid:88) (cid:69)(cid:163) X2(cid:164) (cid:88) (cid:69)|X |r 2 i i i=1 i=1 =Âµr/(râ2)Âµ(râ4)/(râ2) 2 r 2 râ4 â¤ Âµr/2+ Âµ râ2 2 râ2 r â¤Âµr/2+Âµ . 2 r Together (cid:69)|S |r/2â¤2r(râ2)/8(cid:161)Âµ +Âµr/2(cid:162) n r 2",
    "page": 1002,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "APPENDIXB. USEFULINEQUALITIES 983 whichis(B.75)fors=r.Thisshowsthat(B.75)holdsforallsâ¥2. (B.72)and(B.75)imply(B.50)forr >2. â  ProofofKolmogorovâsInequality(B.51).SeeIntroductiontoEconometrics,Theorem7.13. ProofofBillingsleyâsInequality(B.52).WithoutlossofgeneralityassumeÏ2 =1.Definetheevents n (cid:189) (cid:190) A i = |S i |>Î»,max (cid:175) (cid:175)S j (cid:175) (cid:175) â¤Î» j<i Theseeventsaredisjoint.Theirunionis n (cid:189) (cid:190) A=(cid:91) A = max|S |>Î» . i i i=1 iâ¤n Then (cid:34) (cid:35) (cid:34)(cid:40) (cid:41) (cid:35) n (cid:183) Î»(cid:184) n (cid:189) Î»(cid:190) (cid:80)[A]=(cid:80) (cid:91) A â¤(cid:80) |S |> +(cid:80) (cid:91) A â© |S |â¤ . i n i n i=1 2 i=1 2 Sincethe A aredisjointthesecondtermontherightequals i n (cid:183) (cid:189) Î»(cid:190)(cid:184) n (cid:183) (cid:189) Î»(cid:190)(cid:184) (cid:88) (cid:80) A â© |S |â¤ â¤ (cid:88) (cid:80) A â© |S âS |> i n i n i 2 2 i=1 i=1 (cid:110) (cid:111) (cid:110) (cid:111) (cid:110) (cid:111) theinequalitysincetheevents{|S |>Î»}and |S |â¤ Î» imply |S âS |> Î» .TheeventsA and |S âS |> Î» i n 2 n i 2 i n i 2 independent(sinceX areindependent)sofinaltermequals i (cid:88) n (cid:80)[A ](cid:80) (cid:183) |S âS |> Î»(cid:184) â¤ (cid:88) n (cid:80)[A ] 4 var[S âS ]â¤ (cid:88) n (cid:80)[A ] 4 â¤ (cid:80)[A] . i n i 2 i Î»2 n i i Î»2 2 i=1 i=1 i=1 T(cid:112)hefirstinequalityisChebyshevâs,thesecondisvar[S n âS i ]â¤Ï2 n =1,thefinalis4/Î»2<1/2whenÎ»> 2 2.Wehaveshownthat(cid:80)[A]â¤(cid:80)[|S |>Î»/2]+(cid:80)[A]/2,whichuponrearrangementis(B.52). â  n",
    "page": 1003,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "References [1] Abadir,KarimM.andJanR.Magnus(2005):MatrixAlgebra,CambridgeUniversityPress. [2] Acemoglu,Daron,SimonJohnson,JamesA.Robinson(2001): âTheColonialOriginsofCompara- tiveDevelopment:AnEmpiricalInvestigation,âAmericanEconomicReview,91,1369-1401. [3] Acemoglu, Daron, SimonJohnson, JamesA.Robinson(2012): âTheColonialOriginsofCompar- ative Development: An Empirical Investigation: Reply,â American Economic Review, 102, 3077â 3110. [4] Ackerberg,Daniel,C.LanierBenkard,StevenBerry,andArielPakes(2007):âEconometrictoolsfor analyzingmarketoutcomes,âinJamesJ.HeckmanandEdwardE.Leamer(editors)Handbookof Econometrics,Volume6A,4171-4276,NorthHolland. [5] Aitken,AlexanderC.(1935):âOnleastsquaresandlinearcombinationsofobservations,âProceed- ingsoftheRoyalStatisticalSociety,55,42-48. [6] Akaike, Hirotugu(1969).âFittingautoregressivemodelsforprediction,âAnnalsoftheInstituteof StatisticalMathematics,21,243â247. [7] Akaike,Hirotugu(1973).âInformationtheoryandanextensionofthemaximumlikelihoodprin- ciple,âinB.PetrovandF.Csaki(editors),SecondInternationalSymposiumonInformationTheory, 267-281.AkademiaiKiado. [8] Amemiya, Takeshi (1971): âThe estimation of the variances in a variance-components model,â InternationalEconomicReview,12,1-13. [9] Amemiya,Takeshi(1973):âRegressionanalysiswhenthedependentvariableistruncatednormal,â Econometrica,41,997-1016. [10] Amemiya, Takeshi (1974): âThe nonlinear two-stage least-squares estimator,â Journal of Econo- metrics,2,105-110. [11] Amemiya,Takeshi(1977):âThemaximumlikelihoodandnonlinearthree-stageleastsquaresesti- matorinthegeneralnonlinearsimultaneousequationsmodel,âEconometrica,45,955-968. [12] Amemiya,Takeshi(1985):AdvancedEconometrics,HarvardUniversityPress. [13] Amemiya,Takeshi.andThomasE.MaCurdy(1986):âInstrumental-variableestimationofanerror componentsmodel,Econometrica,54,869-881. [14] Anderson,TheodoreW.(1951):âEstimatinglinearrestrictionsonregressioncoefficeintsformulti- variatenormaldistributions,âAnnalsofMathematicalStatistics,22,327-350. 984",
    "page": 1004,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "REFERENCES 985 [15] Anderson,TheodoreW.(2003): AnIntroductiontoMultivariateStatisticalAnalysis,ThirdEdition, Wiley. [16] Anderson,TheodoreW.andChengHsiao(1982):âFormulationandestimationofdynamicmodels usingpaneldata,âJournalofEconometrics,18,47-82. [17] Anderson,TheodoreW.andH.Rubin(1949):âEstimationoftheparametersofasingleequationin acompletesystemofstochasticequations,âTheAnnalsofMathematicalStatistics,20,46-63. [18] Andrews,DonaldW.K.(1984): âNon-strongmixingautoregressiveprocesses,âJournalofApplied Probability,21,930-934. [19] Andrews,DonaldW.K.(1991a): âAsymptoticnormalityofseriesestimatorsfornonparamericand semiparametricregressionmodels,âEconometrica,59,307-345. [20] Andrews, Donald W. K. (1991b): âHeteroskedasticity and autocorrelation consistent covariance matrixestimation,âEconometrica,59,817-858. [21] Andrews, Donald W. K. (1991c): âAsymptotic optimality of generalizedC , cross-validation, and L generalized cross-validation in regression with heteroskedastic errors,â Journal of Econometrics, 47,359-377. [22] Andrews, Donald W. K. (1993), âTests for parameter instability and structural change with un- knownchangepoint,âEconometrica,61,821-8516. [23] Andrews,DonaldW.K.(1994),âAsymptoticsforsemiparametriceconometricmodelsviastochas- ticequicontinuity,âEconometrica,62,43-72. [24] Andrews,DonaldW.K.(2017),âExamplesofL2-completeandboundedly-completedistributions,â JournalofEconometrics,199,213-220. [25] Andrews,DonaldW.K.andWernerPloberger(1994):âOptimaltestswhenanuisanceparameteris presentonlyunderthealternative,âEconometrica,62,1383-1414. [26] Angrist,JoshuaD.,GuidoW.Imbens,andAlanB.Krueger(1999):âJackknifeinstrumentalvariables estimation,âJournalofAppliedEconometrics,14,57-67. [27] Angrist,JoshuaD.,GuidoW.Imbens,andDonaldB.Rubin(1996):âIdentificationofcausaleffects usinginstrumentalvariables,â JournaloftheAmericanStatisticalAssociation,55,650-659. [28] Angrist,JoshuaD.andAlanB.Krueger(1991):âDoescompulsoryschoolattendanceaffectschool- ingandearnings?âQuarterlyJournalofEconomics,91,444-455. [29] Angrist, JoshuaD.andAlanB.Krueger(1995): âSplitsampleinstrumentalvariablesestimatesof thereturntoschooling,âJournalofBusinessandEconomicStatistics,13,225-235. [30] Angrist,JoshuaD.andVictorLavy(1999): âUsingMaimonidesâruletoestimatetheeffectofclass sizeonscholasticachievement,âQuarterlyJournalofEconomics,114,533-575. [31] Angrist,JoshuaD.andJÃ¶rn-SteffenPischke(2009): MostlyHarmlessEconometrics: AnEmpiricists Companion,PrincetonUniversityPress. [32] Arai,Yochi,andHidehikoIchimura(2018):âSimultaneousselectionofoptimalbandwidthsforthe sharpregressiondiscontinuityestimator,âQuantitativeEconomics,9,441-482.",
    "page": 1005,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "REFERENCES 986 [33] Arellano, Manuel (1987): âComputing standard errors for robust within-groups estimators,â Ox- fordBulletinofEconomicsandStatistics,49,431-434. [34] Arellano,Manuel(2003):PanelDataEconometrics,OxfordUniversityPress. [35] Arellano, Manuel and Stephen Bond (1991): âSome tests of specification for panel data: Monte Carlo evidence and an application to employment equations,â Review of Economic Studies, 58, 277-297. [36] Arellano,ManuelandOlympiaBover (1995):âAnotherlookattheinstrumentalvariableestimation oferror-componentsmodels,âJournalofEconometrics,68,29-51. [37] Ash,RobertB.(1972):RealAnalysisandProbability,AcademicPress. [38] Athey,SusanandGuidoW.Imbens(2019): âMachinelearningmethodseconomistsshouldknow about,âmanuscript. [39] Bai, Jushan (2003): âInferential theory for factor models of large dimensions,â Econometrica, 71,135-172. [40] Bai,JushanandSerenaNg(2002):âDeterminingthenumberoffactorsinapproximatefactormod- els,âEconometrica,70,191-221. [41] Bai,JushanandSerenaNg(2006):âConfidenceintervalsfordiffusionindexforecastsandinference forfactor-augmentedregressions,âEconometrica,74,1133-1150. [42] Balestra,PietroandMarcNerlove(1966): âPoolingcrosssectionandtimeseriesdataintheesti- mationofadynamicmodel:Thedemandfornaturalgas,âEconometrica,34,585-612. [43] Baltagi,BadiH.(2013):EconometricAnalysisofPanelData,FifthEdition,Wiley. [44] Barro,RobertJ.(1977): âUnanticipatedmoneygrowthandunemploymentintheUnitedStates,â AmericanEconomicReview,67,101â115 [45] Basmann,RobertL.(1957): âAgeneralizedclassicalmethodoflinearestimationofcoefficientsin astructuralequation,âEconometrica,25,77-83. [46] Basmann,RobertL.(1960): âOnfinitesampledistributionsofgeneralizedclassicallinearidentifi- abilityteststatistics,âJournaloftheAmericanStatisticalAssociation,55,650-659. [47] Baum, Christopher F, Mark E. Schaffer, and Steven Stillman (2003): âInstrumental variables and GMM:Estimationandtesting,âTheStataJournal,3,1-31. [48] Bekker, Paul A. (1994): âAlternative approximations to the distributions of instrumental variable estimators,Econometrica,62,657-681. [49] Belloni, Alexandre, Daniel Chen, Victor Chernozhukov, and Christian B. Hansen (2012): âSparse models and methods for optimal instruments with an application to eminent domain,â Econo- metrica,80,2369-2429. [50] Belloni,Alexandre,VictorChernozhukov(2011): âHighdimensionalsparseeconometricmodels: An introduction,â in Pierre Alquier, Eric Gautier, and Gilles Stoltz (editors) Inverse Problems and High-DimensionalEstimation:LectureNotesinStatistics,Volume203,Springer.",
    "page": 1006,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "REFERENCES 987 [51] Belloni, Alexandre, Victor Chernozhukov (2013): âLeast squares after model selection in high- dimensionalsparsemodels,âBernoulli,19,521-547. [52] Belloni,Alexandre,VictorChernozhukov,DenisChetverikov,ChristianB.Hansen,andKengoKato (2021): âHigh-dimensionaleconometricsandregularizedGMM,âinStevenDurlauf,LarsHansen, JamesHeckmanandRosaMatzkin(editors),HandbookofEconometrics,Volume7b,forthcoming, NorthHolland. [53] Belloni,Alexandre,VictorChernozhukov,DenisChetverikov,andKengoKato(2015): âSomenew asymptotictheoryforleastsquaresseries: Pointwiseanduniformresults,â JournalofEconomet- rics,186,345-366. [54] Belloni, Alexandre, Victor Chernozhukov, and Christian B. Hansen (2013): âInference for high- dimensionalsparseeconometricmodels,âinDaronAcemoglu,ManuelArellano,andEddieDekel (editors), Advances in Economics and Econometrics: 10th World Congress, Volume 3, Cambridge UniversityPress. [55] Belloni, Alexandre, Victor Chernozhukov, and Christian B. Hansen (2014a): âHigh-dimensional methodsandinferenceonstructuralandtreatmenteffects,âJournalofEconomicPerspectives,28, 29-50. [56] Belloni, Alexandre, Victor Chernozhukov, and Christian B. Hansen (2014b): âInference on treat- menteffectsafterselectionamonghigh-dimensionalcontrols,â ReviewofEconomicStudies, 81, 608-650. [57] Bernheim,B.Douglas,JonathanMeerandNevaK.Novarro(2016): âDoconsumersexploitcom- mitment opportunities? Evidence from natural experiments involving liquor consumption,â AmericanEconomicJournal:EconomicPolicy,8,41-69. [58] Berry,Steven(1994):âEstimatingdiscretechoicemodelsofproductdifferentiation,âRANDJournal ofEconomics,25,242-262. [59] Berry,Steven,JamesLevinsohn,andArielPakes(2004):âLimittheoremsforestimatingtheparam- etersofdifferentiatedproductdemandsystems,âReviewofEconomicStudies,71,613-654. [60] Berry,Steven,OliverB.Pakes,andArielPakes(1995): âAutomobilepricesinmarketequilibrium,â Econometrica,63,841-890. [61] Bertrand,Marianne,EstherDuflo,andSendhilMullainathan(2004): âHowmuchshouldwetrust differences-in-differencesestimates?âQuarterlyJournalofEconomics,119,249-275. [62] Beveridge,Stephen,andCharlesR.Nelson(1981): âAnewapproachtothedecompositionofeco- nomic time series into permanent and transitory components with particular attentionto mea- surementofthebusinesscycle,âJournalofMonetaryEconomics,7,151-174. [63] Billingsley,Patrick(1999):ConvergenceofProbabilityMeasures,SecondEdition,Wiley. [64] Black,SandraE.(1999): âDobetterschoolsmatter? Parentalvaluationofelementaryeducation,â QuarterlyJournalofEconomics,114,577-599. [65] Blanchard,OlivierJeanandRobertoPerotti(2002):âAnempiricalcharacterizationofthedynamic effectsofchangesingovernmentspendingandtaxesonoutput,âQuarterlyJournalofEconomics, 117,1329-1368.",
    "page": 1007,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "REFERENCES 988 [66] Blanchard,OlivierJeanandDannyQuah(1989): âThedynamiceffectsofaggregatedemandand supplydisturbances,âAmericanEconomicReview,89,655-673. [67] Blundell, Richard and Stephen Bond (1998): âInitial conditions and moment restrictions in dy- namicpaneldatamodels,âJournalofEconometrics,87,115-143. [68] Bock,MaryEllen(1975):âMinimaxestimatorsofthemeanofamultivariatenormaldistribution,â TheAnnalsofStatistics,3,209-218. [69] Box,GeorgeE.P.andDennisR.Cox,(1964): âAnanalysisoftransformations,âJournaloftheRoyal StatisticalSociety,SeriesB,26,211-252. [70] Breiman,Leo,JerryFriedman,RichardA.Olshen,andCharlesJ.Stone(1984): Classificationand RegressionTrees,Taylor&Francis. [71] Breiman,Leo(1996):âBaggingpredictors,âMachineLearning,24,123-140. [72] Breiman,Leo(2001):âRandomforests,âMachineLearning,45,5-32. [73] Breusch,TrevorS.,GrahamE.MizonandPeterSchmidt(1989): âEfficientestimationusingpanel data,âEconometrica,57,695-700. [74] Brockwell,PeterJ.andRichardA.Davis(1991): TimeSeries: TheoryandMethods,SecondEdition, Springer-Verlag. [75] BÃ¼hlmann, Peter and Sara van de Geer (2011): Statistics for High-Dimensional Data: Methods, TheoryandApplications,Springer-Verlag. [76] BÃ¼hlmann,PeterandBinYu(2002):âAnalyzingbagging,âTheAnnalsofStatistics,30,927-961. [77] Burnham,KennethP.andDavidR.Anderson(1998): ModelSelectionandMultiModelInference: A PracticalInformation-TheoreticApproach,SecondEdition,Springer. [78] Cameron, A. Colin and Pravin K. Trivedi (1998): Regression Analysis of Count Data, Cambridge UniversityPress. [79] Cameron, A. Colin, Johan B. Gelbach, and Douglas L. Miller (2008): âBootstrap-based improve- mentsforinferencewithclusterederrors,âReviewofEconomicsandStatistics,90,414-437. [80] Cameron, A. Colin and Pravin K. Trivedi (2005): Microeconometrics: Methods and Applications, CambridgeUniversityPress. [81] Canay, Ivan A. (2011): âA simple approach to quantile regression for panel data,â Econometrics Journal,14, 368-386. [82] Canova, Fabio (1995): âVector autoregressive models: Specification, estimation, inference, and forecasting,âinM.HashemPeseranandMichaelR.Wickens(editors)HandbookofAppliedEcono- metrics,Volume1:Macroeconomics,Blackwell. [83] Card, David (1995): âUsing geographic variation in college proximity to estimate the return to schooling,â in L. N. Christofides, E. K. Grant, and R. Swidinsky (editors) Aspects of Labor Market Behavior:EssaysinHonourofJohnVanderkamp,UniversityofTorontoPress. [84] Card, DavidandAlanBKrueger(1994): âMinimumwagesandemployment: Acasestudyofthe fast-foodindustryinNewJerseyandPennsylvania,â AmericanEconomicReview,84,772-793.",
    "page": 1008,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "REFERENCES 989 [85] Card, David, Alexandre Mas and Jesse Rothstein (2008): âTipping and the dynamics of segrega- tion,âQuarterlyJournalofEconomics,123,177-218. [86] Card,David,DavidS.Lee,ZhuanPei,andAndreaWeber(2015): âInferenceoncausaleffectsina generalizedregressionkinkdesign,âEconometrica,57,695-700. [87] Cattaneo,MatiasD.,RocÃ­oTitiunik,andGonzaloVazquez-Bare(2017): âComparinginferenceap- proachesforRDdesigns:Areexaminationoftheeffectofheadstartonchildmortality,âJournalof PolicyAnalysisandManagement,36,643-681. [88] Cattaneo,MatiasD.,NicolÃ¡sIdrobo,andRocÃ­oTitiunik(2020): APracticalIntroductiontoRegres- sionDiscontinuityDesigns:Foundations,CambridgeUniversityPress. [89] Cattaneo,MatiasD.,NicolÃ¡sIdrobo,andRocÃ­oTitiunik(2021): APracticalIntroductiontoRegres- sionDiscontinuityDesigns:Extensions,CambridgeUniversityPress. [90] Chamberlain,Gary(1980):âAnalysisofcovariancewithqualitativedata,âReviewofEconomicStud- ies,47,225-238. [91] Chamberlain, Gary (1984): âPanel Data,â in Zvi Griliches and Michael D. Intrilligator (editors) HandbookofEconometrics,Volume2,1247-1318,NorthHolland. [92] Chamberlain,Gary(1987):âAsymptoticefficiencyinestimationwithconditionalmomentrestric- tions,âJournalofEconometrics,34,305-334. [93] Chan,Kung-Sik(1993): âConsistencyandlimitingdistributionoftheleastsquaresestimatorofa thresholdautoregressivemodel,âTheAnnalsofStatistics,21,520-533. [94] Chang,PaoLiandShinichiSakata(2007): âEstimationofimpulseresponsefunctionsusinglong autoregression,âEconometricsJournal,10, 453-469. [95] Chao,JohnC.,NormanR.Swanson,JerryA.Hausman,WhitneyK.Newey,andTiemenWoutersen (2012): âAsymptotic distribution of JIVE in a heteroskedastic IV regression with many instru- ments,âEconometricTheory,28,42-86. [96] Chen,Xiaohong(2007):âLargesamplesieveestimationofsemi-nonparametricmodels,âinJames J. Heckman and Edward E. Leamer, (editors) Handbook of Econometrics, Volume 6B, 5549-5632, NorthHolland. [97] Chen, Xiaohong and Timothy M. Christensen (2015): âOptimal uniform convergence rates and asymptoticnormalityforseriesestimatorsunderweakdependenceandweakconditions,â Journal ofEconometrics,188,447-465. [98] Chen,XiaohongandTimothyM.Christensen(2018):âOptimalsup-normratesanduniforminfer- enceonnonlinearfunctionalsofnonparametricIVregression,âQuantitativeEconomics,9,39-84. [99] Chen, Xiaohong, Zhipeng Liao, and Yixiao Sun (2012): âSieve inference on semi-nonparametric timeseriesmodels,âCowlesFoundationDiscussionPaper#1849. [100] Chen, Xiaohong and Demian Pouzo (2015): âSieve Wald and QLR inferences on semi/nonparametricconditionalmomentmodels,âEconometrica,83,1013-1079. [101] Cheng,Ming-Yen,JianqingFanandJ.S.Marron(1997):âOnautomaticboundarycorrections,âThe AnnalsofStatistics,25,1691-1708.",
    "page": 1009,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "REFERENCES 990 [102] Chernozhukov,Victor,DenisChetverikov,MertDemirer,EstherDuflo,ChristianB.Hansen,Whit- ney Newey, and James Robins (2018): âDouble/debiased machine learning for treatment and structuralparameters,âTheEconometricsJournal,21,C1-C68. [103] Chernozhukov,Victor,IvÃ¡nFernÃ¡ndez-Val,andAlfredGalichon(2010): âQuantileandProbability CurvesWithoutCrossing,âEconometrica,74,1093-1125. [104] Chernozhukov,VictorandChristianB.Hansen(2005):âAnIVmodelofquantiletreatmenteffects,â Econometrica,73,245-261. [105] Chernozhukov,Victor,ChristianB.Hansen,andMartinSpindler(2015):âPost-selectionandpost- regularization inference in linear models with many controls and instruments,â American Eco- nomicReviewPapersandProceedings,105,486-490. [106] Chetverikov,Denis,ZhipengLiao,andVictorChernozhukov(2020): âOncross-validatedLassoin highdimensions,âAnnalsofStatistics,forthcoming. [107] Claeskens, Gerda and Nils Lid Hjort (2003): âThe focused information criterion,â Journal of the AmericanStatisticalAssociation,98,900-945. [108] Claeskens,GerdaandNilsLidHjort(2008):ModelSelectionandModelAveraging,CambridgeUni- versityPress. [109] Conley, Timothy G. and Christopher R. Taber (2011): âInference with âdifference in differencesâ withasmallnumberofpolicychanges,âReviewofEconomicsandStatistics,93,113-125. [110] Cox,Donald,BruceE.Hansen,andEmmanuelJimenez(2004):âHowresponsiveareprivatetrans- fers to income? Evidence from a laissez-faire economy,â Journal of Public Economics, 88, 2193- 2219. [111] Cragg,JohnG.andStephenG.Donald(1993): âTestingidentifiabilityandspecificationininstru- mentalvariablemodels,âEconometricTheory,9,222-240. [112] Craven, Peter and Grace Wahba (1979): âSmoothing noisy data with spline functions: Estimat- ingthe correctdegree ofsmoothing by the method of generalized cross-validationâ, Numerische Mathematik,31,377-403. [113] Das, Mitali, Whitney K. Newey, and Francis Vella (2003): âNonparametric estimation of sample selectionmodels,âTheReviewofEconomicStudies,70,33-58. [114] Davidson,James(2000):EconometricTheory,BlackwellPublishers. [115] Davidson,James(2020):StochasticLimitTheory:AnIntroductionforEconometricians,SecondEdi- tion,OxfordUniversityPress. [116] Davidson,RussellandEmmanuelFlachaire(2008):âThewildbootstrap,tamedatlast,âJournalof Econometrics,146,162-169. [117] Davidson,RussellandJamesG.MacKinnon(1993):EstimationandInferenceinEconometrics,Ox- fordUniversityPress. [118] Davidson,RussellandJamesG.MacKinnon(2004):EconometricTheoryandMethods,OxfordUni- versityPress.",
    "page": 1010,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "REFERENCES 991 [119] Davison,A.C.andD.V.Hinkley(1997):BootstrapMethodsandtheirApplication,CambridgeUni- versityPress. [120] De Luca, Giuseppe, Jan R. Magnus, and Franco Peracchi (2018): âBalanced variable addition in linearmodelsâJournalofEconomicSurveys,31,1183-1200. [121] DiTella, Rafael and Ernesto Schargrodsky (2004): âDo police reduce crime? Estimates using the allocationofpoliceforcesafteraterroristattack,â AmericanEconomicReview,94,115-138. [122] Donald,StephenG.andWhitneyK.Newey(2001):âChoosingthenumberofinstruments,âEcono- metrica,69,1161-1191. [123] Donohue,JohnJ.IIIandStevenD.Levitt(2001):âTheimpactoflegalizedabortiononcrime,âThe QuarterlyJournalofEconomics,116,379-420. [124] Duflo,Esther,PascalineDupas,andMichaelKremer(2011): âPeereffects,teacherincentives,and the impact of tracking: Evidence from a randomized evaluation in Kenya,â American Economic Review,101,1739-1774. [125] Dufour, Jean-Marie (1997): âSome impossibility theorems in econometrics with applications to structuralanddynamicmodels,âEconometrica,65,1365-1387. [126] Durbin,James(1954):âErrorsinvariables,âReviewoftheInternationalStatisticalInstitute,22,23- 32. [127] Durbin, James (1960): âThe fitting of time-series models,â Revue de lâInstitut International de Statistique,28,233-44. [128] Efron,Bradley(1979): âBootstrapmethods: Anotherlookatthejackknife,âAnnalsofStatistics,7, 1-26. [129] Efron,Bradley(1982):TheJackknife,theBootstrap,andOtherResamplingPlans,SocietyforIndus- trialandAppliedMathematics. [130] Efron, Bradley (1987): âBetter bootstrap confidence intervals (with discussion)â, Journal of the AmericanStatisticalAssociation,82,171-200. [131] Efron,Bradley(2010): Large-ScaleInference: EmpiricalBayesMethodsforEstimation,Testingand Prediction,CambridgeUniversityPress. [132] Efron,Bradley(2014):âEstimationandaccuracyaftermodelselection,â(withdiscussion),Journal oftheAmericanStatisticalAssociation,109,991-1007. [133] Efron,BradleyandTrevorHastie(2017): ComputerAgeStatisticalInference: Algorithms,Evidence, andDataScience,CambridgeUniversityPress. [134] Efron,BradleyandRobertJ.Tibshirani(1993):AnIntroductiontotheBootstrap,Chapman-Hall. [135] Eichenbaum,MartinS.,LarsPeterHansen,andKennethJ.Singleton(1988): âAtimeseriesanal- ysisofrepresentativeagentmodelsofconsumptionandleisurechoice,âTheQuarterlyJournalof Economics,103,51-78. [136] Eicker, Friedhelm(1963): âAsymptoticnormalityandconsistencyoftheleastsquaresestimators forfamiliesoflinearregressions,âAnnalsofMathematicalStatistics,34,447-456.",
    "page": 1011,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "REFERENCES 992 [137] Elliott,GrahamandAllanTimmermann(2016):EconomicForecasting,PrincetonUniversityPress. [138] Enders,Walter(2014):AppliedEconomicTimeSeries,FourthEdition,Wiley. [139] Engle,RobertF.andCliveW.J.Granger(1987): âCo-integrationanderrorcorrection: Representa- tion,estimationandtesting,âEconometrica,55,251-276. [140] Fan,Jianqing(1992):âDesign-adaptivenonparametricregression,âJournaloftheAmericanStatis- ticalAssociation,87,998-1004. [141] Fan,Jianqing(1993): âLocallinearregressionsmoothersandtheirminimaxefficiency,âAnnalsof Statistics,21,196-216. [142] Fan, Jianqing and Irene Gijbels (1996): Local Polynomial Modelling and Its Applications, Chapman-Hall. [143] Fan, Jianqing and Qiwei Yao (1998): âEfficient estimation of conditional variance functions in stochasticregression,âBiometrika,85,645-660. [144] Fan,JianqingandQiweiYao(2003): NonlinearTimeSeries: NonparametricandParametricMeth- ods,Springer-Verlag. [145] Foote,ChristopherL.andChristopherF.Goetz(2008):âTheimpactoflegalizedabortiononcrime: Comment,âTheQuarterlyJournalofEconomics,123,407-423. [146] Forinash,ChristopherV.andFrankS.Koppelman(1993):âApplicationandinterpretationofnested logitmodelsofintercitymodechoice,âTransportationResearchRecord,98-106. [147] Freyberger, Joachim (2017): âOn completeness and consistency in nonparametric instrumental variablemodels,âEconometrica,85,1629-1644. [148] Frisch,Ragnar(1933):âEditorial,âEconometrica,1,1-4. [149] Frisch,RagnarandFrederickV.Waugh(1933):âPartialtimeregressionsascomparedwithindivid- ualtrends,âEconometrica,1,387-401. [150] Fuller,WayneA.(1977):âSomepropertiesofamodificationofthelimitedinformationestimator,â Econometrica,45,939-953. [151] Gallant, A. Ronald (1977): âThree-stage least-squares estimation for a system of simultaneous, nonlinear,implicitequations,âJournalofEconometrics,5,71-88. [152] Gallant,A.Ronald(1997):AnIntroductiontoEconometricTheory,PrincetonUniversityPress. [153] Gallant,A.RonaldandDaleW.Jorgenson(1979): âStatisticalinferenceforasystemofnonlinear, implicitequationsinthecontextofinstrumentalvariableestimation,âJournalofEconometrics,11, 275-302. [154] Galton,Francis(1886): âRegressionTowardsMediocrityinHereditaryStature,âTheJournalofthe AnthropologicalInstituteofGreatBritainandIreland,15,246-263. [155] Gardner, Robert (1998): âUnobservable individual effects in unbalanced panel data,â Economics Letters,58,39-42.",
    "page": 1012,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "REFERENCES 993 [156] Gelman,Andrew,andGuidoW.Imbens(2019): âWhyhigh-orderpolynomialsshouldnotbeused inregressiondiscontinuitydesigns,âJournalofBusiness&EconomicStatistics,37,447-456. [157] Godambe,V.P.(1991):EstimatingFunctions,OxfordUniversityPress. [158] Goldberg,PinelopiKoujianou(1995):âProductdifferentiationandoligopolyininternationalmar- kets:ThecaseoftheU.S.automobileindustry,âEconometrica,63,891-951. [159] Goldberger,ArthurS.(1964):EconometricTheory,Wiley. [160] Goldberger,ArthurS.(1968):TopicsinRegressionAnalysis,Macmillan. [161] Goldberger,ArthurS.(1981):âLinearregressionafterselection,â JournalofEconometrics,15,357- 366. [162] Goldberger,ArthurS.(1991):ACourseinEconometrics,HarvardUniversityPress. [163] Goodnight, James H. (1979): âA tutorial on the SWEEP operator,â The American Statistician, 33, 149-158. [164] Gourieroux, Christian(2000): EconometricsofQualitativeDependentVariables, CambridgeUni- versityPress. [165] Granger, Clive W. J. (1969): âInvestigating causal relations by econometric models and cross- spectralmethods,âEconometrica,37,424-438. [166] Granger, Clive W. J. (1981): âSome properties of time series data and their use in econometric specification,âJournalofEconometrics,16,121-130. [167] Granger, Clive W. J. (1989): Forecasting in Business and Economics, Second Edition, Academic Press. [168] Granger,CliveW.J.andPaulNewbold(1974): âSpuriousregressionsineconometrics,âJournalof Econometrics,2,111-120. [169] Granger,CliveW.J.andPaulNewbold(1986): ForecastinginBusinessandEconomicTimeSeries, SecondEdition,AcademicPress. [170] Granger,CliveW.J.andRamuRamanathan(1984): âImprovedmethodsofcombiningforecasts,â JournalofForecasting,3,197-204. [171] Greene,WilliamH.(1981): âOntheasymptoticbiasoftheordinaryleastsquaresestimatorofthe Tobitmodel,âEconometrica,49,505-513, [172] Greene,WilliamH.(2017):EconometricAnalysis,EighthEdition,Pearson. [173] Gregory, Allan W. and Michael R. Veall (1985): âOn formulating Wald tests of nonlinear restric- tions,âEconometrica,53,1465-1468. [174] Haavelmo, Trygve (1944): âThe probability approach in econometrics,â Econometrica, supple- ment,12. [175] Hahn, Jinyong (1996): âA note on bootstrapping generalized method of moments estimators,â EconometricTheory,12,187-197.",
    "page": 1013,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "REFERENCES 994 [176] Hahn,Jinyong,PetraTodd,andWilbertvanderKlaauw(2001): âIdentificationandestimationof treatmenteffectswitharegression-discontinuitydesign,âEconometrica,69,201-209. [177] Hall, BronwynH.andRobertE.Hall(1993): âTheValueandPerformanceofU.S.Corporations,â BrookingsPapersonEconomicActivity,1-49. [178] Hall,Peter(1992):TheBootstrapandEdgeworthExpansion,Springer-Verlag. [179] Hall, Peter (1994): âMethodology and theory for the bootstrap,â in Robert F. Engle and Daniel L. McFadden(editors)HandbookofEconometrics,Volume4,Elsevier. [180] Hall,Peter,andC.C.Heyde(1980):MartingaleLimitTheoryandItsApplication,AcademicPress. [181] Hall, PeterandJoelL.Horowitz(1996): âBootstrapcriticalvaluesfortestsbasedongeneralized- method-of-momentsestimation,âEconometrica,64,891-916. [182] Hall, Robert E. (1978): âStochastic implications of the life cycle-permanent income hypothesis: theoryandevidence,â JournalofPoliticalEconomy,86,971-987. [183] Halmos,PaulR.(1956):LecturesinErgodicTheory,ChelseaPublishing. [184] Hamilton,JamesD.(1994)TimeSeriesAnalysis,PrincetonUniversityPress. [185] Hansen,BruceE.(1992):âConsistentcovariancematrixestimationfordepenendentheterogenous processes,âEconometrica,60,967-972. [186] Hansen, BruceE.(1996): âInferencewhenanuisanceparameterisnotidentifiedunderthenull hypothesis,âEconometrica,64,413-430. [187] Hansen,BruceE.(2000):âSamplesplittingandthresholdestimation,âEconometrica,68,575-603. [188] Hansen, BruceE.(2006): âEdgeworthexpansionsfortheWaldandGMMstatisticsfornonlinear restrictions,âinDeanCorbae,StevenN.DurlaufandBruceE.Hansen(editors)EconometricTheory andPractice:FrontiersofAnalysisandAppliedResearch,CambridgeUniversityPress. [189] Hansen,BruceE.(2007):âLeastsquaresmodelaveraging,âEconometrica,75,1175-1189. [190] Hansen,BruceE.(2014): âModelaveraging,asymptoticrisk,andregressorgroups,âQuantitative Economics,5,495-530. [191] Hansen,BruceE.(2015):âTheintegratedmeansquarederrorofseriesregressionandaRosenthal Hilbert-spaceinequality,âEconometricTheory,31,337-361. [192] Hansen,BruceE.(2020a):IntroductiontoEconometrics,manuscript. [193] Hansen,BruceE.(2020b):âAmodernGauss-Markovtheorem,âmanuscript. [194] Hansen,BruceE.andSeojeongLee(2019):âAsymptoticTheoryforClusteredSamples,âJournalof Econometrics,210,268-290. [195] Hansen,BruceE.andSeojeongLee(2020): âInferenceforiteratedGMMundermisspecificationâ, Econometrica,forthcoming. [196] Hansen,BruceE.andJeffreyRacine(2012):âJackknifemodelaveraging,âJournalofEconometrics, 167,38-46.",
    "page": 1014,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "REFERENCES 995 [197] Hansen,ChristianB.(2007):âAsymptoticpropertiesofarobustvariancematrixestimatorforpanel datawhenT islarge, JournalofEconometrics,141,595-620. [198] Hansen,LarsPeter(1982): âLargesamplepropertiesofgeneralizedmethodofmomentsestima- tors,Econometrica,50,1029-1054. [199] Hansen,LarsPeterandRobertJ.Hodrick(1980):âForwardexchangeratesasoptimalpredictorsof futurespotrates:Aneconometricanalysis,âJournalofPoliticalEconomy,88,829-853. [200] Hansen,LarsPeter,JohnHeaton,andA.Yaron(1996): âFinitesamplepropertiesofsomealterna- tiveGMMestimators,âJournalofBusinessandEconomicStatistics,14,262-280. [201] HÃ¤rdle,Wolfgang(1990):AppliedNonparametricRegression,CambridgeUniversityPress. [202] Harvey,Andrew(1990):TheEconometricAnalysisofTimeSeries,SecondEdition,MITPress. [203] Hastie,Trevor,RobertTibshirani,andJeromeFriedman(2008): TheElementsofStatisticalLearn- ing:DataMining,Inferenece,andPrediction,Springer. [204] Hausman,JerryA.(1978):âSpecificationtestsineconometrics,âEconometrica,46,1251-1271. [205] Hausman,JerryA.,WhitneyK.Newey,TiemenWoutersen,JohnC.Chao,andNormanR.Swanson (2012): âInstrumentalvariableestimationwithheteroskedasticityandmanyinstruments,âQuan- titativeEconomics,3,211-255. [206] Hausman,JerryA.andWilliamE.Taylor(1981):âPaneldataandunobservableindividualeffects,â Econometrica,49,1377-1398. [207] Hayashi,Fumio(2000):Econometrics,PrincetonUniversityPress. [208] Hayashi, FumioandChristopherSims(1983): âNearlyefficientestimationoftimeseriesmodels withpredetermined,butnotexogenous,instruments,âEconometrica,51,783-798. [209] Heckman, James (1979): âSample selection bias as a specification error,â Econometrica, 47, 153- 161. [210] Hinkley,DavidV.(1977):âJackknifinginunbalancedsituations,âTechnometrics,19,285-292. [211] Hoerl, Arthur E. and Robert W. Kennard (1970): âRidge regression: Biased estimation for non- orthogonalproblems,âTechnometrics,12,55-67. [212] Holtz-Eakin,Douglas,WhitneyNeweyandHarveyS.Rosen(1988):âEstimatingvectorautoregres- sionswithpaneldata,âEconometrica,56,1371-1395. [213] HonorÃ©,BoE.(1992): âTrimmedLADandleastsquaresestimationoftruncatedandcensoredre- gressionmodelswithfixedeffects,âEconometrica,60,533-565. [214] Horn,S.D.,R.A.Horn,andD.B.Duncan.(1975): âEstimatingheteroscedasticvariancesinlinear model,âJournaloftheAmericanStatisticalAssociation,70,380-385. [215] Horowitz,Joel(2001):âTheBootstrap,âinJamesJ.HeckmanandEdwardE.Leamer(editors)Hand- bookofEconometrics,Volume5,3159-3228,Elsevier. [216] Horowitz,Joel(2011):âAppliednonparametricinstrumentalvariablesestimation,âEconometrica, 79,347-394.",
    "page": 1015,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "REFERENCES 996 [217] Hsiao,Cheng(2003):AnalysisofPanelData,SecondEdition,CambridgeUniversityPress. [218] Imbens, Guido W., and Joshua D. Angrist (1994): âIdentification and estimation of local average treatmenteffects,âEconometrica,62,467-476. [219] Imbens,GuidoW.,andKarthikKalyanaraman(2012): âOptimalbandwidthchoicefortheregres- siondiscontinuityestimator,âReviewofEconomicStudies,79,933-959. [220] Imbens, Guido W., and Thomas Lemieux (2008): âRegression discontinuity designs: A guide to practice,âJournalofEconometrics,142,615-635. [221] Jackson, Dunham (1912): âOn the approximation by trigonometric sums and polynomials with positivecoefficients,âTransactionsoftheAmericanMathematicalSociety,13,491-515. [222] James,Gareth,DanielaWitten,TrevorHastie,RobertTibshirani(2013):AnIntroductiontoStatisti- calLearning:withApplicationsinR,Springer. [223] James, WilliamandCharlesM.Stein(1961): âEstimationwithquadraticloss,âProceedingsofthe FourthBerkeleySymposiumonMathematicalStatisticsandProbability,1,361-380. [224] Johansen,Soren(1988):âStatisticalanalysisofcointegratingvectors,âJournalofEconomicDynam- icsandControl,12,231-254. [225] Johansen, Soren(1991): âEstimationandhypothesistestingofcointegrationvectorsinthepres- enceoflineartrend,âEconometrica,59,1551-1580. [226] Johansen,Soren(1995):Likelihood-BasedInferenceinCointegratedVectorAuto-RegressiveModels, OxfordUniversityPress. [227] Johnston,JackandJohnDiNardo(1997):EconometricMethods,FourthEdition,McGraw-Hill. [228] JordÃ ,Ãscar(2005): âEstimationandinferenceofimpulseresponsesbylocalprojections,âAmeri- canEconomicReview,95,161-182. [229] Judge, GeorgeG., W.E.Griffiths, R.CarterHill, HelmutLÃ¼tkepohl, andTsoung-ChaoLee(1985): TheTheoryandPracticeofEconometrics,SecondEdition,Wiley. [230] Keating,JohnW.(1992): âStructuralapproachestovectorautoregressions,âFederalReserveBank ofSt.LouisReview,74,37-57. [231] Kilian,Lutz(2009):âNotalloilpriceshocksarealike:Disentanglingdemandandsupplyshocksin thecrudeoilmarket,âAmericanEconomicReview,99,1053-1069. [232] Kilian, LutzandHelmutLÃ¼tkepohl: (2017): StructuralVectorAutoregressiveAnalysis, Cambridge UniversityPress. [233] Kinal, Terrence W. (1980): âThe existence of moments of k-class estimators,â Econometrica, 48, 241-249. [234] Kleibergen, Frank and Richard Paap (2006): âGeneralized reduced rank tests using the singular valuedecomposition,âJournalofEconometrics,133,97-126. [235] Klein,RogerW.andRichardH.Spady(1993): âAnefficientsemiparametricestimatorfordiscrete choice,âEconometrica,61,387-421.",
    "page": 1016,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "REFERENCES 997 [236] Koenker,Roger(2005):QuantileRegression,CambridgeUniversityPress. [237] Koppelman,FrankS.andChieh-HuaWen(2000):âThepairedcombinatoriallogitmodel:proper- ties,estimationandapplication,âTransportationResearchPartB,34,75-89. [238] Kotz, Samuel and Saralees Nadarajah (2000): Extreme Value Distributions: Theory and Applica- tions,ImpericalCollegePress. [239] Kurtz, Thomas G. & Phillip Protter (1991): âWeak limit theorems for stochastic integrals and stochasticdifferentialequations,âTheAnnalsofProbability,19,1035-1070. [240] Kwiatkowski, Denis, Peter C. B. Phillips, Peter Schmidt, and Yongcheol Shin (1992): âTesting the nullhypothesisofstationarityagainstthealternativeofaunitroot,âJournalofEconometrics,54, 159-178. [241] Kyriazidou,Ekaterini(1997): âEstimationofapaneldatasampleselectionmodel,âEconometrica, 65,1335-1364. [242] Lafontaine,FrancineandKennethJ.White(1986): âObtaininganyWaldstatisticyouwant,âEco- nomicsLetters,21,35-40. [243] Lee, David S. (2008): âRandomized experiments from non-random selection in U.S. house elec- tions,âJournalofEconometrics,142,675-697. [244] Lee,DavidS.andThomasLemieux(2010):âRegressiondiscontinuitydesignsineconomics,âJour- nalofEconomicLiterature,48,281-355. [245] Lehmann, Erich L. and George Casella (1998): Theory of Point Estimation, Second Edition, Springer. [246] Lehmann, Erich L. and Joseph P. Romano (2005): Testing Statistical Hypotheses, Third Edition, Springer. [247] Li,Ker-Chau(1986):âAsymptoticoptimalityofC andgeneralizedcross-validationinridgeregres- L sionwithapplicationtosplinesmoothing,â AnnalsofStatistics,14,1101-1112. [248] Li, Ker-Chau (1987): âAsymptotic optimality for C , C , cross-validation and generalized cross- p L validation:DiscreteIndexSet,â AnnalsofStatistics,15,958-975. [249] Li,QiandJeffreyRacine(2007):NonparametricEconometrics,PrincetonUniversityPress. [250] Liu,ReginaY.(1988): âBootstrapproceduresundersomenon-I.I.D.models,âAnnalsofStatistics, 16,1696-1708. [251] Lorentz,GeorgeG.(1986):ApproximationofFunctions,SecondEdition,Chelsea. [252] Lovell,MichaelC.(1963):âSeasonaladjustmentofeconomictimeseries,âJournaloftheAmerican StatisticalAssociation,58,993-1010. [253] Luce,RobertDuncan(1959):IndividualChoiceBehavior:ATheoreticalAnalysis,Wiley. [254] Ludwig,Jens,andDouglasL.Miller(2007): âDoesheadstartimprovechildrenâslifechances? Evi- dencefromaregressiondiscontinuitydesign,âQuarterlyJournalofEconomics,122,159-208. [255] LÃ¼tkepohl,Helmut(2005):NewIntroductiontoMultipleTimeSeriesAnalysis,Springer.",
    "page": 1017,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "REFERENCES 998 [256] MacKinnon,JamesG.andHalbertWhite(1985): âSomeheteroskedasticity-consistentcovariance matrixestimatorswithimprovedfinitesampleproperties,âJournalofEconometrics,29,305-325. [257] Maddala,G.S.(1983): Limited-DependentandQualitativeVariablesinEconometrics,Cambridge UniversityPress. [258] Magnus,JanR.(2017):IntroductiontotheTheoryofEconometrics,VUUniversityPress. [259] Magnus,JanR.,andH.Neudecker(2019):MatrixDifferentialCalculuswithApplicationsinStatis- ticsandEconometrics,ThirdEdition,Wiley. [260] Mallows,C.L.(1973):âSomecommentsonC ,âTechnometrics,15,661-675. p [261] Mammen,Enno(1993): âBootstrapandwildbootstrapforhighdimensionallinearmodels,âAn- nalsofStatistics,21,255-285. [262] Mankiw, N. Gregory, David Romer, and David N. Weil (1992): âA contribution to the empirics of economicgrowth,âTheQuarterlyJournalofEconomics,107,407-437. [263] Manski, Charles F. (1975): âThe maximum score estimator of the stochastic utility model of choice,âJournalofEconometrics,3,205-228. [264] Mariano, Robert S. (1982): âAnalytical small-sample distribution theory in econometrics: the si- multaneousequationscase,âInternationalEconomicReview,23,503-534. [265] McCracken, Michael W. and Serena Ng (2015): âFRED-MD: A monthly database for macroeco- nomicresearch,âworkingpaper2015-012B,FederalReserveBankofSt.Louis. [266] McCrary,Justin(2008): âManipulationoftherunningvariableintheregressiondiscontinuityde- sign:Adensitytest,âJournalofEconometrics,142,698-714. [267] McCulloch,J.Huston(1985):âOnheteros*edasticity,âEconometrica,53,483. [268] McFadden, Daniel (1978): âModeling the choice of residential location,â in A. Karlgvist (editor) SpatialInteractionTheoryandResidentialLocation,75-96,North-Holland. [269] McFadden, Daniel (1981): âEconometric models of probabilistic choice,â in Charles Manski and Daniel McFadden (editors) Structural Analysis of Discrete Data with Econometric Applications, 198-272,MITPress. [270] Mertens,KarelandMortenO.Ravn(2013):âThedynamiceffectsofpersonalandcorporateincome taxchangesintheUnitedStates,âAmericanEconomicReview,103,1212-1247. [271] Meyer,MarcoandJens-PeterKreiss(2015):âOnthevectorautoregressivesievebootstrap,âJournal ofTimeSeriesAnalysis,36,37-397. [272] Mhaskar,HrushikeshN.(1996):IntroductiontotheTheoryofWeightedPolynomialApproximation, WorldScientific. [273] Moulton,BrentR.(1990):âAnillustrationofapitfallinestimatingtheeffectsofaggregatevariables onmicrounits,âReviewofEconomicsandStatistics,72,334-338. [274] Mullainathan, Sendhil and Jann Spiess (2017): âMachine learning: An applied econometric ap- proach,âJournalofEconomicPerspectives,31,87-106.",
    "page": 1018,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "REFERENCES 999 [275] Mundlak,Yair(1961): âEmpiricalproductionfunctionfreeofmanagementbias,âJournalofFarm Economics,43,44-56. [276] Murphy,KevinM.andRobertH.Topel(1985):âEstimationandinferenceintwo-stepeconometric models,âJournalofBusinessandEconomicStatistics,3,370-379. [277] Nadaraya,ÃlizbarA.(1964):âOnestimatingregression,âTheoryofProbabilityandItsApplications, 9,141-142. [278] Nelson, Charles R, and Charles I. Plosser (1982): âTrends and random walks in macroeconomic timeseries:Someevidenceandimplications,âJournalofMonetaryEconomics,10,139-162. [279] Nerlove,Marc(1963):âReturnstoscaleinelectricitysupply,âinCarlChrist,MiltonFriedman,and LeoA.Goodman(editors)MeasurementinEconomics,167-198,StanfordUniversityPress. [280] Nevo, Aviv(2001): âMeasuringmarketpowerintheready-to-eatcerealindustryâ Econometrica, 69,307-342. [281] Newey, Whitney K. (1985): âGeneralized method of moments specification testing,â Journal of Econometrics,29,229-256. [282] Newey,WhitneyK.(1990): âSemiparametricefficiencybounds,âJournalofAppliedEconometrics, 5,99-135. [283] Newey, Whitney K. (1997): âConvergence rates and asymptotic normality for series estimators,â JournalofEconometrics,79,147-168. [284] Newey,WhitneyK.andDanielL.McFadden(1994):âLargesampleestimationandhypothesistest- ing,âinRobertEngleandDanielMcFadden(editors),HandbookofEconometrics,Volume4,2111- 2245,NorthHolland. [285] Newey,WhitneyK.andJamesL.Powell(2003):âInstrumentalvariableestimationofnonparamet- ricmodels,âEconometrica,71,1565-1578. [286] Newey, Whitney K. and Kenneth D. West (1987a): âHypothesis testing with efficient method of momentsestimation,âInternationalEconomicReview,28,777-787. [287] Newey,WhitneyK.andKennethD.West(1987b): âAsimplepositivesemi-definite,heteroskedas- ticityandautocorrelationconsistentcovariancematrix,â Econometrica,55,703-708. [288] Newey, Whitney K. and Kenneth D. West (1994): âAutomatic lag selection in covariance matrix estimation,âReviewofEconomicStudies,631-654. [289] Ng,SerenaandPierrePerron(2001): âLaglengthselectionandtheconstructionofunitroottests withgoodsizeandpower,âEconometrica,69,1519-154. [290] Nickell, Stephen (1981): âBiases in dynamic models with fixed effects,â Econometrica, 49, 1417- 1426. [291] Olsen, Randall J. (1978): âNote on the uniqueness of the maximum likelihood estimator for the Tobitmodel,â Econometrica,46,1211-1215. [292] Oster,Emily(2018): âDiabetesanddiet: Purchasingbehaviorchangeinresponsetohealthinfor- mation,âAmericanEconomicJournal:AppliedEconomics,10,308-348.",
    "page": 1019,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "REFERENCES 1000 [293] Pagan, Adrian (1984): âEconometric issues in the analysis of regressions with generated regres- sors,âInternationalEconomicReview,25,221-247. [294] Pagan, Adrian(1986): âTwostageandrelatedestimatorsandtheirapplications,â ReviewofEco- nomicStudies,53,517-538. [295] Pagan,AdrianandAmanUllah(1999):NonparametricEconometrics,CambridgeUniversityPress. [296] Papageorgiou, Chris, Marianne Saam, and Patrick Schulte (2017): âSubstitution between clean anddirtyenergyinputs: Amacroeconomicperspective,âTheReviewofEconomicsandStatistics, 99,281-290. [297] Park,JoonY.andPeterC.B.Phillips(1988):âOntheformulationofWaldtestsofnonlinearrestric- tions,âEconometrica,56,1065-1083. [298] Park, Joon Y. and Peter C. B. Phillips (1988): âStatistical inference in regressions with integrated processes:Part1,âEconometricTheory,4,468-497. [299] Pham,TuanD.andLanhT.Tran(1985):âSomemixingpropertiesoftimeseriesmodels,âStochastic ProcessesandtheirApplications,19,297-303. [300] Phillips,AlbanW.(1958): âTherelationbetweenunemploymentandtherateofchangeofmoney wageratesintheUnitedKingdom1861â1957,âEconomica,25,283-299. [301] Phillips,G.D.A.andC.Hale(1977):âThebiasofinstrumentalvariableestimatorsofsimultaneous equationsystems,âInternationalEconomicReview,18,219-228. [302] Phillips,PeterC.B.(1983): âExactsmallsampletheoryinthesimultaneousequationsmodel,âin Zvi Griliches and Michael D. Intriligator (editors) Handbook of Econometrics, Volume 1, North- Holland. [303] Phillips, Peter C. B. (1986): âUnderstanding spurious regressions in econometrics,â Journal of Econometrics,33,311-340. [304] Phillips, Peter C. B. and Bruce E. Hansen (1990): âStatistical inference in instrumental variables regressionwithI(1)processes,âReviewofEconomicStudies,57,99-125. [305] Phillips, Peter C. B. and Sam Ouliaris (1990): âAsymptotic properties of residual based tests for cointegration,âEconometrica,58,165-193. [306] Politis,DimitrisN.andTuckerS.McElroy(2020):TimeSeries:AFirstCoursewithBootstrapStarter, Chapman-Hall/CRCPress. [307] Politis,DimitrisN.,JosephP.Romano,andMichaelWolf(1999):Subsampling,Springer. [308] Powell,JamesL.(1986):âLeastabsolutedeviationsestimationforthecensoredregressionmodel,â JournalofEconometrics,25,303-325. [309] Powell,JamesL.(1986):âCensoredregressionquantiles,âJournalofEconometrics,32,143-155. [310] Quenouille, M. H. (1949): âApproximate tests of correlation in time series,â Journal of the Royal StatisticalSocietySeriesB,11,18-84. [311] Ramey, ValerieA,(2016): âMacroeconomicshocksandtheirpropagation,âinJohnB.Taylorand HaraldUhlig(editors)HandbookofMacroeconomics,Volume2,Elsevier.",
    "page": 1020,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "REFERENCES 1001 [312] Ramsey,JamesB.(1969): âTestsforspecificationerrorsinclassicallinearleast-squaresregression analysis,âJournaloftheRoyalStatisticalSocietySeriesB,31,350-371. [313] ReiersÃ¸l,Olav(1945):ConfluenceAnalysisbyMeansofInstrumentalSetsofVariables. [314] Reinhart,CarmenM.andKennethS.Rogoff(2010):âGrowthinatimeofdebt,âAmericanEconomic Review:PapersandProceedings,100,573-578. [315] Robinson,PeterR.(1988)âRoot-n-consistentsemiparametricregression,âEconometrica,56,931- 954. [316] Rubin, Donald B. (1974): âEstimating causal effects of treatments in randomized and non- randomizedstudies,âJournalofEducationalPsychology,66,688-701 [317] Ruud,PaulA.(2000):AnIntroductiontoClassicalEconometricTheory,OxfordUniversityPress. [318] Samuelson,PaulA.(1939): âInteractionsbetweenthemultiplieranalysisandtheprincipleofac- celeration,âReviewofEconomicStatistics,21,75-78. [319] Sargan,J.Dennis(1958):âTheestimationofeconomicrelationshipsusinginstrumentalvariables,â Econometrica,26,393-415. [320] Schumaker,LarryL.(2007): SplineFunctions: BasicTheory,ThirdEdition,CambridgeUniversity Press. [321] Schwarz,Gideon(1978): âEstimatingthedimensionofamodel,âTheAnnalsofStatistics,6,461- 464. [322] Shao,JunandDongshengTu(1995):TheJackknifeandBootstrap,Springer. [323] Shapiro,MatthewD.andMarkW.Watson(1988):âSourcesofbusinesscyclefluctuations,âinStan- leyFischer(editor)NBERMacroeconomicsAnnual,111-148,MITPress. [324] Shibata, Ritei(1980): âAsymptoticallyefficientselectionoftheorderofthemodelforestimating parametersofalinearprocess,âTheAnnalsofStatistics,8,147-164. [325] Sims,ChristopherA.(1972): âMoney,incomeandcausality,âAmericanEconomicReview,62,540- 552. [326] Sims,ChristopherA.(1980):âMacroeconomicsandreality,âEconometrica,48,1-48. [327] Staiger,DouglasandJamesH.Stock(1997): âInstrumentalvariablesregressionwithweakinstru- ments,âEconometrica,65,557-586. [328] Stock, James H. (1987): âAsymptotic properties of least squares estimators of cointegrating vec- tors,âEconometrica,55,1035-1056. [329] Stock, JamesH.andFrancescoTrebbi(2003): âRetrospectives: WhoInventedInstrumentalVari- ableRegression?,âJournalofEconomicPerspectives,17,177-194. [330] Stock,JamesH.andMarkW.Watson(1993):âAsimpleestimatorofcointegratingvectorsinhigher orderintegratedsystems,âEconometrica,61,783-820. [331] Stock,JamesH.andMarkW.Watson(2007): âWhyhasU.S.inflationbecomehardertoforecast?,â JournalofMoney,CreditandBanking,39,3-33.",
    "page": 1021,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "REFERENCES 1002 [332] Stock,JamesH.andMarkW.Watson(2008): âHeteroskedasticity-robuststandarderrorsforfixed effectspaneldataregression,Econometrica,76,155-174. [333] Stock, James H. and Mark W. Watson (2012): âDisentangling the channels of the 2007-09 reces- sion,âBrookingsPapersonEconomicActivity,81-135. [334] Stock,JamesH.andMarkW.Watson(2014):IntroductiontoEconometrics,Thirdedition,Pearson. [335] Stock,JamesH.andMarkW.Watson(2016):âFactormodelsandstructuralvectorautoregressions inmacroeconomics,âinJohnB.TaylorandHaraldUhlig(editors),HandbookofMacroeconomics, Volume2,Elsevier. [336] Stock, JamesH.andJonathanH.Wright(2000): âGMMwithweakidentification,âEconometrica, 68,1055-1096. [337] Stock,JamesH.andMotohiroYogo(2005): âTestingforweakinstrumentsinlinearIVregression,â inDonaldW.K.AndrewsandJamesH.Stock(editors),IdentificationandInferenceforEconometric Models:EssaysinHonorofThomasRothenberg,80-108,CambridgeUniversityPress. [338] Stone,CharlesJ.(1977):âConsistentnonparametricregression,âAnnalsofStatistics,5,595-645. [339] Stone, Marshall H. (1948): âThe generalized Weierstrass approximation theorem,â Mathematics Magazine,21,167-184. [340] Stout,WilliamF.(1974):AlmostSureConvergence,AcademicPress. [341] Theil,Henri.(1953): âRepeatedleastsquaresappliedtocompleteequationsystems,âTheHague, CentralPlanningBureau,mimeo. [342] Theil,Henri(1961):EconomicForecastsandPolicy,NorthHolland. [343] Theil,Henri.(1971):PrinciplesofEconometrics,Wiley. [344] Thistlethwaite,DonaldL.,andDonaldT.Campbell(1960):âRegression-discontinuityanalysis:An alternativetotheex-postfactoexperiment,âJournalofEducationalPsychology,51,309-317. [345] Tibshirani,Robert(1996):âRegressionshrinkageandselectionviathelasso,âJournaloftheAmeri- canStatisticalAssociation,SeriesB,58,267-288. [346] Tikhonov,AndreyNikolayevich(1943): âOnthestabilityofinverseproblems,âDokladyAkademii NaukSSSR,39,195-198. [347] Tobin,James(1958):âEstimationofrelationshipsforlimiteddependentvariables,âEconometrica, 26,24-36. [348] Tong, Howell (1990): Non-linear Time Series: A Dynamical System Approach, Oxford University Press. [349] Train, Kenneth E. (2009): Discrete Choice Methods with Simulation, Second Edition, Cambridge UniversityPress. [350] Tukey, John (1958): âBias and confidence in not quite large samples,â Annals of Mathematical Statistics,29,614.",
    "page": 1022,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "REFERENCES 1003 [351] Tripathi, Gautam(1999): âAmatrixextensionoftheCauchy-Schwarzinequality,âEconomicsLet- ters,63,1-3. [352] von Bahr, Bengt and Carl-Gustav Esseen (1965): âInequalities for the rth absolute moment of a sumofrandomvariables,1â¤r â¤2,âTheAnnalsofMathematicalStatistics,36,299-303. [353] Wager, Stefan, and Susan Athey (2018): âEstimation and inference of heterogeneous treatment effectsusingrandomforests,âJournaloftheAmericanStatisticalAssociation,113,1228-1242. [354] Wager,Stefan,TrevorHastie,andBradleyEfron(2014): âConfidenceintervalsforrandomforests: Thejackknifeandtheinfinitesimaljackknife,âTheJournalofMachineLearningResearch,15,1625- 1651. [355] Wald, Abraham. (1940): âThe fitting of straight lines if both variables are subject to error,â The AnnalsofMathematicalStatistics,11,283-300 [356] Wald, Abraham.(1943): âTestsofstatisticalhypothesesconcerningseveralparameterswhenthe numberofobservationsislarge,âTransactionsoftheAmericanMathematicalSociety,54,426-482. [357] Wasserman,Larry(2006):AllofNonparametricStatistics,Springer. [358] Watson,GeoffreyS.(1964):âSmoothregressionanalysis,âSankyaSeriesA,26,359-372. [359] Watson, Mark W. (1994): âVARs and cointegration,â in Robert Engle and Daniel McFadden (edi- tors),HandbookofEconometrics,Volume4,North-Holland. [360] Weierstrass,Karl(1885): âÃberdieanalytischeDarstellbarkeitsogenannterwillkÃ¼rlicherFunctio- neneinerreellenVerÃ¤nderlichen,âSitzungsberichtederKÃ¶niglichPreuÃischenAkademiederWis- senschaftenzuBerlin. [361] Windmeijer,Frank(2005): âAfinitesamplecorrectionforthevarianceoflinearefficienttwo-step GMMestimatorsâ,JournalofEconometrics,126,25-51. [362] White, Halbert(1980): âAheteroskedasticity-consistentcovariancematrixestimatorandadirect testforheteroskedasticity,âEconometrica,48,817-838. [363] White,Halbert(1982):âInstrumentalvariablesregressionwithindependentobservations,âEcono- metrica,50,483-499. [364] White,Halbert(1984):AsymptoticTheoryforEconometricians,AcademicPress. [365] Halbert White and Ian Domowitz (1984): âNonlinear regression with dependent observations,â Econometrica,52,143-162. [366] Wen, Chieh-Hua and Frank S. Koppelman (2001): âThe generalized nested logit mode,â Trans- portationResearchPartB,35,627-641. [367] Whittle, Peter (1960): âBounds for the moments of linear and quadratic forms in independent variables,âTheoryofProbabilityandItsApplications,5,302-305. [368] Wiener,NobertandPesiR.Masani(1958): âThepredictiontheoryofmultivariatestochasticpro- cesses,II,âActaMathematica,99,93-137. [369] Wooldridge,JeffreyM.(2010): EconometricAnalysisofCrossSectionandPanelData,SecondEdi- tion,MITPress.",
    "page": 1023,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  },
  {
    "text": "REFERENCES 1004 [370] Wooldridge, Jeffrey M. (2015): Introductory Econometrics: A Modern Approach, Sixth Edition, Southwestern. [371] Working,ElmerJ.(1927): âWhatDoStatisticalâDemandCurvesâShow?âQuarterlyJournalofEco- nomics,41,212-35. [372] Wright,PhilipG.(1915):âMooreâsEconomicCycles,âQuarterlyJournalofEconomics,29,631-641. [373] Wright,PhilipG.(1928):TheTariffonAnimalandVegetableOils,MacMillan. [374] Wright,Sewell(1921):âCorrelationandcausation,âJournalofAgriculturalResearch,20,557-585. [375] Wu, De-Min(1973): Alternativetestsofindependencebetweenstochasticregressorsanddistur- bances,âEconometrica,41,733-750. [376] Zellner, Arnold (1962): âAn efficient method of estimating seemingly unrelated regressions, and testsforaggregationbias,âJournaloftheAmericanStatisticalAssociation,57,348-368. [377] Zhang, Fuzhen and Qingling Zhang (2006): âEigenvalue inequalities for matrix product,â IEEE TransactionsonAutomaticControl,51,1506-1509.",
    "page": 1024,
    "chunk_id": 0,
    "source": "Econometrics_by_Bruce_Hansen.pdf"
  }
]